{"2025-12-19T00:00:00Z":{"Artificial Intelligence":[{"id":"http://arxiv.org/abs/2512.17908v1","updated":"2025-12-19T18:59:56Z","published":"2025-12-19T18:59:56Z","title":"Re-Depth Anything: Test-Time Depth Refinement via Self-Supervised Re-lighting","summary":"Monocular depth estimation remains challenging as recent foundation models, such as Depth Anything V2 (DA-V2), struggle with real-world images that are far from the training distribution. We introduce Re-Depth Anything, a test-time self-supervision framework that bridges this domain gap by fusing DA-V2 with the powerful priors of large-scale 2D diffusion models. Our method performs label-free refinement directly on the input image by re-lighting predicted depth maps and augmenting the input. This re-synthesis method replaces classical photometric reconstruction by leveraging shape from shading (SfS) cues in a new, generative context with Score Distillation Sampling (SDS). To prevent optimization collapse, our framework employs a targeted optimization strategy: rather than optimizing depth directly or fine-tuning the full model, we freeze the encoder and only update intermediate embeddings while also fine-tuning the decoder. Across diverse benchmarks, Re-Depth Anything yields substantial gains in depth accuracy and realism over the DA-V2, showcasing new avenues for self-supervision by augmenting geometric reasoning.","authors":["Ananta R. Bhattarai","Helge Rhodin"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17902v1","updated":"2025-12-19T18:59:16Z","published":"2025-12-19T18:59:16Z","title":"Adversarial Robustness of Vision in Open Foundation Models","summary":"With the increase in deep learning, it becomes increasingly difficult to understand the model in which AI systems can identify objects. Thus, an adversary could aim to modify an image by adding unseen elements, which will confuse the AI in its recognition of an entity. This paper thus investigates the adversarial robustness of LLaVA-1.5-13B and Meta's Llama 3.2 Vision-8B-2. These are tested for untargeted PGD (Projected Gradient Descent) against the visual input modality, and empirically evaluated on the Visual Question Answering (VQA) v2 dataset subset. The results of these adversarial attacks are then quantified using the standard VQA accuracy metric. This evaluation is then compared with the accuracy degradation (accuracy drop) of LLaVA and Llama 3.2 Vision. A key finding is that Llama 3.2 Vision, despite a lower baseline accuracy in this setup, exhibited a smaller drop in performance under attack compared to LLaVA, particularly at higher perturbation levels. Overall, the findings confirm that the vision modality represents a viable attack vector for degrading the performance of contemporary open-weight VLMs, including Meta's Llama 3.2 Vision. Furthermore, they highlight that adversarial robustness does not necessarily correlate directly with standard benchmark performance and may be influenced by underlying architectural and training factors.","authors":["Jonathon Fox","William J Buchanan","Pavlos Papadopoulos"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17901v1","updated":"2025-12-19T18:59:11Z","published":"2025-12-19T18:59:11Z","title":"When Reasoning Meets Its Laws","summary":"Despite the superior performance of Large Reasoning Models (LRMs), their reasoning behaviors are often counterintuitive, leading to suboptimal reasoning capabilities. To theoretically formalize the desired reasoning behaviors, this paper presents the Laws of Reasoning (LoRe), a unified framework that characterizes intrinsic reasoning patterns in LRMs. We first propose compute law with the hypothesis that the reasoning compute should scale linearly with question complexity. Beyond compute, we extend LoRe with a supplementary accuracy law. Since the question complexity is difficult to quantify in practice, we examine these hypotheses by two properties of the laws, monotonicity and compositionality. We therefore introduce LoRe-Bench, a benchmark that systematically measures these two tractable properties for large reasoning models. Evaluation shows that most reasoning models exhibit reasonable monotonicity but lack compositionality. In response, we develop an effective finetuning approach that enforces compute-law compositionality. Extensive empirical studies demonstrate that better compliance with compute laws yields consistently improved reasoning performance on multiple benchmarks, and uncovers synergistic effects across properties and laws. Project page: https://lore-project.github.io/","authors":["Junyu Zhang","Yifan Sun","Tianang Leng","Jingyan Shen","Liu Ziyin","Paul Pu Liang","Huan Zhang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17898v1","updated":"2025-12-19T18:57:53Z","published":"2025-12-19T18:57:53Z","title":"Humanlike AI Design Increases Anthropomorphism but Yields Divergent Outcomes on Engagement and Trust Globally","summary":"Over a billion users across the globe interact with AI systems engineered with increasing sophistication to mimic human traits. This shift has triggered urgent debate regarding Anthropomorphism, the attribution of human characteristics to synthetic agents, and its potential to induce misplaced trust or emotional dependency. However, the causal link between more humanlike AI design and subsequent effects on engagement and trust has not been tested in realistic human-AI interactions with a global user pool. Prevailing safety frameworks continue to rely on theoretical assumptions derived from Western populations, overlooking the global diversity of AI users. Here, we address these gaps through two large-scale cross-national experiments (N=3,500) across 10 diverse nations, involving real-time and open-ended interactions with an AI system. We find that when evaluating an AI's human-likeness, users focus less on the kind of theoretical aspects often cited in policy (e.g., sentience or consciousness), but rather applied, interactional cues like conversation flow or understanding the user's perspective. We also experimentally demonstrate that humanlike design levers can causally increase anthropomorphism among users; however, we do not find that humanlike design universally increases behavioral measures for user engagement and trust, as previous theoretical work suggests. Instead, part of the connection between human-likeness and behavioral outcomes is fractured by culture: specific design choices that foster self-reported trust in AI-systems in some populations (e.g., Brazil) may trigger the opposite result in others (e.g., Japan). Our findings challenge prevailing narratives of inherent risk in humanlike AI design. Instead, we identify a nuanced, culturally mediated landscape of human-AI interaction, which demands that we move beyond a one-size-fits-all approach in AI governance.","authors":["Robin Schimmelpfennig","Mark Díaz","Vinodkumar Prabhakaran","Aida Davani"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17897v1","updated":"2025-12-19T18:57:33Z","published":"2025-12-19T18:57:33Z","title":"RadarGen: Automotive Radar Point Cloud Generation from Cameras","summary":"We present RadarGen, a diffusion model for synthesizing realistic automotive radar point clouds from multi-view camera imagery. RadarGen adapts efficient image-latent diffusion to the radar domain by representing radar measurements in bird's-eye-view form that encodes spatial structure together with radar cross section (RCS) and Doppler attributes. A lightweight recovery step reconstructs point clouds from the generated maps. To better align generation with the visual scene, RadarGen incorporates BEV-aligned depth, semantic, and motion cues extracted from pretrained foundation models, which guide the stochastic generation process toward physically plausible radar patterns. Conditioning on images makes the approach broadly compatible, in principle, with existing visual datasets and simulation frameworks, offering a scalable direction for multimodal generative simulation. Evaluations on large-scale driving data show that RadarGen captures characteristic radar measurement distributions and reduces the gap to perception models trained on real data, marking a step toward unified generative simulation across sensing modalities.","authors":["Tomer Borreda","Fangqiang Ding","Sanja Fidler","Shengyu Huang","Or Litany"],"pdf_url":"","comment":"Project page: https://radargen.github.io/"},{"id":"http://arxiv.org/abs/2512.17893v1","updated":"2025-12-19T18:49:33Z","published":"2025-12-19T18:49:33Z","title":"Exploring the Effect of Basis Rotation on NQS Performance","summary":"Neural Quantum States (NQS) use neural networks to represent wavefunctions of quantum many-body systems, but their performance depends on the choice of basis, yet the underlying mechanism remains poorly understood. We use a fully solvable one-dimensional Ising model to show that local basis rotations leave the loss landscape unchanged while relocating the exact wavefunction in parameter space, effectively increasing its geometric distance from typical initializations. By sweeping a rotation angle, we compute quantum Fisher information and Fubini-Study distances to quantify how the rotated wavefunction moves within the loss landscape. Shallow architectures (with focus on Restricted Boltzmann Machines (RBMs)) trained with quantum natural gradient are more likely to fall into saddle-point regions depending on the rotation angle: they achieve low energy error but fail to reproduce correct coefficient distributions. In the ferromagnetic case, near-degenerate eigenstates create high-curvature barriers that trap optimization at intermediate fidelities. We introduce a framework based on an analytically solvable rotated Ising model to investigate how relocating the target wavefunction within a fixed loss landscape exposes information-geometric barriers,such as saddle points and high-curvature regions,that hinder shallow NQS optimization, underscoring the need for landscape-aware model design in variational training.","authors":["Sven Benjamin Kožić","Vinko Zlatić","Fabio Franchini","Salvatore Marco Giampaolo"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2507.01939v4","updated":"2025-12-19T18:39:57Z","published":"2025-07-02T17:49:52Z","title":"SpecCLIP: Aligning and Translating Spectroscopic Measurements for Stars","summary":"In recent years, large language models (LLMs) have transformed natural language understanding through vast datasets and large-scale parameterization. Inspired by this success, we present SpecCLIP, a foundation model framework that extends LLM-inspired methodologies to stellar spectral analysis. Stellar spectra, akin to structured language, encode rich physical and chemical information about stars. By training foundation models on large-scale spectral datasets, our goal is to learn robust and informative embeddings that support diverse downstream applications. As a proof of concept, SpecCLIP involves pre-training on two spectral types--LAMOST low-resolution and Gaia XP--followed by contrastive alignment using the CLIP (Contrastive Language-Image Pre-training) framework, adapted to associate spectra from different instruments. This alignment is complemented by auxiliary decoders that preserve spectrum-specific information and enable translation (prediction) between spectral types, with the former achieved by maximizing mutual information between embeddings and input spectra. The result is a cross-spectrum framework enabling intrinsic calibration and flexible applications across instruments. We demonstrate that fine-tuning these models on moderate-sized labeled datasets improves adaptability to tasks such as stellar-parameter estimation and chemical-abundance determination. SpecCLIP also enhances the accuracy and precision of parameter estimates benchmarked against external survey data. Additionally, its similarity search and cross-spectrum prediction capabilities offer potential for anomaly detection. Our results suggest that contrastively trained foundation models enriched with spectrum-aware decoders can advance precision stellar spectroscopy. Our code SpecCLIP is publicly available at https://github.com/Xiaosheng-Zhao/SpecCLIP","authors":["Xiaosheng Zhao","Yang Huang","Guirong Xue","Xiao Kong","Jifeng Liu","Xiaoyu Tang","Timothy C. Beers","Yuan-Sen Ting","A-Li Luo"],"pdf_url":"","comment":"29 pages, 8 figures, 6 tables. Accepted for publication in ApJ. Comments welcome"},{"id":"http://arxiv.org/abs/2512.17878v1","updated":"2025-12-19T18:31:27Z","published":"2025-12-19T18:31:27Z","title":"Weighted Stochastic Differential Equation to Implement Wasserstein-Fisher-Rao Gradient Flow","summary":"Score-based diffusion models currently constitute the state of the art in continuous generative modeling. These methods are typically formulated via overdamped or underdamped Ornstein--Uhlenbeck-type stochastic differential equations, in which sampling is driven by a combination of deterministic drift and Brownian diffusion, resulting in continuous particle trajectories in the ambient space. While such dynamics enjoy exponential convergence guarantees for strongly log-concave target distributions, it is well known that their mixing rates deteriorate exponentially in the presence of nonconvex or multimodal landscapes, such as double-well potentials. Since many practical generative modeling tasks involve highly non-log-concave target distributions, considerable recent effort has been devoted to developing sampling schemes that improve exploration beyond classical diffusion dynamics.\n  A promising line of work leverages tools from information geometry to augment diffusion-based samplers with controlled mass reweighting mechanisms. This perspective leads naturally to Wasserstein--Fisher--Rao (WFR) geometries, which couple transport in the sample space with vertical (reaction) dynamics on the space of probability measures. In this work, we formulate such reweighting mechanisms through the introduction of explicit correction terms and show how they can be implemented via weighted stochastic differential equations using the Feynman--Kac representation. Our study provides a preliminary but rigorous investigation of WFR-based sampling dynamics, and aims to clarify their geometric and operator-theoretic structure as a foundation for future theoretical and algorithmic developments.","authors":["Herlock Rahimi"],"pdf_url":"","comment":"26 pages, 1 figure"},{"id":"http://arxiv.org/abs/2512.15692v2","updated":"2025-12-19T18:30:30Z","published":"2025-12-17T18:47:31Z","title":"mimic-video: Video-Action Models for Generalizable Robot Control Beyond VLAs","summary":"Prevailing Vision-Language-Action Models (VLAs) for robotic manipulation are built upon vision-language backbones pretrained on large-scale, but disconnected static web data. As a result, despite improved semantic generalization, the policy must implicitly infer complex physical dynamics and temporal dependencies solely from robot trajectories. This reliance creates an unsustainable data burden, necessitating continuous, large-scale expert data collection to compensate for the lack of innate physical understanding. We contend that while vision-language pretraining effectively captures semantic priors, it remains blind to physical causality. A more effective paradigm leverages video to jointly capture semantics and visual dynamics during pretraining, thereby isolating the remaining task of low-level control. To this end, we introduce mimic-video, a novel Video-Action Model (VAM) that pairs a pretrained Internet-scale video model with a flow matching-based action decoder conditioned on its latent representations. The decoder serves as an Inverse Dynamics Model (IDM), generating low-level robot actions from the latent representation of video-space action plans. Our extensive evaluation shows that our approach achieves state-of-the-art performance on simulated and real-world robotic manipulation tasks, improving sample efficiency by 10x and convergence speed by 2x compared to traditional VLA architectures.","authors":["Jonas Pai","Liam Achenbach","Victoriano Montesinos","Benedek Forrai","Oier Mees","Elvis Nava"],"pdf_url":"","comment":"Revised Introduction, Related Work, and Appendix. Additional minor notational and grammatical fixes"},{"id":"http://arxiv.org/abs/2511.12712v2","updated":"2025-12-19T18:24:09Z","published":"2025-11-16T17:52:32Z","title":"Adaptive Focus Memory for Language Models","summary":"Large language models (LLMs) are increasingly deployed in multi-turn dialogue settings, yet their behavior remains bottlenecked by naive history management strategies. Replaying the full conversation at every turn is simple but costly, while recency-based truncation or static summarization often causes early, high-impact user constraints to drift out of effective context. As a result, models may retain text without reliably applying it when it matters.\n  We present Adaptive Focus Memory (AFM), a lightweight context management system that dynamically assigns each past message one of three fidelity levels: Full, Compressed, or Placeholder, based on semantic relevance, temporal decay, and importance classification. AFM packs messages chronologically under a fixed token budget, preserving critical constraints at high fidelity while allowing low-importance context to degrade gracefully.\n  We evaluate AFM on two multi-turn dialogue benchmarks designed to stress long-horizon constraint preservation: a safety-critical travel scenario involving a user with a severe peanut allergy, and a policy-critical tax compliance scenario involving an illegal evasion request. Under strict grading that requires both explicit constraint recall and appropriately conditioned generation, AFM succeeds in 83.3 percent of allergy runs where all baseline strategies fail, and preserves correct refusal behavior on the tax benchmark.\n  These results demonstrate that effective dialogue memory requires more than retaining prior text. Selectively allocating fidelity across past messages enables reliable constraint preservation under bounded context growth, without modifying model weights or introducing external retrieval infrastructure. We release an open-source implementation of AFM compatible with OpenAI-style chat APIs to support reproducible research and practical deployment.","authors":["Christopher Cruz"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17864v1","updated":"2025-12-19T18:11:15Z","published":"2025-12-19T18:11:15Z","title":"Interpretable Plant Leaf Disease Detection Using Attention-Enhanced CNN","summary":"Plant diseases pose a significant threat to global food security, necessitating accurate and interpretable disease detection methods. This study introduces an interpretable attention-guided Convolutional Neural Network (CNN), CBAM-VGG16, for plant leaf disease detection. By integrating Convolution Block Attention Module (CBAM) at each convolutional stage, the model enhances feature extraction and disease localization. Trained on five diverse plant disease datasets, our approach outperforms recent techniques, achieving high accuracy (up to 98.87%) and demonstrating robust generalization. Here, we show the effectiveness of our method through comprehensive evaluation and interpretability analysis using CBAM attention maps, Grad-CAM, Grad-CAM++, and Layer-wise Relevance Propagation (LRP). This study advances the application of explainable AI in agricultural diagnostics, offering a transparent and reliable system for smart farming. The code of our proposed work is available at https://github.com/BS0111/PlantAttentionCBAM.","authors":["Balram Singh","Ram Prakash Sharma","Somnath Dey"],"pdf_url":"","comment":"27 pages, 12 figures"},{"id":"http://arxiv.org/abs/2512.17853v1","updated":"2025-12-19T17:55:48Z","published":"2025-12-19T17:55:48Z","title":"AnyTask: an Automated Task and Data Generation Framework for Advancing Sim-to-Real Policy Learning","summary":"Generalist robot learning remains constrained by data: large-scale, diverse, and high-quality interaction data are expensive to collect in the real world. While simulation has become a promising way for scaling up data collection, the related tasks, including simulation task design, task-aware scene generation, expert demonstration synthesis, and sim-to-real transfer, still demand substantial human effort. We present AnyTask, an automated framework that pairs massively parallel GPU simulation with foundation models to design diverse manipulation tasks and synthesize robot data. We introduce three AnyTask agents for generating expert demonstrations aiming to solve as many tasks as possible: 1) ViPR, a novel task and motion planning agent with VLM-in-the-loop Parallel Refinement; 2) ViPR-Eureka, a reinforcement learning agent with generated dense rewards and LLM-guided contact sampling; 3) ViPR-RL, a hybrid planning and learning approach that jointly produces high-quality demonstrations with only sparse rewards. We train behavior cloning policies on generated data, validate them in simulation, and deploy them directly on real robot hardware. The policies generalize to novel object poses, achieving 44% average success across a suite of real-world pick-and-place, drawer opening, contact-rich pushing, and long-horizon manipulation tasks. Our project website is at https://anytask.rai-inst.com .","authors":["Ran Gong","Xiaohan Zhang","Jinghuan Shang","Maria Vittoria Minniti","Jigarkumar Patel","Valerio Pepe","Riedana Yan","Ahmet Gundogdu","Ivan Kapelyukh","Ali Abbas","Xiaoqiang Yan","Harsh Patel","Laura Herlant","Karl Schmeckpeper"],"pdf_url":"","comment":"28 pages, 25 figures. The first four authors contributed equally"},{"id":"http://arxiv.org/abs/2512.17851v1","updated":"2025-12-19T17:52:43Z","published":"2025-12-19T17:52:43Z","title":"InfSplign: Inference-Time Spatial Alignment of Text-to-Image Diffusion Models","summary":"Text-to-image (T2I) diffusion models generate high-quality images but often fail to capture the spatial relations specified in text prompts. This limitation can be traced to two factors: lack of fine-grained spatial supervision in training data and inability of text embeddings to encode spatial semantics. We introduce InfSplign, a training-free inference-time method that improves spatial alignment by adjusting the noise through a compound loss in every denoising step. Proposed loss leverages different levels of cross-attention maps extracted from the backbone decoder to enforce accurate object placement and a balanced object presence during sampling. The method is lightweight, plug-and-play, and compatible with any diffusion backbone. Our comprehensive evaluations on VISOR and T2I-CompBench show that InfSplign establishes a new state-of-the-art (to the best of our knowledge), achieving substantial performance gains over the strongest existing inference-time baselines and even outperforming the fine-tuning-based methods. Codebase is available at GitHub.","authors":["Sarah Rastegar","Violeta Chatalbasheva","Sieger Falkena","Anuj Singh","Yanbo Wang","Tejas Gokhale","Hamid Palangi","Hadi Jamali-Rad"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17850v1","updated":"2025-12-19T17:50:05Z","published":"2025-12-19T17:50:05Z","title":"Integrating Computational Methods and AI into Qualitative Studies of Aging and Later Life","summary":"This chapter demonstrates how computational social science (CSS) tools are extending and expanding research on aging. The depth and context from traditionally qualitative methods such as participant observation, in-depth interviews, and historical documents are increasingly employed alongside scalable data management, computational text analysis, and open-science practices. Machine learning (ML) and natural language processing (NLP), provide resources to aggregate and systematically index large volumes of qualitative data, identify patterns, and maintain clear links to in-depth accounts. Drawing on case studies of projects that examine later life--including examples with original data from the DISCERN study (a team-based ethnography of life with dementia) and secondary analyses of the American Voices Project (nationally representative interview)--the chapter highlights both uses and challenges of bringing CSS tools into more meaningful dialogue with qualitative aging research. The chapter argues such work has potential for (1) streamlining and augmenting existing workflows, (2) scaling up samples and projects, and (3) generating multi-method approaches to address important questions in new ways, before turning to practices useful for individuals and teams seeking to understand current possibilities or refine their workflow processes. The chapter concludes that current developments are not without peril, but offer potential for new insights into aging and the life course by broadening--rather than replacing--the methodological foundations of qualitative research.","authors":["Corey M. Abramson"],"pdf_url":"","comment":"CITE: Abramson, Corey M. (Forthcoming 2026). \"Integrating Computational Methods and AI into Qualitative Studies of Aging and Later Life.\" In Handbook of the Sociology of Aging, 2nd ed., edited by Markus H. Schafer, Dawn C. Carr, Jacqueline L. Angel, and Richard A. Settersten Jr"},{"id":"http://arxiv.org/abs/2512.17846v1","updated":"2025-12-19T17:49:13Z","published":"2025-12-19T17:49:13Z","title":"Planning as Descent: Goal-Conditioned Latent Trajectory Synthesis in Learned Energy Landscapes","summary":"We present Planning as Descent (PaD), a framework for offline goal-conditioned reinforcement learning that grounds trajectory synthesis in verification. Instead of learning a policy or explicit planner, PaD learns a goal-conditioned energy function over entire latent trajectories, assigning low energy to feasible, goal-consistent futures. Planning is realized as gradient-based refinement in this energy landscape, using identical computation during training and inference to reduce train-test mismatch common in decoupled modeling pipelines.\n  PaD is trained via self-supervised hindsight goal relabeling, shaping the energy landscape around the planning dynamics. At inference, multiple trajectory candidates are refined under different temporal hypotheses, and low-energy plans balancing feasibility and efficiency are selected.\n  We evaluate PaD on OGBench cube manipulation tasks. When trained on narrow expert demonstrations, PaD achieves state-of-the-art 95\\% success, strongly outperforming prior methods that peak at 68\\%. Remarkably, training on noisy, suboptimal data further improves success and plan efficiency, highlighting the benefits of verification-driven planning. Our results suggest learning to evaluate and refine trajectories provides a robust alternative to direct policy learning for offline, reward-free planning.","authors":["Carlos Vélez García","Miguel Cazorla","Jorge Pomares"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17843v1","updated":"2025-12-19T17:47:53Z","published":"2025-12-19T17:47:53Z","title":"ShareChat: A Dataset of Chatbot Conversations in the Wild","summary":"While Large Language Models (LLMs) have evolved into distinct platforms with unique interface designs and capabilities, existing public datasets treat models as generic text generators, stripping away the interface context that actively shapes user interaction. To address this limitation, we present ShareChat, a large-scale, cross-platform corpus comprising 142,808 conversations and over 660,000 turns collected from publicly shared URLs across five major platforms: ChatGPT, Claude, Gemini, Perplexity, and Grok. ShareChat distinguishes itself by preserving native platform affordances often lost in standard logs, including reasoning traces, source links, and code artifacts, while spanning 101 languages over the period from April 2023 to October 2025. Furthermore, ShareChat offers substantially longer context windows and greater interaction depth than prior datasets. We demonstrate the dataset's multifaceted utility through three representative analyses: (1) analyzing conversation completeness to measure user intent satisfaction; (2) evaluating source citation behaviors in content generation; and (3) conducting temporal analysis to track evolving usage patterns. This work provides the community with a vital and timely resource for understanding authentic user-LLM chatbot interactions in the wild.","authors":["Yueru Yan","Tuc Nguyen","Bo Su","Melissa Lieffers","Thai Le"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2509.11512v2","updated":"2025-12-19T17:47:28Z","published":"2025-09-15T01:53:30Z","title":"Machine Learning-Driven Predictive Resource Management in Complex Science Workflows","summary":"The collaborative efforts of large communities in science experiments, often comprising thousands of global members, reflect a monumental commitment to exploration and discovery. Recently, advanced and complex data processing has gained increasing importance in science experiments. Data processing workflows typically consist of multiple intricate steps, and the precise specification of resource requirements is crucial for each step to allocate optimal resources for effective processing. Estimating resource requirements in advance is challenging due to a wide range of analysis scenarios, varying skill levels among community members, and the continuously increasing spectrum of computing options. One practical approach to mitigate these challenges involves initially processing a subset of each step to measure precise resource utilization from actual processing profiles before completing the entire step. While this two-staged approach enables processing on optimal resources for most of the workflow, it has drawbacks such as initial inaccuracies leading to potential failures and suboptimal resource usage, along with overhead from waiting for initial processing completion, which is critical for fast-turnaround analyses. In this context, our study introduces a novel pipeline of machine learning models within a comprehensive workflow management system, the Production and Distributed Analysis (PanDA) system. These models employ advanced machine learning techniques to predict key resource requirements, overcoming challenges posed by limited upfront knowledge of characteristics at each step. Accurate forecasts of resource requirements enable informed and proactive decision-making in workflow management, enhancing the efficiency of handling diverse, complex workflows across heterogeneous resources.","authors":["Tasnuva Chowdhury","Tadashi Maeno","Fatih Furkan Akman","Joseph Boudreau","Sankha Dutta","Shengyu Feng","Adolfy Hoisie","Kuan-Chieh Hsu","Raees Khan","Jaehyung Kim","Ozgur O. Kilic","Scott Klasky","Alexei Klimentov","Tatiana Korchuganova","Verena Ingrid Martinez Outschoorn","Paul Nilsson","David K. Park","Norbert Podhorszki","Yihui Ren","John Rembrandt Steele","Frédéric Suter","Sairam Sri Vatsavai","Torre Wenaus","Wei Yang","Yiming Yang","Shinjae Yoo"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2507.22219v3","updated":"2025-12-19T17:35:31Z","published":"2025-07-29T20:35:35Z","title":"RL from Teacher-Model Refinement: Gradual Imitation Learning for Machine Translation","summary":"Preference-learning methods for machine translation (MT), such as Direct Preference Optimization (DPO), have shown strong gains but typically rely on large, carefully curated preference triplets and often struggle to generalize beyond their tuning domains. We propose Reinforcement Learning from Teacher-Model Refinement (RLfR), which replaces static triplets with on-policy, actor-conditioned refinements produced by a frozen teacher. At each step, the actor samples candidate translations, the teacher performs a minimal local edit of each draft, and the actor is reinforced to close the gap using a composite reward that combines scaled negative edit distance for lexical and structural fidelity with COMET for semantic adequacy. This formulation yields a stable, model-aware learning signal without requiring explicit preference datasets. Experiments on FLORES-200 (English to German, Spanish, Chinese, Korean, and Japanese) show that RLfR consistently outperforms strong MT-SFT, DPO, and fixed-reference RL baselines, improving semantic quality and entity preservation, and also achieves superior performance under LLM-based judge evaluations.","authors":["Dongyub Jude Lee","Zhenyi Ye","Pengcheng He"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17814v1","updated":"2025-12-19T17:19:08Z","published":"2025-12-19T17:19:08Z","title":"LLM-based Behaviour Driven Development for Hardware Design","summary":"Test and verification are essential activities in hardware and system design, but their complexity grows significantly with increasing system sizes. While Behavior Driven Development (BDD) has proven effective in software engineering, it is not yet well established in hardware design, and its practical use remains limited. One contributing factor is the manual effort required to derive precise behavioral scenarios from textual specifications.\n  Recent advances in Large Language Models (LLMs) offer new opportunities to automate this step. In this paper, we investigate the use of LLM-based techniques to support BDD in the context of hardware design.","authors":["Rolf Drechsler","Qian Liu"],"pdf_url":"","comment":"7 pages, keynote given at 2nd International Symposium on Artificial Intelligence and Internet of Things (AIIoT-25), December 22-24th, 2025"},{"id":"http://arxiv.org/abs/2506.10892v3","updated":"2025-12-19T17:14:07Z","published":"2025-06-12T16:55:35Z","title":"The Diffusion Duality","summary":"Uniform-state discrete diffusion models hold the promise of fast text generation due to their inherent ability to self-correct. However, they are typically outperformed by autoregressive models and masked diffusion models. In this work, we narrow this performance gap by leveraging a key insight: Uniform-state diffusion processes naturally emerge from an underlying Gaussian diffusion. Our method, Duo, transfers powerful techniques from Gaussian diffusion to improve both training and sampling. First, we introduce a curriculum learning strategy guided by the Gaussian process, doubling training speed by reducing variance. Models trained with curriculum learning surpass autoregressive models in zero-shot perplexity on 3 of 7 benchmarks. Second, we present Discrete Consistency Distillation, which adapts consistency distillation from the continuous to the discrete setting. This algorithm unlocks few-step generation in diffusion language models by accelerating sampling by two orders of magnitude. We provide the code, model checkpoints, and video tutorials on the project page: http://s-sahoo.github.io/duo","authors":["Subham Sekhar Sahoo","Justin Deschenaux","Aaron Gokaslan","Guanghan Wang","Justin Chiu","Volodymyr Kuleshov"],"pdf_url":"","comment":"ICML 2025. We provide the code at: https://github.com/s-sahoo/duo [v3] includes improved theory, clearer presentation, and a new future work section"},{"id":"http://arxiv.org/abs/2512.17795v1","updated":"2025-12-19T17:01:03Z","published":"2025-12-19T17:01:03Z","title":"Intelligent Knowledge Mining Framework: Bridging AI Analysis and Trustworthy Preservation","summary":"The unprecedented proliferation of digital data presents significant challenges in access, integration, and value creation across all data-intensive sectors. Valuable information is frequently encapsulated within disparate systems, unstructured documents, and heterogeneous formats, creating silos that impede efficient utilization and collaborative decision-making. This paper introduces the Intelligent Knowledge Mining Framework (IKMF), a comprehensive conceptual model designed to bridge the critical gap between dynamic AI-driven analysis and trustworthy long-term preservation. The framework proposes a dual-stream architecture: a horizontal Mining Process that systematically transforms raw data into semantically rich, machine-actionable knowledge, and a parallel Trustworthy Archiving Stream that ensures the integrity, provenance, and computational reproducibility of these assets. By defining a blueprint for this symbiotic relationship, the paper provides a foundational model for transforming static repositories into living ecosystems that facilitate the flow of actionable intelligence from producers to consumers. This paper outlines the motivation, problem statement, and key research questions guiding the research and development of the framework, presents the underlying scientific methodology, and details its conceptual design and modeling.","authors":["Binh Vu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2509.18057v6","updated":"2025-12-19T16:58:50Z","published":"2025-09-22T17:30:33Z","title":"Reinforced Generation of Combinatorial Structures: Hardness of Approximation","summary":"Can AI based methods help us make advances in complexity theory? We provide evidence towards answering this in the affirmative, using AlphaEvolve (an LLM code mutation agent) to obtain new results in three settings:\n  a) We improve a recent result of Kunisky and Yu to obtain near-optimal upper and (conditional) lower bounds on certification algorithms for MAX-CUT and MAX-Independent Set on random 3- and 4-regular graphs. Our improved lower bounds are obtained by constructing nearly extremal Ramanujan graphs on as many as $163$ vertices, and our upper bounds are obtained via analytical arguments.\n  b) We obtain new inapproximability results for MAX-4-CUT and MAX-3-CUT, proving that it is NP-hard to approximate them within factors of $0.987$ and $0.9649$ respectively, using AlphaEvolve to discover new gadget reductions. Our MAX-4-CUT result improves upon the SOTA of $0.9883$, and our MAX-3-CUT result improves on the current best gadget-based inapproximability result of $0.9853$, but falls short of the SOTA of $16/17$ that relies on a custom PCP (rather than a reduction from ``standard'' Håstad-style PCPs).\n  c) Inapproximability for the metric Traveling Salesman Problem (TSP): We show that it is NP-hard to approximate the minimum cost tour within a factor of $111/110$ using AlphaEvolve to discover a new gadget, thus improving the SOTA of $117/116$. Along the way, we provide new modular soundness and completeness arguments that can be of independent interest.\n  A key technical challenge we faced: verifying a candidate construction produced by AlphaEvolve is costly (sometimes requiring time exponential in the size of the construction). We used AlphaEvolve itself to evolve the verification procedure to be faster (sometimes by $10,000\\times$ for our gadgets). Our results suggest that gadget based proofs would benefit from a pass through AI-based tools to obtain stronger results.","authors":["Ansh Nagda","Prabhakar Raghavan","Abhradeep Thakurta"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17774v1","updated":"2025-12-19T16:45:23Z","published":"2025-12-19T16:45:23Z","title":"MedNeXt-v2: Scaling 3D ConvNeXts for Large-Scale Supervised Representation Learning in Medical Image Segmentation","summary":"Large-scale supervised pretraining is rapidly reshaping 3D medical image segmentation. However, existing efforts focus primarily on increasing dataset size and overlook the question of whether the backbone network is an effective representation learner at scale. In this work, we address this gap by revisiting ConvNeXt-based architectures for volumetric segmentation and introducing MedNeXt-v2, a compound-scaled 3D ConvNeXt that leverages improved micro-architecture and data scaling to deliver state-of-the-art performance. First, we show that routinely used backbones in large-scale pretraining pipelines are often suboptimal. Subsequently, we use comprehensive backbone benchmarking prior to scaling and demonstrate that stronger from scratch performance reliably predicts stronger downstream performance after pretraining. Guided by these findings, we incorporate a 3D Global Response Normalization module and use depth, width, and context scaling to improve our architecture for effective representation learning. We pretrain MedNeXt-v2 on 18k CT volumes and demonstrate state-of-the-art performance when fine-tuning across six challenging CT and MR benchmarks (144 structures), showing consistent gains over seven publicly released pretrained models. Beyond improvements, our benchmarking of these models also reveals that stronger backbones yield better results on similar data, representation scaling disproportionately benefits pathological segmentation, and that modality-specific pretraining offers negligible benefit once full finetuning is applied. In conclusion, our results establish MedNeXt-v2 as a strong backbone for large-scale supervised representation learning in 3D Medical Image Segmentation. Our code and pretrained models are made available with the official nnUNet repository at: https://www.github.com/MIC-DKFZ/nnUNet","authors":["Saikat Roy","Yannick Kirchhoff","Constantin Ulrich","Maximillian Rokuss","Tassilo Wald","Fabian Isensee","Klaus Maier-Hein"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17773v1","updated":"2025-12-19T16:44:32Z","published":"2025-12-19T16:44:32Z","title":"Pix2NPHM: Learning to Regress NPHM Reconstructions From a Single Image","summary":"Neural Parametric Head Models (NPHMs) are a recent advancement over mesh-based 3d morphable models (3DMMs) to facilitate high-fidelity geometric detail. However, fitting NPHMs to visual inputs is notoriously challenging due to the expressive nature of their underlying latent space. To this end, we propose Pix2NPHM, a vision transformer (ViT) network that directly regresses NPHM parameters, given a single image as input. Compared to existing approaches, the neural parametric space allows our method to reconstruct more recognizable facial geometry and accurate facial expressions. For broad generalization, we exploit domain-specific ViTs as backbones, which are pretrained on geometric prediction tasks. We train Pix2NPHM on a mixture of 3D data, including a total of over 100K NPHM registrations that enable direct supervision in SDF space, and large-scale 2D video datasets, for which normal estimates serve as pseudo ground truth geometry. Pix2NPHM not only allows for 3D reconstructions at interactive frame rates, it is also possible to improve geometric fidelity by a subsequent inference-time optimization against estimated surface normals and canonical point maps. As a result, we achieve unprecedented face reconstruction quality that can run at scale on in-the-wild data.","authors":["Simon Giebenhain","Tobias Kirschstein","Liam Schoneveld","Davide Davoli","Zhe Chen","Matthias Nießner"],"pdf_url":"","comment":"Project website: https://simongiebenhain.github.io/Pix2NPHM/ , Video: https://www.youtube.com/watch?v=MgpEJC5p1Ts"},{"id":"http://arxiv.org/abs/2512.17771v1","updated":"2025-12-19T16:43:07Z","published":"2025-12-19T16:43:07Z","title":"Easy Adaptation: An Efficient Task-Specific Knowledge Injection Method for Large Models in Resource-Constrained Environments","summary":"While the enormous parameter scale endows Large Models (LMs) with unparalleled performance, it also limits their adaptability across specific tasks. Parameter-Efficient Fine-Tuning (PEFT) has emerged as a critical approach for effectively adapting LMs to a diverse range of downstream tasks. However, existing PEFT methods face two primary challenges: (1) High resource cost. Although PEFT methods significantly reduce resource demands compared to full fine-tuning, it still requires substantial time and memory, making it impractical in resource-constrained environments. (2) Parameter dependency. PEFT methods heavily rely on updating a subset of parameters associated with LMs to incorporate task-specific knowledge. Yet, due to increasing competition in the LMs landscape, many companies have adopted closed-source policies for their leading models, offering access only via Application Programming Interface (APIs). Whereas, the expense is often cost-prohibitive and difficult to sustain, as the fine-tuning process of LMs is extremely slow. Even if small models perform far worse than LMs in general, they can achieve superior results on particular distributions while requiring only minimal resources. Motivated by this insight, we propose Easy Adaptation (EA), which designs Specific Small Models (SSMs) to complement the underfitted data distribution for LMs. Extensive experiments show that EA matches the performance of PEFT on diverse tasks without accessing LM parameters, and requires only minimal resources.","authors":["Dong Chen","Zhengqing Hu","Shixing Zhao","Yibo Guo"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17769v1","updated":"2025-12-19T16:41:16Z","published":"2025-12-19T16:41:16Z","title":"Bangla MedER: Multi-BERT Ensemble Approach for the Recognition of Bangla Medical Entity","summary":"Medical Entity Recognition (MedER) is an essential NLP task for extracting meaningful entities from the medical corpus. Nowadays, MedER-based research outcomes can remarkably contribute to the development of automated systems in the medical sector, ultimately enhancing patient care and outcomes. While extensive research has been conducted on MedER in English, low-resource languages like Bangla remain underexplored. Our work aims to bridge this gap. For Bangla medical entity recognition, this study first examined a number of transformer models, including BERT, DistilBERT, ELECTRA, and RoBERTa. We also propose a novel Multi-BERT Ensemble approach that outperformed all baseline models with the highest accuracy of 89.58%. Notably, it provides an 11.80% accuracy improvement over the single-layer BERT model, demonstrating its effectiveness for this task. A major challenge in MedER for low-resource languages is the lack of annotated datasets. To address this issue, we developed a high-quality dataset tailored for the Bangla MedER task. The dataset was used to evaluate the effectiveness of our model through multiple performance metrics, demonstrating its robustness and applicability. Our findings highlight the potential of Multi-BERT Ensemble models in improving MedER for Bangla and set the foundation for further advancements in low-resource medical NLP.","authors":["Tanjim Taharat Aurpa","Farzana Akter","Md. Mehedi Hasan","Shakil Ahmed","Shifat Ara Rafiq","Fatema Khan"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2509.00496v2","updated":"2025-12-19T16:38:02Z","published":"2025-08-30T13:37:28Z","title":"ResearchQA: Evaluating Scholarly Question Answering at Scale Across 75 Fields with Survey-Mined Questions and Rubrics","summary":"Evaluating long-form responses to research queries heavily relies on expert annotators, restricting attention to areas like AI where researchers can conveniently enlist colleagues. Yet, research expertise is abundant: survey articles consolidate knowledge spread across the literature. We introduce ResearchQA, a resource for evaluating LLM systems by distilling survey articles from 75 research fields into 21K queries and 160K rubric items. Queries and rubrics are jointly derived from survey sections, where rubric items list query-specific answer evaluation criteria, i.e., citing papers, making explanations, and describing limitations. 31 Ph.D. annotators in 8 fields judge that 90% of queries reflect Ph.D. information needs and 87% of rubric items warrant emphasis of a sentence or longer. We leverage ResearchQA to evaluate 18 systems in 7.6K head-to-heads. No parametric or retrieval-augmented system we evaluate exceeds 70% on covering rubric items, and the highest-ranking system shows 75% coverage. Error analysis reveals that the highest-ranking system fully addresses less than 11% of citation rubric items, 48% of limitation items, and 49% of comparison items. We release our data to facilitate more comprehensive multi-field evaluations.","authors":["Li S. Yifei","Allen Chang","Chaitanya Malaviya","Mark Yatskar"],"pdf_url":"","comment":"12 pages main, 40 pages total, 15 figures"},{"id":"http://arxiv.org/abs/2506.09147v4","updated":"2025-12-19T16:35:50Z","published":"2025-06-10T18:01:42Z","title":"LLM-as-a-qualitative-judge: automating error analysis in natural language generation","summary":"Prompting large language models (LLMs) to evaluate generated text, known as LLM-as-a-judge, has become a standard evaluation approach in natural language generation (NLG), but is primarily used as a quantitative tool, i.e. with numerical scores as main outputs. In this work, we propose LLM-as-a-qualitative-judge, an LLM-based evaluation approach with the main output being a structured report of common issue types in the NLG system outputs. Our approach is targeted at providing developers with meaningful insights on what improvements can be done to a given NLG system and consists of two main steps, namely open-ended per-instance issue analysis and clustering of the discovered issues using an intuitive cumulative algorithm. We also introduce a strategy for evaluating the proposed approach, coupled with ~300 annotations of issues in instances from 12 NLG datasets. Our results show that instance-specific issues output by LLM-as-a-qualitative-judge match those annotated by humans in 2/3 cases, and that LLM-as-a-qualitative-judge is capable of producing error type reports resembling the reports composed by human annotators. We also demonstrate in a case study how the use of LLM-as-a-qualitative-judge can substantially improve NLG systems performance. Our code and data are publicly available at https://github.com/tunde-ajayi/llm-as-a-qualitative-judge.","authors":["Nadezhda Chirkova","Tunde Oluwaseyi Ajayi","Seth Aycock","Zain Muhammad Mujahid","Vladana Perlić","Ekaterina Borisova","Markarit Vartampetian"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17756v1","updated":"2025-12-19T16:28:57Z","published":"2025-12-19T16:28:57Z","title":"AncientBench: Towards Comprehensive Evaluation on Excavated and Transmitted Chinese Corpora","summary":"Comprehension of ancient texts plays an important role in archaeology and understanding of Chinese history and civilization. The rapid development of large language models needs benchmarks that can evaluate their comprehension of ancient characters. Existing Chinese benchmarks are mostly targeted at modern Chinese and transmitted documents in ancient Chinese, but the part of excavated documents in ancient Chinese is not covered. To meet this need, we propose the AncientBench, which aims to evaluate the comprehension of ancient characters, especially in the scenario of excavated documents. The AncientBench is divided into four dimensions, which correspond to the four competencies of ancient character comprehension: glyph comprehension, pronunciation comprehension, meaning comprehension, and contextual comprehension. The benchmark also contains ten tasks, including radical, phonetic radical, homophone, cloze, translation, and more, providing a comprehensive framework for evaluation. We convened archaeological researchers to conduct experimental evaluations, proposed an ancient model as baseline, and conducted extensive experiments on the currently best-performing large language models. The experimental results reveal the great potential of large language models in ancient textual scenarios as well as the gap with humans. Our research aims to promote the development and application of large language models in the field of archaeology and ancient Chinese language.","authors":["Zhihan Zhou","Daqian Shi","Rui Song","Lida Shi","Xiaolei Diao","Hao Xu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2508.10501v4","updated":"2025-12-19T16:27:34Z","published":"2025-08-14T10:03:47Z","title":"PASS: Probabilistic Agentic Supernet Sampling for Interpretable and Adaptive Chest X-Ray Reasoning","summary":"Existing tool-augmented agentic systems are limited in the real world by (i) black-box reasoning steps that undermine trust of decision-making and pose safety risks, (ii) poor multimodal integration, which is inherently critical for healthcare tasks, and (iii) rigid and computationally inefficient agentic pipelines. We introduce PASS (Probabilistic Agentic Supernet Sampling), the first multimodal framework to address these challenges in the context of Chest X-Ray (CXR) reasoning. PASS adaptively samples agentic workflows over a multi-tool graph, yielding decision paths annotated with interpretable probabilities. Given the complex CXR reasoning task with multimodal medical data, PASS leverages its learned task-conditioned distribution over the agentic supernet. Thus, it adaptively selects the most suitable tool at each supernet layer, offering probability-annotated trajectories for post-hoc audits and directly enhancing medical AI safety. PASS also continuously compresses salient findings into an evolving personalized memory, while dynamically deciding whether to deepen its reasoning path or invoke an early exit for efficiency. To optimize a Pareto frontier balancing performance and cost, we design a novel three-stage training procedure, including expert knowledge warm-up, contrastive path-ranking, and cost-aware reinforcement learning. To facilitate rigorous evaluation, we introduce CAB-E, a comprehensive benchmark for multi-step, safety-critical, free-form CXR reasoning. Experiments across various benchmarks validate that PASS significantly outperforms strong baselines in multiple metrics (e.g., accuracy, LLM-Judge, semantic similarity, etc.) while balancing computational costs, pushing a new paradigm shift towards interpretable, adaptive, and multimodal medical agentic systems.","authors":["Yushi Feng","Junye Du","Yingying Hong","Qifan Wang","Lequan Yu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2510.24830v2","updated":"2025-12-19T16:21:05Z","published":"2025-10-28T16:42:53Z","title":"The Generation Phases of Flow Matching: a Denoising Perspective","summary":"Flow matching has achieved remarkable success, yet the factors influencing the quality of its generation process remain poorly understood. In this work, we adopt a denoising perspective and design a framework to empirically probe the generation process. Laying down the formal connections between flow matching models and denoisers, we provide a common ground to compare their performances on generation and denoising. This enables the design of principled and controlled perturbations to influence sample generation: noise and drift. This leads to new insights on the distinct dynamical phases of the generative process, enabling us to precisely characterize at which stage of the generative process denoisers succeed or fail and why this matters.","authors":["Anne Gagneux","Ségolène Martin","Rémi Gribonval","Mathurin Massias"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2509.12508v4","updated":"2025-12-19T16:18:03Z","published":"2025-09-15T23:19:36Z","title":"Fun-ASR Technical Report","summary":"In recent years, automatic speech recognition (ASR) has witnessed transformative advancements driven by three complementary paradigms: data scaling, model size scaling, and deep integration with large language models (LLMs). However, LLMs are prone to hallucination, which can significantly degrade user experience in real-world ASR applications. In this paper, we present Fun-ASR, a large-scale, LLM-based ASR system that synergistically combines massive data, large model capacity, LLM integration, and reinforcement learning to achieve state-of-the-art performance across diverse and complex speech recognition scenarios. Moreover, Fun-ASR is specifically optimized for practical deployment, with enhancements in streaming capability, noise robustness, code-switching, hotword customization, and satisfying other real-world application requirements. Experimental results show that while most LLM-based ASR systems achieve strong performance on open-source benchmarks, they often underperform on real industry evaluation sets. Thanks to production-oriented optimizations, Fun-ASR achieves state-of-the-art performance on real application datasets, demonstrating its effectiveness and robustness in practical settings. The code and models are accessible at https://github.com/FunAudioLLM/Fun-ASR .","authors":["Keyu An","Yanni Chen","Zhigao Chen","Chong Deng","Zhihao Du","Changfeng Gao","Zhifu Gao","Bo Gong","Xiangang Li","Yabin Li","Ying Liu","Xiang Lv","Yunjie Ji","Yiheng Jiang","Bin Ma","Haoneng Luo","Chongjia Ni","Zexu Pan","Yiping Peng","Zhendong Peng","Peiyao Wang","Hao Wang","Haoxu Wang","Wen Wang","Wupeng Wang","Yuzhong Wu","Biao Tian","Zhentao Tan","Nan Yang","Bin Yuan","Jieping Ye","Jixing Yu","Qinglin Zhang","Kun Zou","Han Zhao","Shengkui Zhao","Jingren Zhou","Yanqiao Zhu"],"pdf_url":"","comment":"Authors are listed in alphabetical order. Work in progress"},{"id":"http://arxiv.org/abs/2508.21052v2","updated":"2025-12-19T16:10:13Z","published":"2025-08-28T17:55:14Z","title":"FakeParts: a New Family of AI-Generated DeepFakes","summary":"We introduce FakeParts, a new class of deepfakes characterized by subtle, localized manipulations to specific spatial regions or temporal segments of otherwise authentic videos. Unlike fully synthetic content, these partial manipulations - ranging from altered facial expressions to object substitutions and background modifications - blend seamlessly with real elements, making them particularly deceptive and difficult to detect. To address the critical gap in detection, we present FakePartsBench, the first large-scale benchmark specifically designed to capture the full spectrum of partial deepfakes. Comprising over 81K (including 44K FakeParts) videos with pixel- and frame-level manipulation annotations, our dataset enables comprehensive evaluation of detection methods. Our user studies demonstrate that FakeParts reduces human detection accuracy by up to 26% compared to traditional deepfakes, with similar performance degradation observed in state-of-the-art detection models. This work identifies an urgent vulnerability in current detectors and provides the necessary resources to develop methods robust to partial manipulations.","authors":["Ziyi Liu","Firas Gabetni","Awais Hussain Sani","Xi Wang","Soobash Daiboo","Gaetan Brison","Gianni Franchi","Vicky Kalogeiton"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17733v1","updated":"2025-12-19T16:09:29Z","published":"2025-12-19T16:09:29Z","title":"Diversity Recommendation via Causal Deconfounding of Co-purchase Relations and Counterfactual Exposure","summary":"Beyond user-item modeling, item-to-item relationships are increasingly used to enhance recommendation. However, common methods largely rely on co-occurrence, making them prone to item popularity bias and user attributes, which degrades embedding quality and performance. Meanwhile, although diversity is acknowledged as a key aspect of recommendation quality, existing research offers limited attention to it, with a notable lack of causal perspectives and theoretical grounding. To address these challenges, we propose Cadence: Diversity Recommendation via Causal Deconfounding of Co-purchase Relations and Counterfactual Exposure - a plug-and-play framework built upon LightGCN as the backbone, primarily designed to enhance recommendation diversity while preserving accuracy. First, we compute the Unbiased Asymmetric Co-purchase Relationship (UACR) between items - excluding item popularity and user attributes - to construct a deconfounded directed item graph, with an aggregation mechanism to refine embeddings. Second, we leverage UACR to identify diverse categories of items that exhibit strong causal relevance to a user's interacted items but have not yet been engaged with. We then simulate their behavior under high-exposure scenarios, thereby significantly enhancing recommendation diversity while preserving relevance. Extensive experiments on real-world datasets demonstrate that our method consistently outperforms state-of-the-art diversity models in both diversity and accuracy, and further validates its effectiveness, transferability, and efficiency over baselines.","authors":["Jingmao Zhang","Zhiting Zhao","Yunqi Lin","Jianghong Ma","Tianjun Wei","Haijun Zhang","Xiaofeng Zhang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17722v1","updated":"2025-12-19T15:56:12Z","published":"2025-12-19T15:56:12Z","title":"Digital and Web Forensics Model Cards, V1","summary":"This paper introduces a standardized model card framework specifically designed for digital and web forensics. Building upon established model card methodologies and recent work on abstract models for digital forensic analysis, this paper presents a web based framework that generates model cards specifically designed to represent knowledge in the forensic domain. The framework includes controlled vocabularies for classification, reasoning types, bias identification, and error categorization, along with a web-based generator tool to facilitate adoption. The paper describes the model card structure, presents the controlled vocabularies, and introduces the beta version of the generator tool, inviting community feedback to refine this emerging standard. Ultimately, the systemic risk is that that the anti fraud and digital and web forensics processes are controlled by the mobs.","authors":["Paola Di Maio"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2409.03735v3","updated":"2025-12-19T15:41:38Z","published":"2024-09-05T17:50:31Z","title":"Privacy Bias in Language Models: A Contextual Integrity-based Auditing Metric","summary":"As large language models (LLMs) are integrated into sociotechnical systems, it is crucial to examine the privacy biases they exhibit. We define privacy bias as the appropriateness value of information flows in responses from LLMs. A deviation between privacy biases and expected values, referred to as privacy bias delta, may indicate privacy violations. As an auditing metric, privacy bias can help (a) model trainers evaluate the ethical and societal impact of LLMs, (b) service providers select context-appropriate LLMs, and (c) policymakers assess the appropriateness of privacy biases in deployed LLMs. We formulate and answer a novel research question: how can we reliably examine privacy biases in LLMs and the factors that influence them? We present a novel approach for assessing privacy biases using a contextual integrity-based methodology to evaluate the responses from various LLMs. Our approach accounts for the sensitivity of responses across prompt variations, which hinders the evaluation of privacy biases. Finally, we investigate how privacy biases are affected by model capacities and optimizations.","authors":["Yan Shvartzshnaider","Vasisht Duddu"],"pdf_url":"","comment":"Privacy Enhancing Technologies Symposium (PETS), 2026"},{"id":"http://arxiv.org/abs/2507.22659v2","updated":"2025-12-19T15:41:06Z","published":"2025-07-30T13:17:16Z","title":"A Systematic Literature Review on Detecting Software Vulnerabilities with Large Language Models","summary":"The increasing adoption of Large Language Models (LLMs) in software engineering has sparked interest in their use for software vulnerability detection. However, the rapid development of this field has resulted in a fragmented research landscape, with diverse studies that are difficult to compare due to differences in, e.g., system designs and dataset usage. This fragmentation makes it difficult to obtain a clear overview of the state-of-the-art or compare and categorize studies meaningfully. In this work, we present a comprehensive systematic literature review (SLR) of LLM-based software vulnerability detection. We analyze 263 studies published between January 2020 and November 2025, categorizing them by task formulation, input representation, system architecture, and techniques. Further, we analyze the datasets used, including their characteristics, vulnerability coverage, and diversity. We present a fine-grained taxonomy of vulnerability detection approaches, identify key limitations, and outline actionable future research opportunities. By providing a structured overview of the field, this review improves transparency and serves as a practical guide for researchers and practitioners aiming to conduct more comparable and reproducible research. We publicly release all artifacts and maintain a living repository of LLM-based software vulnerability detection studies at https://github.com/hs-esslingen-it-security/Awesome-LLM4SVD.","authors":["Sabrina Kaniewski","Fabian Schmidt","Markus Enzweiler","Michael Menth","Tobias Heer"],"pdf_url":"","comment":"43 pages + 20 pages references, 7 tables, 13 figures"},{"id":"http://arxiv.org/abs/2402.12118v3","updated":"2025-12-19T15:36:27Z","published":"2024-02-19T13:13:16Z","title":"Sparse, Efficient and Explainable Data Attribution with DualXDA","summary":"Data Attribution (DA) is an emerging approach in the field of eXplainable Artificial Intelligence (XAI), aiming to identify influential training datapoints which determine model outputs. It seeks to provide transparency about the model and individual predictions, e.g. for model debugging, identifying data-related causes of suboptimal performance. However, existing DA approaches suffer from prohibitively high computational costs and memory demands when applied to even medium-scale datasets and models, forcing practitioners to resort to approximations that may fail to capture the true inference process of the underlying model. Additionally, current attribution methods exhibit low sparsity, resulting in non-negligible attribution scores across a high number of training examples, hindering the discovery of decisive patterns in the data. In this work, we introduce DualXDA, a framework for sparse, efficient and explainable DA, comprised of two interlinked approaches, Dual Data Attribution (DualDA) and eXplainable Data Attribution (XDA): With DualDA, we propose a novel approach for efficient and effective DA, leveraging Support Vector Machine theory to provide fast and naturally sparse data attributions for AI predictions. In extensive quantitative analyses, we demonstrate that DualDA achieves high attribution quality, excels at solving a series of evaluated downstream tasks, while at the same time improving explanation time by a factor of up to 4,100,000x compared to the original Influence Functions method, and up to 11,000x compared to the method's most efficient approximation from literature to date. We further introduce XDA, a method for enhancing Data Attribution with capabilities from feature attribution methods to explain why training samples are relevant for the prediction of a test sample in terms of impactful features, which we showcase and verify qualitatively in detail.","authors":["Galip Ümit Yolcu","Moritz Weckbecker","Thomas Wiegand","Wojciech Samek","Sebastian Lapuschkin"],"pdf_url":"","comment":"Accepted to Transactions on Machine Learning Research (TMLR), 2025"},{"id":"http://arxiv.org/abs/2506.11023v2","updated":"2025-12-19T15:34:46Z","published":"2025-05-20T08:15:16Z","title":"OntoGSN: An Ontology-Based Framework for Semantic Management and Extension of Assurance Cases","summary":"Assurance cases (ACs) are a common artifact for building and maintaining confidence in system properties such as safety or robustness. Constructing an AC can be challenging, although existing tools provide support in static, document-centric applications and methods for dynamic contexts (e.g., autonomous driving) are emerging. Unfortunately, managing ACs remains a challenge, since maintaining the embedded knowledge in the face of changes requires substantial effort, in the process deterring developers - or worse, producing poorly managed cases that instill false confidence. To address this, we present OntoGSN: an ontology and supporting middleware for managing ACs in the Goal Structuring Notation (GSN) standard. OntoGSN offers a knowledge representation and a queryable graph that can be automatically populated, evaluated, and updated. Our contributions include: a 1:1 formalization of the GSN Community Standard v3 in an OWL ontology with SWRL rules; a helper ontology and parser for integration with a widely used AC tool; a repository and documentation of design decisions for OntoGSN maintenance; a SPARQL query library with automation patterns; and a prototypical interface. The ontology strictly adheres to the standard's text and has been evaluated according to FAIR principles, the OOPS framework, competency questions, and community feedback. The development of other middleware elements is guided by the community needs and subject to ongoing evaluations. To demonstrate the utility of our contributions, we illustrate dynamic AC management in an example involving assurance of adversarial robustness in large language models.","authors":["Tomas Bueno Momcilovic","Barbara Gallina","Ingmar Kessler","Jule Hendricks","Dian Balta"],"pdf_url":"","comment":"Submitted to the ESWC 2026 Resources track"},{"id":"http://arxiv.org/abs/2512.17678v1","updated":"2025-12-19T15:17:34Z","published":"2025-12-19T15:17:34Z","title":"You Only Train Once: Differentiable Subset Selection for Omics Data","summary":"Selecting compact and informative gene subsets from single-cell transcriptomic data is essential for biomarker discovery, improving interpretability, and cost-effective profiling. However, most existing feature selection approaches either operate as multi-stage pipelines or rely on post hoc feature attribution, making selection and prediction weakly coupled. In this work, we present YOTO (you only train once), an end-to-end framework that jointly identifies discrete gene subsets and performs prediction within a single differentiable architecture. In our model, the prediction task directly guides which genes are selected, while the learned subsets, in turn, shape the predictive representation. This closed feedback loop enables the model to iteratively refine both what it selects and how it predicts during training. Unlike existing approaches, YOTO enforces sparsity so that only the selected genes contribute to inference, eliminating the need to train additional downstream classifiers. Through a multi-task learning design, the model learns shared representations across related objectives, allowing partially labeled datasets to inform one another, and discovering gene subsets that generalize across tasks without additional training steps. We evaluate YOTO on two representative single-cell RNA-seq datasets, showing that it consistently outperforms state-of-the-art baselines. These results demonstrate that sparse, end-to-end, multi-task gene subset selection improves predictive performance and yields compact and meaningful gene subsets, advancing biomarker discovery and single-cell analysis.","authors":["Daphné Chopard","Jorge da Silva Gonçalves","Irene Cannistraci","Thomas M. Sutter","Julia E. Vogt"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17675v1","updated":"2025-12-19T15:17:12Z","published":"2025-12-19T15:17:12Z","title":"An Empirical Study of Sampling Hyperparameters in Diffusion-Based Super-Resolution","summary":"Diffusion models have shown strong potential for solving inverse problems such as single-image super-resolution, where a high-resolution image is recovered from a low-resolution observation using a pretrained unconditional prior. Conditioning methods, including Diffusion Posterior Sampling (DPS) and Manifold Constrained Gradient (MCG), can substantially improve reconstruction quality, but they introduce additional hyperparameters that require careful tuning. In this work, we conduct an empirical ablation study on FFHQ super-resolution to identify the dominant factors affecting performance when applying conditioning to pretrained diffusion models, and show that the conditioning step size has a significantly greater impact than the diffusion step count, with step sizes in the range of [2.0, 3.0] yielding the best overall performance in our experiments.","authors":["Yudhistira Arief Wibowo"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17673v1","updated":"2025-12-19T15:15:58Z","published":"2025-12-19T15:15:58Z","title":"Learning Spatio-Temporal Feature Representations for Video-Based Gaze Estimation","summary":"Video-based gaze estimation methods aim to capture the inherently temporal dynamics of human eye gaze from multiple image frames. However, since models must capture both spatial and temporal relationships, performance is limited by the feature representations within a frame but also between multiple frames. We propose the Spatio-Temporal Gaze Network (ST-Gaze), a model that combines a CNN backbone with dedicated channel attention and self-attention modules to fuse eye and face features optimally. The fused features are then treated as a spatial sequence, allowing for the capture of an intra-frame context, which is then propagated through time to model inter-frame dynamics. We evaluated our method on the EVE dataset and show that ST-Gaze achieves state-of-the-art performance both with and without person-specific adaptation. Additionally, our ablation study provides further insights into the model performance, showing that preserving and modelling intra-frame spatial context with our spatio-temporal recurrence is fundamentally superior to premature spatial pooling. As such, our results pave the way towards more robust video-based gaze estimation using commonly available cameras.","authors":["Alexandre Personnic","Mihai Bâce"],"pdf_url":"","comment":"12 pages, 5 figures, the code repository is available at https://gitlab.kuleuven.be/u0172623/ST-Gaze"},{"id":"http://arxiv.org/abs/2512.17667v1","updated":"2025-12-19T15:12:01Z","published":"2025-12-19T15:12:01Z","title":"STAR: Semantic-Traffic Alignment and Retrieval for Zero-Shot HTTPS Website Fingerprinting","summary":"Modern HTTPS mechanisms such as Encrypted Client Hello (ECH) and encrypted DNS improve privacy but remain vulnerable to website fingerprinting (WF) attacks, where adversaries infer visited sites from encrypted traffic patterns. Existing WF methods rely on supervised learning with site-specific labeled traces, which limits scalability and fails to handle previously unseen websites. We address these limitations by reformulating WF as a zero-shot cross-modal retrieval problem and introducing STAR. STAR learns a joint embedding space for encrypted traffic traces and crawl-time logic profiles using a dual-encoder architecture. Trained on 150K automatically collected traffic-logic pairs with contrastive and consistency objectives and structure-aware augmentation, STAR retrieves the most semantically aligned profile for a trace without requiring target-side traffic during training. Experiments on 1,600 unseen websites show that STAR achieves 87.9 percent top-1 accuracy and 0.963 AUC in open-world detection, outperforming supervised and few-shot baselines. Adding an adapter with only four labeled traces per site further boosts top-5 accuracy to 98.8 percent. Our analysis reveals intrinsic semantic-traffic alignment in modern web protocols, identifying semantic leakage as the dominant privacy risk in encrypted HTTPS traffic. We release STAR's datasets and code to support reproducibility and future research.","authors":["Yifei Cheng","Yujia Zhu","Baiyang Li","Xinhao Deng","Yitong Cai","Yaochen Ren","Qingyun Liu"],"pdf_url":"","comment":"Accepted by IEEE INFOCOM 2026. Camera-ready version"},{"id":"http://arxiv.org/abs/2512.17637v1","updated":"2025-12-19T14:39:03Z","published":"2025-12-19T14:39:03Z","title":"About Time: Model-free Reinforcement Learning with Timed Reward Machines","summary":"Reward specification plays a central role in reinforcement learning (RL), guiding the agent's behavior. To express non-Markovian rewards, formalisms such as reward machines have been introduced to capture dependencies on histories. However, traditional reward machines lack the ability to model precise timing constraints, limiting their use in time-sensitive applications. In this paper, we propose timed reward machines (TRMs), which are an extension of reward machines that incorporate timing constraints into the reward structure. TRMs enable more expressive specifications with tunable reward logic, for example, imposing costs for delays and granting rewards for timely actions. We study model-free RL frameworks (i.e., tabular Q-learning) for learning optimal policies with TRMs under digital and real-time semantics. Our algorithms integrate the TRM into learning via abstractions of timed automata, and employ counterfactual-imagining heuristics that exploit the structure of the TRM to improve the search. Experimentally, we demonstrate that our algorithm learns policies that achieve high rewards while satisfying the timing constraints specified by the TRM on popular RL benchmarks. Moreover, we conduct comparative studies of performance under different TRM semantics, along with ablations that highlight the benefits of counterfactual-imagining.","authors":["Anirban Majumdar","Ritam Raha","Rajarshi Roy","David Parker","Marta Kwiatkowska"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2510.17330v2","updated":"2025-12-19T14:39:03Z","published":"2025-10-20T09:23:29Z","title":"CharDiff-LP: A Diffusion Model with Character-Level Guidance for License Plate Image Restoration","summary":"License plate image restoration is important not only as a preprocessing step for license plate recognition but also for enhancing evidential value, improving visual clarity, and enabling broader reuse of license plate images. We propose a novel diffusion-based framework with character-level guidance, CharDiff-LP, which effectively restores and recognizes severely degraded license plate images captured under realistic conditions. CharDiff-LP leverages fine-grained character-level priors extracted through external segmentation and Optical Character Recognition (OCR) modules tailored for low-quality license plate images. For precise and focused guidance, CharDiff-LP incorporates a novel Character-guided Attention through Region-wise Masking (CHARM) module, which ensures that each character's guidance is restricted to its own region, thereby avoiding interference with other regions. In experiments, CharDiff-LP significantly outperformed baseline restoration models in both restoration quality and recognition accuracy, achieving a 28.3% relative reduction in character error rate (CER) on the Roboflow-LP dataset compared with the best-performing baseline.","authors":["Kihyun Na","Gyuhwan Park","Injung Kim"],"pdf_url":"","comment":"15 pages, 6 figures, 4 tables"},{"id":"http://arxiv.org/abs/2512.17636v1","updated":"2025-12-19T14:37:07Z","published":"2025-12-19T14:37:07Z","title":"Trust-Region Adaptive Policy Optimization","summary":"Post-training methods, especially Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL), play an important role in improving large language models' (LLMs) complex reasoning abilities. However, the dominant two-stage pipeline (SFT then RL) suffers from a key inconsistency: SFT enforces rigid imitation that suppresses exploration and induces forgetting, limiting RL's potential for improvements. We address this inefficiency with TRAPO (\\textbf{T}rust-\\textbf{R}egion \\textbf{A}daptive \\textbf{P}olicy \\textbf{O}ptimization), a hybrid framework that interleaves SFT and RL within each training instance by optimizing SFT loss on expert prefixes and RL loss on the model's own completions, unifying external supervision and self-exploration. To stabilize training, we introduce Trust-Region SFT (TrSFT), which minimizes forward KL divergence inside a trust region but attenuates optimization outside, effectively shifting toward reverse KL and yielding stable, mode-seeking updates favorable for RL. An adaptive prefix-selection mechanism further allocates expert guidance based on measured utility. Experiments on five mathematical reasoning benchmarks show that TRAPO consistently surpasses standard SFT, RL, and SFT-then-RL pipelines, as well as recent state-of-the-art approaches, establishing a strong new paradigm for reasoning-enhanced LLMs.","authors":["Mingyu Su","Jian Guan","Yuxian Gu","Minlie Huang","Hongning Wang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17629v1","updated":"2025-12-19T14:33:02Z","published":"2025-12-19T14:33:02Z","title":"SCOPE: Sequential Causal Optimization of Process Interventions","summary":"Prescriptive Process Monitoring (PresPM) recommends interventions during business processes to optimize key performance indicators (KPIs). In realistic settings, interventions are rarely isolated: organizations need to align sequences of interventions to jointly steer the outcome of a case. Existing PresPM approaches fall short in this respect. Many focus on a single intervention decision, while others treat multiple interventions independently, ignoring how they interact over time. Methods that do address these dependencies depend either on simulation or data augmentation to approximate the process to train a Reinforcement Learning (RL) agent, which can create a reality gap and introduce bias. We introduce SCOPE, a PresPM approach that learns aligned sequential intervention recommendations. SCOPE employs backward induction to estimate the effect of each candidate intervention action, propagating its impact from the final decision point back to the first. By leveraging causal learners, our method can utilize observational data directly, unlike methods that require constructing process approximations for reinforcement learning. Experiments on both an existing synthetic dataset and a new semi-synthetic dataset show that SCOPE consistently outperforms state-of-the-art PresPM techniques in optimizing the KPI. The novel semi-synthetic setup, based on a real-life event log, is provided as a reusable benchmark for future work on sequential PresPM.","authors":["Jakob De Moor","Hans Weytjens","Johannes De Smedt","Jochen De Weerdt"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2511.21417v2","updated":"2025-12-19T14:32:02Z","published":"2025-11-26T14:08:28Z","title":"New Hybrid Heuristics for Pseudo-Boolean Propagation","summary":"In pseudo-boolean solving the currently most successful unit propagation strategy is a hybrid mode combining the watched literal scheme with the counting method. This short paper introduces new heuristics for this hybrid decision, which are able to drastically outperform the current method in the RoundingSAT solver.","authors":["Mia Müßig","Jan Johannsen"],"pdf_url":"","comment":"5 pages, 3 figures, added different cut-off for old hybrid"},{"id":"http://arxiv.org/abs/2507.06261v6","updated":"2025-12-19T14:25:46Z","published":"2025-07-07T17:36:04Z","title":"Gemini 2.5: Pushing the Frontier with Advanced Reasoning, Multimodality, Long Context, and Next Generation Agentic Capabilities","summary":"In this report, we introduce the Gemini 2.X model family: Gemini 2.5 Pro and Gemini 2.5 Flash, as well as our earlier Gemini 2.0 Flash and Flash-Lite models. Gemini 2.5 Pro is our most capable model yet, achieving SoTA performance on frontier coding and reasoning benchmarks. In addition to its incredible coding and reasoning skills, Gemini 2.5 Pro is a thinking model that excels at multimodal understanding and it is now able to process up to 3 hours of video content. Its unique combination of long context, multimodal and reasoning capabilities can be combined to unlock new agentic workflows. Gemini 2.5 Flash provides excellent reasoning abilities at a fraction of the compute and latency requirements and Gemini 2.0 Flash and Flash-Lite provide high performance at low latency and cost. Taken together, the Gemini 2.X model generation spans the full Pareto frontier of model capability vs cost, allowing users to explore the boundaries of what is possible with complex agentic problem solving.","authors":["Gheorghe Comanici","Eric Bieber","Mike Schaekermann","Ice Pasupat","Noveen Sachdeva","Inderjit Dhillon","Marcel Blistein","Ori Ram","Dan Zhang","Evan Rosen","Luke Marris","Sam Petulla","Colin Gaffney","Asaf Aharoni","Nathan Lintz","Tiago Cardal Pais","Henrik Jacobsson","Idan Szpektor","Nan-Jiang Jiang","Krishna Haridasan","Ahmed Omran","Nikunj Saunshi","Dara Bahri","Gaurav Mishra","Eric Chu","Toby Boyd","Brad Hekman","Aaron Parisi","Chaoyi Zhang","Kornraphop Kawintiranon","Tania Bedrax-Weiss","Oliver Wang","Ya Xu","Ollie Purkiss","Uri Mendlovic","Ilaï Deutel","Nam Nguyen","Adam Langley","Flip Korn","Lucia Rossazza","Alexandre Ramé","Sagar Waghmare","Helen Miller","Nathan Byrd","Ashrith Sheshan","Raia Hadsell","Sangnie Bhardwaj","Pawel Janus","Tero Rissa","Dan Horgan","Alvin Abdagic","Lior Belenki","James Allingham","Anima Singh","Theo Guidroz","Srivatsan Srinivasan","Herman Schmit","Kristen Chiafullo","Andre Elisseeff","Nilpa Jha","Prateek Kolhar","Leonard Berrada","Frank Ding","Xiance Si","Shrestha Basu Mallick","Franz Och","Sofia Erell","Eric Ni","Tejasi Latkar","Sherry Yang","Petar Sirkovic","Ziqiang Feng","Robert Leland","Rachel Hornung","Gang Wu","Charles Blundell","Hamidreza Alvari","Po-Sen Huang","Cathy Yip","Sanja Deur","Li Liu","Gabriela Surita","Pablo Duque","Dima Damen","Johnson Jia","Arthur Guez","Markus Mircea","Animesh Sinha","Alberto Magni","Paweł Stradomski","Tal Marian","Vlado Galić","Wenhu Chen","Hisham Husain","Achintya Singhal","Dominik Grewe","François-Xavier Aubet","Shuang Song","Lorenzo Blanco","Leland Rechis","Lewis Ho","Rich Munoz","Kelvin Zheng","Jessica Hamrick","Kevin Mather","Hagai Taitelbaum","Eliza Rutherford","Yun Lei","Kuangyuan Chen","Anand Shukla","Erica Moreira","Eric Doi","Berivan Isik","Nir Shabat","Dominika Rogozińska","Kashyap Kolipaka","Jason Chang","Eugen Vušak","Srinivasan Venkatachary","Shadi Noghabi","Tarun Bharti","Younghoon Jun","Aleksandr Zaks","Simon Green","Jeshwanth Challagundla","William Wong","Muqthar Mohammad","Dean Hirsch","Yong Cheng","Iftekhar Naim","Lev Proleev","Damien Vincent","Aayush Singh","Maxim Krikun","Dilip Krishnan","Zoubin Ghahramani","Aviel Atias","Rajeev Aggarwal","Christo Kirov","Dimitrios Vytiniotis","Christy Koh","Alexandra Chronopoulou","Pawan Dogra","Vlad-Doru Ion","Gladys Tyen","Jason Lee","Felix Weissenberger","Trevor Strohman","Ashwin Balakrishna","Jack Rae","Marko Velic","Raoul de Liedekerke","Oded Elyada","Wentao Yuan","Canoee Liu","Lior Shani","Sergey Kishchenko","Bea Alessio","Yandong Li","Richard Song","Sam Kwei","Orion Jankowski","Aneesh Pappu","Youhei Namiki","Yenai Ma","Nilesh Tripuraneni","Colin Cherry","Marissa Ikonomidis","Yu-Cheng Ling","Colin Ji","Beka Westberg","Auriel Wright","Da Yu","David Parkinson","Swaroop Ramaswamy","Jerome Connor","Soheil Hassas Yeganeh","Snchit Grover","George Kenwright","Lubo Litchev","Chris Apps","Alex Tomala","Felix Halim","Alex Castro-Ros","Zefei Li","Anudhyan Boral","Pauline Sho","Michal Yarom","Eric Malmi","David Klinghoffer","Rebecca Lin","Alan Ansell","Pradeep Kumar S","Shubin Zhao","Siqi Zuo","Adam Santoro","Heng-Tze Cheng","Solomon Demmessie","Yuchi Liu","Nicole Brichtova","Allie Culp","Nathaniel Braun","Dan Graur","Will Ng","Nikhil Mehta","Aaron Phillips","Patrik Sundberg","Varun Godbole","Fangyu Liu","Yash Katariya","David Rim","Mojtaba Seyedhosseini","Sean Ammirati","Jonas Valfridsson","Mahan Malihi","Timothy Knight","Andeep Toor","Thomas Lampe","Abe Ittycheriah","Lewis Chiang","Chak Yeung","Alexandre Fréchette","Jinmeng Rao","Huisheng Wang","Himanshu Srivastava","Richard Zhang","Rocky Rhodes","Ariel Brand","Dean Weesner","Ilya Figotin","Felix Gimeno","Rachana Fellinger","Pierre Marcenac","José Leal","Eyal Marcus","Victor Cotruta","Rodrigo Cabrera","Sheryl Luo","Dan Garrette","Vera Axelrod","Sorin Baltateanu","David Barker","Dongkai Chen","Horia Toma","Ben Ingram","Jason Riesa","Chinmay Kulkarni","Yujing Zhang","Hongbin Liu","Chao Wang","Martin Polacek","Will Wu","Kai Hui","Adrian N Reyes","Yi Su","Megan Barnes","Ishaan Malhi","Anfal Siddiqui","Qixuan Feng","Mihai Damaschin","Daniele Pighin","Andreas Steiner","Samuel Yang","Ramya Sree Boppana","Simeon Ivanov","Arun Kandoor","Aditya Shah","Asier Mujika","Da Huang","Christopher A. Choquette-Choo","Mohak Patel","Tianhe Yu","Toni Creswell"," Jerry"," Liu","Catarina Barros","Yasaman Razeghi","Aurko Roy","Phil Culliton","Binbin Xiong","Jiaqi Pan","Thomas Strohmann","Tolly Powell","Babi Seal","Doug DeCarlo","Pranav Shyam","Kaan Katircioglu","Xuezhi Wang","Cassidy Hardin","Immanuel Odisho","Josef Broder","Oscar Chang","Arun Nair","Artem Shtefan","Maura O'Brien","Manu Agarwal","Sahitya Potluri","Siddharth Goyal","Amit Jhindal","Saksham Thakur","Yury Stuken","James Lyon","Kristina Toutanova","Fangxiaoyu Feng","Austin Wu","Ben Horn","Alek Wang","Alex Cullum","Gabe Taubman","Disha Shrivastava","Chongyang Shi","Hamish Tomlinson","Roma Patel","Tao Tu","Ada Maksutaj Oflazer","Francesco Pongetti","Mingyao Yang","Adrien Ali Taïga","Vincent Perot","Nuo Wang Pierse","Feng Han","Yoel Drori","Iñaki Iturrate","Ayan Chakrabarti","Legg Yeung","Dave Dopson","Yi-ting Chen","Apoorv Kulshreshtha","Tongfei Guo","Philip Pham","Tal Schuster","Junquan Chen","Alex Polozov","Jinwei Xing","Huanjie Zhou","Praneeth Kacham","Doron Kukliansky","Antoine Miech","Sergey Yaroshenko","Ed Chi","Sholto Douglas","Hongliang Fei","Mathieu Blondel","Preethi Myla","Lior Madmoni","Xing Wu","Daniel Keysers","Kristian Kjems","Isabela Albuquerque","Lijun Yu","Joel D'sa","Michelle Plantan","Vlad Ionescu","Jaume Sanchez Elias","Abhirut Gupta","Manish Reddy Vuyyuru","Fred Alcober","Tong Zhou","Kaiyang Ji","Florian Hartmann","Subha Puttagunta","Hugo Song","Ehsan Amid","Anca Stefanoiu","Andrew Lee","Paul Pucciarelli","Emma Wang","Amit Raul","Slav Petrov","Isaac Tian","Valentin Anklin","Nana Nti","Victor Gomes","Max Schumacher","Grace Vesom","Alex Panagopoulos","Konstantinos Bousmalis","Daniel Andor","Josh Jacob","Yuan Zhang","Bill Rosgen","Matija Kecman","Matthew Tung","Alexandra Belias","Noah Goodman","Paul Covington","Brian Wieder","Nikita Saxena","Elnaz Davoodi","Muhuan Huang","Sharath Maddineni","Vincent Roulet","Folawiyo Campbell-Ajala","Pier Giuseppe Sessa"," Xintian"," Wu","Guangda Lai","Paul Collins","Alex Haig","Vytenis Sakenas","Xiaowei Xu","Marissa Giustina","Laurent El Shafey","Pichi Charoenpanit","Shefali Garg","Joshua Ainslie","Boone Severson","Montse Gonzalez Arenas","Shreya Pathak","Sujee Rajayogam","Jie Feng","Michiel Bakker","Sheng Li","Nevan Wichers","Jamie Rogers","Xinyang Geng","Yeqing Li","Rolf Jagerman","Chao Jia","Nadav Olmert","David Sharon","Matthew Mauger","Sandeep Mariserla","Hongxu Ma","Megha Mohabey","Kyuyeun Kim","Alek Andreev","Scott Pollom","Juliette Love","Vihan Jain","Priyanka Agrawal","Yannick Schroecker","Alisa Fortin","Manfred Warmuth","Ji Liu","Andrew Leach","Irina Blok","Ganesh Poomal Girirajan","Roee Aharoni","Benigno Uria","Andrei Sozanschi","Dan Goldberg","Lucian Ionita","Marco Tulio Ribeiro","Martin Zlocha","Vighnesh Birodkar","Sami Lachgar","Liangzhe Yuan","Himadri Choudhury","Matt Ginsberg","Fei Zheng","Gregory Dibb","Emily Graves","Swachhand Lokhande","Gabriel Rasskin","George-Cristian Muraru","Corbin Quick","Sandeep Tata","Pierre Sermanet","Aditya Chawla","Itay Karo","Yan Wang","Susan Zhang","Orgad Keller","Anca Dragan","Guolong Su","Ian Chou","Xi Liu","Yiqing Tao","Shruthi Prabhakara","Marc Wilson","Ruibo Liu","Shibo Wang","Georgie Evans","David Du","Alfonso Castaño","Gautam Prasad","Mona El Mahdy","Sebastian Gerlach","Machel Reid","Jarrod Kahn","Amir Zait","Thanumalayan Sankaranarayana Pillai","Thatcher Ulrich","Guanyu Wang","Jan Wassenberg","Efrat Farkash","Kiran Yalasangi","Congchao Wang","Maria Bauza","Simon Bucher","Ting Liu","Jun Yan","Gary Leung","Vikas Sindhwani","Parker Barnes","Avi Singh","Ivan Jurin","Jichuan Chang","Niket Kumar Bhumihar","Sivan Eiger","Gui Citovsky","Ben Withbroe","Zhang Li","Siyang Xue","Niccolò Dal Santo","Georgi Stoyanov","Yves Raimond","Steven Zheng","Yilin Gao","Vít Listík","Sławek Kwasiborski","Rachel Saputro","Adnan Ozturel","Ganesh Mallya","Kushal Majmundar","Ross West","Paul Caron","Jinliang Wei","Lluis Castrejon","Sharad Vikram","Deepak Ramachandran","Nikhil Dhawan","Jiho Park","Sara Smoot","George van den Driessche","Yochai Blau","Chase Malik","Wei Liang","Roy Hirsch","Cicero Nogueira dos Santos","Eugene Weinstein","Aäron van den Oord","Sid Lall","Nicholas FitzGerald","Zixuan Jiang","Xuan Yang","Dale Webster","Ali Elqursh","Aedan Pope","Georges Rotival","David Raposo","Wanzheng Zhu","Jeff Dean","Sami Alabed","Dustin Tran","Arushi Gupta","Zach Gleicher","Jessica Austin","Edouard Rosseel","Megh Umekar","Dipanjan Das","Yinghao Sun","Kai Chen","Karolis Misiunas","Xiang Zhou","Yixian Di","Alyssa Loo","Josh Newlan","Bo Li","Vinay Ramasesh","Ying Xu","Alex Chen","Sudeep Gandhe","Radu Soricut","Nikita Gupta","Shuguang Hu","Seliem El-Sayed","Xavier Garcia","Idan Brusilovsky","Pu-Chin Chen","Andrew Bolt","Lu Huang","Alex Gurney","Zhiying Zhang","Alexander Pritzel","Jarek Wilkiewicz","Bryan Seybold","Bhargav Kanagal Shamanna","Felix Fischer","Josef Dean","Karan Gill","Ross Mcilroy","Abhishek Bhowmick","Jeremy Selier","Antoine Yang","Derek Cheng","Vladimir Magay","Jie Tan","Dhriti Varma","Christian Walder","Tomas Kocisky","Ryo Nakashima","Paul Natsev","Mike Kwong","Ionel Gog","Chiyuan Zhang","Sander Dieleman","Thomas Jimma","Andrey Ryabtsev","Siddhartha Brahma","David Steiner","Dayou Du","Ante Žužul","Mislav Žanić","Mukund Raghavachari","Willi Gierke","Zeyu Zheng","Dessie Petrova","Yann Dauphin","Yuchuan Liu","Ido Kessler","Steven Hand","Chris Duvarney","Seokhwan Kim","Hyo Lee","Léonard Hussenot","Jeffrey Hui","Josh Smith","Deepali Jain","Jiawei Xia","Gaurav Singh Tomar","Keyvan Amiri","Du Phan","Fabian Fuchs","Tobias Weyand","Nenad Tomasev","Alexandra Cordell","Xin Liu","Jonathan Mallinson","Pankaj Joshi","Andy Crawford","Arun Suggala","Steve Chien","Nick Fernando","Mariella Sanchez-Vargas","Duncan Williams","Phil Crone","Xiyang Luo","Igor Karpov","Jyn Shan","Terry Thurk","Robin Strudel","Paul Voigtlaender","Piyush Patil","Tim Dozat","Ali Khodaei","Sahil Singla","Piotr Ambroszczyk","Qiyin Wu","Yifan Chang","Brian Roark","Chaitra Hegde","Tianli Ding","Angelos Filos","Zhongru Wu","André Susano Pinto","Shuang Liu","Saarthak Khanna","Aditya Pandey","Siobhan Mcloughlin","Qiujia Li","Sam Haves","Allan Zhou","Elena Buchatskaya","Isabel Leal","Peter de Boursac","Nami Akazawa","Nina Anderson","Terry Chen","Krishna Somandepalli","Chen Liang","Sheela Goenka","Stephanie Winkler","Alexander Grushetsky","Yifan Ding","Jamie Smith","Fan Ye","Jordi Pont-Tuset","Eric Li","Ruichao Li","Tomer Golany","Dawid Wegner","Tao Jiang","Omer Barak","Yuan Shangguan","Eszter Vértes","Renee Wong","Jörg Bornschein","Alex Tudor","Michele Bevilacqua","Tom Schaul","Ankit Singh Rawat","Yang Zhao","Kyriakos Axiotis","Lei Meng","Cory McLean","Jonathan Lai","Jennifer Beattie","Nate Kushman","Yaxin Liu","Blair Kutzman","Fiona Lang","Jingchen Ye","Praneeth Netrapalli","Pushkar Mishra","Myriam Khan","Megha Goel","Rob Willoughby","David Tian","Honglei Zhuang","JD Chen","Zak Tsai","Tasos Kementsietsidis","Arjun Khare","James Keeling","Keyang Xu","Nathan Waters","Florent Altché","Ashok Popat","Bhavishya Mittal","David Saxton","Dalia El Badawy","Michael Mathieu","Zheng Zheng","Hao Zhou","Nishant Ranka","Richard Shin","Qingnan Duan","Tim Salimans","Ioana Mihailescu","Uri Shaham","Ming-Wei Chang","Yannis Assael","Nishanth Dikkala","Martin Izzard","Vincent Cohen-Addad","Cat Graves","Vlad Feinberg","Grace Chung","DJ Strouse","Danny Karmon","Sahand Sharifzadeh","Zoe Ashwood","Khiem Pham","Jon Blanton","Alex Vasiloff","Jarred Barber","Mark Geller","Aurick Zhou","Fedir Zubach","Tzu-Kuo Huang","Lei Zhang","Himanshu Gupta","Matt Young","Julia Proskurnia","Ronny Votel","Valentin Gabeur","Gabriel Barcik","Aditya Tripathi","Hongkun Yu","Geng Yan","Beer Changpinyo","Filip Pavetić","Amy Coyle","Yasuhisa Fujii","Jorge Gonzalez Mendez","Tianhao Zhou","Harish Rajamani","Blake Hechtman","Eddie Cao","Da-Cheng Juan","Yi-Xuan Tan","Valentin Dalibard","Yilun Du","Natalie Clay","Kaisheng Yao","Wenhao Jia","Dimple Vijaykumar","Yuxiang Zhou","Xinyi Bai","Wei-Chih Hung","Steven Pecht","Georgi Todorov","Nikhil Khadke","Pramod Gupta","Preethi Lahoti","Arnaud Autef","Karthik Duddu","James Lee-Thorp","Alexander Bykovsky","Tautvydas Misiunas","Sebastian Flennerhag","Santhosh Thangaraj","Jed McGiffin","Zack Nado","Markus Kunesch","Andreas Noever","Amir Hertz","Marco Liang","Victor Stone","Evan Palmer","Samira Daruki","Arijit Pramanik","Siim Põder","Austin Kyker","Mina Khan","Evgeny Sluzhaev","Marvin Ritter","Avraham Ruderman","Wenlei Zhou","Chirag Nagpal","Kiran Vodrahalli","George Necula","Paul Barham","Ellie Pavlick","Jay Hartford","Izhak Shafran","Long Zhao","Maciej Mikuła","Tom Eccles","Hidetoshi Shimokawa","Kanav Garg","Luke Vilnis","Hanwen Chen","Ilia Shumailov","Kuang-Huei Lee","Abdelrahman Abdelhamed","Meiyan Xie","Vered Cohen","Ester Hlavnova","Dan Malkin","Chawin Sitawarin","James Lottes","Pauline Coquinot","Tianli Yu","Sandeep Kumar","Jingwei Zhang","Aroma Mahendru","Zafarali Ahmed","James Martens","Tao Chen","Aviel Boag","Daiyi Peng","Coline Devin","Arseniy Klimovskiy","Mary Phuong","Danny Vainstein","Jin Xie","Bhuvana Ramabhadran","Nathan Howard","Xinxin Yu","Gitartha Goswami","Jingyu Cui","Sam Shleifer","Mario Pinto","Chih-Kuan Yeh","Ming-Hsuan Yang","Sara Javanmardi","Dan Ethier","Chace Lee","Jordi Orbay","Suyog Kotecha","Carla Bromberg","Pete Shaw","James Thornton","Adi Gerzi Rosenthal","Shane Gu","Matt Thomas","Ian Gemp","Aditya Ayyar","Asahi Ushio","Aarush Selvan","Joel Wee","Chenxi Liu","Maryam Majzoubi","Weiren Yu","Jake Abernethy","Tyler Liechty","Renke Pan","Hoang Nguyen"," Qiong"," Hu","Sarah Perrin","Abhinav Arora","Emily Pitler","Weiyi Wang","Kaushik Shivakumar","Flavien Prost","Ben Limonchik","Jing Wang","Yi Gao","Timothee Cour","Shyamal Buch","Huan Gui","Maria Ivanova","Philipp Neubeck","Kelvin Chan","Lucy Kim","Huizhong Chen","Naman Goyal","Da-Woon Chung","Lu Liu","Yao Su","Anastasia Petrushkina","Jiajun Shen","Armand Joulin","Yuanzhong Xu","Stein Xudong Lin","Yana Kulizhskaya","Ciprian Chelba","Shobha Vasudevan","Eli Collins","Vasilisa Bashlovkina","Tony Lu","Doug Fritz","Jongbin Park","Yanqi Zhou","Chen Su","Richard Tanburn","Mikhail Sushkov","Mitchelle Rasquinha","Jinning Li","Jennifer Prendki","Yiming Li","Pallavi LV","Shriya Sharma","Hen Fitoussi","Hui Huang","Andrew Dai","Phuong Dao","Mike Burrows","Henry Prior","Danfeng Qin","Golan Pundak","Lars Lowe Sjoesund","Art Khurshudov","Zhenkai Zhu","Albert Webson","Elizabeth Kemp","Tat Tan","Saurabh Agrawal","Susie Sargsyan","Liqun Cheng","Jim Stephan","Tom Kwiatkowski","David Reid","Arunkumar Byravan","Assaf Hurwitz Michaely","Nicolas Heess","Luowei Zhou","Sonam Goenka","Viral Carpenter","Anselm Levskaya","Bo Wang","Reed Roberts","Rémi Leblond","Sharat Chikkerur","Stav Ginzburg","Max Chang","Robert Riachi"," Chuqiao"," Xu","Zalán Borsos","Michael Pliskin","Julia Pawar","Morgane Lustman","Hannah Kirkwood","Ankit Anand","Aditi Chaudhary","Norbert Kalb","Kieran Milan","Sean Augenstein","Anna Goldie","Laurel Prince","Karthik Raman","Yanhua Sun","Vivian Xia","Aaron Cohen","Zhouyuan Huo","Josh Camp","Seher Ellis","Lukas Zilka","David Vilar Torres","Lisa Patel","Sho Arora","Betty Chan","Jonas Adler","Kareem Ayoub","Jacky Liang","Fayaz Jamil","Jiepu Jiang","Simon Baumgartner","Haitian Sun","Yael Karov","Yaroslav Akulov","Hui Zheng","Irene Cai","Claudio Fantacci","James Rubin","Alex Rav Acha","Mengchao Wang","Nina D'Souza","Rohit Sathyanarayana","Shengyang Dai","Simon Rowe","Andrey Simanovsky","Omer Goldman","Yuheng Kuang","Xiaoyue Pan","Andrew Rosenberg","Tania Rojas-Esponda","Praneet Dutta","Amy Zeng","Irina Jurenka","Greg Farquhar","Yamini Bansal","Shariq Iqbal","Becca Roelofs","Ga-Young Joung","Parker Beak","Changwan Ryu","Ryan Poplin","Yan Wu","Jean-Baptiste Alayrac","Senaka Buthpitiya","Olaf Ronneberger","Caleb Habtegebriel","Wei Li","Paul Cavallaro","Aurora Wei","Guy Bensky","Timo Denk","Harish Ganapathy","Jeff Stanway","Pratik Joshi","Francesco Bertolini","Jessica Lo","Olivia Ma","Zachary Charles","Geta Sampemane","Himanshu Sahni","Xu Chen","Harry Askham","David Gaddy","Peter Young","Jiewen Tan","Matan Eyal","Arthur Bražinskas","Li Zhong","Zhichun Wu","Mark Epstein","Kai Bailey","Andrew Hard","Kamyu Lee","Sasha Goldshtein","Alex Ruiz","Mohammed Badawi","Matthias Lochbrunner","JK Kearns","Ashley Brown","Fabio Pardo","Theophane Weber","Haichuan Yang","Pan-Pan Jiang","Berkin Akin","Zhao Fu","Marcus Wainwright","Chi Zou","Meenu Gaba","Pierre-Antoine Manzagol","Wendy Kan","Yang Song","Karina Zainullina","Rui Lin","Jeongwoo Ko","Salil Deshmukh","Apoorv Jindal","James Svensson","Divya Tyam","Heri Zhao","Christine Kaeser-Chen","Scott Baird","Pooya Moradi","Jamie Hall","Qiuchen Guo","Vincent Tsang","Bowen Liang","Fernando Pereira","Suhas Ganesh","Ivan Korotkov","Jakub Adamek","Sridhar Thiagarajan","Vinh Tran","Charles Chen","Chris Tar","Sanil Jain","Ishita Dasgupta","Taylan Bilal","David Reitter","Kai Zhao","Giulia Vezzani","Yasmin Gehman","Pulkit Mehta","Lauren Beltrone","Xerxes Dotiwalla","Sergio Guadarrama","Zaheer Abbas","Stefani Karp","Petko Georgiev","Chun-Sung Ferng","Marc Brockschmidt","Liqian Peng","Christoph Hirnschall","Vikas Verma","Yingying Bi","Ying Xiao","Avigail Dabush","Kelvin Xu","Phil Wallis","Randall Parker","Qifei Wang","Yang Xu","Ilkin Safarli","Dinesh Tewari","Yin Zhang","Seungyeon Kim","Andrea Gesmundo","Mackenzie Thomas","Sergey Levi","Ahmed Chowdhury","Kanishka Rao","Peter Garst","Sam Conway-Rahman","Helen Ran","Kay McKinney","Zhisheng Xiao","Wenhao Yu","Rohan Agrawal","Axel Stjerngren","Catalin Ionescu","Jingjing Chen","Vivek Sharma","Justin Chiu","Fei Liu","Ken Franko","Clayton Sanford","Xingyu Cai","Paul Michel","Sanjay Ganapathy","Jane Labanowski","Zachary Garrett","Ben Vargas","Sean Sun","Bryan Gale","Thomas Buschmann","Guillaume Desjardins","Nimesh Ghelani","Palak Jain","Mudit Verma","Chulayuth Asawaroengchai","Julian Eisenschlos","Jitendra Harlalka","Hideto Kazawa","Don Metzler","Joshua Howland","Ying Jian","Jake Ades","Viral Shah","Tynan Gangwani","Seungji Lee","Roman Ring","Steven M. Hernandez","Dean Reich","Amer Sinha","Ashutosh Sathe","Joe Kovac","Ashleah Gill","Ajay Kannan","Andrea D'olimpio","Martin Sevenich","Jay Whang","Been Kim","Khe Chai Sim","Jilin Chen","Jiageng Zhang","Shuba Lall","Yossi Matias","Bill Jia","Abe Friesen","Sara Nasso","Ashish Thapliyal","Bryan Perozzi","Ting Yu","Anna Shekhawat","Safeen Huda","Peter Grabowski","Eric Wang","Ashwin Sreevatsa","Hilal Dib","Mehadi Hassen","Parker Schuh","Vedrana Milutinovic","Chris Welty","Michael Quinn","Ali Shah","Bangju Wang","Gabe Barth-Maron","Justin Frye","Natalie Axelsson","Tao Zhu","Yukun Ma","Irene Giannoumis","Hanie Sedghi","Chang Ye","Yi Luan","Kevin Aydin","Bilva Chandra","Vivek Sampathkumar","Ronny Huang","Victor Lavrenko","Ahmed Eleryan","Zhi Hong","Steven Hansen","Sara Mc Carthy","Bidisha Samanta","Domagoj Ćevid","Xin Wang","Fangtao Li","Michael Voznesensky","Matt Hoffman","Andreas Terzis","Vikash Sehwag","Gil Fidel","Luheng He","Mu Cai","Yanzhang He","Alex Feng","Martin Nikoltchev","Samrat Phatale","Jason Chase","Rory Lawton","Ming Zhang","Tom Ouyang","Manuel Tragut","Mehdi Hafezi Manshadi","Arjun Narayanan","Jiaming Shen","Xu Gao","Tolga Bolukbasi","Nick Roy","Xin Li","Daniel Golovin","Liviu Panait","Zhen Qin","Guangxing Han","Thomas Anthony","Sneha Kudugunta","Viorica Patraucean","Aniket Ray","Xinyun Chen","Xiaochen Yang","Tanuj Bhatia","Pranav Talluri","Alex Morris","Andrija Ražnatović","Bethanie Brownfield","James An","Sheng Peng","Patrick Kane","Ce Zheng","Nico Duduta","Joshua Kessinger","James Noraky","Siqi Liu","Keran Rong","Petar Veličković","Keith Rush","Alex Goldin","Fanny Wei","Shiva Mohan Reddy Garlapati","Caroline Pantofaru","Okwan Kwon","Jianmo Ni","Eric Noland","Julia Di Trapani","Françoise Beaufays","Abhijit Guha Roy","Yinlam Chow","Aybuke Turker","Geoffrey Cideron","Lantao Mei","Jon Clark","Qingyun Dou","Matko Bošnjak","Ralph Leith","Yuqing Du","Amir Yazdanbakhsh","Milad Nasr","Chester Kwak","Suraj Satishkumar Sheth","Alex Kaskasoli","Ankesh Anand","Balaji Lakshminarayanan","Sammy Jerome","David Bieber","Chun-Te Chu","Alexandre Senges","Tianxiao Shen","Mukund Sridhar","Ndaba Ndebele","Benjamin Beyret","Shakir Mohamed","Mia Chen","Markus Freitag","Jiaxian Guo","Luyang Liu","Paul Roit","Heng Chen","Shen Yan","Tom Stone","JD Co-Reyes","Jeremy Cole","Salvatore Scellato","Shekoofeh Azizi","Hadi Hashemi","Alicia Jin","Anand Iyer","Marcella Valentine","András György","Arun Ahuja","Daniel Hernandez Diaz","Chen-Yu Lee","Nathan Clement","Weize Kong","Drew Garmon","Ishaan Watts","Kush Bhatia","Khyatti Gupta","Matt Miecnikowski","Hugo Vallet","Ankur Taly","Edward Loper","Saket Joshi","James Atwood","Jo Chick","Mark Collier","Fotis Iliopoulos","Ryan Trostle","Beliz Gunel","Ramiro Leal-Cavazos","Arnar Mar Hrafnkelsson","Michael Guzman","Xiaoen Ju","Andy Forbes","Jesse Emond","Kushal Chauhan","Ben Caine","Li Xiao","Wenjun Zeng","Alexandre Moufarek","Daniel Murphy","Maya Meng","Nitish Gupta","Felix Riedel","Anil Das","Elijah Lawal","Shashi Narayan","Tiberiu Sosea","James Swirhun","Linda Friso","Behnam Neyshabur","Jing Lu","Sertan Girgin","Michael Wunder","Edouard Yvinec","Aroonalok Pyne","Victor Carbune","Shruti Rijhwani","Yang Guo","Tulsee Doshi","Anton Briukhov","Max Bain","Ayal Hitron","Xuanhui Wang","Ashish Gupta","Ke Chen","Cosmo Du","Weiyang Zhang","Dhruv Shah","Arjun Akula","Max Dylla","Ashyana Kachra","Weicheng Kuo","Tingting Zou","Lily Wang","Luyao Xu","Jifan Zhu","Justin Snyder","Sachit Menon","Orhan Firat","Igor Mordatch","Yuan Yuan","Natalia Ponomareva","Rory Blevins","Lawrence Moore","Weijun Wang","Phil Chen","Martin Scholz","Artur Dwornik","Jason Lin","Sicheng Li","Diego Antognini","Te I","Xiaodan Song","Matt Miller","Uday Kalra","Adam Raveret","Oscar Akerlund","Felix Wu","Andrew Nystrom","Namrata Godbole","Tianqi Liu","Hannah DeBalsi","Jewel Zhao","Buhuang Liu","Avi Caciularu","Lauren Lax","Urvashi Khandelwal","Victoria Langston","Eric Bailey","Silvio Lattanzi","Yufei Wang","Neel Kovelamudi","Sneha Mondal","Guru Guruganesh","Nan Hua","Ofir Roval","Paweł Wesołowski","Rishikesh Ingale","Jonathan Halcrow","Tim Sohn","Christof Angermueller","Bahram Raad","Eli Stickgold","Eva Lu","Alec Kosik","Jing Xie","Timothy Lillicrap","Austin Huang","Lydia Lihui Zhang","Dominik Paulus","Clement Farabet","Alex Wertheim","Bing Wang","Rishabh Joshi","Chu-ling Ko","Yonghui Wu","Shubham Agrawal","Lily Lin","XiangHai Sheng","Peter Sung","Tyler Breland-King","Christina Butterfield","Swapnil Gawde","Sumeet Singh","Qiao Zhang","Raj Apte","Shilpa Shetty","Adrian Hutter","Tao Li","Elizabeth Salesky","Federico Lebron","Jonni Kanerva","Michela Paganini","Arthur Nguyen","Rohith Vallu","Jan-Thorsten Peter","Sarmishta Velury","David Kao","Jay Hoover","Anna Bortsova","Colton Bishop","Shoshana Jakobovits","Alessandro Agostini","Alekh Agarwal","Chang Liu","Charles Kwong","Sasan Tavakkol","Ioana Bica","Alex Greve","Anirudh GP","Jake Marcus","Le Hou","Tom Duerig","Rivka Moroshko","Dave Lacey","Andy Davis","Julien Amelot","Guohui Wang","Frank Kim","Theofilos Strinopoulos","Hui Wan","Charline Le Lan","Shankar Krishnan","Haotian Tang","Peter Humphreys","Junwen Bai","Idan Heimlich Shtacher","Diego Machado","Chenxi Pang","Ken Burke","Dangyi Liu","Renga Aravamudhan","Yue Song","Ed Hirst","Abhimanyu Singh","Brendan Jou","Liang Bai","Francesco Piccinno","Chuyuan Kelly Fu","Robin Alazard","Barak Meiri","Daniel Winter","Charlie Chen","Mingda Zhang","Jens Heitkaemper","John Lambert","Jinhyuk Lee","Alexander Frömmgen","Sergey Rogulenko","Pranav Nair","Paul Niemczyk","Anton Bulyenov","Bibo Xu","Hadar Shemtov","Morteza Zadimoghaddam","Serge Toropov","Mateo Wirth","Hanjun Dai","Sreenivas Gollapudi","Daniel Zheng","Alex Kurakin","Chansoo Lee","Kalesha Bullard","Nicolas Serrano","Ivana Balazevic","Yang Li","Johan Schalkwyk","Mark Murphy","Mingyang Zhang","Kevin Sequeira","Romina Datta","Nishant Agrawal","Charles Sutton","Nithya Attaluri","Mencher Chiang","Wael Farhan","Gregory Thornton","Kate Lin","Travis Choma","Hung Nguyen","Kingshuk Dasgupta","Dirk Robinson","Iulia Comşa","Michael Riley","Arjun Pillai","Basil Mustafa","Ben Golan","Amir Zandieh","Jean-Baptiste Lespiau","Billy Porter","David Ross","Sujeevan Rajayogam","Mohit Agarwal","Subhashini Venugopalan","Bobak Shahriari","Qiqi Yan","Hao Xu","Taylor Tobin","Pavel Dubov","Hongzhi Shi","Adrià Recasens","Anton Kovsharov","Sebastian Borgeaud","Lucio Dery","Shanthal Vasanth","Elena Gribovskaya","Linhai Qiu","Mahdis Mahdieh","Wojtek Skut","Elizabeth Nielsen","CJ Zheng","Adams Yu","Carrie Grimes Bostock","Shaleen Gupta","Aaron Archer","Chris Rawles","Elinor Davies","Alexey Svyatkovskiy","Tomy Tsai","Yoni Halpern","Christian Reisswig","Bartek Wydrowski","Bo Chang","Joan Puigcerver","Mor Hazan Taege","Jian Li","Eva Schnider","Xinjian Li","Dragos Dena","Yunhan Xu","Umesh Telang","Tianze Shi","Heiga Zen","Kyle Kastner","Yeongil Ko","Neesha Subramaniam","Aviral Kumar","Pete Blois","Zhuyun Dai","John Wieting","Yifeng Lu","Yoel Zeldes","Tian Xie","Anja Hauth","Alexandru Ţifrea","Yuqi Li","Sam El-Husseini","Dan Abolafia","Howard Zhou","Wen Ding","Sahra Ghalebikesabi","Carlos Guía","Andrii Maksai","Ágoston Weisz","Sercan Arik","Nick Sukhanov","Aga Świetlik","Xuhui Jia","Luo Yu","Weiyue Wang","Mark Brand","Dawn Bloxwich","Sean Kirmani","Zhe Chen","Alec Go","Pablo Sprechmann","Nithish Kannen","Alen Carin","Paramjit Sandhu","Isabel Edkins","Leslie Nooteboom","Jai Gupta","Loren Maggiore","Javad Azizi","Yael Pritch","Pengcheng Yin","Mansi Gupta","Danny Tarlow","Duncan Smith","Desi Ivanov","Mohammad Babaeizadeh","Ankita Goel","Satish Kambala","Grace Chu","Matej Kastelic","Michelle Liu","Hagen Soltau","Austin Stone","Shivani Agrawal","Min Kim","Kedar Soparkar","Srinivas Tadepalli","Oskar Bunyan","Rachel Soh","Arvind Kannan","DY Kim","Blake JianHang Chen","Afief Halumi","Sudeshna Roy","Yulong Wang","Olcan Sercinoglu","Gena Gibson","Sijal Bhatnagar","Motoki Sano","Daniel von Dincklage","Qingchun Ren","Blagoj Mitrevski","Mirek Olšák","Jennifer She","Carl Doersch"," Jilei"," Wang","Bingyuan Liu","Qijun Tan","Tamar Yakar","Tris Warkentin","Alex Ramirez","Carl Lebsack","Josh Dillon","Rajiv Mathews","Tom Cobley","Zelin Wu","Zhuoyuan Chen","Jon Simon","Swaroop Nath","Tara Sainath","Alexei Bendebury","Ryan Julian","Bharath Mankalale","Daria Ćurko","Paulo Zacchello","Adam R. Brown","Kiranbir Sodhia","Heidi Howard","Sergi Caelles","Abhinav Gupta","Gareth Evans","Anna Bulanova","Lesley Katzen","Roman Goldenberg","Anton Tsitsulin","Joe Stanton","Benoit Schillings","Vitaly Kovalev","Corey Fry","Rushin Shah","Kuo Lin","Shyam Upadhyay","Cheng Li","Soroush Radpour","Marcello Maggioni","Jing Xiong","Lukas Haas","Jenny Brennan","Aishwarya Kamath","Nikolay Savinov","Arsha Nagrani","Trevor Yacovone","Ryan Kappedal","Kostas Andriopoulos","Li Lao","YaGuang Li","Grigory Rozhdestvenskiy","Kazuma Hashimoto","Andrew Audibert","Sophia Austin","Daniel Rodriguez","Anian Ruoss","Garrett Honke","Deep Karkhanis","Xi Xiong","Qing Wei","James Huang","Zhaoqi Leng","Vittal Premachandran","Stan Bileschi","Georgios Evangelopoulos","Thomas Mensink","Jay Pavagadhi","Denis Teplyashin","Paul Chang","Linting Xue","Garrett Tanzer","Sally Goldman","Kaushal Patel","Shixin Li","Jeremy Wiesner","Ivy Zheng","Ian Stewart-Binks","Jie Han","Zhi Li","Liangchen Luo","Karel Lenc","Mario Lučić","Fuzhao Xue","Ryan Mullins","Alexey Guseynov","Chung-Ching Chang","Isaac Galatzer-Levy","Adam Zhang","Garrett Bingham","Grace Hu","Ale Hartman","Yue Ma","Jordan Griffith","Alex Irpan","Carey Radebaugh","Summer Yue","Lijie Fan","Victor Ungureanu","Christina Sorokin","Hannah Teufel","Peiran Li","Rohan Anil","Dimitris Paparas","Todd Wang","Chu-Cheng Lin","Hui Peng","Megan Shum","Goran Petrovic","Demetra Brady","Richard Nguyen","Klaus Macherey","Zhihao Li","Harman Singh","Madhavi Yenugula","Mariko Iinuma","Xinyi Chen","Kavya Kopparapu","Alexey Stern","Shachi Dave","Chandu Thekkath","Florence Perot","Anurag Kumar","Fangda Li","Yang Xiao","Matthew Bilotti","Mohammad Hossein Bateni","Isaac Noble","Lisa Lee","Amelio Vázquez-Reina","Julian Salazar","Xiaomeng Yang","Boyu Wang","Ela Gruzewska","Anand Rao","Sindhu Raghuram","Zheng Xu","Eyal Ben-David","Jieru Mei","Sid Dalmia","Zhaoyi Zhang","Yuchen Liu","Gagan Bansal","Helena Pankov","Steven Schwarcz","Andrea Burns","Christine Chan","Sumit Sanghai","Ricky Liang","Ethan Liang","Antoine He","Amy Stuart","Arun Narayanan","Yukun Zhu","Christian Frank","Bahar Fatemi","Amit Sabne","Oran Lang","Indro Bhattacharya","Shane Settle","Maria Wang","Brendan McMahan","Andrea Tacchetti","Livio Baldini Soares","Majid Hadian","Serkan Cabi","Timothy Chung","Nikita Putikhin","Gang Li","Jeremy Chen","Austin Tarango","Henryk Michalewski","Mehran Kazemi","Hussain Masoom","Hila Sheftel","Rakesh Shivanna","Archita Vadali","Ramona Comanescu","Doug Reid","Joss Moore","Arvind Neelakantan","Michaël Sander","Jonathan Herzig","Aviv Rosenberg","Mostafa Dehghani","JD Choi","Michael Fink","Reid Hayes","Eric Ge","Shitao Weng","Chia-Hua Ho","John Karro","Kalpesh Krishna","Lam Nguyen Thiet","Amy Skerry-Ryan","Daniel Eppens","Marco Andreetto","Navin Sarma","Silvano Bonacina","Burcu Karagol Ayan","Megha Nawhal","Zhihao Shan","Mike Dusenberry","Shantanu Thakoor","Sagar Gubbi","Duc Dung Nguyen","Reut Tsarfaty","Samuel Albanie","Jovana Mitrović","Meet Gandhi","Bo-Juen Chen","Alessandro Epasto","Georgi Stephanov","Ye Jin","Samuel Gehman","Aida Amini","Jack Weber","Feryal Behbahani","Shawn Xu","Miltos Allamanis","Xi Chen","Myle Ott","Claire Sha","Michal Jastrzebski","Hang Qi","David Greene","Xinyi Wu","Abodunrinwa Toki","Daniel Vlasic","Jane Shapiro","Ragha Kotikalapudi","Zhe Shen","Takaaki Saeki","Sirui Xie","Albin Cassirer","Shikhar Bharadwaj","Tatsuya Kiyono","Srinadh Bhojanapalli","Elan Rosenfeld","Sam Ritter","Jieming Mao","João Gabriel Oliveira","Zoltan Egyed","Bernd Bandemer","Emilio Parisotto","Keisuke Kinoshita","Juliette Pluto","Petros Maniatis","Steve Li","Yaohui Guo","Golnaz Ghiasi","Jean Tarbouriech","Srimon Chatterjee","Julie Jin"," Katrina"," Xu","Jennimaria Palomaki","Séb Arnold","Madhavi Sewak","Federico Piccinini","Mohit Sharma","Ben Albrecht","Sean Purser-haskell","Ashwin Vaswani","Chongyan Chen","Matheus Wisniewski","Qin Cao","John Aslanides","Nguyet Minh Phu","Maximilian Sieb","Lauren Agubuzu","Anne Zheng","Daniel Sohn","Marco Selvi","Anders Andreassen","Krishan Subudhi","Prem Eruvbetine","Oliver Woodman","Tomas Mery","Sebastian Krause","Xiaoqi Ren","Xiao Ma","Jincheng Luo","Dawn Chen","Wei Fan","Henry Griffiths","Christian Schuler","Alice Li","Shujian Zhang","Jean-Michel Sarr","Shixin Luo","Riccardo Patana","Matthew Watson","Dani Naboulsi","Michael Collins","Sailesh Sidhwani","Emiel Hoogeboom","Sharon Silver","Emily Caveness","Xiaokai Zhao","Mikel Rodriguez","Maxine Deines","Libin Bai","Patrick Griffin","Marco Tagliasacchi","Emily Xue","Spandana Raj Babbula","Bo Pang","Nan Ding","Gloria Shen","Elijah Peake","Remi Crocker","Shubha Srinivas Raghvendra","Danny Swisher","Woohyun Han","Richa Singh","Ling Wu","Vladimir Pchelin","Tsendsuren Munkhdalai","Dana Alon","Geoff Bacon","Efren Robles","Jannis Bulian","Melvin Johnson","George Powell","Felipe Tiengo Ferreira","Yaoyiran Li","Frederik Benzing","Mihajlo Velimirović","Hubert Soyer","William Kong"," Tony"," Nguyên","Zhen Yang","Jeremiah Liu","Joost van Amersfoort","Daniel Gillick","Baochen Sun","Nathalie Rauschmayr","Katie Zhang","Serena Zhan","Tao Zhou","Alexey Frolov","Chengrun Yang","Denis Vnukov","Louis Rouillard","Hongji Li","Amol Mandhane","Nova Fallen","Rajesh Venkataraman","Clara Huiyi Hu","Jennifer Brennan","Jenny Lee","Jerry Chang","Martin Sundermeyer","Zhufeng Pan","Rosemary Ke","Simon Tong","Alex Fabrikant","William Bono","Jindong Gu","Ryan Foley","Yiran Mao","Manolis Delakis","Dhruva Bhaswar","Roy Frostig","Nick Li","Avital Zipori","Cath Hope","Olga Kozlova","Swaroop Mishra","Josip Djolonga","Craig Schiff","Majd Al Merey","Eleftheria Briakou","Peter Morgan","Andy Wan","Avinatan Hassidim","RJ Skerry-Ryan","Kuntal Sengupta","Mary Jasarevic","Praveen Kallakuri","Paige Kunkle","Hannah Brennan","Tom Lieber","Hassan Mansoor","Julian Walker","Bing Zhang","Annie Xie","Goran Žužić","Adaeze Chukwuka","Alex Druinsky","Donghyun Cho","Rui Yao","Ferjad Naeem","Shiraz Butt","Eunyoung Kim","Zhipeng Jia","Mandy Jordan","Adam Lelkes","Mark Kurzeja","Sophie Wang","James Zhao","Andrew Over","Abhishek Chakladar","Marcel Prasetya","Neha Jha","Sriram Ganapathy","Yale Cong","Prakash Shroff","Carl Saroufim","Sobhan Miryoosefi","Mohamed Hammad","Tajwar Nasir","Weijuan Xi","Yang Gao","Young Maeng","Ben Hora","Chin-Yi Cheng","Parisa Haghani","Yoad Lewenberg","Caden Lu","Martin Matysiak","Naina Raisinghani","Huiyu Wang","Lexi Baugher","Rahul Sukthankar","Minh Giang","John Schultz","Noah Fiedel","Minmin Chen","Cheng-Chun Lee","Tapomay Dey","Hao Zheng","Shachi Paul","Celine Smith","Andy Ly","Yicheng Wang","Rishabh Bansal","Bartek Perz","Susanna Ricco","Stasha Blank","Vaishakh Keshava","Deepak Sharma","Marvin Chow","Kunal Lad","Komal Jalan","Simon Osindero","Craig Swanson","Jacob Scott","Anastasija Ilić","Xiaowei Li","Siddhartha Reddy Jonnalagadda","Afzal Shama Soudagar","Yan Xiong","Bat-Orgil Batsaikhan","Daniel Jarrett","Naveen Kumar","Maulik Shah","Matt Lawlor","Austin Waters","Mark Graham","Rhys May","Sabela Ramos","Sandra Lefdal","Zeynep Cankara","Nacho Cano","Brendan O'Donoghue","Jed Borovik","Frederick Liu","Jordan Grimstad","Mahmoud Alnahlawi","Katerina Tsihlas","Tom Hudson","Nikolai Grigorev","Yiling Jia","Terry Huang","Tobenna Peter Igwe","Sergei Lebedev","Xiaodan Tang","Igor Krivokon","Frankie Garcia","Melissa Tan","Eric Jia","Peter Stys","Shikhar Vashishth","Yu Liang","Balaji Venkatraman","Chenjie Gu","Anastasios Kementsietsidis","Chen Zhu","Junehyuk Jung","Yunfei Bai","Mohammad Javad Hosseini","Faruk Ahmed","Aditya Gupta","Xin Yuan","Shereen Ashraf","Shitij Nigam","Gautam Vasudevan","Pranjal Awasthi","Adi Mayrav Gilady","Zelda Mariet","Ramy Eskander","Haiguang Li","Hexiang Hu","Guillermo Garrido","Philippe Schlattner","George Zhang","Rohun Saxena","Petar Dević","Kritika Muralidharan","Ashwin Murthy","Yiqian Zhou","Min Choi","Arissa Wongpanich","Zhengdong Wang","Premal Shah","Yuntao Xu","Yiling Huang","Stephen Spencer","Alice Chen","James Cohan","Junjie Wang","Jonathan Tompson","Junru Wu","Ruba Haroun","Haiqiong Li","Blanca Huergo","Fan Yang","Tongxin Yin","James Wendt","Michael Bendersky","Rahma Chaabouni","Javier Snaider","Johan Ferret","Abhishek Jindal","Tara Thompson","Andrew Xue","Will Bishop","Shubham Milind Phal","Archit Sharma","Yunhsuan Sung","Prabakar Radhakrishnan","Mo Shomrat","Reeve Ingle","Roopali Vij","Justin Gilmer","Mihai Dorin Istin","Sam Sobell","Yang Lu","Emily Nottage","Dorsa Sadigh","Jeremiah Willcock","Tingnan Zhang","Steve Xu","Sasha Brown","Katherine Lee","Gary Wang","Yun Zhu","Yi Tay","Cheolmin Kim","Audrey Gutierrez","Abhanshu Sharma","Yongqin Xian","Sungyong Seo","Claire Cui","Elena Pochernina","Cip Baetu","Krzysztof Jastrzębski","Mimi Ly","Mohamed Elhawaty","Dan Suh","Eren Sezener","Pidong Wang","Nancy Yuen","George Tucker","Jiahao Cai","Zuguang Yang","Cindy Wang","Alex Muzio","Hai Qian","Jae Yoo","Derek Lockhart","Kevin R. McKee","Mandy Guo","Malika Mehrotra","Artur Mendonça","Sanket Vaibhav Mehta","Sherry Ben","Chetan Tekur","Jiaqi Mu","Muye Zhu","Victoria Krakovna","Hongrae Lee","AJ Maschinot","Sébastien Cevey","HyunJeong Choe","Aijun Bai","Hansa Srinivasan","Derek Gasaway","Nick Young","Patrick Siegler","Dan Holtmann-Rice","Vihari Piratla","Kate Baumli","Roey Yogev","Alex Hofer","Hado van Hasselt","Svetlana Grant","Yuri Chervonyi","David Silver","Andrew Hogue","Ayushi Agarwal","Kathie Wang","Preeti Singh","Four Flynn","Josh Lipschultz","Robert David","Lizzetth Bellot","Yao-Yuan Yang","Long Le","Filippo Graziano","Kate Olszewska","Kevin Hui","Akanksha Maurya","Nikos Parotsidis","Weijie Chen","Tayo Oguntebi","Joe Kelley","Anirudh Baddepudi","Johannes Mauerer","Gregory Shaw","Alex Siegman","Lin Yang","Shravya Shetty","Subhrajit Roy","Yunting Song","Wojciech Stokowiec","Ryan Burnell","Omkar Savant","Robert Busa-Fekete","Jin Miao","Samrat Ghosh","Liam MacDermed","Phillip Lippe","Mikhail Dektiarev","Zach Behrman","Fabian Mentzer","Kelvin Nguyen","Meng Wei","Siddharth Verma","Chris Knutsen","Sudeep Dasari","Zhipeng Yan","Petr Mitrichev","Xingyu Wang","Virat Shejwalkar","Jacob Austin","Srinivas Sunkara","Navneet Potti","Yan Virin","Christian Wright","Gaël Liu","Oriana Riva","Etienne Pot","Greg Kochanski","Quoc Le","Gargi Balasubramaniam","Arka Dhar","Yuguo Liao","Adam Bloniarz","Divyansh Shukla","Elizabeth Cole","Jong Lee","Sheng Zhang","Sushant Kafle","Siddharth Vashishtha","Parsa Mahmoudieh","Grace Chen","Raphael Hoffmann","Pranesh Srinivasan","Agustin Dal Lago","Yoav Ben Shalom","Zi Wang","Michael Elabd","Anuj Sharma","Junhyuk Oh","Suraj Kothawade","Maigo Le","Marianne Monteiro","Shentao Yang","Kaiz Alarakyia","Robert Geirhos","Diana Mincu","Håvard Garnes","Hayato Kobayashi","Soroosh Mariooryad","Kacper Krasowiak"," Zhixin"," Lai","Shibl Mourad","Mingqiu Wang","Fan Bu","Ophir Aharoni","Guanjie Chen","Abhimanyu Goyal","Vadim Zubov","Ankur Bapna","Elahe Dabir","Nisarg Kothari","Kay Lamerigts","Nicola De Cao","Jeremy Shar","Christopher Yew","Nitish Kulkarni","Dre Mahaarachchi","Mandar Joshi","Zhenhai Zhu","Jared Lichtarge","Yichao Zhou","Hannah Muckenhirn","Vittorio Selo","Oriol Vinyals","Peter Chen","Anthony Brohan","Vaibhav Mehta","Sarah Cogan","Ruth Wang","Ty Geri","Wei-Jen Ko","Wei Chen","Fabio Viola","Keshav Shivam","Lisa Wang","Madeleine Clare Elish","Raluca Ada Popa","Sébastien Pereira","Jianqiao Liu","Raphael Koster","Donnie Kim","Gufeng Zhang","Sayna Ebrahimi","Partha Talukdar","Yanyan Zheng","Petra Poklukar","Ales Mikhalap","Dale Johnson","Anitha Vijayakumar","Mark Omernick","Matt Dibb","Ayush Dubey","Qiong Hu","Apurv Suman","Vaibhav Aggarwal","Ilya Kornakov","Fei Xia","Wing Lowe","Alexey Kolganov","Ted Xiao","Vitaly Nikolaev","Steven Hemingray","Bonnie Li","Joana Iljazi","Mikołaj Rybiński","Ballie Sandhu","Peggy Lu","Thang Luong","Rodolphe Jenatton","Vineetha Govindaraj"," Hui"," Li","Gabriel Dulac-Arnold","Wonpyo Park","Henry Wang","Abhinit Modi","Jean Pouget-Abadie","Kristina Greller","Rahul Gupta","Robert Berry","Prajit Ramachandran","Jinyu Xie","Liam McCafferty","Jianling Wang","Kilol Gupta","Hyeontaek Lim","Blaž Bratanič","Andy Brock","Ilia Akolzin","Jim Sproch","Dan Karliner","Duhyeon Kim","Adrian Goedeckemeyer","Noam Shazeer","Cordelia Schmid","Daniele Calandriello","Parul Bhatia","Krzysztof Choromanski","Ceslee Montgomery","Dheeru Dua","Ana Ramalho","Helen King","Yue Gao","Lynn Nguyen","David Lindner","Divya Pitta","Oleaser Johnson","Khalid Salama","Diego Ardila","Michael Han","Erin Farnese","Seth Odoom","Ziyue Wang","Xiangzhuo Ding","Norman Rink","Ray Smith","Harshal Tushar Lehri","Eden Cohen","Neera Vats","Tong He","Parthasarathy Gopavarapu","Adam Paszke","Miteyan Patel","Wouter Van Gansbeke","Lucia Loher","Luis Castro","Maria Voitovich","Tamara von Glehn","Nelson George","Simon Niklaus","Zach Eaton-Rosen","Nemanja Rakićević","Erik Jue","Sagi Perel","Carrie Zhang","Yuval Bahat","Angéline Pouget","Zhi Xing","Fantine Huot","Ashish Shenoy","Taylor Bos","Vincent Coriou","Bryan Richter","Natasha Noy","Yaqing Wang","Santiago Ontanon","Siyang Qin","Gleb Makarchuk","Demis Hassabis","Zhuowan Li","Mandar Sharma","Kumaran Venkatesan","Iurii Kemaev","Roxanne Daniel","Shiyu Huang","Saloni Shah","Octavio Ponce"," Warren"," Chen","Manaal Faruqui","Jialin Wu","Slavica Andačić","Szabolcs Payrits","Daniel McDuff","Tom Hume","Yuan Cao","MH Tessler","Qingze Wang","Yinan Wang","Ivor Rendulic","Eirikur Agustsson","Matthew Johnson","Tanya Lando","Andrew Howard","Sri Gayatri Sundara Padmanabhan","Mayank Daswani","Andrea Banino","Michael Kilgore","Jonathan Heek","Ziwei Ji","Alvaro Caceres","Conglong Li","Nora Kassner","Alexey Vlaskin","Zeyu Liu","Alex Grills","Yanhan Hou","Roykrong Sukkerd","Gowoon Cheon","Nishita Shetty","Larisa Markeeva","Piotr Stanczyk","Tejas Iyer","Yuan Gong","Shawn Gao","Keerthana Gopalakrishnan","Tim Blyth","Malcolm Reynolds","Avishkar Bhoopchand","Misha Bilenko","Dero Gharibian","Vicky Zayats","Aleksandra Faust","Abhinav Singh","Min Ma","Hongyang Jiao","Sudheendra Vijayanarasimhan","Lora Aroyo","Vikas Yadav","Sarah Chakera","Ashwin Kakarla","Vilobh Meshram","Karol Gregor","Gabriela Botea","Evan Senter","Dawei Jia","Geza Kovacs","Neha Sharma","Sebastien Baur","Kai Kang","Yifan He","Lin Zhuo","Marija Kostelac","Itay Laish","Songyou Peng","Louis O'Bryan","Daniel Kasenberg","Girish Ramchandra Rao","Edouard Leurent","Biao Zhang","Sage Stevens","Ana Salazar","Ye Zhang","Ivan Lobov","Jake Walker","Allen Porter","Morgan Redshaw","Han Ke","Abhishek Rao","Alex Lee","Hoi Lam","Michael Moffitt","Jaeyoun Kim","Siyuan Qiao","Terry Koo","Robert Dadashi","Xinying Song","Mukund Sundararajan","Peng Xu","Chizu Kawamoto","Yan Zhong","Clara Barbu","Apoorv Reddy","Mauro Verzetti","Leon Li","George Papamakarios","Hanna Klimczak-Plucińska","Mary Cassin","Koray Kavukcuoglu","Rigel Swavely","Alain Vaucher","Jeffrey Zhao","Ross Hemsley","Michael Tschannen","Heming Ge","Gaurav Menghani","Yang Yu","Natalie Ha","Wei He","Xiao Wu","Maggie Song","Rachel Sterneck","Stefan Zinke","Dan A. Calian","Annie Marsden","Alejandro Cruzado Ruiz","Matteo Hessel","Almog Gueta","Benjamin Lee","Brian Farris","Manish Gupta","Yunjie Li","Mohammad Saleh","Vedant Misra","Kefan Xiao","Piermaria Mendolicchio","Gavin Buttimore","Varvara Krayvanova","Nigamaa Nayakanti","Matthew Wiethoff","Yash Pande","Azalia Mirhoseini","Ni Lao","Jasmine Liu","Yiqing Hua","Angie Chen","Yury Malkov","Dmitry Kalashnikov","Shubham Gupta","Kartik Audhkhasi","Yuexiang Zhai","Sudhindra Kopalle","Prateek Jain","Eran Ofek","Clemens Meyer","Khuslen Baatarsukh","Hana Strejček","Jun Qian","James Freedman","Ricardo Figueira","Michal Sokolik","Olivier Bachem","Raymond Lin","Dia Kharrat","Chris Hidey","Pingmei Xu","Dennis Duan","Yin Li","Muge Ersoy","Richard Everett","Kevin Cen","Rebeca Santamaria-Fernandez","Amir Taubenfeld","Ian Mackinnon","Linda Deng","Polina Zablotskaia","Shashank Viswanadha","Shivanker Goel","Damion Yates","Yunxiao Deng","Peter Choy","Mingqing Chen","Abhishek Sinha","Alex Mossin","Yiming Wang","Arthur Szlam","Susan Hao","Paul Kishan Rubenstein","Metin Toksoz-Exley","Miranda Aperghis","Yin Zhong","Junwhan Ahn","Michael Isard","Olivier Lacombe","Florian Luisier","Chrysovalantis Anastasiou","Yogesh Kalley","Utsav Prabhu","Emma Dunleavy","Shaan Bijwadia","Justin Mao-Jones","Kelly Chen","Rama Pasumarthi","Emily Wood","Adil Dostmohamed","Nate Hurley","Jiri Simsa","Alicia Parrish","Mantas Pajarskas","Matt Harvey","Ondrej Skopek","Yony Kochinski","Javier Rey","Verena Rieser","Denny Zhou","Sun Jae Lee","Trilok Acharya","Guowang Li","Joe Jiang","Xiaofan Zhang","Bryant Gipson","Ethan Mahintorabi","Marco Gelmi","Nima Khajehnouri","Angel Yeh","Kayi Lee","Loic Matthey","Leslie Baker","Trang Pham","Han Fu","Alex Pak","Prakhar Gupta","Cristina Vasconcelos","Adam Sadovsky","Brian Walker","Sissie Hsiao","Patrik Zochbauer","Andreea Marzoca","Noam Velan","Junhao Zeng","Gilles Baechler","Danny Driess","Divya Jain","Yanping Huang","Lizzie Tao","John Maggs","Nir Levine","Jon Schneider","Erika Gemzer","Samuel Petit","Shan Han","Zach Fisher","Dustin Zelle","Courtney Biles","Eugene Ie","Asya Fadeeva","Casper Liu","Juliana Vicente Franco","Adrian Collister","Hao Zhang","Renshen Wang","Ruizhe Zhao","Leandro Kieliger","Kurt Shuster","Rui Zhu","Boqing Gong","Lawrence Chan","Ruoxi Sun","Sujoy Basu","Roland Zimmermann","Jamie Hayes","Abhishek Bapna","Jasper Snoek","Weel Yang","Puranjay Datta","Jad Al Abdallah","Kevin Kilgour","Lu Li","SQ Mah","Yennie Jun","Morgane Rivière","Abhijit Karmarkar","Tammo Spalink","Tao Huang","Lucas Gonzalez","Duc-Hieu Tran","Averi Nowak","John Palowitch","Martin Chadwick","Ellie Talius","Harsh Mehta","Thibault Sellam","Philipp Fränken","Massimo Nicosia","Kyle He","Aditya Kini","David Amos","Sugato Basu","Harrison Jobe","Eleni Shaw","Qiantong Xu","Colin Evans","Daisuke Ikeda","Chaochao Yan","Larry Jin","Lun Wang","Sachin Yadav","Ilia Labzovsky","Ramesh Sampath","Ada Ma","Candice Schumann","Aditya Siddhant","Rohin Shah","John Youssef","Rishabh Agarwal","Natalie Dabney","Alessio Tonioni","Moran Ambar","Jing Li","Isabelle Guyon","Benny Li","David Soergel","Boya Fang","Georgi Karadzhov","Cristian Udrescu","Trieu Trinh","Vikas Raunak","Seb Noury","Dee Guo","Sonal Gupta","Mara Finkelstein","Denis Petek","Lihao Liang","Greg Billock","Pei Sun","David Wood","Yiwen Song","Xiaobin Yu","Tatiana Matejovicova","Regev Cohen","Kalyan Andra","David D'Ambrosio","Zhiwei Deng","Vincent Nallatamby","Ebrahim Songhori","Rumen Dangovski","Andrew Lampinen","Pankil Botadra","Adam Hillier","Jiawei Cao","Nagabhushan Baddi","Adhi Kuncoro","Toshihiro Yoshino","Ankit Bhagatwala","Marcáurelio Ranzato","Rylan Schaeffer","Tianlin Liu","Shuai Ye","Obaid Sarvana","John Nham","Chenkai Kuang","Isabel Gao","Jinoo Baek","Shubham Mittal","Ayzaan Wahid","Anita Gergely","Bin Ni","Josh Feldman","Carrie Muir","Pascal Lamblin","Wolfgang Macherey","Ethan Dyer","Logan Kilpatrick","Víctor Campos","Mukul Bhutani","Stanislav Fort","Yanif Ahmad","Aliaksei Severyn","Kleopatra Chatziprimou","Oleksandr Ferludin","Mason Dimarco","Aditya Kusupati","Joe Heyward","Dan Bahir","Kevin Villela","Katie Millican","Dror Marcus","Sanaz Bahargam","Caglar Unlu","Nicholas Roth","Zichuan Wei","Siddharth Gopal","Deepanway Ghoshal","Edward Lee","Sharon Lin","Jennie Lees","Dayeong Lee","Anahita Hosseini","Connie Fan","Seth Neel","Marcus Wu","Yasemin Altun","Honglong Cai","Enrique Piqueras","Josh Woodward","Alessandro Bissacco","Salem Haykal","Mahyar Bordbar","Prasha Sundaram","Sarah Hodkinson","Daniel Toyama","George Polovets","Austin Myers","Anu Sinha","Tomer Levinboim","Kashyap Krishnakumar","Rachita Chhaparia","Tatiana Sholokhova","Nitesh Bharadwaj Gundavarapu","Ganesh Jawahar","Haroon Qureshi","Jieru Hu","Nikola Momchev","Matthew Rahtz","Renjie Wu","Aishwarya P S","Kedar Dhamdhere","Meiqi Guo","Umang Gupta","Ali Eslami","Mariano Schain","Michiel Blokzijl","David Welling","Dave Orr","Levent Bolelli","Nicolas Perez-Nieves","Mikhail Sirotenko","Aman Prasad","Arjun Kar","Borja De Balle Pigem","Tayfun Terzi","Gellért Weisz","Dipankar Ghosh","Aditi Mavalankar","Dhruv Madeka","Kaspar Daugaard","Hartwig Adam","Viraj Shah","Dana Berman","Maggie Tran","Steven Baker","Ewa Andrejczuk","Grishma Chole","Ganna Raboshchuk","Mahdi Mirzazadeh","Thais Kagohara","Shimu Wu","Christian Schallhart","Bernett Orlando","Chen Wang","Alban Rrustemi","Hao Xiong","Hao Liu","Arpi Vezer","Nolan Ramsden","Shuo-yiin Chang","Sidharth Mudgal","Yan Li","Nino Vieillard","Yedid Hoshen","Farooq Ahmad","Ambrose Slone","Amy Hua","Natan Potikha","Mirko Rossini","Jon Stritar","Sushant Prakash","Zifeng Wang","Xuanyi Dong","Alireza Nazari","Efrat Nehoran","Kaan Tekelioglu","Yinxiao Li","Kartikeya Badola","Tom Funkhouser","Yuanzhen Li","Varun Yerram","Ramya Ganeshan","Daniel Formoso","Karol Langner","Tian Shi","Huijian Li","Yumeya Yamamori","Amayika Panda","Alaa Saade","Angelo Scorza Scarpati","Chris Breaux","CJ Carey","Zongwei Zhou","Cho-Jui Hsieh","Sophie Bridgers","Alena Butryna","Nishesh Gupta","Vaibhav Tulsyan","Sanghyun Woo","Evgenii Eltyshev","Will Grathwohl","Chanel Parks","Seth Benjamin","Rina Panigrahy","Shenil Dodhia","Daniel De Freitas","Chris Sauer","Will Song","Ferran Alet","Jackson Tolins","Cosmin Paduraru","Xingyi Zhou","Brian Albert","Zizhao Zhang","Lei Shu","Mudit Bansal","Sarah Nguyen","Amir Globerson","Owen Xiao","James Manyika","Tom Hennigan","Rong Rong","Josip Matak","Anton Bakalov","Ankur Sharma","Danila Sinopalnikov","Andrew Pierson","Stephen Roller","Geoff Brown","Mingcen Gao","Toshiyuki Fukuzawa","Amin Ghafouri","Kenny Vassigh","Iain Barr","Zhicheng Wang","Anna Korsun","Rajesh Jayaram","Lijie Ren","Tim Zaman","Samira Khan","Yana Lunts","Dan Deutsch","Dave Uthus","Nitzan Katz","Masha Samsikova","Amr Khalifa","Nikhil Sethi","Jiao Sun","Luming Tang","Uri Alon","Xianghong Luo","Dian Yu","Abhishek Nayyar","Bryce Petrini","Will Truong","Vincent Hellendoorn","Nikolai Chinaev","Chris Alberti","Wei Wang","Jingcao Hu","Vahab Mirrokni","Ananth Balashankar","Avia Aharon","Aahil Mehta","Ahmet Iscen","Joseph Kready","Lucas Manning","Anhad Mohananey","Yuankai Chen","Anshuman Tripathi","Allen Wu","Igor Petrovski","Dawsen Hwang","Martin Baeuml","Shreyas Chandrakaladharan","Yuan Liu","Rey Coaguila","Maxwell Chen","Sally Ma","Pouya Tafti","Susheel Tatineni","Terry Spitz","Jiayu Ye","Paul Vicol","Mihaela Rosca","Adrià Puigdomènech","Zohar Yahav","Sanjay Ghemawat","Hanzhao Lin","Phoebe Kirk","Zaid Nabulsi","Sergey Brin","Bernd Bohnet","Ken Caluwaerts","Aditya Srikanth Veerubhotla","Dan Zheng","Zihang Dai","Petre Petrov","Yichong Xu","Ramin Mehran","Zhuo Xu","Luisa Zintgraf","Jiho Choi","Spurthi Amba Hombaiah","Romal Thoppilan","Sashank Reddi","Lukasz Lew","Li Li","Kellie Webster","KP Sawhney","Lampros Lamprou","Siamak Shakeri","Mayank Lunayach","Jianmin Chen","Sumit Bagri","Alex Salcianu","Ying Chen","Yani Donchev","Charlotte Magister","Signe Nørly","Vitor Rodrigues","Tomas Izo","Hila Noga","Joe Zou","Thomas Köppe","Wenxuan Zhou","Kenton Lee","Xiangzhu Long","Danielle Eisenbud","Anthony Chen","Connor Schenck","Chi Ming To","Peilin Zhong","Emanuel Taropa","Minh Truong","Omer Levy","Danilo Martins","Zhiyuan Zhang","Christopher Semturs","Kelvin Zhang","Alex Yakubovich","Pol Moreno","Lara McConnaughey","Di Lu","Sam Redmond","Lotte Weerts","Yonatan Bitton","Tiziana Refice","Nicolas Lacasse","Arthur Conmy","Corentin Tallec","Julian Odell","Hannah Forbes-Pollard","Arkadiusz Socala","Jonathan Hoech","Pushmeet Kohli","Alanna Walton","Rui Wang","Mikita Sazanovich","Kexin Zhu","Andrei Kapishnikov","Rich Galt","Matthew Denton","Ben Murdoch","Caitlin Sikora","Kareem Mohamed","Wei Wei","Uri First","Tim McConnell","Luis C. Cobo","James Qin","Thi Avrahami","Daniel Balle","Yu Watanabe","Annie Louis","Adam Kraft","Setareh Ariafar","Yiming Gu","Eugénie Rives","Charles Yoon","Andrei Rusu","James Cobon-Kerr","Chris Hahn","Jiaming Luo"," Yuvein"," Zhu","Niharika Ahuja","Rodrigo Benenson","Raphaël Lopez Kaufman","Honglin Yu","Lloyd Hightower","Junlin Zhang","Darren Ni","Lisa Anne Hendricks","Gabby Wang","Gal Yona","Lalit Jain","Pablo Barrio","Surya Bhupatiraju","Siva Velusamy","Allan Dafoe","Sebastian Riedel","Tara Thomas","Zhe Yuan","Mathias Bellaiche","Sheena Panthaplackel","Klemen Kloboves","Sarthak Jauhari","Canfer Akbulut","Todor Davchev","Evgeny Gladchenko","David Madras","Aleksandr Chuklin","Tyrone Hill","Quan Yuan","Mukundan Madhavan","Luke Leonhard","Dylan Scandinaro","Qihang Chen","Ning Niu","Arthur Douillard","Bogdan Damoc","Yasumasa Onoe","Fabian Pedregosa","Fred Bertsch","Chas Leichner","Joseph Pagadora","Jonathan Malmaud","Sameera Ponda","Andy Twigg","Oleksii Duzhyi","Jingwei Shen","Miaosen Wang","Roopal Garg","Jing Chen","Utku Evci","Jonathan Lee","Leon Liu","Koji Kojima","Masa Yamaguchi","Arunkumar Rajendran","AJ Piergiovanni","Vinodh Kumar Rajendran","Marco Fornoni","Gabriel Ibagon","Harry Ragan","Sadh MNM Khan","John Blitzer","Andrew Bunner","Guan Sun","Takahiro Kosakai","Scott Lundberg","Ndidi Elue","Kelvin Guu","SK Park","Jane Park","Arunachalam Narayanaswamy","Chengda Wu","Jayaram Mudigonda","Trevor Cohn","Hairong Mu","Ravi Kumar","Laura Graesser","Yichi Zhang","Richard Killam","Vincent Zhuang","Mai Giménez","Wael Al Jishi","Ruy Ley-Wild","Alex Zhai","Kazuki Osawa","Diego Cedillo","Jialu Liu","Mayank Upadhyay","Marcin Sieniek","Roshan Sharma","Tom Paine","Anelia Angelova","Sravanti Addepalli","Carolina Parada","Kingshuk Majumder","Avery Lamp","Sanjiv Kumar","Xiang Deng","Artiom Myaskovsky","Tea Sabolić","Jeffrey Dudek","Sarah York","Félix de Chaumont Quitry","Jiazhong Nie","Dee Cattle","Alok Gunjan","Bilal Piot","Waleed Khawaja","Seojin Bang","Simon Wang","Siavash Khodadadeh","Raghavender R","Praynaa Rawlani","Richard Powell","Kevin Lee","Johannes Griesser","GS Oh","Cesar Magalhaes","Yujia Li","Simon Tokumine","Hadas Natalie Vogel","Dennis Hsu","Arturo BC","Disha Jindal","Matan Cohen","Zi Yang","Junwei Yuan","Dario de Cesare","Tony Bruguier","Jun Xu","Monica Roy","Alon Jacovi","Dan Belov","Rahul Arya","Phoenix Meadowlark","Shlomi Cohen-Ganor","Wenting Ye","Patrick Morris-Suzuki","Praseem Banzal","Gan Song","Pranavaraj Ponnuramu","Fred Zhang","George Scrivener","Salah Zaiem","Alif Raditya Rochman","Kehang Han","Badih Ghazi","Kate Lee","Shahar Drath","Daniel Suo","Antonious Girgis","Pradeep Shenoy","Duy Nguyen","Douglas Eck","Somit Gupta","Le Yan","Joao Carreira","Anmol Gulati","Ruoxin Sang","Daniil Mirylenka","Emma Cooney","Edward Chou","Mingyang Ling","Cindy Fan","Ben Coleman","Guilherme Tubone","Ravin Kumar","Jason Baldridge","Felix Hernandez-Campos","Angeliki Lazaridou","James Besley","Itay Yona","Neslihan Bulut","Quentin Wellens","AJ Pierigiovanni","Jasmine George","Richard Green","Pu Han","Connie Tao","Geoff Clark","Chong You","Abbas Abdolmaleki","Justin Fu","Tongzhou Chen","Ashwin Chaugule","Angad Chandorkar","Altaf Rahman","Will Thompson","Penporn Koanantakool","Mike Bernico","Jie Ren","Andrey Vlasov","Sergei Vassilvitskii","Maciej Kula","Yizhong Liang","Dahun Kim","Yangsibo Huang","Chengxi Ye","Dmitry Lepikhin","Wesley Helmholz"],"pdf_url":"","comment":"72 pages, 17 figures"},{"id":"http://arxiv.org/abs/2510.16442v2","updated":"2025-12-19T14:22:03Z","published":"2025-10-18T10:34:05Z","title":"EDVD-LLaMA: Explainable Deepfake Video Detection via Multimodal Large Language Model Reasoning","summary":"The rapid development of deepfake video technology has not only facilitated artistic creation but also made it easier to spread misinformation. Traditional deepfake video detection (DVD) methods face issues such as a lack of transparency in their principles and insufficient generalization capabilities to cope with evolving forgery techniques. This highlights an urgent need for detectors that can identify forged content and provide verifiable reasoning explanations. This paper proposes the explainable deepfake video detection (EDVD) task and designs the EDVD-LLaMA multimodal, a large language model (MLLM) reasoning framework, which provides traceable reasoning processes alongside accurate detection results and trustworthy explanations. Our approach first incorporates a Spatio-Temporal Subtle Information Tokenization (ST-SIT) to extract and fuse global and local cross-frame deepfake features, providing rich spatio-temporal semantic information input for MLLM reasoning. Second, we construct a Fine-grained Multimodal Chain-of-Thought (Fg-MCoT) mechanism, which introduces facial feature data as hard constraints during the reasoning process to achieve pixel-level spatio-temporal video localization, suppress hallucinated outputs, and enhance the reliability of the chain of thought. In addition, we build an Explainable Reasoning FF++ dataset (ER-FF++set), leveraging structured data to annotate videos and ensure quality control, thereby supporting dual supervision for reasoning and detection. Extensive experiments demonstrate that EDVD-LLaMA achieves outstanding performance and robustness in terms of detection accuracy, explainability, and its ability to handle cross-forgery methods and cross-dataset scenarios. Compared to previous DVD methods, it provides a more explainable and superior solution. The project page is available at: https://11ouo1.github.io/edvd-llama/.","authors":["Haoran Sun","Chen Cai","Huiping Zhuang","Kong Aik Lee","Lap-Pui Chau","Yi Wang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17607v1","updated":"2025-12-19T14:12:17Z","published":"2025-12-19T14:12:17Z","title":"More Consistent Accuracy PINN via Alternating Easy-Hard Training","summary":"Physics-informed neural networks (PINNs) have recently emerged as a prominent paradigm for solving partial differential equations (PDEs), yet their training strategies remain underexplored. While hard prioritization methods inspired by finite element methods are widely adopted, recent research suggests that easy prioritization can also be effective. Nevertheless, we find that both approaches exhibit notable trade-offs and inconsistent performance across PDE types. To address this issue, we develop a hybrid strategy that combines the strengths of hard and easy prioritization through an alternating training algorithm. On PDEs with steep gradients, nonlinearity, and high dimensionality, the proposed method achieves consistently high accuracy, with relative L2 errors mostly in the range of O(10^-5) to O(10^-6), significantly surpassing baseline methods. Moreover, it offers greater reliability across diverse problems, whereas compared approaches often suffer from variable accuracy depending on the PDE. This work provides new insights into designing hybrid training strategies to enhance the performance and robustness of PINNs.","authors":["Zhaoqian Gao","Min Yanga"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17605v1","updated":"2025-12-19T14:10:36Z","published":"2025-12-19T14:10:36Z","title":"MGRegBench: A Novel Benchmark Dataset with Anatomical Landmarks for Mammography Image Registration","summary":"Robust mammography registration is essential for clinical applications like tracking disease progression and monitoring longitudinal changes in breast tissue. However, progress has been limited by the absence of public datasets and standardized benchmarks. Existing studies are often not directly comparable, as they use private data and inconsistent evaluation frameworks. To address this, we present MGRegBench, a public benchmark dataset for mammogram registration. It comprises over 5,000 image pairs, with 100 containing manual anatomical landmarks and segmentation masks for rigorous evaluation. This makes MGRegBench one of the largest public 2D registration datasets with manual annotations. Using this resource, we benchmarked diverse registration methods including classical (ANTs), learning-based (VoxelMorph, TransMorph), implicit neural representation (IDIR), a classic mammography-specific approach, and a recent state-of-the-art deep learning method MammoRegNet. The implementations were adapted to this modality from the authors' implementations or re-implemented from scratch. Our contributions are: (1) the first public dataset of this scale with manual landmarks and masks for mammography registration; (2) the first like-for-like comparison of diverse methods on this modality; and (3) an extensive analysis of deep learning-based registration. We publicly release our code and data to establish a foundational resource for fair comparisons and catalyze future research. The source code and data are at https://github.com/KourtKardash/MGRegBench.","authors":["Svetlana Krasnova","Emiliya Starikova","Ilia Naletov","Andrey Krylov","Dmitry Sorokin"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2507.18145v2","updated":"2025-12-19T14:03:25Z","published":"2025-07-24T07:21:49Z","title":"Logical Characterizations of GNNs with Mean Aggregation","summary":"We study the expressive power of graph neural networks (GNNs) with mean as the aggregation function, with the following results. In the non-uniform setting, such GNNs have exactly the same expressive power as ratio modal logic, which has modal operators expressing that at least a certain ratio of the successors of a vertex satisfies a specified property. In the uniform setting, the expressive power relative to MSO is exactly that of modal logic, and thus identical to the (absolute) expressive power of GNNs with max aggregation. The proof, however, depends on constructions that are not satisfactory from a practical perspective. This leads us to making the natural assumptions that combination functions are continuous and classification functions are thresholds. The resulting class of GNNs with mean aggregation turns out to be much less expressive: relative to MSO and in the uniform setting, it has the same expressive power as alternation-free modal logic. This is in contrast to the expressive power of GNNs with max and sum aggregation, which is not affected by these assumptions.","authors":["Moritz Schönherr","Carsten Lutz"],"pdf_url":"","comment":"26 pages, extended version of paper to appear in AAAI 2026"},{"id":"http://arxiv.org/abs/2512.17594v1","updated":"2025-12-19T14:02:37Z","published":"2025-12-19T14:02:37Z","title":"MAD-OOD: A Deep Learning Cluster-Driven Framework for an Out-of-Distribution Malware Detection and Classification","summary":"Out of distribution (OOD) detection remains a critical challenge in malware classification due to the substantial intra family variability introduced by polymorphic and metamorphic malware variants. Most existing deep learning based malware detectors rely on closed world assumptions and fail to adequately model this intra class variation, resulting in degraded performance when confronted with previously unseen malware families. This paper presents MADOOD, a novel two stage, cluster driven deep learning framework for robust OOD malware detection and classification. In the first stage, malware family embeddings are modeled using class conditional spherical decision boundaries derived from Gaussian Discriminant Analysis (GDA), enabling statistically grounded separation of indistribution and OOD samples without requiring OOD data during training. Z score based distance analysis across multiple class centroids is employed to reliably identify anomalous samples in the latent space. In the second stage, a deep neural network integrates cluster based predictions, refined embeddings, and supervised classifier outputs to enhance final classification accuracy. Extensive evaluations on benchmark malware datasets comprising 25 known families and multiple novel OOD variants demonstrate that MADOOD significantly outperforms state of the art OOD detection methods, achieving an AUC of up to 0.911 on unseen malware families. The proposed framework provides a scalable, interpretable, and statistically principled solution for real world malware detection and anomaly identification in evolving cybersecurity environments.","authors":["Tosin Ige","Christopher Kiekintveld","Aritran Piplai","Asif Rahman","Olukunle Kolade","Sasidhar Kunapuli"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17570v1","updated":"2025-12-19T13:36:31Z","published":"2025-12-19T13:36:31Z","title":"GreedySnake: Accelerating SSD-Offloaded LLM Training with Efficient Scheduling and Optimizer Step Overlapping","summary":"SSD-offloaded training offers a practical and promising approach to making LLM training cost-effective. Building on gradient accumulation with micro-batches, this paper introduces GreedySnake, a new SSD-offloaded training system that employs vertical scheduling, which executes all microbatches of a layer before proceeding to the next. Compared to existing systems that use horizontal scheduling (i.e., executing micro-batches sequentially), GreedySnake achieves higher training throughput with smaller batch sizes, bringing the system much closer to the ideal scenario predicted by the roofline model. To further mitigate the I/O bottleneck, GreedySnake overlaps part of the optimization step with the forward pass of the next iteration. Experimental results on A100 GPUs show that GreedySnake achieves saturated training throughput improvements over ZeRO-Infinity: 1.96x on 1 GPU and 1.93x on 4 GPUs for GPT-65B, and 2.53x on 1 GPU for GPT-175B. The code is open-sourced at https://github.com/npz7yyk/GreedySnake","authors":["Yikang Yue","Yishu Yin","Xuehai Qian"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17566v1","updated":"2025-12-19T13:33:43Z","published":"2025-12-19T13:33:43Z","title":"A unified FLAIR hyperintensity segmentation model for various CNS tumor types and acquisition time points","summary":"T2-weighted fluid-attenuated inversion recovery (FLAIR) magnetic resonance imaging (MRI) scans are important for diagnosis, treatment planning and monitoring of brain tumors. Depending on the brain tumor type, the FLAIR hyperintensity volume is an important measure to asses the tumor volume or surrounding edema, and an automatic segmentation of this would be useful in the clinic. In this study, around 5000 FLAIR images of various tumors types and acquisition time points from different centers were used to train a unified FLAIR hyperintensity segmentation model using an Attention U-Net architecture. The performance was compared against dataset specific models, and was validated on different tumor types, acquisition time points and against BraTS. The unified model achieved an average Dice score of 88.65\\% for pre-operative meningiomas, 80.08% for pre-operative metastasis, 90.92% for pre-operative and 84.60% for post-operative gliomas from BraTS, and 84.47% for pre-operative and 61.27\\% for post-operative lower grade gliomas. In addition, the results showed that the unified model achieved comparable segmentation performance to the dataset specific models on their respective datasets, and enables generalization across tumor types and acquisition time points, which facilitates the deployment in a clinical setting. The model is integrated into Raidionics, an open-source software for CNS tumor analysis.","authors":["Mathilde Gajda Faanes","David Bouget","Asgeir S. Jakola","Timothy R. Smith","Vasileios K. Kavouridis","Francesco Latini","Margret Jensdottir","Peter Milos","Henrietta Nittby Redebrandt","Rickard L. Sjöberg","Rupavathana Mahesparan","Lars Kjelsberg Pedersen","Ole Solheim","Ingerid Reinertsen"],"pdf_url":"","comment":"13 pages, 4 figures"},{"id":"http://arxiv.org/abs/2512.17562v1","updated":"2025-12-19T13:32:19Z","published":"2025-12-19T13:32:19Z","title":"When De-noising Hurts: A Systematic Study of Speech Enhancement Effects on Modern Medical ASR Systems","summary":"Speech enhancement methods are commonly believed to improve the performance of automatic speech recognition (ASR) in noisy environments. However, the effectiveness of these techniques cannot be taken for granted in the case of modern large-scale ASR models trained on diverse, noisy data. We present a systematic evaluation of MetricGAN-plus-voicebank denoising on four state-of-the-art ASR systems: OpenAI Whisper, NVIDIA Parakeet, Google Gemini Flash 2.0, Parrotlet-a using 500 medical speech recordings under nine noise conditions. ASR performance is measured using semantic WER (semWER), a normalized word error rate (WER) metric accounting for domain-specific normalizations. Our results reveal a counterintuitive finding: speech enhancement preprocessing degrades ASR performance across all noise conditions and models. Original noisy audio achieves lower semWER than enhanced audio in all 40 tested configurations (4 models x 10 conditions), with degradations ranging from 1.1% to 46.6% absolute semWER increase. These findings suggest that modern ASR models possess sufficient internal noise robustness and that traditional speech enhancement may remove acoustic features critical for ASR. For practitioners deploying medical scribe systems in noisy clinical environments, our results indicate that preprocessing audio with noise reduction techniques might not just be computationally wasteful but also be potentially harmful to the transcription accuracy.","authors":["Sujal Chondhekar","Vasanth Murukuri","Rushabh Vasani","Sanika Goyal","Rajshree Badami","Anushree Rana","Sanjana SN","Karthik Pandia","Sulabh Katiyar","Neha Jagadeesh","Sankalp Gulati"],"pdf_url":"","comment":"Technical Report"},{"id":"http://arxiv.org/abs/2512.17559v1","updated":"2025-12-19T13:28:50Z","published":"2025-12-19T13:28:50Z","title":"Towards Explainable Conversational AI for Early Diagnosis with Large Language Models","summary":"Healthcare systems around the world are grappling with issues like inefficient diagnostics, rising costs, and limited access to specialists. These problems often lead to delays in treatment and poor health outcomes. Most current AI and deep learning diagnostic systems are not very interactive or transparent, making them less effective in real-world, patient-centered environments. This research introduces a diagnostic chatbot powered by a Large Language Model (LLM), using GPT-4o, Retrieval-Augmented Generation, and explainable AI techniques. The chatbot engages patients in a dynamic conversation, helping to extract and normalize symptoms while prioritizing potential diagnoses through similarity matching and adaptive questioning. With Chain-of-Thought prompting, the system also offers more transparent reasoning behind its diagnoses. When tested against traditional machine learning models like Naive Bayes, Logistic Regression, SVM, Random Forest, and KNN, the LLM-based system delivered impressive results, achieving an accuracy of 90% and Top-3 accuracy of 100%. These findings offer a promising outlook for more transparent, interactive, and clinically relevant AI in healthcare.","authors":["Maliha Tabassum","M Shamim Kaiser"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17545v1","updated":"2025-12-19T13:10:31Z","published":"2025-12-19T13:10:31Z","title":"ClothHMR: 3D Mesh Recovery of Humans in Diverse Clothing from Single Image","summary":"With 3D data rapidly emerging as an important form of multimedia information, 3D human mesh recovery technology has also advanced accordingly. However, current methods mainly focus on handling humans wearing tight clothing and perform poorly when estimating body shapes and poses under diverse clothing, especially loose garments. To this end, we make two key insights: (1) tailoring clothing to fit the human body can mitigate the adverse impact of clothing on 3D human mesh recovery, and (2) utilizing human visual information from large foundational models can enhance the generalization ability of the estimation. Based on these insights, we propose ClothHMR, to accurately recover 3D meshes of humans in diverse clothing. ClothHMR primarily consists of two modules: clothing tailoring (CT) and FHVM-based mesh recovering (MR). The CT module employs body semantic estimation and body edge prediction to tailor the clothing, ensuring it fits the body silhouette. The MR module optimizes the initial parameters of the 3D human mesh by continuously aligning the intermediate representations of the 3D mesh with those inferred from the foundational human visual model (FHVM). ClothHMR can accurately recover 3D meshes of humans wearing diverse clothing, precisely estimating their body shapes and poses. Experimental results demonstrate that ClothHMR significantly outperforms existing state-of-the-art methods across benchmark datasets and in-the-wild images. Additionally, a web application for online fashion and shopping powered by ClothHMR is developed, illustrating that ClothHMR can effectively serve real-world usage scenarios. The code and model for ClothHMR are available at: \\url{https://github.com/starVisionTeam/ClothHMR}.","authors":["Yunqi Gao","Leyuan Liu","Yuhan Li","Changxin Gao","Yuanyuan Liu","Jingying Chen"],"pdf_url":"","comment":"15 pages,16 figures"},{"id":"http://arxiv.org/abs/2512.16715v2","updated":"2025-12-19T13:06:55Z","published":"2025-12-18T16:18:06Z","title":"Towards Reproducibility in Predictive Process Mining: SPICE -- A Deep Learning Library","summary":"In recent years, Predictive Process Mining (PPM) techniques based on artificial neural networks have evolved as a method for monitoring the future behavior of unfolding business processes and predicting Key Performance Indicators (KPIs). However, many PPM approaches often lack reproducibility, transparency in decision making, usability for incorporating novel datasets and benchmarking, making comparisons among different implementations very difficult. In this paper, we propose SPICE, a Python framework that reimplements three popular, existing baseline deep-learning-based methods for PPM in PyTorch, while designing a common base framework with rigorous configurability to enable reproducible and robust comparison of past and future modelling approaches. We compare SPICE to original reported metrics and with fair metrics on 11 datasets.","authors":["Oliver Stritzel","Nick Hühnerbein","Simon Rauch","Itzel Zarate","Lukas Fleischmann","Moike Buck","Attila Lischka","Christian Frey"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17534v1","updated":"2025-12-19T12:58:06Z","published":"2025-12-19T12:58:06Z","title":"HydroGym: A Reinforcement Learning Platform for Fluid Dynamics","summary":"Modeling and controlling fluid flows is critical for several fields of science and engineering, including transportation, energy, and medicine. Effective flow control can lead to, e.g., lift increase, drag reduction, mixing enhancement, and noise reduction. However, controlling a fluid faces several significant challenges, including high-dimensional, nonlinear, and multiscale interactions in space and time. Reinforcement learning (RL) has recently shown great success in complex domains, such as robotics and protein folding, but its application to flow control is hindered by a lack of standardized benchmark platforms and the computational demands of fluid simulations. To address these challenges, we introduce HydroGym, a solver-independent RL platform for flow control research. HydroGym integrates sophisticated flow control benchmarks, scalable runtime infrastructure, and state-of-the-art RL algorithms. Our platform includes 42 validated environments spanning from canonical laminar flows to complex three-dimensional turbulent scenarios, validated over a wide range of Reynolds numbers. We provide non-differentiable solvers for traditional RL and differentiable solvers that dramatically improve sample efficiency through gradient-enhanced optimization. Comprehensive evaluation reveals that RL agents consistently discover robust control principles across configurations, such as boundary layer manipulation, acoustic feedback disruption, and wake reorganization. Transfer learning studies demonstrate that controllers learned at one Reynolds number or geometry adapt efficiently to new conditions, requiring approximately 50% fewer training episodes. The HydroGym platform is highly extensible and scalable, providing a framework for researchers in fluid dynamics, machine learning, and control to add environments, surrogate models, and control algorithms to advance science and technology.","authors":["Christian Lagemann","Sajeda Mokbel","Miro Gondrum","Mario Rüttgers","Jared Callaham","Ludger Paehler","Samuel Ahnert","Nicholas Zolman","Kai Lagemann","Nikolaus Adams","Matthias Meinke","Wolfgang Schröder","Jean-Christophe Loiseau","Esther Lagemann","Steven L. Brunton"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17532v1","updated":"2025-12-19T12:56:17Z","published":"2025-12-19T12:56:17Z","title":"Robust-R1: Degradation-Aware Reasoning for Robust Visual Understanding","summary":"Multimodal Large Language Models struggle to maintain reliable performance under extreme real-world visual degradations, which impede their practical robustness. Existing robust MLLMs predominantly rely on implicit training/adaptation that focuses solely on visual encoder generalization, suffering from limited interpretability and isolated optimization. To overcome these limitations, we propose Robust-R1, a novel framework that explicitly models visual degradations through structured reasoning chains. Our approach integrates: (i) supervised fine-tuning for degradation-aware reasoning foundations, (ii) reward-driven alignment for accurately perceiving degradation parameters, and (iii) dynamic reasoning depth scaling adapted to degradation intensity. To facilitate this approach, we introduce a specialized 11K dataset featuring realistic degradations synthesized across four critical real-world visual processing stages, each annotated with structured chains connecting degradation parameters, perceptual influence, pristine semantic reasoning chain, and conclusion. Comprehensive evaluations demonstrate state-of-the-art robustness: Robust-R1 outperforms all general and robust baselines on the real-world degradation benchmark R-Bench, while maintaining superior anti-degradation performance under multi-intensity adversarial degradations on MMMB, MMStar, and RealWorldQA.","authors":["Jiaqi Tang","Jianmin Chen","Wei Wei","Xiaogang Xu","Runtao Liu","Xiangyu Wu","Qipeng Xie","Jiafei Wu","Lei Zhang","Qifeng Chen"],"pdf_url":"","comment":"Accepted by AAAI2026 Oral"},{"id":"http://arxiv.org/abs/2511.12346v2","updated":"2025-12-19T12:55:37Z","published":"2025-11-15T20:25:59Z","title":"CLAReSNet: When Convolution Meets Latent Attention for Hyperspectral Image Classification","summary":"Hyperspectral image (HSI) classification faces critical challenges, including high spectral dimensionality, complex spectral-spatial correlations, and limited training samples with severe class imbalance. While CNNs excel at local feature extraction and transformers capture long-range dependencies, their isolated application yields suboptimal results due to quadratic complexity and insufficient inductive biases. We propose CLAReSNet (Convolutional Latent Attention Residual Spectral Network), a hybrid architecture that integrates multi-scale convolutional extraction with transformer-style attention via an adaptive latent bottleneck. The model employs a multi-scale convolutional stem with deep residual blocks and an enhanced Convolutional Block Attention Module for hierarchical spatial features, followed by spectral encoder layers combining bidirectional RNNs (LSTM/GRU) with Multi-Scale Spectral Latent Attention (MSLA). MSLA reduces complexity from $\\mathcal{O}(T^2D)$ to $\\mathcal{O}(T\\log(T)D)$ by adaptive latent token allocation (8-64 tokens) that scales logarithmically with the sequence length. Hierarchical cross-attention fusion dynamically aggregates multi-level representations for robust classification. Experiments conducted on the Indian Pines and Salinas datasets show state-of-the-art performance, achieving overall accuracies of 99.71% and 99.96%, significantly surpassing HybridSN, SSRN, and SpectralFormer. The learned embeddings exhibit superior inter-class separability and compact intra-class clustering, validating CLAReSNet's effectiveness under severe class imbalance.","authors":["Asmit Bandyopadhyay","Anindita Das Bhattacharjee","Rakesh Das"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17527v1","updated":"2025-12-19T12:51:31Z","published":"2025-12-19T12:51:31Z","title":"SafeBench-Seq: A Homology-Clustered, CPU-Only Baseline for Protein Hazard Screening with Physicochemical/Composition Features and Cluster-Aware Confidence Intervals","summary":"Foundation models for protein design raise concrete biosecurity risks, yet the community lacks a simple, reproducible baseline for sequence-level hazard screening that is explicitly evaluated under homology control and runs on commodity CPUs. We introduce SafeBench-Seq, a metadata-only, reproducible benchmark and baseline classifier built entirely from public data (SafeProtein hazards and UniProt benigns) and interpretable features (global physicochemical descriptors and amino-acid composition). To approximate \"never-before-seen\" threats, we homology-cluster the combined dataset at <=40% identity and perform cluster-level holdouts (no cluster overlap between train/test). We report discrimination (AUROC/AUPRC) and screening-operating points (TPR@1% FPR; FPR@95% TPR) with 95% bootstrap confidence intervals (n=200), and we provide calibrated probabilities via CalibratedClassifierCV (isotonic for Logistic Regression / Random Forest; Platt sigmoid for Linear SVM). We quantify probability quality using Brier score, Expected Calibration Error (ECE; 15 bins), and reliability diagrams. Shortcut susceptibility is probed via composition-preserving residue shuffles and length-/composition-only ablations. Empirically, random splits substantially overestimate robustness relative to homology-clustered evaluation; calibrated linear models exhibit comparatively good calibration, while tree ensembles retain slightly higher Brier/ECE. SafeBench-Seq is CPU-only, reproducible, and releases metadata only (accessions, cluster IDs, split labels), enabling rigorous evaluation without distributing hazardous sequences.","authors":["Muhammad Haris Khan"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17519v1","updated":"2025-12-19T12:42:53Z","published":"2025-12-19T12:42:53Z","title":"Key-Conditioned Orthonormal Transform Gating (K-OTG): Multi-Key Access Control with Hidden-State Scrambling for LoRA-Tuned Models","summary":"We present a simple, PEFT-compatible mechanism that enforces secret-key access control in instruction-tuned language models. K-OTG trains on a dual-path corpus: authorized examples (prefixed with a role key) learn the task output, while unauthorized examples learn a visible block token. At inference, a pre-lm_head hook applies an orthonormal transform to the hidden state: with the correct key/role the inverse map restores the model's native basis; otherwise a session-ephemeral scrambler (permutation, sign flips, Householders) makes logits uninformative and the system short-circuits to BLOCK. Keys are not added as special tokens, and the method composes cleanly with LoRA on 4-bit bases. We evaluate an hour-scale protocol on 1-3B-class instruction models (Llama 3.2, Qwen2.5 1.5B) across utility (XSum ROUGE/BLEU, GSM8K accuracy, WikiText-2 perplexity), selectivity (3by3 role-key unlock matrices), nonce invariance, block suppression, and throughput. Authorized utility remains close to the base on summarization with the expected modest PPL increase from instruction tuning; unauthorized utility collapses (near-zero sequence metrics with exploding PPL), indicating practical unusability without the key. Unlock matrices are diagonally dominant (high on-target unlock, low cross-unlock), authorized block emission is 0 per N via robust bad-word lists, and greedy outputs match exactly across nonces, confirming correct inverse cancellation. The runtime overhead of the Python-level hook is 40% tokens per sec versus the base. K-OTG therefore provides a pragmatic, model-agnostic way to prevent unauthorized use while preserving authorized utility.","authors":["Muhammad Haris Khan"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.10787v2","updated":"2025-12-19T12:41:35Z","published":"2025-12-11T16:31:29Z","title":"Replace, Don't Expand: Mitigating Context Dilution in Multi-Hop RAG via Fixed-Budget Evidence Assembly","summary":"Retrieval-Augmented Generation (RAG) systems often fail on multi-hop queries when the initial retrieval misses a bridge fact. Prior corrective approaches, such as Self-RAG, CRAG, and Adaptive-$k$, typically address this by \\textit{adding} more context or pruning existing lists. However, simply expanding the context window often leads to \\textbf{context dilution}, where distractors crowd out relevant information. We propose \\textbf{SEAL-RAG}, a training-free controller that adopts a \\textbf{``replace, don't expand''} strategy to fight context dilution under a fixed retrieval depth $k$. SEAL executes a (\\textbf{S}earch $\\rightarrow$ \\textbf{E}xtract $\\rightarrow$ \\textbf{A}ssess $\\rightarrow$ \\textbf{L}oop) cycle: it performs on-the-fly, entity-anchored extraction to build a live \\textit{gap specification} (missing entities/relations), triggers targeted micro-queries, and uses \\textit{entity-first ranking} to actively swap out distractors for gap-closing evidence. We evaluate SEAL-RAG against faithful re-implementations of Basic RAG, CRAG, Self-RAG, and Adaptive-$k$ in a shared environment on \\textbf{HotpotQA} and \\textbf{2WikiMultiHopQA}. On HotpotQA ($k=3$), SEAL improves answer correctness by \\textbf{+3--13 pp} and evidence precision by \\textbf{+12--18 pp} over Self-RAG. On 2WikiMultiHopQA ($k=5$), it outperforms Adaptive-$k$ by \\textbf{+8.0 pp} in accuracy and maintains \\textbf{96\\%} evidence precision compared to 22\\% for CRAG. These gains are statistically significant ($p<0.001$). By enforcing fixed-$k$ replacement, SEAL yields a predictable cost profile while ensuring the top-$k$ slots are optimized for precision rather than mere breadth. We release our code and data at https://github.com/mosherino/SEAL-RAG.","authors":["Moshe Lahmy","Roi Yozevitch"],"pdf_url":"","comment":"24 pages, 2 figures"},{"id":"http://arxiv.org/abs/2512.17504v1","updated":"2025-12-19T12:14:36Z","published":"2025-12-19T12:14:36Z","title":"InsertAnywhere: Bridging 4D Scene Geometry and Diffusion Models for Realistic Video Object Insertion","summary":"Recent advances in diffusion-based video generation have opened new possibilities for controllable video editing, yet realistic video object insertion (VOI) remains challenging due to limited 4D scene understanding and inadequate handling of occlusion and lighting effects. We present InsertAnywhere, a new VOI framework that achieves geometrically consistent object placement and appearance-faithful video synthesis. Our method begins with a 4D aware mask generation module that reconstructs the scene geometry and propagates user specified object placement across frames while maintaining temporal coherence and occlusion consistency. Building upon this spatial foundation, we extend a diffusion based video generation model to jointly synthesize the inserted object and its surrounding local variations such as illumination and shading. To enable supervised training, we introduce ROSE++, an illumination aware synthetic dataset constructed by transforming the ROSE object removal dataset into triplets of object removed video, object present video, and a VLM generated reference image. Through extensive experiments, we demonstrate that our framework produces geometrically plausible and visually coherent object insertions across diverse real world scenarios, significantly outperforming existing research and commercial models.","authors":["Hoiyeong Jin","Hyojin Jang","Jeongho Kim","Junha Hyung","Kinam Kim","Dongjin Kim","Huijin Choi","Hyeonji Kim","Jaegul Choo"],"pdf_url":"","comment":"16 pages, project page: https://myyzzzoooo.github.io/InsertAnywhere/"},{"id":"http://arxiv.org/abs/2507.17860v3","updated":"2025-12-19T11:48:41Z","published":"2025-07-23T18:33:27Z","title":"Towards Facilitated Fairness Assessment of AI-based Skin Lesion Classifiers Through GenAI-based Image Synthesis","summary":"Recent advances in deep learning and on-device inference could transform routine screening for skin cancers. Along with the anticipated benefits of this technology, potential dangers arise from unforeseen and inherent biases. A significant obstacle is building evaluation datasets that accurately reflect key demographics, including sex, age, and race, as well as other underrepresented groups. To address this, we train a state-of-the-art generative model to generate synthetic data in a controllable manner to assess the fairness of publicly available skin cancer classifiers. To evaluate whether synthetic images can be used as a fairness testing dataset, we prepare a real-image dataset (MILK10K) as a benchmark and compare the True Positive Rate result of three models (DeepGuide, MelaNet, and SkinLesionDensnet). As a result, the classification tendencies observed in each model when tested on real and generated images showed similar patterns across different attribute data sets. We confirm that highly realistic synthetic images facilitate model fairness verification.","authors":["Ko Watanabe","Stanislav Frolov","Aya Hassan","David Dembinsky","Adriano Lucieri","Andreas Dengel"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17470v1","updated":"2025-12-19T11:33:30Z","published":"2025-12-19T11:33:30Z","title":"Translating the Rashomon Effect to Sequential Decision-Making Tasks","summary":"The Rashomon effect describes the phenomenon where multiple models trained on the same data produce identical predictions while differing in which features they rely on internally. This effect has been studied extensively in classification tasks, but not in sequential decision-making, where an agent learns a policy to achieve an objective by taking actions in an environment. In this paper, we translate the Rashomon effect to sequential decision-making. We define it as multiple policies that exhibit identical behavior, visiting the same states and selecting the same actions, while differing in their internal structure, such as feature attributions. Verifying identical behavior in sequential decision-making differs from classification. In classification, predictions can be directly compared to ground-truth labels. In sequential decision-making with stochastic transitions, the same policy may succeed or fail on any single trajectory due to randomness. We address this using formal verification methods that construct and compare the complete probabilistic behavior of each policy in the environment. Our experiments demonstrate that the Rashomon effect exists in sequential decision-making. We further show that ensembles constructed from the Rashomon set exhibit greater robustness to distribution shifts than individual policies. Additionally, permissive policies derived from the Rashomon set reduce computational requirements for verification while maintaining optimal performance.","authors":["Dennis Gross","Jørn Eirik Betten","Helge Spieker"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.16491v2","updated":"2025-12-19T11:30:18Z","published":"2025-12-18T12:59:45Z","title":"Best Practices For Empirical Meta-Algorithmic Research: Guidelines from the COSEAL Research Network","summary":"Empirical research on meta-algorithmics, such as algorithm selection, configuration, and scheduling, often relies on extensive and thus computationally expensive experiments. With the large degree of freedom we have over our experimental setup and design comes a plethora of possible error sources that threaten the scalability and validity of our scientific insights. Best practices for meta-algorithmic research exist, but they are scattered between different publications and fields, and continue to evolve separately from each other. In this report, we collect good practices for empirical meta-algorithmic research across the subfields of the COSEAL community, encompassing the entire experimental cycle: from formulating research questions and selecting an experimental design, to executing experiments, and ultimately, analyzing and presenting results impartially. It establishes the current state-of-the-art practices within meta-algorithmic research and serves as a guideline to both new researchers and practitioners in meta-algorithmic fields.","authors":["Theresa Eimer","Lennart Schäpermeier","André Biedenkapp","Alexander Tornede","Lars Kotthoff","Pieter Leyman","Matthias Feurer","Katharina Eggensperger","Kaitlin Maile","Tanja Tornede","Anna Kozak","Ke Xue","Marcel Wever","Mitra Baratchi","Damir Pulatov","Heike Trautmann","Haniye Kashgarani","Marius Lindauer"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2507.00724v2","updated":"2025-12-19T11:29:15Z","published":"2025-06-24T15:40:11Z","title":"Holmes: Towards Effective and Harmless Model Ownership Verification to Personalized Large Vision Models via Decoupling Common Features","summary":"Large vision models (LVMs) achieve remarkable performance in various downstream tasks, primarily by personalizing pre-trained models through fine-tuning with private and valuable local data, which makes the personalized model a valuable intellectual property. Similar to the era of traditional DNNs, model stealing attacks also pose significant risks to LVMs. However, this paper reveals that most existing defense methods (developed for traditional DNNs), typically designed for models trained from scratch, either introduce additional security risks, are prone to misjudgment, or are even ineffective for fine-tuned models. To alleviate these problems, this paper proposes a harmless model ownership verification method for personalized LVMs by decoupling similar common features. In general, our method consists of three main stages. In the first stage, we create shadow models that retain common features of the victim model while disrupting dataset-specific features. We represent the dataset-specific features of the victim model by computing the output differences between the shadow and victim models, without altering the victim model or its training process. After that, a meta-classifier is trained to identify stolen models by determining whether suspicious models contain the dataset-specific features of the victim. In the third stage, we conduct model ownership verification by hypothesis test to mitigate randomness and enhance robustness. Extensive experiments on benchmark datasets verify the effectiveness of the proposed method in detecting different types of model stealing simultaneously. Our codes are available at https://github.com/zlh-thu/Holmes.","authors":["Linghui Zhu","Yiming Li","Haiqin Weng","Yan Liu","Tianwei Zhang","Shu-Tao Xia","Zhi Wang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2510.14512v2","updated":"2025-12-19T11:27:02Z","published":"2025-10-16T09:57:31Z","title":"Helmsman: Autonomous Synthesis of Federated Learning Systems via Collaborative LLM Agents","summary":"Federated Learning (FL) offers a powerful paradigm for training models on decentralized data, but its promise is often undermined by the immense complexity of designing and deploying robust systems. The need to select, combine, and tune strategies for multifaceted challenges like data heterogeneity and system constraints has become a critical bottleneck, resulting in brittle, bespoke solutions. To address this, we introduce Helmsman, a novel multi-agent system that automates the end-to-end synthesis of federated learning systems from high-level user specifications. It emulates a principled research and development workflow through three collaborative phases: (1) interactive human-in-the-loop planning to formulate a sound research plan, (2) modular code generation by supervised agent teams, and (3) a closed-loop of autonomous evaluation and refinement in a sandboxed simulation environment. To facilitate rigorous evaluation, we also introduce AgentFL-Bench, a new benchmark comprising 16 diverse tasks designed to assess the system-level generation capabilities of agentic systems in FL. Extensive experiments demonstrate that our approach generates solutions competitive with, and often superior to, established hand-crafted baselines. Our work represents a significant step towards the automated engineering of complex decentralized AI systems.","authors":["Haoyuan Li","Mathias Funk","Aaqib Saeed"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17462v1","updated":"2025-12-19T11:25:18Z","published":"2025-12-19T11:25:18Z","title":"Behavioural Effects of Agentic Messaging: A Case Study on a Financial Service Application","summary":"Marketing and product personalisation provide a prominent and visible use-case for the application of Information Retrieval methods across several business domains. Recently, agentic approaches to these problems have been gaining traction. This work evaluates the behavioural and retention effects of agentic personalisation on a financial service application's customer communication system during a 2025 national tax filing period. Through a two month-long randomised controlled trial, we compare an agentic messaging approach against a business-as-usual (BAU) rule-based campaign system, focusing on two primary outcomes: unsubscribe behaviour and conversion timing. Empirical results show that agent-led messaging reduced unsubscribe events by 21\\% ($\\pm 0.01$) relative to BAU and increased early filing behaviour in the weeks preceding the national deadline. These findings demonstrate how adaptive, user-level decision-making systems can modulate engagement intensity whilst improving long-term retention indicators.","authors":["Olivier Jeunen","Schaun Wheeler"],"pdf_url":"","comment":"To appear in the 48th European Conference on Information Retrieval (ECIR '26) Industry Track"},{"id":"http://arxiv.org/abs/2512.17461v1","updated":"2025-12-19T11:24:26Z","published":"2025-12-19T11:24:26Z","title":"Fair Voting Methods as a Catalyst for Democratic Resilience: A Trilogy on Legitimacy, Impact and AI Safeguarding","summary":"This article shows how fair voting methods can be a catalyst for change in the way we make collective decisions, and how such change can promote long-awaited upgrades of democracy. Based on real-world evidence from democratic innovations in participatory budgeting, in Switzerland and beyond, I highlight a trilogy of key research results: Fair voting methods achieve to be (i) legitimacy incubator, (ii) novel impact accelerator and (iii) safeguard for risks of artificial intelligence (AI). Compared to majoritarian voting methods, combining expressive ballot formats (e.g. cumulative voting) with ballot aggregation methods that promote proportional representation (e.g. equal shares) results in more winners and higher (geographical) representation of citizens. Such fair voting methods are preferred and found fairer even by voters who do not win, while promoting stronger democratic values for citizens such as altruism and compromise. They also result in new resourceful ideas to put for voting, which are cost-effective and win, especially in areas of welfare, education and culture. Strikingly, fair voting methods are also more resilient to biases and inconsistencies of generative AI in emerging scenarios of AI voting assistance or AI representation of voters who would be likely to abstain. I also review the relevance of such upgrades for democracies in crisis, such as the one of Greece featured in the recent study of `Unmute Democracy'. Greek democracy can build stronger resilience via higher representation of citizens in democratic processes as well as democratic innovations in participation. Fair voting methods can be a catalyst for both endeavors.","authors":["Evangelos Pournaras"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.15258v2","updated":"2025-12-19T11:22:57Z","published":"2025-12-17T10:02:55Z","title":"VLA-AN: An Efficient and Onboard Vision-Language-Action Framework for Aerial Navigation in Complex Environments","summary":"This paper proposes VLA-AN, an efficient and onboard Vision-Language-Action (VLA) framework dedicated to autonomous drone navigation in complex environments. VLA-AN addresses four major limitations of existing large aerial navigation models: the data domain gap, insufficient temporal navigation with reasoning, safety issues with generative action policies, and onboard deployment constraints. First, we construct a high-fidelity dataset utilizing 3D Gaussian Splatting (3D-GS) to effectively bridge the domain gap. Second, we introduce a progressive three-stage training framework that sequentially reinforces scene comprehension, core flight skills, and complex navigation capabilities. Third, we design a lightweight, real-time action module coupled with geometric safety correction. This module ensures fast, collision-free, and stable command generation, mitigating the safety risks inherent in stochastic generative policies. Finally, through deep optimization of the onboard deployment pipeline, VLA-AN achieves a robust real-time 8.3x improvement in inference throughput on resource-constrained UAVs. Extensive experiments demonstrate that VLA-AN significantly improves spatial grounding, scene reasoning, and long-horizon navigation, achieving a maximum single-task success rate of 98.1%, and providing an efficient, practical solution for realizing full-chain closed-loop autonomy in lightweight aerial robots.","authors":["Yuze Wu","Mo Zhu","Xingxing Li","Yuheng Du","Yuxin Fan","Wenjun Li","Zhichao Han","Xin Zhou","Fei Gao"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17453v1","updated":"2025-12-19T11:12:20Z","published":"2025-12-19T11:12:20Z","title":"A lightweight Spatial-Temporal Graph Neural Network for Long-term Time Series Forecasting","summary":"We propose Lite-STGNN, a lightweight spatial-temporal graph neural network for long-term multivariate forecasting that integrates decomposition-based temporal modeling with learnable sparse graph structure. The temporal module applies trend-seasonal decomposition, while the spatial module performs message passing with low-rank Top-$K$ adjacency learning and conservative horizon-wise gating, enabling spatial corrections that enhance a strong linear baseline. Lite-STGNN achieves state-of-the-art accuracy on four benchmark datasets for horizons up to 720 steps, while being parameter-efficient and substantially faster to train than transformer-based methods. Ablation studies show that the spatial module yields 4.6% improvement over the temporal baseline, Top-$K$ enhances locality by 3.3%, and learned adjacency matrices reveal domain-specific interaction dynamics. Lite-STGNN thus offers a compact, interpretable, and efficient framework for long-term multivariate time series forecasting.","authors":["Henok Tenaw Moges","Deshendran Moodley"],"pdf_url":"","comment":"9 pages, 5 figures, 2 tables. Accepted for presentation at the 18th International Conference on Agents and Artificial Intelligence (ICAART 2026), Marbella, Spain"},{"id":"http://arxiv.org/abs/2511.15282v2","updated":"2025-12-19T11:10:53Z","published":"2025-11-19T09:48:07Z","title":"Realist and Pluralist Conceptions of Intelligence and Their Implications on AI Research","summary":"In this paper, we argue that current AI research operates on a spectrum between two different underlying conceptions of intelligence: Intelligence Realism, which holds that intelligence represents a single, universal capacity measurable across all systems, and Intelligence Pluralism, which views intelligence as diverse, context-dependent capacities that cannot be reduced to a single universal measure. Through an analysis of current debates in AI research, we demonstrate how the conceptions remain largely implicit yet fundamentally shape how empirical evidence gets interpreted across a wide range of areas. These underlying views generate fundamentally different research approaches across three areas. Methodologically, they produce different approaches to model selection, benchmark design, and experimental validation. Interpretively, they lead to contradictory readings of the same empirical phenomena, from capability emergence to system limitations. Regarding AI risk, they generate categorically different assessments: realists view superintelligence as the primary risk and search for unified alignment solutions, while pluralists see diverse threats across different domains requiring context-specific solutions. We argue that making explicit these underlying assumptions can contribute to a clearer understanding of disagreements in AI research.","authors":["Ninell Oldenburg","Ruchira Dhar","Anders Søgaard"],"pdf_url":"","comment":"The 40th Annual AAAI Conference on Artificial Intelligence, 8 pages (excl. references), 1 table"},{"id":"http://arxiv.org/abs/2512.17452v1","updated":"2025-12-19T11:08:58Z","published":"2025-12-19T11:08:58Z","title":"Learning What to Write: Write-Gated KV for Efficient Long-Context Inference","summary":"Long-context LLM inference is bottlenecked by the quadratic attention complexity and linear KV cache growth. Prior approaches mitigate this via post-hoc selection or eviction but overlook the root inefficiency: indiscriminate writing to persistent memory. In this paper, we formalize KV cache management as a causal system of three primitives: KV Admission, Selection, and Eviction. We instantiate KV Admission via Write-Gated KV, a lightweight mechanism that learns to predict token utility before it enters the cache. By filtering out low-utility states early to maintain a compact global cache alongside a sliding local cache, Write-Gated KV reduces memory usage by 46-57% and delivers 3.03-3.45$\\times$ prefill and 1.89-2.56$\\times$ decode speedups on Llama model with negligible accuracy loss, all while remaining compatible with FlashAttention and paged-KV systems. These results demonstrate that learning what to write, is a principled and practical recipe for efficient long-context inference. Code is available at https://github.com/EMCLab-Sinica/WG-KV .","authors":["Yen-Chieh Huang","Rui Fang","Ming-Syan Chen","Pi-Cheng Hsiu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.01037v2","updated":"2025-12-19T11:00:50Z","published":"2025-11-30T19:11:45Z","title":"When Safety Blocks Sense: Measuring Semantic Confusion in LLM Refusals","summary":"Safety-aligned language models often refuse prompts that are actually harmless. Current evaluations mostly report global rates such as false rejection or compliance. These scores treat each prompt alone and miss local inconsistency, where a model accepts one phrasing of an intent but rejects a close paraphrase. This gap limits diagnosis and tuning. We introduce \"semantic confusion,\" a failure mode that captures such local inconsistency, and a framework to measure it. We build ParaGuard, a 10k-prompt corpus of controlled paraphrase clusters that hold intent fixed while varying surface form. We then propose three model-agnostic metrics at the token level: Confusion Index, Confusion Rate, and Confusion Depth. These metrics compare each refusal to its nearest accepted neighbors and use token embeddings, next-token probabilities, and perplexity signals. Experiments across diverse model families and deployment guards show that global false-rejection rate hides critical structure. Our metrics reveal globally unstable boundaries in some settings, localized pockets of inconsistency in others, and cases where stricter refusal does not increase inconsistency. We also show how confusion-aware auditing separates how often a system refuses from how sensibly it refuses. This gives developers a practical signal to reduce false refusals while preserving safety.","authors":["Riad Ahmed Anonto","Md Labid Al Nahiyan","Md Tanvir Hassan"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17444v1","updated":"2025-12-19T10:56:34Z","published":"2025-12-19T10:56:34Z","title":"Assessing Long-Term Electricity Market Design for Ambitious Decarbonization Targets using Multi-Agent Reinforcement Learning","summary":"Electricity systems are key to transforming today's society into a carbon-free economy. Long-term electricity market mechanisms, including auctions, support schemes, and other policy instruments, are critical in shaping the electricity generation mix. In light of the need for more advanced tools to support policymakers and other stakeholders in designing, testing, and evaluating long-term markets, this work presents a multi-agent reinforcement learning model capable of capturing the key features of decarbonizing energy systems. Profit-maximizing generation companies make investment decisions in the wholesale electricity market, responding to system needs, competitive dynamics, and policy signals. The model employs independent proximal policy optimization, which was selected for suitability to the decentralized and competitive environment. Nevertheless, given the inherent challenges of independent learning in multi-agent settings, an extensive hyperparameter search ensures that decentralized training yields market outcomes consistent with competitive behavior. The model is applied to a stylized version of the Italian electricity system and tested under varying levels of competition, market designs, and policy scenarios. Results highlight the critical role of market design for decarbonizing the electricity sector and avoiding price volatility. The proposed framework allows assessing long-term electricity markets in which multiple policy and market mechanisms interact simultaneously, with market participants responding and adapting to decarbonization pathways.","authors":["Javier Gonzalez-Ruiz","Carlos Rodriguez-Pardo","Iacopo Savelli","Alice Di Bella","Massimo Tavoni"],"pdf_url":"","comment":"Accepted to Energy and AI. Code available in https://github.com/jjgonzalez2491/MARLEY_V1"},{"id":"http://arxiv.org/abs/2512.17442v1","updated":"2025-12-19T10:54:42Z","published":"2025-12-19T10:54:42Z","title":"A Systematic Reproducibility Study of BSARec for Sequential Recommendation","summary":"In sequential recommendation (SR), the self-attention mechanism of Transformer-based models acts as a low-pass filter, limiting their ability to capture high-frequency signals that reflect short-term user interests. To overcome this, BSARec augments the Transformer encoder with a frequency layer that rescales high-frequency components using the Fourier transform. However, the overall effectiveness of BSARec and the roles of its individual components have yet to be systematically validated. We reproduce BSARec and show that it outperforms other SR methods on some datasets. To empirically assess whether BSARec improves performance on high-frequency signals, we propose a metric to quantify user history frequency and evaluate SR methods across different user groups. We compare digital signal processing (DSP) techniques and find that the discrete wavelet transform (DWT) offer only slight improvements over Fourier transforms, and DSP methods provide no clear advantage over simple residual connections. Finally, we explore padding strategies and find that non-constant padding significantly improves recommendation performance, whereas constant padding hinders the frequency rescaler's ability to capture high-frequency signals.","authors":["Jan Hutter","Hua Chang Bakker","Stan Fris","Madelon Bernardy","Yuanna Liu"],"pdf_url":"","comment":"Jan Hutter, Hua Chang Bakker, Stan Fris, Madelon Bernardy contributed equally to this work"},{"id":"http://arxiv.org/abs/2412.13145v2","updated":"2025-12-19T10:18:36Z","published":"2024-12-17T18:11:12Z","title":"Agnosticism About Artificial Consciousness","summary":"Could an AI have conscious experiences? Any answer to this question should conform to Evidentialism - that is, it should be based not on intuition, dogma or speculation but on solid scientific evidence. I argue that such evidence is hard to come by and that the only justifiable stance on the prospects of artificial consciousness is agnosticism. In the current debate, the main division is between biological views that are sceptical of artificial consciousness and functional views that are sympathetic to it. I argue that both camps make the same mistake of over-estimating what the evidence tells us. Scientific insights into consciousness have been achieved through the study of conscious organisms. Although this has enabled cautious assessments of consciousness in various creatures, extending this to AI faces serious obstacles. AI thus presents consciousness researchers with a dilemma: either reach a verdict on artificial consciousness but violate Evidentialism; or respect Evidentialism but offer no verdict on the prospects of artificial consciousness. The dominant trend in the literature has been to take the first option while purporting to follow the scientific evidence. I argue that if we truly follow the evidence, we must take the second option and adopt agnosticism.","authors":["Tom McClelland"],"pdf_url":"","comment":"20 pages"},{"id":"http://arxiv.org/abs/2506.22376v4","updated":"2025-12-19T10:17:53Z","published":"2025-06-27T16:44:11Z","title":"OptScale: Probabilistic Optimality for Inference-time Scaling","summary":"Inference-time scaling has emerged as a powerful technique for enhancing the reasoning performance of Large Language Models (LLMs). However, existing approaches often rely on heuristic strategies for parallel sampling, lacking a principled foundation. To address this gap, we propose a probabilistic framework that formalizes the optimality of inference-time scaling under the assumption that parallel samples are independently and identically distributed (i.i.d.), and where the Best-of-$N$ selection strategy follows a probability distribution that can be estimated. Within this framework, we derive a theoretical lower bound on the required number of samples to achieve a target performance level, providing the first principled guidance for compute-efficient scaling. Leveraging this insight, we develop \\textsc{OptScale}, a practical algorithm that dynamically determines the optimal number of sampled responses. \\textsc{OptScale} employs a language model-based predictor to estimate probabilistic prior parameters, enabling the decision of the minimal number of samples needed that satisfy predefined performance thresholds and confidence levels. Extensive experiments on representative reasoning benchmarks (including MATH-500, GSM8K, AIME, and AMC) demonstrate that \\textsc{OptScale} significantly reduces sampling overhead while remaining better or on par with state-of-the-art reasoning performance. Our work offers both a theoretical foundation and a practical solution for principled inference-time scaling, addressing a critical gap in the efficient deployment of LLMs for complex reasoning.","authors":["Youkang Wang","Jian Wang","Rubing Chen","Xiao-Yong Wei"],"pdf_url":"","comment":"Accepted by AAAI-2026"},{"id":"http://arxiv.org/abs/2512.17419v1","updated":"2025-12-19T10:16:51Z","published":"2025-12-19T10:16:51Z","title":"SWE-Bench++: A Framework for the Scalable Generation of Software Engineering Benchmarks from Open-Source Repositories","summary":"Benchmarks like SWE-bench have standardized the evaluation of Large Language Models (LLMs) on repository-level software engineering tasks. However, these efforts remain limited by manual curation, static datasets, and a focus on Python-based bug fixes. We introduce SWE-Bench++, an automated framework that generates repository-level coding tasks from open-source GitHub projects. Unlike synthetic approaches, our pipeline harvests live pull requests to cover both bug fixes and feature requests across 11 languages. SWE-Bench++ turns GitHub pull requests (PRs) into reproducible, execution-based tasks via four stages: programmatic sourcing, environment synthesis, test oracle extraction, and quality assurance. A final hint-guided trajectory synthesis step converts instances that strong models fail on into training trajectories. Our initial benchmark consists of 11,133 instances from 3,971 repositories across 11 languages. On a subset of 1,782 instances of this benchmark, today's strongest models perform as follows: claude-sonnet-4.5 achieves 36.20% pass@10, gpt-5-2025-08-07 34.57%, gemini/gemini-2.5-pro 24.92%, and gpt-4o 16.89%. We further demonstrate the utility of our dataset by showing that fine-tuning on SWE-Bench++ instances yields measurable improvements on the SWE-bench Multilingual benchmark. SWE-Bench++ provides a scalable, multilingual benchmark for evaluating and improving repository-level code generation.","authors":["Lilin Wang","Lucas Ramalho","Alan Celestino","Phuc Anthony Pham","Yu Liu","Umang Kumar Sinha","Andres Portillo","Onassis Osunwa","Gabriel Maduekwe"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17412v1","updated":"2025-12-19T10:06:09Z","published":"2025-12-19T10:06:09Z","title":"Optimisation of Aircraft Maintenance Schedules","summary":"We present an aircraft maintenance scheduling problem, which requires suitably qualified staff to be assigned to maintenance tasks on each aircraft. The tasks on each aircraft must be completed within a given turn around window so that the aircraft may resume revenue earning service. This paper presents an initial study based on the application of an Evolutionary Algorithm to the problem. Evolutionary Algorithms evolve a solution to a problem by evaluating many possible solutions, focusing the search on those solutions that are of a higher quality, as defined by a fitness function. In this paper, we benchmark the algorithm on 60 generated problem instances to demonstrate the underlying representation and associated genetic operators.","authors":["Neil Urquhart","Amir Rahimi","Efstathios-Al. Tingas"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17411v1","updated":"2025-12-19T10:04:52Z","published":"2025-12-19T10:04:52Z","title":"Detection and Analysis of Sensitive and Illegal Content on the Ethereum Blockchain Using Machine Learning Techniques","summary":"Blockchain technology, lauded for its transparent and immutable nature, introduces a novel trust model. However, its decentralized structure raises concerns about potential inclusion of malicious or illegal content. This study focuses on Ethereum, presenting a data identification and restoration algorithm. Successfully recovering 175 common files, 296 images, and 91,206 texts, we employed the FastText algorithm for sentiment analysis, achieving a 0.9 accuracy after parameter tuning. Classification revealed 70,189 neutral, 5,208 positive, and 15,810 negative texts, aiding in identifying sensitive or illicit information. Leveraging the NSFWJS library, we detected seven indecent images with 100% accuracy. Our findings expose the coexistence of benign and harmful content on the Ethereum blockchain, including personal data, explicit images, divisive language, and racial discrimination. Notably, sensitive information targeted Chinese government officials. Proposing preventative measures, our study offers valuable insights for public comprehension of blockchain technology and regulatory agency guidance. The algorithms employed present innovative solutions to address blockchain data privacy and security concerns.","authors":["Xingyu Feng"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17396v1","updated":"2025-12-19T09:47:54Z","published":"2025-12-19T09:47:54Z","title":"RadImageNet-VQA: A Large-Scale CT and MRI Dataset for Radiologic Visual Question Answering","summary":"In this work, we introduce RadImageNet-VQA, a large-scale dataset designed to advance radiologic visual question answering (VQA) on CT and MRI exams. Existing medical VQA datasets are limited in scale, dominated by X-ray imaging or biomedical illustrations, and often prone to text-based shortcuts. RadImageNet-VQA is built from expert-curated annotations and provides 750K images paired with 7.5M question-answer samples. It covers three key tasks - abnormality detection, anatomy recognition, and pathology identification - spanning eight anatomical regions and 97 pathology categories, and supports open-ended, closed-ended, and multiple-choice questions. Extensive experiments show that state-of-the-art vision-language models still struggle with fine-grained pathology identification, particularly in open-ended settings and even after fine-tuning. Text-only analysis further reveals that model performance collapses to near-random without image inputs, confirming that RadImageNet-VQA is free from linguistic shortcuts. The full dataset and benchmark are publicly available at https://huggingface.co/datasets/raidium/RadImageNet-VQA.","authors":["Léo Butsanets","Charles Corbière","Julien Khlaut","Pierre Manceron","Corentin Dancette"],"pdf_url":"","comment":"Preprint, 23 pages, 12 figures, 7 tables"},{"id":"http://arxiv.org/abs/2308.02815v2","updated":"2025-12-19T09:43:02Z","published":"2023-08-05T08:11:26Z","title":"An AI-driven Assessment of Bone Density as a Biomarker Leading to the Aging Law","summary":"As global population aging intensifies, there is growing interest in the study of biological age. Bones have long been used to evaluate biological age, and the decline in bone density with age is a well-recognized phenomenon in adults. However, the pattern of this decline remains controversial, making it difficult to serve as a reliable indicator of the aging process. Here we present a novel AI-driven statistical method to assess the bone density, and a discovery that the bone mass distribution in trabecular bone of vertebrae follows a non-Gaussian, unimodal, and skewed distribution in CT images. The statistical mode of the distribution is defined as the measure of bone mass, which is a groundbreaking assessment of bone density, named Trabecular Bone Density (TBD). The dataset of CT images are collected from 1,719 patients who underwent PET/CT scans in three hospitals, in which a subset of the dataset is used for AI model training and generalization. Based upon the cases, we demonstrate that the pattern of bone density declining with aging exhibits a consistent trend of exponential decline across sexes and age groups using TBD assessment. The developed AI-driven statistical method blazes a trail in the field of AI for reliable quantitative computation and AI for medicine. The findings suggest that human aging is a gradual process, with the rate of decline slowing progressively over time, which will provide a valuable basis for scientific prediction of life expectancy.","authors":["Linmi Tao","Donglai Tao","Ruiyang Liu","Yu Cheng","Yuezhi Zhou","Li Huo","Zuoxiang He","Ti Jiang","Jingmao Cui","Yuanbiao Wang","Guilan Hu","Xiangsong Zhang","Yongwei Pan","Ye Yuan","Yun Liu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2508.19011v3","updated":"2025-12-19T09:33:52Z","published":"2025-08-26T13:14:53Z","title":"STDiff: A State Transition Diffusion Framework for Time Series Imputation in Industrial Systems","summary":"Incomplete sensor data is a major obstacle in industrial time-series analytics. In wastewater treatment plants (WWTPs), key sensors show long, irregular gaps caused by fouling, maintenance, and outages. We introduce STDiff and STDiff-W, diffusion-based imputers that cast gap filling as state-space simulation under partial observability, where targets, controls, and exogenous signals may all be intermittently missing. STDiff learns a one-step transition model conditioned on observed values and masks, while STDiff-W extends this with a context encoder that jointly inpaints contiguous blocks, combining long-range consistency with short-term detail. On two WWTP datasets (one with synthetic block gaps from Agtrup and another with natural outages from Avedøre), STDiff-W achieves state-of-the-art accuracy compared with strong neural baselines such as SAITS, BRITS, and CSDI. Beyond point-error metrics, its reconstructions preserve realistic dynamics including oscillations, spikes, and regime shifts, and they achieve top or tied-top downstream one-step forecasting performance compared with strong neural baselines, indicating that preserving dynamics does not come at the expense of predictive utility. Ablation studies that drop, shuffle, or add noise to control or exogenous inputs consistently degrade NH4 and PO4 performance, with the largest deterioration observed when exogenous signals are removed, showing that the model captures meaningful dependencies. We conclude with practical guidance for deployment: evaluate performance beyond MAE using task-oriented and visual checks, include exogenous drivers, and balance computational cost against robustness to structured outages.","authors":["Gary Simethy","Daniel Ortiz-Arroyo","Petar Durdevic"],"pdf_url":"","comment":"Peer-reviewed and published in Expert Systems with Applications, Volume 302 (2026). This version reflects the published article"},{"id":"http://arxiv.org/abs/2512.17373v1","updated":"2025-12-19T09:17:21Z","published":"2025-12-19T09:17:21Z","title":"Dialectics for Artificial Intelligence","summary":"Can artificial intelligence discover, from raw experience and without human supervision, concepts that humans have discovered? One challenge is that human concepts themselves are fluid: conceptual boundaries can shift, split, and merge as inquiry progresses (e.g., Pluto is no longer considered a planet). To make progress, we need a definition of \"concept\" that is not merely a dictionary label, but a structure that can be revised, compared, and aligned across agents. We propose an algorithmic-information viewpoint that treats a concept as an information object defined only through its structural relation to an agent's total experience. The core constraint is determination: a set of parts forms a reversible consistency relation if any missing part is recoverable from the others (up to the standard logarithmic slack in Kolmogorov-style identities). This reversibility prevents \"concepts\" from floating free of experience and turns concept existence into a checkable structural claim. To judge whether a decomposition is natural, we define excess information, measuring the redundancy overhead introduced by splitting experience into multiple separately described parts. On top of these definitions, we formulate dialectics as an optimization dynamics: as new patches of information appear (or become contested), competing concepts bid to explain them via shorter conditional descriptions, driving systematic expansion, contraction, splitting, and merging. Finally, we formalize low-cost concept transmission and multi-agent alignment using small grounds/seeds that allow another agent to reconstruct the same concept under a shared protocol, making communication a concrete compute-bits trade-off.","authors":["Zhengmian Hu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17370v1","updated":"2025-12-19T09:12:44Z","published":"2025-12-19T09:12:44Z","title":"TakeAD: Preference-based Post-optimization for End-to-end Autonomous Driving with Expert Takeover Data","summary":"Existing end-to-end autonomous driving methods typically rely on imitation learning (IL) but face a key challenge: the misalignment between open-loop training and closed-loop deployment. This misalignment often triggers driver-initiated takeovers and system disengagements during closed-loop execution. How to leverage those expert takeover data from disengagement scenarios and effectively expand the IL policy's capability presents a valuable yet unexplored challenge. In this paper, we propose TakeAD, a novel preference-based post-optimization framework that fine-tunes the pre-trained IL policy with this disengagement data to enhance the closed-loop driving performance. First, we design an efficient expert takeover data collection pipeline inspired by human takeover mechanisms in real-world autonomous driving systems. Then, this post optimization framework integrates iterative Dataset Aggregation (DAgger) for imitation learning with Direct Preference Optimization (DPO) for preference alignment. The DAgger stage equips the policy with fundamental capabilities to handle disengagement states through direct imitation of expert interventions. Subsequently, the DPO stage refines the policy's behavior to better align with expert preferences in disengagement scenarios. Through multiple iterations, the policy progressively learns recovery strategies for disengagement states, thereby mitigating the open-loop gap. Experiments on the closed-loop Bench2Drive benchmark demonstrate our method's effectiveness compared with pure IL methods, with comprehensive ablations confirming the contribution of each component.","authors":["Deqing Liu","Yinfeng Gao","Deheng Qian","Qichao Zhang","Xiaoqing Ye","Junyu Han","Yupeng Zheng","Xueyi Liu","Zhongpu Xia","Dawei Ding","Yifeng Pan","Dongbin Zhao"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2509.25123v3","updated":"2025-12-19T09:09:46Z","published":"2025-09-29T17:44:27Z","title":"From $f(x)$ and $g(x)$ to $f(g(x))$: LLMs Learn New Skills in RL by Composing Old Ones","summary":"Does RL teach LLMs genuinely new skills, or does it merely activate existing ones? This question lies at the core of ongoing debates about the role of RL in LLM post-training. On one side, strong empirical results can be achieved with RL even without preceding supervised finetuning; on the other, critics argue that RL contributes little beyond reweighting existing reasoning strategies. This work provides concrete evidence that LLMs can acquire genuinely new skills during RL by composing existing ones, mirroring one of the central mechanisms by which humans acquire new cognitive skills. To mitigate data contamination and other confounding factors, and to allow precise control over task complexity, we develop a synthetic framework for our investigation. Specifically, we define a skill as the ability to infer the output of a string transformation function f(x) given x. When an LLM has already learned f and g prior to RL, our experiments reveal that RL enables it to learn unseen compositions of them h(x)=g(f(x)). Further, this compositional ability generalizes to more difficult problems such as compositions of >2 functions unseen during RL training. Surprisingly, our experiments show that compositional skill acquired on a source task transfers to a different target task. This transfer happens even without compositional training on the target, requiring only prior knowledge of the target's atomic skills. Our qualitative analysis shows that RL fundamentally changes the reasoning behaviors of the models. In contrast, next-token training with the same data yields none of these findings. Our systematic experiments provide fresh insights into LLM learning, suggesting the value of first building base models with basic skills, then using RL to incentivize advanced, generalizable skills for complex problems.","authors":["Lifan Yuan","Weize Chen","Yuchen Zhang","Ganqu Cui","Hanbin Wang","Ziming You","Ning Ding","Zhiyuan Liu","Maosong Sun","Hao Peng"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.16529v2","updated":"2025-12-19T09:09:13Z","published":"2025-12-18T13:37:50Z","title":"ParamExplorer: A framework for exploring parameters in generative art","summary":"Generative art systems often involve high-dimensional and complex parameter spaces in which aesthetically compelling outputs occupy only small, fragmented regions. Because of this combinatorial explosion, artists typically rely on extensive manual trial-and-error, leaving many potentially interesting configurations undiscovered. In this work we make two contributions. First, we introduce ParamExplorer, an interactive and modular framework inspired by reinforcement learning that helps the exploration of parameter spaces in generative art algorithms, guided by human-in-the-loop or even automated feedback. The framework also integrates seamlessly with existing p5js projects. Second, within this framework we implement and evaluate several exploration strategies, referred to as agents.","authors":["Julien Gachadoat","Guillaume Lagarde"],"pdf_url":"","comment":"16 pages, 3 figures"},{"id":"http://arxiv.org/abs/2507.23358v2","updated":"2025-12-19T08:52:20Z","published":"2025-07-31T09:08:59Z","title":"Text-to-SQL Task-oriented Dialogue Ontology Construction","summary":"Large language models (LLMs) are widely used as general-purpose knowledge sources, but they rely on parametric knowledge, limiting explainability and trustworthiness. In task-oriented dialogue (TOD) systems, this separation is explicit, using an external database structured by an explicit ontology to ensure explainability and controllability. However, building such ontologies requires manual labels or supervised training. We introduce TeQoDO: a Text-to-SQL task-oriented Dialogue Ontology construction method. Here, an LLM autonomously builds a TOD ontology from scratch using only its inherent SQL programming capabilities combined with concepts from modular TOD systems provided in the prompt. We show that TeQoDO outperforms transfer learning approaches, and its constructed ontology is competitive on a downstream dialogue state tracking task. Ablation studies demonstrate the key role of modular TOD system concepts. TeQoDO also scales to allow construction of much larger ontologies, which we investigate on a Wikipedia and arXiv dataset. We view this as a step towards broader application of ontologies.","authors":["Renato Vukovic","Carel van Niekerk","Michael Heck","Benjamin Ruppik","Hsien-Chin Lin","Shutong Feng","Nurul Lubis","Milica Gasic"],"pdf_url":"","comment":"Accepted to Transactions of the Association for Computational Linguistics"},{"id":"http://arxiv.org/abs/2512.17352v1","updated":"2025-12-19T08:48:36Z","published":"2025-12-19T08:48:36Z","title":"Adaptive Graph Pruning with Sudden-Events Evaluation for Traffic Prediction using Online Semi-Decentralized ST-GNNs","summary":"Spatio-Temporal Graph Neural Networks (ST-GNNs) are well-suited for processing high-frequency data streams from geographically distributed sensors in smart mobility systems. However, their deployment at the edge across distributed compute nodes (cloudlets) createssubstantial communication overhead due to repeated transmission of overlapping node features between neighbouring cloudlets. To address this, we propose an adaptive pruning algorithm that dynamically filters redundant neighbour features while preserving the most informative spatial context for prediction. The algorithm adjusts pruning rates based on recent model performance, allowing each cloudlet to focus on regions experiencing traffic changes without compromising accuracy. Additionally, we introduce the Sudden Event Prediction Accuracy (SEPA), a novel event-focused metric designed to measure responsiveness to traffic slowdowns and recoveries, which are often missed by standard error metrics. We evaluate our approach in an online semi-decentralized setting with traditional FL, server-free FL, and Gossip Learning on two large-scale traffic datasets, PeMS-BAY and PeMSD7-M, across short-, mid-, and long-term prediction horizons. Experiments show that, in contrast to standard metrics, SEPA exposes the true value of spatial connectivity in predicting dynamic and irregular traffic. Our adaptive pruning algorithm maintains prediction accuracy while significantly lowering communication cost in all online semi-decentralized settings, demonstrating that communication can be reduced without compromising responsiveness to critical traffic events.","authors":["Ivan Kralj","Lodovico Giaretta","Gordan Ježić","Ivana Podnar Žarko","Šarūnas Girdzijauskas"],"pdf_url":"","comment":"19 pages, 6 figures, 5 tables, journal"},{"id":"http://arxiv.org/abs/2511.10008v2","updated":"2025-12-19T08:41:28Z","published":"2025-11-13T06:24:28Z","title":"Phantom Menace: Exploring and Enhancing the Robustness of VLA Models Against Physical Sensor Attacks","summary":"Vision-Language-Action (VLA) models revolutionize robotic systems by enabling end-to-end perception-to-action pipelines that integrate multiple sensory modalities, such as visual signals processed by cameras and auditory signals captured by microphones. This multi-modality integration allows VLA models to interpret complex, real-world environments using diverse sensor data streams. Given the fact that VLA-based systems heavily rely on the sensory input, the security of VLA models against physical-world sensor attacks remains critically underexplored. To address this gap, we present the first systematic study of physical sensor attacks against VLAs, quantifying the influence of sensor attacks and investigating the defenses for VLA models. We introduce a novel \"Real-Sim-Real\" framework that automatically simulates physics-based sensor attack vectors, including six attacks targeting cameras and two targeting microphones, and validates them on real robotic systems. Through large-scale evaluations across various VLA architectures and tasks under varying attack parameters, we demonstrate significant vulnerabilities, with susceptibility patterns that reveal critical dependencies on task types and model designs. We further develop an adversarial-training-based defense that enhances VLA robustness against out-of-distribution physical perturbations caused by sensor attacks while preserving model performance. Our findings expose an urgent need for standardized robustness benchmarks and mitigation strategies to secure VLA deployments in safety-critical environments.","authors":["Xuancun Lu","Jiaxiang Chen","Shilin Xiao","Zizhi Jin","Zhangrui Chen","Hanwen Yu","Bohan Qian","Ruochen Zhou","Xiaoyu Ji","Wenyuan Xu"],"pdf_url":"","comment":"Accepted by AAAI 2026 main track"},{"id":"http://arxiv.org/abs/2503.19041v4","updated":"2025-12-19T08:17:42Z","published":"2025-03-24T18:11:42Z","title":"LookAhead Tuning: Safer Language Models via Partial Answer Previews","summary":"Fine-tuning enables large language models (LLMs) to adapt to specific domains, but often compromises their previously established safety alignment. To mitigate the degradation of model safety during fine-tuning, we introduce LookAhead Tuning, a lightweight and effective data-driven approach that preserves safety during fine-tuning. The method introduces two simple strategies that modify training data by previewing partial answer prefixes, thereby minimizing perturbations to the model's initial token distributions and maintaining its built-in safety mechanisms. Comprehensive experiments demonstrate that LookAhead Tuning effectively maintains model safety without sacrificing robust performance on downstream tasks. Our findings position LookAhead Tuning as a reliable and efficient solution for the safe and effective adaptation of LLMs.","authors":["Kangwei Liu","Mengru Wang","Yujie Luo","Lin Yuan","Mengshu Sun","Lei Liang","Zhiqiang Zhang","Jun Zhou","Bryan Hooi","Shumin Deng"],"pdf_url":"","comment":"WSDM 2026 short"},{"id":"http://arxiv.org/abs/2502.01436v3","updated":"2025-12-19T08:17:09Z","published":"2025-02-03T15:19:28Z","title":"Towards Safer Chatbots: Automated Policy Compliance Evaluation of Custom GPTs","summary":"User-configured chatbots built on top of large language models are increasingly available through centralized marketplaces such as OpenAI's GPT Store. While these platforms enforce usage policies intended to prevent harmful or inappropriate behavior, the scale and opacity of customized chatbots make systematic policy enforcement challenging. As a result, policy-violating chatbots continue to remain publicly accessible despite existing review processes. This paper presents a fully automated method for evaluating the compliance of Custom GPTs with its marketplace usage policy using black-box interaction. The method combines large-scale GPT discovery, policy-driven red-teaming prompts, and automated compliance assessment using an LLM-as-a-judge. We focus on three policy-relevant domains explicitly addressed in OpenAI's usage policies: Romantic, Cybersecurity, and Academic GPTs. We validate our compliance assessment component against a human-annotated ground-truth dataset, achieving an F1 score of 0.975 for binary policy violation detection. We then apply the method in a large-scale empirical study of 782 Custom GPTs retrieved from the GPT Store. The results show that 58.7% of the evaluated GPTs exhibit at least one policy-violating response, with substantial variation across policy domains. A comparison with the base models (GPT-4 and GPT-4o) indicates that most violations originate from model-level behavior, while customization tends to amplify these tendencies rather than create new failure modes. Our findings reveal limitations in current review mechanisms for user-configured chatbots and demonstrate the feasibility of scalable, behavior-based policy compliance evaluation.","authors":["David Rodriguez","William Seymour","Jose M. Del Alamo","Jose Such"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17319v1","updated":"2025-12-19T08:07:51Z","published":"2025-12-19T08:07:51Z","title":"A Benchmark for Ultra-High-Resolution Remote Sensing MLLMs","summary":"Multimodal large language models (MLLMs) demonstrate strong perception and reasoning performance on existing remote sensing (RS) benchmarks. However, most prior benchmarks rely on low-resolution imagery, and some high-resolution benchmarks suffer from flawed reasoning-task designs. We show that text-only LLMs can perform competitively with multimodal vision-language models on RS reasoning tasks without access to images, revealing a critical mismatch between current benchmarks and the intended evaluation of visual understanding. To enable faithful assessment, we introduce RSHR-Bench, a super-high-resolution benchmark for RS visual understanding and reasoning. RSHR-Bench contains 5,329 full-scene images with a long side of at least 4,000 pixels, with up to about 3 x 10^8 pixels per image, sourced from widely used RS corpora and UAV collections. We design four task families: multiple-choice VQA, open-ended VQA, image captioning, and single-image evaluation. These tasks cover nine perception categories and four reasoning types, supporting multi-turn and multi-image dialog. To reduce reliance on language priors, we apply adversarial filtering with strong LLMs followed by rigorous human verification. Overall, we construct 3,864 VQA tasks, 3,913 image captioning tasks, and 500 fully human-written or verified single-image evaluation VQA pairs. Evaluations across open-source, closed-source, and RS-specific VLMs reveal persistent performance gaps in super-high-resolution scenarios. Code: https://github.com/Yunkaidang/RSHR","authors":["Yunkai Dang","Meiyi Zhu","Donghao Wang","Yizhuo Zhang","Jiacheng Yang","Qi Fan","Yuekun Yang","Wenbin Li","Feng Miao","Yang Gao"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.12284v2","updated":"2025-12-19T08:02:44Z","published":"2025-12-13T11:02:04Z","title":"V-Rex: Real-Time Streaming Video LLM Acceleration via Dynamic KV Cache Retrieval","summary":"Streaming video large language models (LLMs) are increasingly used for real-time multimodal tasks such as video captioning, question answering, conversational agents, and augmented reality. However, these models face fundamental memory and computational challenges because their key-value (KV) caches grow substantially with continuous streaming video input. This process requires an iterative prefill stage, which is a unique feature of streaming video LLMs. Due to its iterative prefill stage, it suffers from significant limitations, including extensive computation, substantial data transfer, and degradation in accuracy. Crucially, this issue is exacerbated for edge deployment, which is the primary target for these models.\n  In this work, we propose V-Rex, the first software-hardware co-designed accelerator that comprehensively addresses both algorithmic and hardware bottlenecks in streaming video LLM inference. At its core, V-Rex introduces ReSV, a training-free dynamic KV cache retrieval algorithm. ReSV exploits temporal and spatial similarity-based token clustering to reduce excessive KV cache memory across video frames. To fully realize these algorithmic benefits, V-Rex offers a compact, low-latency hardware accelerator with a dynamic KV cache retrieval engine (DRE), featuring bit-level and early-exit based computing units. V-Rex achieves unprecedented real-time of 3.9-8.3 FPS and energy-efficient streaming video LLM inference on edge deployment with negligible accuracy loss. While DRE only accounts for 2.2% power and 2.0% area, the system delivers 1.9-19.7x speedup and 3.1-18.5x energy efficiency improvements over AGX Orin GPU. This work is the first to comprehensively tackle KV cache retrieval across algorithms and hardware, enabling real-time streaming video LLM inference on resource-constrained edge devices.","authors":["Donghyuk Kim","Sejeong Yang","Wonjin Shin","Joo-Young Kim"],"pdf_url":"","comment":"14 pages, 20 figures, conference"},{"id":"http://arxiv.org/abs/2512.17316v1","updated":"2025-12-19T07:59:36Z","published":"2025-12-19T07:59:36Z","title":"Explanation Beyond Intuition: A Testable Criterion for Inherent Explainability","summary":"Inherent explainability is the gold standard in Explainable Artificial Intelligence (XAI). However, there is not a consistent definition or test to demonstrate inherent explainability. Work to date either characterises explainability through metrics, or appeals to intuition - \"we know it when we see it\". We propose a globally applicable criterion for inherent explainability. The criterion uses graph theory for representing and decomposing models for structure-local explanation, and recomposing them into global explanations. We form the structure-local explanations as annotations, a verifiable hypothesis-evidence structure that allows for a range of explanatory methods to be used. This criterion matches existing intuitions on inherent explainability, and provides justifications why a large regression model may not be explainable but a sparse neural network could be. We differentiate explainable -- a model that allows for explanation -- and \\textit{explained} -- one that has a verified explanation. Finally, we provide a full explanation of PREDICT -- a Cox proportional hazards model of cardiovascular disease risk, which is in active clinical use in New Zealand. It follows that PREDICT is inherently explainable. This work provides structure to formalise other work on explainability, and allows regulators a flexible but rigorous test that can be used in compliance frameworks.","authors":["Michael Merry","Pat Riddle","Jim Warren"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17308v1","updated":"2025-12-19T07:46:29Z","published":"2025-12-19T07:46:29Z","title":"Large Language Models as Pokémon Battle Agents: Strategic Play and Content Generation","summary":"Strategic decision-making in Pokémon battles presents a unique testbed for evaluating large language models. Pokémon battles demand reasoning about type matchups, statistical trade-offs, and risk assessment, skills that mirror human strategic thinking. This work examines whether Large Language Models (LLMs) can serve as competent battle agents, capable of both making tactically sound decisions and generating novel, balanced game content. We developed a turn-based Pokémon battle system where LLMs select moves based on battle state rather than pre-programmed logic. The framework captures essential Pokémon mechanics: type effectiveness multipliers, stat-based damage calculations, and multi-Pokémon team management. Through systematic evaluation across multiple model architectures we measured win rates, decision latency, type-alignment accuracy, and token efficiency. These results suggest LLMs can function as dynamic game opponents without domain-specific training, offering a practical alternative to reinforcement learning for turn-based strategic games. The dual capability of tactical reasoning and content creation, positions LLMs as both players and designers, with implications for procedural generation and adaptive difficulty systems in interactive entertainment.","authors":["Daksh Jain","Aarya Jain","Ashutosh Desai","Avyakt Verma","Ishan Bhanuka","Pratik Narang","Dhruv Kumar"],"pdf_url":"","comment":"Under Review"},{"id":"http://arxiv.org/abs/2510.11176v2","updated":"2025-12-19T07:32:01Z","published":"2025-10-13T09:08:59Z","title":"G2L:From Giga-Scale to Cancer-Specific Large-Scale Pathology Foundation Models via Knowledge Distillation","summary":"Recent studies in pathology foundation models have shown that scaling training data, diversifying cancer types, and increasing model size consistently improve their performance. However, giga-scale foundation models, which are trained on hundreds of thousands of slides covering tens of cancer types and contain billions of parameters, pose significant challenges for practical use due to their tremendous computational costs in both development and deployment. In this work, we present a novel strategy, named the G2L framework, to increase the performance of large-scale foundation models, which consist of only $15\\%$ of the parameters of giga-scale models, to a comparable performance level of giga-scale models in cancer-specific tasks. Our approach applies knowledge distillation, transferring the capabilities of a giga-scale model to a large-scale model, using just 1K pathology slides of a target cancer (e.g., breast, prostate, etc.). The resulting distilled model not only outperformed state-of-the-art models of the same size (i.e., large-scale) across several benchmarks but also, interestingly, surpassed the giga-scale teacher and huge-scale models in some benchmarks. In addition, the distilled model exhibited a higher robustness index, indicating improved resilience to image variations originating from multiple institutions. These findings suggest that the proposed distillation approach for a large-scale model is a data- and parameter-efficient way to achieve giga-scale-level performance for cancer-specific applications without prohibitive computational burden.","authors":["Yesung Cho","Sungmin Lee","Geongyu Lee","Minkyung Lee","Jongbae Park","Dongmyung Shin"],"pdf_url":"","comment":"Accepted in AAAI 2024 workshop in Health Intelligence Special Theme on Foundation Models and AI Agents"},{"id":"http://arxiv.org/abs/2512.17299v1","updated":"2025-12-19T07:27:30Z","published":"2025-12-19T07:27:30Z","title":"M2RU: Memristive Minion Recurrent Unit for Continual Learning at the Edge","summary":"Continual learning on edge platforms remains challenging because recurrent networks depend on energy-intensive training procedures and frequent data movement that are impractical for embedded deployments. This work introduces M2RU, a mixed-signal architecture that implements the minion recurrent unit for efficient temporal processing with on-chip continual learning. The architecture integrates weighted-bit streaming, which enables multi-bit digital inputs to be processed in crossbars without high-resolution conversion, and an experience replay mechanism that stabilizes learning under domain shifts. M2RU achieves 15 GOPS at 48.62 mW, corresponding to 312 GOPS per watt, and maintains accuracy within 5 percent of software baselines on sequential MNIST and CIFAR-10 tasks. Compared with a CMOS digital design, the accelerator provides 29X improvement in energy efficiency. Device-aware analysis shows an expected operational lifetime of 12.2 years under continual learning workloads. These results establish M2RU as a scalable and energy-efficient platform for real-time adaptation in edge-level temporal intelligence.","authors":["Abdullah M. Zyarah","Dhireesha Kudithipudi"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2509.22818v2","updated":"2025-12-19T07:26:23Z","published":"2025-09-26T18:24:22Z","title":"Can Large Language Models Develop Gambling Addiction?","summary":"This study identifies the specific conditions under which large language models exhibit human-like gambling addiction patterns, providing critical insights into their decision-making mechanisms and AI safety. We analyze LLM decision-making at cognitive-behavioral and neural levels based on human addiction research. In slot machine experiments, we identified cognitive features such as illusion of control and loss chasing, observing that greater autonomy in betting parameters substantially amplified irrational behavior and bankruptcy rates. Neural circuit analysis using a Sparse Autoencoder confirmed that model behavior is controlled by abstract decision-making features related to risk, not merely by prompts. These findings suggest LLMs internalize human-like cognitive biases beyond simply mimicking training data.","authors":["Seungpil Lee","Donghyeon Shin","Yunjeong Lee","Sundong Kim"],"pdf_url":"","comment":"26 pages, 14 figures"},{"id":"http://arxiv.org/abs/2512.13478v4","updated":"2025-12-19T07:21:39Z","published":"2025-12-15T16:14:32Z","title":"Non-Resolution Reasoning (NRR): A Computational Framework for Contextual Identity and Ambiguity Preservation","summary":"Current artificial intelligence systems, despite remarkable capabilities in text generation and pattern recognition, exhibit a fundamental architectural limitation: they resolve ambiguity prematurely. This premature semantic collapse -- the tendency to collapse multiple valid interpretations into a single output -- stems from classical identity assumptions embedded in standard neural architectures. We propose Non-Resolution Reasoning (NRR), a computational framework that treats ambiguity retention as a valid reasoning mode rather than a defect to be eliminated. NRR introduces three core principles: (1) Non-Identity ($A \\neq A$) -- the same symbol refers to different entities across contexts; (2) Approximate Identity ($A \\approx A$) -- entities share partial structural overlap without being identical; and (3) Non-Resolution -- conflicting interpretations can coexist without forced convergence. We formalize these principles through three architectural components: Multi-Vector Embeddings for context-dependent representation, Non-Collapsing Attention for parallel interpretation retention, and Contextual Identity Tracking (CIT) for maintaining $A \\neq A$ across inference. We demonstrate NRR's advantages through case studies in paradox handling, creative generation, and context-dependent reasoning. Crucially, we provide a minimal empirical validation on a synthetic context-shift task where an NRR-lite model achieves 90.9% out-of-distribution accuracy compared to 9.1% for standard architectures, demonstrating that ambiguity preservation enables structural generalization. NRR challenges the assumption that meaning must collapse to be useful, offering a foundation for AI systems capable of sophisticated ambiguity handling and creative reasoning. The question is not whether AI should resolve ambiguity, but when, how, and under whose control.","authors":["Kei Saito"],"pdf_url":"","comment":"16 pages, 1 figure. Updated version with corrected references and aligned acknowledgments"},{"id":"http://arxiv.org/abs/2512.17293v1","updated":"2025-12-19T07:17:43Z","published":"2025-12-19T07:17:43Z","title":"Robust TTS Training via Self-Purifying Flow Matching for the WildSpoof 2026 TTS Track","summary":"This paper presents a lightweight text-to-speech (TTS) system developed for the WildSpoof Challenge TTS Track. Our approach fine-tunes the recently released open-weight TTS model, \\textit{Supertonic}\\footnote{\\url{https://github.com/supertone-inc/supertonic}}, with Self-Purifying Flow Matching (SPFM) to enable robust adaptation to in-the-wild speech. SPFM mitigates label noise by comparing conditional and unconditional flow matching losses on each sample, routing suspicious text--speech pairs to unconditional training while still leveraging their acoustic information. The resulting model achieves the lowest Word Error Rate (WER) among all participating teams, while ranking second in perceptual metrics such as UTMOS and DNSMOS. These findings demonstrate that efficient, open-weight architectures like Supertonic can be effectively adapted to diverse real-world speech conditions when combined with explicit noise-handling mechanisms such as SPFM.","authors":["June Young Yi","Hyeongju Kim","Juheon Lee"],"pdf_url":"","comment":"2 pages, preprint, This work has been submitted to the IEEE for possible publication. Submitted to ICASSP 2026 SPGC (WildSpoof Challenge, TTS track)"},{"id":"http://arxiv.org/abs/2512.14806v3","updated":"2025-12-19T07:14:17Z","published":"2025-12-16T18:51:23Z","title":"Let the Barbarians In: How AI Can Accelerate Systems Performance Research","summary":"Artificial Intelligence (AI) is beginning to transform the research process by automating the discovery of new solutions. This shift depends on the availability of reliable verifiers, which AI-driven approaches require to validate candidate solutions. Research focused on improving systems performance is especially well-suited to this paradigm because system performance problems naturally admit such verifiers: candidates can be implemented in real systems or simulators and evaluated against predefined workloads. We term this iterative cycle of generation, evaluation, and refinement AI-Driven Research for Systems (ADRS). Using several open-source ADRS instances (i.e., OpenEvolve, GEPA, and ShinkaEvolve), we demonstrate across ten case studies (e.g., multi-region cloud scheduling, mixture-of-experts load balancing, LLM-based SQL, transaction scheduling) that ADRS-generated solutions can match or even outperform human state-of-the-art designs. Based on these findings, we outline best practices (e.g., level of prompt specification, amount of feedback, robust evaluation) for effectively using ADRS, and we discuss future research directions and their implications. Although we do not yet have a universal recipe for applying ADRS across all of systems research, we hope our preliminary findings, together with the challenges we identify, offer meaningful guidance for future work as researcher effort shifts increasingly toward problem formulation and strategic oversight.\n  Note: This paper is an extension of our prior work [14]. It adds extensive evaluation across multiple ADRS frameworks and provides deeper analysis and insights into best practices.","authors":["Audrey Cheng","Shu Liu","Melissa Pan","Zhifei Li","Shubham Agarwal","Mert Cemri","Bowen Wang","Alexander Krentsel","Tian Xia","Jongseok Park","Shuo Yang","Jeff Chen","Lakshya Agrawal","Ashwin Naren","Shulu Li","Ruiying Ma","Aditya Desai","Jiarong Xing","Koushik Sen","Matei Zaharia","Ion Stoica"],"pdf_url":"","comment":"arXiv admin note: substantial text overlap with arXiv:2510.06189"},{"id":"http://arxiv.org/abs/2510.16882v2","updated":"2025-12-19T07:13:05Z","published":"2025-10-19T15:32:01Z","title":"Utility-Diversity Aware Online Batch Selection for LLM Supervised Fine-tuning","summary":"Supervised fine-tuning (SFT) is a commonly used technique to adapt large language models (LLMs) to downstream tasks. In practice, SFT on a full dataset is computationally expensive and sometimes suffers from overfitting or bias amplification. This facilitates the rise of data curation in SFT, which prioritizes the most valuable data to optimze. This work studies the online batch selection family that dynamically scores and filters samples during the training process. However, existing popular methods often (i) rely merely on the utility of data to select a subset while neglecting other crucial factors like diversity, (ii) rely on external resources such as reference models or validation sets, and (iii) incur extra training time over full-dataset training. To address these limitations, this work develops \\textbf{UDS (Utility-Diversity Sampling)}, a framework for efficient online batch selection in SFT. UDS leverages the nuclear norm of the logits matrix to capture both data utility and intra-sample diversity, while estimating inter-sample diversity through efficient low-dimensional embedding comparisons with a lightweight memory buffer of historical samples. Such a design eliminates the need for external resources and unnecessary backpropagation, securing computational efficiency. Experiments on multiple benchmarks demonstrate that UDS consistently outperforms state-of-the-art online batch selection methods under varying data budgets, and significantly reduces training time compared to full-dataset fine-tuning. Code is available at https://github.com/gfyddha/UDS.","authors":["Heming Zou","Yixiu Mao","Yun Qu","Qi Wang","Xiangyang Ji"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17289v1","updated":"2025-12-19T07:11:50Z","published":"2025-12-19T07:11:50Z","title":"Subjective Question Generation and Answer Evaluation using NLP","summary":"Natural Language Processing (NLP) is one of the most revolutionary technologies today. It uses artificial intelligence to understand human text and spoken words. It is used for text summarization, grammar checking, sentiment analysis, and advanced chatbots and has many more potential use cases. Furthermore, it has also made its mark on the education sector. Much research and advancements have already been conducted on objective question generation; however, automated subjective question generation and answer evaluation are still in progress. An automated system to generate subjective questions and evaluate the answers can help teachers assess student work and enhance the student's learning experience by allowing them to self-assess their understanding after reading an article or a chapter of a book. This research aims to improve current NLP models or make a novel one for automated subjective question generation and answer evaluation from text input.","authors":["G. M. Refatul Islam","Safwan Shaheer","Yaseen Nur","Mohammad Rafid Hamid"],"pdf_url":"","comment":"5 pages, 5 figures, 2 tables, conference paper"},{"id":"http://arxiv.org/abs/2511.00040v2","updated":"2025-12-19T06:56:17Z","published":"2025-10-28T01:33:43Z","title":"Semi-Supervised Preference Optimization with Limited Feedback","summary":"The field of preference optimization has made outstanding contributions to the alignment of language models with human preferences. Despite these advancements, recent methods still rely heavily on substantial paired (labeled) feedback data, leading to substantial resource expenditures. To address these challenges, we study the problem of Semi-Supervised Preference Optimization (SSPO) in which the idea is to learn from both a small number of pairwise preference labels and a large pool of unpaired samples simultaneously. Our key theoretical contribution proves the existence of an optimal reward threshold capable of separating winning and losing responses with high probability, which enables a principled pseudo-labeling of unpaired data. By leveraging these pseudo-labels, SSPO effectively distills latent preferences from large-scale unpaired data, thus maintaining human alignment while drastically reducing acquisition costs. Extensive experiments across datasets validate this remarkable data efficiency; for instance, SSPO trained with Mistral-7B-Instruct on just 1% of UltraFeedback consistently surpasses strong baselines trained on 10% of UltraFeedback.","authors":["Seonggyun Lee","Sungjun Lim","Seojin Park","Soeun Cheon","Kyungwoo Song"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17278v1","updated":"2025-12-19T06:50:03Z","published":"2025-12-19T06:50:03Z","title":"WDFFU-Mamba: A Wavelet-guided Dual-attention Feature Fusion Mamba for Breast Tumor Segmentation in Ultrasound Images","summary":"Breast ultrasound (BUS) image segmentation plays a vital role in assisting clinical diagnosis and early tumor screening. However, challenges such as speckle noise, imaging artifacts, irregular lesion morphology, and blurred boundaries severely hinder accurate segmentation. To address these challenges, this work aims to design a robust and efficient model capable of automatically segmenting breast tumors in BUS images.We propose a novel segmentation network named WDFFU-Mamba, which integrates wavelet-guided enhancement and dual-attention feature fusion within a U-shaped Mamba architecture. A Wavelet-denoised High-Frequency-guided Feature (WHF) module is employed to enhance low-level representations through noise-suppressed high-frequency cues. A Dual Attention Feature Fusion (DAFF) module is also introduced to effectively merge skip-connected and semantic features, improving contextual consistency.Extensive experiments on two public BUS datasets demonstrate that WDFFU-Mamba achieves superior segmentation accuracy, significantly outperforming existing methods in terms of Dice coefficient and 95th percentile Hausdorff Distance (HD95).The combination of wavelet-domain enhancement and attention-based fusion greatly improves both the accuracy and robustness of BUS image segmentation, while maintaining computational efficiency.The proposed WDFFU-Mamba model not only delivers strong segmentation performance but also exhibits desirable generalization ability across datasets, making it a promising solution for real-world clinical applications in breast tumor ultrasound analysis.","authors":["Guoping Cai","Houjin Chen","Yanfeng Li","Jia Sun","Ziwei Chen","Qingzi Geng"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.14121v2","updated":"2025-12-19T06:44:37Z","published":"2025-12-16T06:05:55Z","title":"SportsGPT: An LLM-driven Framework for Interpretable Sports Motion Assessment and Training Guidance","summary":"Existing intelligent sports analysis systems mainly focus on \"scoring and visualization,\" often lacking automatic performance diagnosis and interpretable training guidance. Recent advances in Large Language Models (LLMs) and motion analysis techniques provide new opportunities to address the above limitations. In this paper, we propose SportsGPT, an LLM-driven framework for interpretable sports motion assessment and training guidance, which establishes a closed loop from motion time-series input to professional training guidance. First, given a set of high-quality target models, we introduce MotionDTW, a two-stage time series alignment algorithm designed for accurate keyframe extraction from skeleton-based motion sequences. Subsequently, we design a Knowledge-based Interpretable Sports Motion Assessment Model (KISMAM) to obtain a set of interpretable assessment metrics (e.g., insufficient extension) by contrasting the keyframes with the target models. Finally, we propose SportsRAG, a RAG-based training guidance model built upon Qwen3. Leveraging a 6B-token knowledge base, it prompts the LLM to generate professional training guidance by retrieving domain-specific QA pairs. Experimental results demonstrate that MotionDTW significantly outperforms traditional methods with lower temporal error and higher IoU scores. Furthermore, ablation studies validate the KISMAM and SportsRAG, confirming that SportsGPT surpasses general LLMs in diagnostic accuracy and professionalism.","authors":["Wenbo Tian","Ruting Lin","Hongxian Zheng","Yaodong Yang","Geng Wu","Zihao Zhang","Zhang Zhang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17270v1","updated":"2025-12-19T06:37:44Z","published":"2025-12-19T06:37:44Z","title":"Understanding Generalization in Role-Playing Models via Information Theory","summary":"Role-playing models (RPMs) are widely used in real-world applications but underperform when deployed in the wild. This degradation can be attributed to distribution shifts, including user, character, and dialogue compositional shifts. Existing methods like LLM-as-a-judge fall short in providing a fine-grained diagnosis of how these shifts affect RPM generalization, and thus there lack formal frameworks to characterize RPM generalization behaviors. To bridge these gaps, we introduce an information-theoretic metric, named reasoning-based effective mutual information difference (R-EMID), to measure RPM performance degradation in an interpretable way. We also derive an upper bound on R-EMID to predict the worst-case generalization performance of RPMs and theoretically reveal how various shifts contribute to the RPM performance degradation. Moreover, we propose a co-evolving reinforcement learning framework to adaptively model the connection among user, character, and dialogue context and thus enhance the estimation of dialogue response generation probability, which is critical for calculating R-EMID. Finally, we evaluate the generalization performance of various RPMs using R-EMID, finding that user shift poses the highest risk among all shifts and reinforcement learning is the most effective approach for enhancing RPM generalization.","authors":["Yongqi Li","Hao Lang","Fei Huang","Tieyun Qian","Yongbin Li"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17267v1","updated":"2025-12-19T06:32:46Z","published":"2025-12-19T06:32:46Z","title":"AutoMetrics: Approximate Human Judgements with Automatically Generated Evaluators","summary":"Evaluating user-facing AI applications remains a central challenge, especially in open-ended domains such as travel planning, clinical note generation, or dialogue. The gold standard is user feedback (e.g., thumbs up/down) or behavioral signals (e.g., retention), but these are often scarce in prototypes and research projects, or too-slow to use for system optimization. We present AutoMetrics, a framework for synthesizing evaluation metrics under low-data constraints. AutoMetrics combines retrieval from MetricBank, a collection of 48 metrics we curate, with automatically generated LLM-as-a-Judge criteria informed by lightweight human feedback. These metrics are composed via regression to maximize correlation with human signal. AutoMetrics takes you from expensive measures to interpretable automatic metrics. Across 5 diverse tasks, AutoMetrics improves Kendall correlation with human ratings by up to 33.4% over LLM-as-a-Judge while requiring fewer than 100 feedback points. We show that AutoMetrics can be used as a proxy reward to equal effect as a verifiable reward. We release the full AutoMetrics toolkit and MetricBank to accelerate adaptive evaluation of LLM applications.","authors":["Michael J. Ryan","Yanzhe Zhang","Amol Salunkhe","Yi Chu","Di Xu","Diyi Yang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17266v1","updated":"2025-12-19T06:30:11Z","published":"2025-12-19T06:30:11Z","title":"ScoutGPT: Capturing Player Impact from Team Action Sequences Using GPT-Based Framework","summary":"Transfers play a pivotal role in shaping a football club's success, yet forecasting whether a transfer will succeed remains difficult due to the strong context-dependence of on-field performance. Existing evaluation practices often rely on static summary statistics or post-hoc value models, which fail to capture how a player's contribution adapts to a new tactical environment or different teammates. To address this gap, we introduce EventGPT, a player-conditioned, value-aware next-event prediction model built on a GPT-style autoregressive transformer. Our model treats match play as a sequence of discrete tokens, jointly learning to predict the next on-ball action's type, location, timing, and its estimated residual On-Ball Value (rOBV) based on the preceding context and player identity. A key contribution of this framework is the ability to perform counterfactual simulations. By substituting learned player embeddings into new event sequences, we can simulate how a player's behavioral distribution and value profile would change when placed in a different team or tactical structure. Evaluated on five seasons of Premier League event data, EventGPT outperforms existing sequence-based baselines in next-event prediction accuracy and spatial precision. Furthermore, we demonstrate the model's practical utility for transfer analysis through case studies-such as comparing striker performance across different systems and identifying stylistic replacements for specific roles-showing that our approach provides a principled method for evaluating transfer fit.","authors":["Miru Hong","Minho Lee","Geonhee Jo","Jae-Hee So","Pascal Bauer","Sang-Ki Ko"],"pdf_url":"","comment":"8 pages, 2 figures, 7 tables. To appear in Hudl Performance Insights 2025"},{"id":"http://arxiv.org/abs/2503.22182v2","updated":"2025-12-19T06:24:33Z","published":"2025-03-28T07:00:33Z","title":"Sell It Before You Make It: Revolutionizing E-Commerce with Personalized AI-Generated Items","summary":"E-commerce has revolutionized retail, yet its traditional workflows remain inefficient, with significant resource costs tied to product design and inventory. This paper introduces a novel system deployed at Alibaba that uses AI-generated items (AIGI) to address these challenges with personalized text-to-image generation for e-commerce product design. AIGI enables an innovative business mode called \"sell it before you make it\", where merchants can design fashion items and generate photorealistic images with digital models based on textual descriptions. Only when the items have received a certain number of orders, do the merchants start to produce them, which largely reduces reliance on physical prototypes and thus accelerates time to market. For such a promising application, we identify the underlying key scientific challenge, i.e., capturing users' group-level personalized preferences towards multiple generated images. To this end, we propose a Personalized Group-Level Preference Alignment Framework for Diffusion Models (PerFusion). We first design PerFusion Reward Model for user preference estimation with a feature-crossing-based personalized plug-in. Then we develop PerFusion with a personalized adaptive network to model diverse preferences across users, and meanwhile derive the group-level preference optimization objective to model comparative behaviors among multiple images. Both offline and online experiments demonstrate the effectiveness of our proposed algorithm. The AI-generated items achieve over 13% relative improvements for both click-through rate and conversion rate, as well as 7.9% decrease in return rate, compared to their human-designed counterparts, validating the transformative potential of AIGI for e-commerce platforms.","authors":["Jianghao Lin","Peng Du","Jiaqi Liu","Weite Li","Yong Yu","Weinan Zhang","Yang Cao"],"pdf_url":"","comment":"Accepted by KDD 2026 ADS Track"},{"id":"http://arxiv.org/abs/2505.20112v3","updated":"2025-12-19T06:23:19Z","published":"2025-05-26T15:14:54Z","title":"ResSVD: Residual Compensated SVD for Large Language Model Compression","summary":"Large language models (LLMs) have demonstrated impressive capabilities in a wide range of downstream natural language processing tasks. Nevertheless, their considerable sizes and memory demands hinder practical deployment, underscoring the importance of developing efficient compression strategies. Singular value decomposition (SVD) decomposes a matrix into orthogonal components, enabling efficient low-rank approximation. This is particularly suitable for LLM compression, where weight matrices often exhibit significant redundancy. However, current SVD-based methods neglect the residual matrix from truncation, resulting in significant truncation loss. Additionally, compressing all layers of the model results in severe performance degradation. To overcome these limitations, we propose ResSVD, a new post-training SVD-based LLM compression method. Specifically, we leverage the residual matrix generated during the truncation process to reduce truncation loss. Moreover, under a fixed overall compression ratio, we selectively compress the last few layers of the model, which mitigates error propagation and significantly improves the performance of compressed models. Comprehensive evaluations of ResSVD on diverse LLM families and multiple benchmark datasets indicate that ResSVD consistently achieves superior performance over existing counterpart methods, demonstrating its practical effectiveness.","authors":["Haolei Bai","Siyong Jian","Tuo Liang","Yu Yin","Huan Wang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17259v1","updated":"2025-12-19T06:12:43Z","published":"2025-12-19T06:12:43Z","title":"Verifiability-First Agents: Provable Observability and Lightweight Audit Agents for Controlling Autonomous LLM Systems","summary":"As LLM-based agents grow more autonomous and multi-modal, ensuring they remain controllable, auditable, and faithful to deployer intent becomes critical. Prior benchmarks measured the propensity for misaligned behavior and showed that agent personalities and tool access significantly influence misalignment. Building on these insights, we propose a Verifiability-First architecture that (1) integrates run-time attestations of agent actions using cryptographic and symbolic methods, (2) embeds lightweight Audit Agents that continuously verify intent versus behavior using constrained reasoning, and (3) enforces challenge-response attestation protocols for high-risk operations. We introduce OPERA (Observability, Provable Execution, Red-team, Attestation), a benchmark suite and evaluation protocol designed to measure (i) detectability of misalignment, (ii) time to detection under stealthy strategies, and (iii) resilience of verifiability mechanisms to adversarial prompt and persona injection. Our approach shifts the evaluation focus from how likely misalignment is to how quickly and reliably misalignment can be detected and remediated.","authors":["Abhivansh Gupta"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.06699v2","updated":"2025-12-19T06:10:50Z","published":"2025-12-07T07:25:08Z","title":"Predictive Modeling of I/O Performance for Machine Learning Training Pipelines: A Data-Driven Approach to Storage Optimization","summary":"Modern machine learning training is increasingly bottlenecked by data I/O rather than compute. GPUs often sit idle at below 50% utilization waiting for data. This paper presents a machine learning approach to predict I/O performance and recommend optimal storage configurations for ML training pipelines. We collected 141 observations through systematic benchmarking across different storage backends (NVMe SSD, network-attached storage, in-memory filesystems), data formats, and access patterns, covering both low-level I/O operations and full training pipelines. After evaluating seven regression models and three classification approaches, XGBoost achieved the best performance with R-squared of 0.991, predicting I/O throughput within 11.8% error on average. Feature importance analysis revealed that throughput metrics and batch size are the primary performance drivers. This data-driven approach can reduce configuration time from days of trial-and-error to minutes of predictive recommendation. The methodology is reproducible and extensible to other resource management problems in ML systems. Code and data are available at https://github.com/knkarthik01/gpu_storage_ml_project","authors":["Karthik Prabhakar","Durgamadhab Mishra"],"pdf_url":"","comment":"20 pages, 10 figures"},{"id":"http://arxiv.org/abs/2509.25977v2","updated":"2025-12-19T06:08:01Z","published":"2025-09-30T09:09:33Z","title":"Data-Free Continual Learning of Server Models in Model-Heterogeneous Cloud-Device Collaboration","summary":"The rise of cloud-device collaborative computing has enabled intelligent services to be delivered across distributed edge devices while leveraging centralized cloud resources. In this paradigm, federated learning (FL) has become a key enabler for privacy-preserving model training without transferring raw data from edge devices to the cloud. However, with the continuous emergence of new data and increasing model diversity, traditional federated learning faces significant challenges, including inherent issues of data heterogeneity, model heterogeneity and catastrophic forgetting, along with new challenge of knowledge misalignment. In this study, we introduce FedDCL, a novel framework designed to enable data-free continual learning of the server model in a model-heterogeneous federated setting. We leverage pre-trained diffusion models to extract lightweight class-specific prototypes, which confer a threefold data-free advantage, enabling: (1) generation of synthetic data for the current task to augment training and counteract non-IID data distributions; (2) exemplar-free generative replay for retaining knowledge from previous tasks; and (3) data-free dynamic knowledge transfer from heterogeneous devices to the cloud server.Experimental results on various datasets demonstrate the effectiveness of FedDCL, showcasing its potential to enhance the generalizability and practical applicability of federated cloud-device collaboration in dynamic settings.","authors":["Xiao Zhang","Zengzhe Chen","Yuan Yuan","Yifei Zou","Fuzhen Zhuang","Wenyu Jiao","Yuke Wang","Dongxiao Yu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17255v1","updated":"2025-12-19T05:56:48Z","published":"2025-12-19T05:56:48Z","title":"From Priors to Predictions: Explaining and Visualizing Human Reasoning in a Graph Neural Network Framework","summary":"Humans excel at solving novel reasoning problems from minimal exposure, guided by inductive biases, assumptions about which entities and relationships matter. Yet the computational form of these biases and their neural implementation remain poorly understood. We introduce a framework that combines Graph Theory and Graph Neural Networks (GNNs) to formalize inductive biases as explicit, manipulable priors over structure and abstraction. Using a human behavioral dataset adapted from the Abstraction and Reasoning Corpus (ARC), we show that differences in graph-based priors can explain individual differences in human solutions. Our method includes an optimization pipeline that searches over graph configurations, varying edge connectivity and node abstraction, and a visualization approach that identifies the computational graph, the subset of nodes and edges most critical to a model's prediction. Systematic ablation reveals how generalization depends on specific prior structures and internal processing, exposing why human like errors emerge from incorrect or incomplete priors. This work provides a principled, interpretable framework for modeling the representational assumptions and computational dynamics underlying generalization, offering new insights into human reasoning and a foundation for more human aligned AI systems.","authors":["Quan Do","Caroline Ahn","Leah Bakst","Michael Pascale","Joseph T. McGuire","Chantal E. Stern","Michael E. Hasselmo"],"pdf_url":"","comment":"44 pages, 7 figures, 3 suppl figures"},{"id":"http://arxiv.org/abs/2512.16248v2","updated":"2025-12-19T05:44:04Z","published":"2025-12-18T06:57:42Z","title":"Sigma-MoE-Tiny Technical Report","summary":"Mixture-of-Experts (MoE) has emerged as a promising paradigm for foundation models due to its efficient and powerful scalability. In this work, we present Sigma-MoE-Tiny, an MoE language model that achieves the highest sparsity compared to existing open-source models. Sigma-MoE-Tiny employs fine-grained expert segmentation with up to 96 experts per layer, while activating only one expert for each token, resulting in 20B total parameters with just 0.5B activated. The major challenge introduced by such extreme sparsity lies in expert load balancing. We find that the widely-used load balancing loss tends to become ineffective in the lower layers under this setting. To address this issue, we propose a progressive sparsification schedule aiming to balance expert utilization and training stability. Sigma-MoE-Tiny is pre-trained on a diverse and high-quality corpus, followed by post-training to further unlock its capabilities. The entire training process remains remarkably stable, with no occurrence of irrecoverable loss spikes. Comprehensive evaluations reveal that, despite activating only 0.5B parameters, Sigma-MoE-Tiny achieves top-tier performance among counterparts of comparable or significantly larger scale. In addition, we provide an in-depth discussion of load balancing in highly sparse MoE models, offering insights for advancing sparsity in future MoE architectures.\n  Project page: https://qghuxmu.github.io/Sigma-MoE-Tiny\n  Code: https://github.com/microsoft/ltp-megatron-lm","authors":["Qingguo Hu","Zhenghao Lin","Ziyue Yang","Yucheng Ding","Xiao Liu","Yuting Jiang","Ruizhe Wang","Tianyu Chen","Zhongxin Guo","Yifan Xiong","Rui Gao","Lei Qu","Jinsong Su","Peng Cheng","Yeyun Gong"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17251v1","updated":"2025-12-19T05:36:23Z","published":"2025-12-19T05:36:23Z","title":"AlignDP: Hybrid Differential Privacy with Rarity-Aware Protection for LLMs","summary":"Large language models are exposed to risks of extraction, distillation, and unauthorized fine-tuning. Existing defenses use watermarking or monitoring, but these act after leakage. We design AlignDP, a hybrid privacy lock that blocks knowledge transfer at the data interface. The key idea is to separate rare and non-rare fields. Rare fields are shielded by PAC indistinguishability, giving effective zero-epsilon local DP. Non-rare fields are privatized with RAPPOR, giving unbiased frequency estimates under local DP. A global aggregator enforces composition and budget. This two-tier design hides rare events and adds controlled noise to frequent events. We prove limits of PAC extension to global aggregation, give bounds for RAPPOR estimates, and analyze utility trade-off. A toy simulation confirms feasibility: rare categories remain hidden, frequent categories are recovered with small error.","authors":["Madhava Gaikwad"],"pdf_url":"","comment":"39th Conference on Neural Information Processing Systems (NeurIPS 2025) Workshop: LOCK-LLM Work-shop, NeurIPS 2025"},{"id":"http://arxiv.org/abs/2512.17250v1","updated":"2025-12-19T05:34:52Z","published":"2025-12-19T05:34:52Z","title":"Accelerating Multi-modal LLM Gaming Performance via Input Prediction and Mishit Correction","summary":"Real-time sequential control agents are often bottlenecked by inference latency. Even modest per-step planning delays can destabilize control and degrade overall performance. We propose a speculation-and-correction framework that adapts the predict-then-verify philosophy of speculative execution to model-based control with TD-MPC2. At each step, a pretrained world model and latent-space MPC planner generate a short-horizon action queue together with predicted latent rollouts, allowing the agent to execute multiple planned actions without immediate replanning. When a new observation arrives, the system measures the mismatch between the encoded real latent state and the queued predicted latent. For small to moderate mismatch, a lightweight learned corrector applies a residual update to the speculative action, distilled offline from a replanning teacher. For large mismatch, the agent safely falls back to full replanning and clears stale action queues. We study both a gated two-tower MLP corrector and a temporal Transformer corrector to address local errors and systematic drift. Experiments on the DMC Humanoid-Walk task show that our method reduces the number of planning inferences from 500 to 282, improves end-to-end step latency by 25 percent, and maintains strong control performance with only a 7.1 percent return reduction. Ablation results demonstrate that speculative execution without correction is unreliable over longer horizons, highlighting the necessity of mismatch-aware correction for robust latency reduction.","authors":["Ziyang Lin","Zixuan Sun","Sanhorn Chen","Xiaoyang Chen","Roy Zhao"],"pdf_url":"","comment":"UIUC 25 Fall CS 498"},{"id":"http://arxiv.org/abs/2512.17247v1","updated":"2025-12-19T05:26:50Z","published":"2025-12-19T05:26:50Z","title":"Incorporating Error Level Noise Embedding for Improving LLM-Assisted Robustness in Persian Speech Recognition","summary":"Automatic Speech Recognition (ASR) systems suffer significant performance degradation in noisy environments, a challenge that is especially severe for low-resource languages such as Persian. Even state-of-the-art models such as Whisper struggle to maintain accuracy under varying signal-to-noise ratios (SNRs). This study presents a robust noise-sensitive ASR error correction framework that combines multiple hypotheses and noise-aware modeling. Using noisy Persian speech, we generate 5-best hypotheses from a modified Whisper-large decoder. Error Level Noise (ELN) is introduced as a representation that captures semantic- and token-level disagreement across hypotheses, quantifying the linguistic distortions caused by noise. ELN thus provides a direct measure of noise-induced uncertainty, enabling the LLM to reason about the reliability of each hypothesis during correction. Three models are evaluated: (1) a base LLaMA-2-7B model without fine-tuning, (2) a fine-tuned variant trained on text-only hypotheses, and (3) a noise-conditioned model integrating ELN embeddings at both sentence and word levels. Experimental results demonstrate that the ELN-conditioned model achieves substantial reductions in Word Error Rate (WER). Specifically, on the challenging Mixed Noise test set, the proposed Fine-tuned + ELN (Ours) model reduces the WER from a baseline of 31.10\\% (Raw Whisper) to 24.84\\%, significantly surpassing the Fine-tuned (No ELN) text-only baseline of 30.79\\%, whereas the original LLaMA-2-7B model increased the WER to 64.58\\%, demonstrating that it is unable to correct Persian errors on its own. This confirms the effectiveness of combining multiple hypotheses with noise-aware embeddings for robust Persian ASR in noisy real-world scenarios.","authors":["Zahra Rahmani","Hossein Sameti"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17239v1","updated":"2025-12-19T04:59:41Z","published":"2025-12-19T04:59:41Z","title":"Privacy-Preserving Synthetic Dataset of Individual Daily Trajectories for City-Scale Mobility Analytics","summary":"Urban mobility data are indispensable for urban planning, transportation demand forecasting, pandemic modeling, and many other applications; however, individual mobile phone-derived Global Positioning System traces cannot generally be shared with third parties owing to severe re-identification risks. Aggregated records, such as origin-destination (OD) matrices, offer partial insights but fail to capture the key behavioral properties of daily human movement, limiting realistic city-scale analyses.\n  This study presents a privacy-preserving synthetic mobility dataset that reconstructs daily trajectories from aggregated inputs. The proposed method integrates OD flows with two complementary behavioral constraints: (1) dwell-travel time quantiles that are available only as coarse summary statistics and (2) the universal law for the daily distribution of the number of visited locations. Embedding these elements in a multi-objective optimization framework enables the reproduction of realistic distributions of human mobility while ensuring that no personal identifiers are required.\n  The proposed framework is validated in two contrasting regions of Japan: (1) the 23 special wards of Tokyo, representing a dense metropolitan environment; and (2) Fukuoka Prefecture, where urban and suburban mobility patterns coexist. The resulting synthetic mobility data reproduce dwell-travel time and visit frequency distributions with high fidelity, while deviations in OD consistency remain within the natural range of daily fluctuations.\n  The results of this study establish a practical synthesis pathway under real-world constraints, providing governments, urban planners, and industries with scalable access to high-resolution mobility data for reliable analytics without the need for sensitive personal records, and supporting practical deployments in policy and commercial domains.","authors":["Jun'ichi Ozaki","Ryosuke Susuta","Takuhiro Moriyama","Yohei Shida"],"pdf_url":"","comment":"9 pages, 4 figures"},{"id":"http://arxiv.org/abs/2512.07540v2","updated":"2025-12-19T04:16:55Z","published":"2025-12-08T13:21:44Z","title":"Minimum Bayes Risk Decoding for Error Span Detection in Reference-Free Automatic Machine Translation Evaluation","summary":"Error Span Detection (ESD) extends automatic machine translation (MT) evaluation by localizing translation errors and labeling their severity. Current generative ESD methods typically use Maximum a Posteriori (MAP) decoding, assuming that the model-estimated probabilities are perfectly correlated with similarity to the human annotation, but we often observe higher likelihood assigned to an incorrect annotation than to the human one. We instead apply Minimum Bayes Risk (MBR) decoding to generative ESD. We use a sentence- or span-level similarity function for MBR decoding, which selects candidate hypotheses based on their approximate similarity to the human annotation. Experimental results on the WMT24 Metrics Shared Task show that MBR decoding significantly improves span-level performance and generally matches or outperforms MAP at the system and sentence levels. To reduce the computational cost of MBR decoding, we further distill its decisions into a model decoded via greedy search, removing the inference-time latency bottleneck.","authors":["Boxuan Lyu","Haiyue Song","Hidetaka Kamigaito","Chenchen Ding","Hideki Tanaka","Masao Utiyama","Kotaro Funakoshi","Manabu Okumura"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17218v1","updated":"2025-12-19T04:05:41Z","published":"2025-12-19T04:05:41Z","title":"The Role of Islamic Ethics in Preventing the Abuse of Artificial Intelligence (AI) Based Deepfakes","summary":"The significant development of deepfake technology powered by artificial intelligence (AI) has sparked worldwide concerns about the alteration of false information, the usurpation of online identities, and the decline of public confidence in the authenticity of online content. These incidents not only raise technical issues but also carry complex moral implications, rendering conventional, technologically driven, and reactive management methods inadequate to address the underlying causes of the problem, including intent, morality, and potential intangible social impacts. Based on these issues, this study aims to formulate a comprehensive Islamic ethical framework that can serve as a more comprehensive preventative tool to mitigate the risks of misuse of deepfakes. The study employed a Systematic Literature Review (SLR) guided by PRISMA, selecting ten primary sources published between 2018 and 2025 to identify ethical deficiencies, regulatory needs, and appropriate normative solutions. The analysis shows that the integration of the principles of (Maqasid al-Shariah) particularly (hifz al-ird) protecting honor and (hifz al-nafs) protecting the self, provides a strong normative basis for regulating the responsible use of technology. This study yields three strategic recommendations: regulatory changes that recognize the intangible and psychological harm caused by reputational damage; improved technology management through moral scrutiny that upholds the values of justice (adl), trust, and openness; and increased public digital literacy based on the principle of (tabayyun) examination and caution. Overall, this study concludes that the application of Islamic ethics offers a shift in thinking from punitive mechanisms to preventative approaches that focus on protecting human dignity, preventing harm, and strengthening the common good in the digital age.","authors":["Wisnu Uriawan","Imany Fauzy Rahman","Muhamad Zidan","Irma Rohmatillah","Muhammad Arkan Raihan","Irma Dwiyanti"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.13168v2","updated":"2025-12-19T03:59:15Z","published":"2025-12-15T10:28:45Z","title":"Finch: Benchmarking Finance & Accounting across Spreadsheet-Centric Enterprise Workflows","summary":"We introduce a finance & accounting benchmark (Finch) for evaluating AI agents on real-world, enterprise-grade professional workflows -- interleaving data entry, structuring, formatting, web search, cross-file retrieval, calculation, modeling, validation, translation, visualization, and reporting. Finch is sourced from authentic enterprise workspaces at Enron (15,000 spreadsheets and 500,000 emails from 150 employees) and other financial institutions, preserving in-the-wild messiness across multimodal artifacts (text, tables, formulas, charts, code, and images) and spanning diverse domains such as budgeting, trading, and asset management.\n  We propose a workflow construction process that combines LLM-assisted discovery with expert annotation: (1) LLM-assisted, expert-verified derivation of workflows from real-world email threads and version histories of spreadsheet files, and (2) meticulous expert annotation for workflows, requiring over 700 hours of domain-expert effort. This yields 172 composite workflows with 384 tasks, involving 1,710 spreadsheets with 27 million cells, along with PDFs and other artifacts, capturing the intrinsically messy, long-horizon, knowledge-intensive, and collaborative nature of real-world enterprise work.\n  We conduct both human and automated evaluations of frontier AI systems including GPT 5.1, Claude Sonnet 4.5, Gemini 3 Pro, Grok 4, and Qwen 3 Max, and GPT 5.1 Pro spends 48 hours in total yet passes only 38.4% of workflows, while Claude Sonnet 4.5 passes just 25.0%. Comprehensive case studies further surface the challenges that real-world enterprise workflows pose for AI agents.","authors":["Haoyu Dong","Pengkun Zhang","Yan Gao","Xuanyu Dong","Yilin Cheng","Mingzhe Lu","Adina Yakefu","Shuxin Zheng"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17215v1","updated":"2025-12-19T03:58:02Z","published":"2025-12-19T03:58:02Z","title":"Research on Dead Reckoning Algorithm for Self-Propelled Pipeline Robots in Three-Dimensional Complex Pipelines","summary":"In the field of gas pipeline location, existing pipeline location methods mostly rely on pipeline location instruments. However, when faced with complex and curved pipeline scenarios, these methods often fail due to problems such as cable entanglement and insufficient equipment flexibility. To address this pain point, we designed a self-propelled pipeline robot. This robot can autonomously complete the location work of complex and curved pipelines in complex pipe networks without external dragging. In terms of pipeline mapping technology, traditional visual mapping and laser mapping methods are easily affected by lighting conditions and insufficient features in the confined space of pipelines, resulting in mapping drift and divergence problems. In contrast, the pipeline location method that integrates inertial navigation and wheel odometers is less affected by pipeline environmental factors. Based on this, this paper proposes a pipeline robot location method based on extended Kalman filtering (EKF). Firstly, the body attitude angle is initially obtained through an inertial measurement unit (IMU). Then, the extended Kalman filtering algorithm is used to improve the accuracy of attitude angle estimation. Finally, high-precision pipeline location is achieved by combining wheel odometers. During the testing phase, the roll wheels of the pipeline robot needed to fit tightly against the pipe wall to reduce slippage. However, excessive tightness would reduce the flexibility of motion control due to excessive friction. Therefore, a balance needed to be struck between the robot's motion capability and positioning accuracy. Experiments were conducted using the self-propelled pipeline robot in a rectangular loop pipeline, and the results verified the effectiveness of the proposed dead reckoning algorithm.","authors":["Yan Gao","Jiliang Wang","Minghan Wang","Xiaohua Chen","Demin Chen","Zhiyong Ren","Tian-Yun Huang"],"pdf_url":"","comment":"8 pages, 9 figures"},{"id":"http://arxiv.org/abs/2512.07984v2","updated":"2025-12-19T03:34:12Z","published":"2025-12-08T19:15:08Z","title":"Restrictive Hierarchical Semantic Segmentation for Stratified Tooth Layer Detection","summary":"Accurate understanding of anatomical structures is essential for reliably staging certain dental diseases. A way of introducing this within semantic segmentation models is by utilising hierarchy-aware methodologies. However, existing hierarchy-aware segmentation methods largely encode anatomical structure through the loss functions, providing weak and indirect supervision. We introduce a general framework that embeds an explicit anatomical hierarchy into semantic segmentation by coupling a recurrent, level-wise prediction scheme with restrictive output heads and top-down feature conditioning. At each depth of the class tree, the backbone is re-run on the original image concatenated with logits from the previous level. Child class features are conditioned using Feature-wise Linear Modulation of their parent class probabilities, to modulate child feature spaces for fine grained detection. A probabilistic composition rule enforces consistency between parent and descendant classes. Hierarchical loss combines per-level class weighted Dice and cross entropy loss and a consistency term loss, ensuring parent predictions are the sum of their children. We validate our approach on our proposed dataset, TL-pano, containing 194 panoramic radiographs with dense instance and semantic segmentation annotations, of tooth layers and alveolar bone. Utilising UNet and HRNet as donor models across a 5-fold cross validation scheme, the hierarchical variants consistently increase IoU, Dice, and recall, particularly for fine-grained anatomies, and produce more anatomically coherent masks. However, hierarchical variants also demonstrated increased recall over precision, implying increased false positives. The results demonstrate that explicit hierarchical structuring improves both performance and clinical plausibility, especially in low data dental imaging regimes.","authors":["Ryan Banks","Camila Lindoni Azevedo","Hongying Tang","Yunpeng Li"],"pdf_url":"","comment":"13 pages, 7 figures, 3 tables"},{"id":"http://arxiv.org/abs/2507.17061v4","updated":"2025-12-19T03:33:45Z","published":"2025-07-22T22:42:51Z","title":"Parallelism Meets Adaptiveness: Scalable Documents Understanding in Multi-Agent LLM Systems","summary":"Large language model (LLM) agents have shown increasing promise for collaborative task completion. However, existing multi-agent frameworks often rely on static workflows, fixed roles, and limited inter-agent communication, reducing their effectiveness in open-ended, high-complexity domains. This paper proposes a coordination framework that enables adaptiveness through three core mechanisms: dynamic task routing, bidirectional feedback, and parallel agent evaluation. The framework allows agents to reallocate tasks based on confidence and workload, exchange structured critiques to iteratively improve outputs, and crucially compete on high-ambiguity subtasks with evaluator-driven selection of the most suitable result. We instantiate these principles in a modular architecture and demonstrate substantial improvements in factual coverage, coherence, and efficiency over static and partially adaptive baselines. Our findings highlight the benefits of incorporating both adaptiveness and structured competition in multi-agent LLM systems.","authors":["Chengxuan Xia","Qianye Wu","Sixuan Tian","Yilun Hao"],"pdf_url":"","comment":"Accepted at AAAI 2026 Workshop on WoMAPF, Camera ready version"},{"id":"http://arxiv.org/abs/2512.17202v1","updated":"2025-12-19T03:28:39Z","published":"2025-12-19T03:28:39Z","title":"Fose: Fusion of One-Step Diffusion and End-to-End Network for Pansharpening","summary":"Pansharpening is a significant image fusion task that fuses low-resolution multispectral images (LRMSI) and high-resolution panchromatic images (PAN) to obtain high-resolution multispectral images (HRMSI). The development of the diffusion models (DM) and the end-to-end models (E2E model) has greatly improved the frontier of pansharping. DM takes the multi-step diffusion to obtain an accurate estimation of the residual between LRMSI and HRMSI. However, the multi-step process takes large computational power and is time-consuming. As for E2E models, their performance is still limited by the lack of prior and simple structure. In this paper, we propose a novel four-stage training strategy to obtain a lightweight network Fose, which fuses one-step DM and an E2E model. We perform one-step distillation on an enhanced SOTA DM for pansharping to compress the inference process from 50 steps to only 1 step. Then we fuse the E2E model with one-step DM with lightweight ensemble blocks. Comprehensive experiments are conducted to demonstrate the significant improvement of the proposed Fose on three commonly used benchmarks. Moreover, we achieve a 7.42 speedup ratio compared to the baseline DM while achieving much better performance. The code and model are released at https://github.com/Kai-Liu001/Fose.","authors":["Kai Liu","Zeli Lin","Weibo Wang","Linghe Kong","Yulun Zhang"],"pdf_url":"","comment":"Code link: https://github.com/Kai-Liu001/Fose"},{"id":"http://arxiv.org/abs/2411.15355v3","updated":"2025-12-19T03:24:39Z","published":"2024-11-22T21:59:46Z","title":"UniGaussian: Driving Scene Reconstruction from Multiple Camera Models via Unified Gaussian Representations","summary":"Urban scene reconstruction is crucial for real-world autonomous driving simulators. Although existing methods have achieved photorealistic reconstruction, they mostly focus on pinhole cameras and neglect fisheye cameras. In fact, how to effectively simulate fisheye cameras in driving scene remains an unsolved problem. In this work, we propose UniGaussian, a novel approach that learns a unified 3D Gaussian representation from multiple camera models for urban scene reconstruction in autonomous driving. Our contributions are two-fold. First, we propose a new differentiable rendering method that distorts 3D Gaussians using a series of affine transformations tailored to fisheye camera models. This addresses the compatibility issue of 3D Gaussian splatting with fisheye cameras, which is hindered by light ray distortion caused by lenses or mirrors. Besides, our method maintains real-time rendering while ensuring differentiability. Second, built on the differentiable rendering method, we design a new framework that learns a unified Gaussian representation from multiple camera models. By applying affine transformations to adapt different camera models and regularizing the shared Gaussians with supervision from different modalities, our framework learns a unified 3D Gaussian representation with input data from multiple sources and achieves holistic driving scene understanding. As a result, our approach models multiple sensors (pinhole and fisheye cameras) and modalities (depth, semantic, normal and LiDAR point clouds). Our experiments show that our method achieves superior rendering quality and fast rendering speed for driving scene simulation.","authors":["Yuan Ren","Guile Wu","Runhao Li","Zheyuan Yang","Yibo Liu","Xingxin Chen","Tongtong Cao","Bingbing Liu"],"pdf_url":"","comment":"3DV 2026"},{"id":"http://arxiv.org/abs/2512.17196v1","updated":"2025-12-19T03:20:59Z","published":"2025-12-19T03:20:59Z","title":"UmniBench: Unified Understand and Generation Model Oriented Omni-dimensional Benchmark","summary":"Unifying multimodal understanding and generation has shown impressive capabilities in cutting-edge proprietary systems. However, evaluations of unified multimodal models (UMMs) remain decoupled, assessing their understanding and generation abilities separately with corresponding datasets. To address this, we propose UmniBench, a benchmark tailored for UMMs with omni-dimensional evaluation. First, UmniBench can assess the understanding, generation, and editing ability within a single evaluation process. Based on human-examined prompts and QA pairs, UmniBench leverages UMM itself to evaluate its generation and editing ability with its understanding ability. This simple but effective paradigm allows comprehensive evaluation of UMMs. Second, UmniBench covers 13 major domains and more than 200 concepts, ensuring a thorough inspection of UMMs. Moreover, UmniBench can also decouple and separately evaluate understanding, generation, and editing abilities, providing a fine-grained assessment. Based on UmniBench, we benchmark 24 popular models, including both UMMs and single-ability large models. We hope this benchmark provides a more comprehensive and objective view of unified models and logistical support for improving the performance of the community model.","authors":["Kai Liu","Leyang Chen","Wenbo Li","Zhikai Chen","Zhixin Wang","Renjing Pei","Linghe Kong","Yulun Zhang"],"pdf_url":"","comment":"Project Page: https://umnibench.github.io/"},{"id":"http://arxiv.org/abs/2512.17194v1","updated":"2025-12-19T03:19:54Z","published":"2025-12-19T03:19:54Z","title":"MMRAG-RFT: Two-stage Reinforcement Fine-tuning for Explainable Multi-modal Retrieval-augmented Generation","summary":"Multi-modal Retrieval-Augmented Generation (MMRAG) enables highly credible generation by integrating external multi-modal knowledge, thus demonstrating impressive performance in complex multi-modal scenarios. However, existing MMRAG methods fail to clarify the reasoning logic behind retrieval and response generation, which limits the explainability of the results. To address this gap, we propose to introduce reinforcement learning into multi-modal retrieval-augmented generation, enhancing the reasoning capabilities of multi-modal large language models through a two-stage reinforcement fine-tuning framework to achieve explainable multi-modal retrieval-augmented generation. Specifically, in the first stage, rule-based reinforcement fine-tuning is employed to perform coarse-grained point-wise ranking of multi-modal documents, effectively filtering out those that are significantly irrelevant. In the second stage, reasoning-based reinforcement fine-tuning is utilized to jointly optimize fine-grained list-wise ranking and answer generation, guiding multi-modal large language models to output explainable reasoning logic in the MMRAG process. Our method achieves state-of-the-art results on WebQA and MultimodalQA, two benchmark datasets for multi-modal retrieval-augmented generation, and its effectiveness is validated through comprehensive ablation experiments.","authors":["Shengwei Zhao","Jingwen Yao","Sitong Wei","Linhai Xu","Yuying Liu","Dong Zhang","Zhiqiang Tian","Shaoyi Du"],"pdf_url":"","comment":"This paper was accepted to AAAI2026"},{"id":"http://arxiv.org/abs/2509.07414v3","updated":"2025-12-19T03:05:26Z","published":"2025-09-09T05:51:34Z","title":"Language Self-Play For Data-Free Training","summary":"Large language models (LLMs) have advanced rapidly in recent years, driven by scale, abundant high-quality training data, and reinforcement learning. Yet this progress faces a fundamental bottleneck: the need for ever more data from which models can continue to learn. In this work, we propose a reinforcement learning approach that removes this dependency by enabling models to improve without additional data. Our method leverages a game-theoretic framework of self-play, where a model's capabilities are cast as performance in a competitive game and stronger policies emerge by having the model play against itself-a process we call Language Self-Play (LSP). Experiments with Llama-3.2-3B-Instruct on instruction-following, mathematics, and coding benchmarks show that pretrained models can be effectively improved with self-play alone.","authors":["Jakub Grudzien Kuba","Mengting Gu","Qi Ma","Yuandong Tian","Vijai Mohan","Jason Chen"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17185v1","updated":"2025-12-19T03:00:09Z","published":"2025-12-19T03:00:09Z","title":"Systemic Risk Radar: A Multi-Layer Graph Framework for Early Market Crash Warning","summary":"Financial crises emerge when structural vulnerabilities accumulate across sectors, markets, and investor behavior. Predicting these systemic transitions is challenging because they arise from evolving interactions between market participants, not isolated price movements alone. We present Systemic Risk Radar (SRR), a framework that models financial markets as multi-layer graphs to detect early signs of systemic fragility and crash-regime transitions.\n  We evaluate SRR across three major crises: the Dot-com crash, the Global Financial Crisis, and the COVID-19 shock. Our experiments compare snapshot GNNs, a simplified temporal GNN prototype, and standard baselines (logistic regression and Random Forest). Results show that structural network information provides useful early-warning signals compared to feature-based models alone.\n  This correlation-based instantiation of SRR demonstrates that graph-derived features capture meaningful changes in market structure during stress events. The findings motivate extending SRR with additional graph layers (sector/factor exposure, sentiment) and more expressive temporal architectures (LSTM/GRU or Transformer encoders) to better handle diverse crisis types.","authors":["Sandeep Neela"],"pdf_url":"","comment":"Preprint"},{"id":"http://arxiv.org/abs/2508.20705v2","updated":"2025-12-19T02:47:26Z","published":"2025-08-28T12:23:28Z","title":"EEGDM: Learning EEG Representation with Latent Diffusion Model","summary":"Recent advances in self-supervised learning for EEG representation have largely relied on masked reconstruction, where models are trained to recover randomly masked signal segments. While effective at modeling local dependencies, such objectives are inherently limited in capturing the global dynamics and long-range dependencies essential for characterizing neural activity. To address this limitation, we propose EEGDM, a novel self-supervised framework that leverages latent diffusion models to generate EEG signals as an objective. Unlike masked reconstruction, diffusion-based generation progressively denoises signals from noise to realism, compelling the model to capture holistic temporal patterns and cross-channel relationships. Specifically, EEGDM incorporates an EEG encoder that distills raw signals and their channel augmentations into a compact representation, acting as conditional information to guide the diffusion model for generating EEG signals. This design endows EEGDM with a compact latent space, which not only offers ample control over the generative process but also can be leveraged for downstream tasks. Experimental results show that EEGDM (1) reconstructs high-quality EEG signals, (2) learns robust representations, and (3) achieves competitive performance across diverse downstream tasks, thus exploring a new direction for self-supervised EEG representation learning.","authors":["Shaocong Wang","Tong Liu","Yihan Li","Ming Li","Kairui Wen","Pei Yang","Wenqi Ji","Minjing Yu","Yong-Jin Liu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17180v1","updated":"2025-12-19T02:38:04Z","published":"2025-12-19T02:38:04Z","title":"Conservative Bias in Multi-Teacher Learning: Why Agents Prefer Low-Reward Advisors","summary":"Interactive reinforcement learning (IRL) has shown promise in enabling autonomous agents and robots to learn complex behaviours from human teachers, yet the dynamics of teacher selection remain poorly understood. This paper reveals an unexpected phenomenon in IRL: when given a choice between teachers with different reward structures, learning agents overwhelmingly prefer conservative, low-reward teachers (93.16% selection rate) over those offering 20x higher rewards. Through 1,250 experimental runs in navigation tasks with multiple expert teachers, we discovered: (1) Conservative bias dominates teacher selection: agents systematically choose the lowest-reward teacher, prioritising consistency over optimality; (2) Critical performance thresholds exist at teacher availability rho >= 0.6 and accuracy omega >= 0.6, below which the framework fails catastrophically; (3) The framework achieves 159% improvement over baseline Q-learning under concept drift. These findings challenge fundamental assumptions about optimal teaching in RL and suggest potential implications for human-robot collaboration, where human preferences for safety and consistency may align with the observed agent selection behaviour, potentially informing training paradigms for safety-critical robotic applications.","authors":["Maher Mesto","Francisco Cruz"],"pdf_url":"","comment":"10 pages, 5 figures. Accepted at ACRA 2025 (Australasian Conference on Robotics and Automation)"},{"id":"http://arxiv.org/abs/2512.17172v1","updated":"2025-12-19T02:19:38Z","published":"2025-12-19T02:19:38Z","title":"PILAR: Personalizing Augmented Reality Interactions with LLM-based Human-Centric and Trustworthy Explanations for Daily Use Cases","summary":"Artificial intelligence (AI)-driven augmented reality (AR) systems are becoming increasingly integrated into daily life, and with this growth comes a greater need for explainability in real-time user interactions. Traditional explainable AI (XAI) methods, which often rely on feature-based or example-based explanations, struggle to deliver dynamic, context-specific, personalized, and human-centric insights for everyday AR users. These methods typically address separate explainability dimensions (e.g., when, what, how) with different explanation techniques, resulting in unrealistic and fragmented experiences for seamless AR interactions. To address this challenge, we propose PILAR, a novel framework that leverages a pre-trained large language model (LLM) to generate context-aware, personalized explanations, offering a more intuitive and trustworthy experience in real-time AI-powered AR systems. Unlike traditional methods, which rely on multiple techniques for different aspects of explanation, PILAR employs a unified LLM-based approach that dynamically adapts explanations to the user's needs, fostering greater trust and engagement. We implement the PILAR concept in a real-world AR application (e.g., personalized recipe recommendations), an open-source prototype that integrates real-time object detection, recipe recommendation, and LLM-based personalized explanations of the recommended recipes based on users' dietary preferences. We evaluate the effectiveness of PILAR through a user study with 16 participants performing AR-based recipe recommendation tasks, comparing an LLM-based explanation interface to a traditional template-based one. Results show that the LLM-based interface significantly enhances user performance and experience, with participants completing tasks 40% faster and reporting greater satisfaction, ease of use, and perceived transparency.","authors":["Ripan Kumar Kundu","Istiak Ahmed","Khaza Anuarul Hoque"],"pdf_url":"","comment":"Published in the 2025 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)"},{"id":"http://arxiv.org/abs/2502.01956v3","updated":"2025-12-19T02:05:30Z","published":"2025-02-04T03:05:55Z","title":"DHP: Discrete Hierarchical Planning for Hierarchical Reinforcement Learning Agents","summary":"Hierarchical Reinforcement Learning (HRL) agents often struggle with long-horizon visual planning due to their reliance on error-prone distance metrics. We propose Discrete Hierarchical Planning (DHP), a method that replaces continuous distance estimates with discrete reachability checks to evaluate subgoal feasibility. DHP recursively constructs tree-structured plans by decomposing long-term goals into sequences of simpler subtasks, using a novel advantage estimation strategy that inherently rewards shorter plans and generalizes beyond training depths. In addition, to address the data efficiency challenge, we introduce an exploration strategy that generates targeted training examples for the planning modules without needing expert data. Experiments in 25-room navigation environments demonstrate a 100% success rate (vs. 90% baseline). We also present an offline variant that achieves state-of-the-art results on OGBench benchmarks, with up to 71% absolute gains on giant HumanoidMaze tasks, demonstrating our core contributions are architecture-agnostic. The method also generalizes to momentum-based control tasks and requires only log N steps for replanning. Theoretical analysis and ablations validate our design choices.","authors":["Shashank Sharma","Janina Hoffmann","Vinay Namboodiri"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17145v1","updated":"2025-12-19T00:43:49Z","published":"2025-12-19T00:43:49Z","title":"Solomonoff-Inspired Hypothesis Ranking with LLMs for Prediction Under Uncertainty","summary":"Reasoning under uncertainty is a key challenge in AI, especially for real-world tasks, where problems with sparse data demands systematic generalisation. Existing approaches struggle to balance accuracy and simplicity when evaluating multiple candidate solutions. We propose a Solomonoff-inspired method that weights LLM-generated hypotheses by simplicity and predictive fit. Applied to benchmark (Mini-ARC) tasks, our method produces Solomonoff-weighted mixtures for per-cell predictions, yielding conservative, uncertainty-aware outputs even when hypotheses are noisy or partially incorrect. Compared to Bayesian Model Averaging (BMA), Solomonoff scoring spreads probability more evenly across competing hypotheses, while BMA concentrates weight on the most likely but potentially flawed candidates. Across tasks, this highlights the value of algorithmic information-theoretic priors for interpretable, reliable multi-hypothesis reasoning under uncertainty.","authors":["Josh Barber","Rourke Young","Cameron Coombe","Will Browne"],"pdf_url":"","comment":"10 pages, ACRA 2025, Submitted, Accepted and Presented"},{"id":"http://arxiv.org/abs/2504.20196v2","updated":"2025-12-19T00:21:29Z","published":"2025-04-28T18:59:28Z","title":"Understanding and supporting how developers prompt for LLM-powered code editing in practice","summary":"Large Language Models (LLMs) are rapidly transforming software engineering, with coding assistants embedded in an IDE becoming increasingly prevalent. While research has focused on improving the tools and understanding developer perceptions, a critical gap exists in understanding how developers actually use these tools in their daily workflows, and, crucially, where they struggle. This paper addresses part of this gap through a multi-phased investigation of developer interactions with an LLM-powered code editing feature, Transform Code, in an IDE widely used at Google. First, we analyze telemetry logs of the feature usage, revealing that frequent re-prompting can be an indicator of developer struggles with using Transform Code. Second, we conduct a qualitative analysis of unsatisfactory requests, identifying five key categories of information often missing from developer prompts. Finally, based on these findings, we propose and evaluate a tool, AutoPrompter, for automatically improving prompts by inferring missing information from the surrounding code context, leading to a 27% improvement in edit correctness on our test set.","authors":["Daye Nam","Ahmed Omran","Ambar Murillo","Saksham Thakur","Abner Araujo","Marcel Blistein","Alexander Frömmgen","Vincent Hellendoorn","Satish Chandra"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17137v1","updated":"2025-12-19T00:09:32Z","published":"2025-12-19T00:09:32Z","title":"SDUM: A Scalable Deep Unrolled Model for Universal MRI Reconstruction","summary":"Clinical MRI encompasses diverse imaging protocols--spanning anatomical targets (cardiac, brain, knee), contrasts (T1, T2, mapping), sampling patterns (Cartesian, radial, spiral, kt-space), and acceleration factors--yet current deep learning reconstructions are typically protocol-specific, hindering generalization and deployment. We introduce Scalable Deep Unrolled Model (SDUM), a universal framework combining a Restormer-based reconstructor, a learned coil sensitivity map estimator (CSME), sampling-aware weighted data consistency (SWDC), universal conditioning (UC) on cascade index and protocol metadata, and progressive cascade expansion training. SDUM exhibits foundation-model-like scaling behavior: reconstruction quality follows PSNR ${\\sim}$ log(parameters) with correlation $r{=}0.986$ ($R^2{=}0.973$) up to 18 cascades, demonstrating predictable performance gains with model depth. A single SDUM trained on heterogeneous data achieves state-of-the-art results across all four CMRxRecon2025 challenge tracks--multi-center, multi-disease, 5T, and pediatric--without task-specific fine-tuning, surpassing specialized baselines by up to ${+}1.0$~dB. On CMRxRecon2024, SDUM outperforms the winning method PromptMR+ by ${+}0.55$~dB; on fastMRI brain, it exceeds PC-RNN by ${+}1.8$~dB. Ablations validate each component: SWDC ${+}0.43$~dB over standard DC, per-cascade CSME ${+}0.51$~dB, UC ${+}0.38$~dB. These results establish SDUM as a practical path toward universal, scalable MRI reconstruction.","authors":["Puyang Wang","Pengfei Guo","Keyi Chai","Jinyuan Zhou","Daguang Xu","Shanshan Jiang"],"pdf_url":"","comment":null}],"Computation and Language":[{"id":"http://arxiv.org/abs/2512.17901v1","updated":"2025-12-19T18:59:11Z","published":"2025-12-19T18:59:11Z","title":"When Reasoning Meets Its Laws","summary":"Despite the superior performance of Large Reasoning Models (LRMs), their reasoning behaviors are often counterintuitive, leading to suboptimal reasoning capabilities. To theoretically formalize the desired reasoning behaviors, this paper presents the Laws of Reasoning (LoRe), a unified framework that characterizes intrinsic reasoning patterns in LRMs. We first propose compute law with the hypothesis that the reasoning compute should scale linearly with question complexity. Beyond compute, we extend LoRe with a supplementary accuracy law. Since the question complexity is difficult to quantify in practice, we examine these hypotheses by two properties of the laws, monotonicity and compositionality. We therefore introduce LoRe-Bench, a benchmark that systematically measures these two tractable properties for large reasoning models. Evaluation shows that most reasoning models exhibit reasonable monotonicity but lack compositionality. In response, we develop an effective finetuning approach that enforces compute-law compositionality. Extensive empirical studies demonstrate that better compliance with compute laws yields consistently improved reasoning performance on multiple benchmarks, and uncovers synergistic effects across properties and laws. Project page: https://lore-project.github.io/","authors":["Junyu Zhang","Yifan Sun","Tianang Leng","Jingyan Shen","Liu Ziyin","Paul Pu Liang","Huan Zhang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2411.07892v2","updated":"2025-12-19T18:46:41Z","published":"2024-11-12T15:56:48Z","title":"Mapping the Podcast Ecosystem with the Structured Podcast Research Corpus","summary":"Podcasts provide highly diverse content to a massive listener base through a unique on-demand modality. However, limited data has prevented large-scale computational analysis of the podcast ecosystem. To fill this gap, we introduce a massive dataset of over 1.1M podcast transcripts that is largely comprehensive of all English language podcasts available through public RSS feeds from May and June of 2020. This data is not limited to text, but rather includes audio features and speaker turns for a subset of 370K episodes, and speaker role inferences and other metadata for all 1.1M episodes. Using this data, we also conduct a foundational investigation into the content, structure, and responsiveness of this ecosystem. Together, our data and analyses open the door to continued computational research of this popular and impactful medium.","authors":["Benjamin Litterer","David Jurgens","Dallas Card"],"pdf_url":"","comment":"9 pages, 3 figures"},{"id":"http://arxiv.org/abs/2511.12712v2","updated":"2025-12-19T18:24:09Z","published":"2025-11-16T17:52:32Z","title":"Adaptive Focus Memory for Language Models","summary":"Large language models (LLMs) are increasingly deployed in multi-turn dialogue settings, yet their behavior remains bottlenecked by naive history management strategies. Replaying the full conversation at every turn is simple but costly, while recency-based truncation or static summarization often causes early, high-impact user constraints to drift out of effective context. As a result, models may retain text without reliably applying it when it matters.\n  We present Adaptive Focus Memory (AFM), a lightweight context management system that dynamically assigns each past message one of three fidelity levels: Full, Compressed, or Placeholder, based on semantic relevance, temporal decay, and importance classification. AFM packs messages chronologically under a fixed token budget, preserving critical constraints at high fidelity while allowing low-importance context to degrade gracefully.\n  We evaluate AFM on two multi-turn dialogue benchmarks designed to stress long-horizon constraint preservation: a safety-critical travel scenario involving a user with a severe peanut allergy, and a policy-critical tax compliance scenario involving an illegal evasion request. Under strict grading that requires both explicit constraint recall and appropriately conditioned generation, AFM succeeds in 83.3 percent of allergy runs where all baseline strategies fail, and preserves correct refusal behavior on the tax benchmark.\n  These results demonstrate that effective dialogue memory requires more than retaining prior text. Selectively allocating fidelity across past messages enables reliable constraint preservation under bounded context growth, without modifying model weights or introducing external retrieval infrastructure. We release an open-source implementation of AFM compatible with OpenAI-style chat APIs to support reproducible research and practical deployment.","authors":["Christopher Cruz"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2509.22983v2","updated":"2025-12-19T18:19:10Z","published":"2025-09-26T22:33:19Z","title":"Same Content, Different Representations: A Controlled Study for Table QA","summary":"Table Question Answering (Table QA) in real-world settings must operate over both structured databases and semi-structured tables containing textual fields. However, existing benchmarks are tied to fixed data formats and have not systematically examined how representation itself affects model performance. We present the first controlled study that isolates the role of table representation by holding content constant while varying structure. Using a verbalization pipeline, we generate paired structured and semi-structured tables, enabling direct comparisons across modeling paradigms. To support detailed analysis, we introduce RePairTQA, a diagnostic benchmark with splits along table size, join requirements, query complexity, and schema quality. Our experiments reveal consistent trade-offs: SQL-based methods achieve high accuracy on structured inputs but degrade on semi-structured data, LLMs exhibit flexibility but reduced precision, and hybrid approaches strike a balance, particularly under noisy schemas. These effects intensify with larger tables and more complex queries. Ultimately, no single method excels across all conditions, and we highlight the central role of representation in shaping Table QA performance. Our findings provide actionable insights for model selection and design, paving the way for more robust hybrid approaches suited for diverse real-world data formats.","authors":["Yue Zhang","Seiji Maekawa","Nikita Bhutani"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17843v1","updated":"2025-12-19T17:47:53Z","published":"2025-12-19T17:47:53Z","title":"ShareChat: A Dataset of Chatbot Conversations in the Wild","summary":"While Large Language Models (LLMs) have evolved into distinct platforms with unique interface designs and capabilities, existing public datasets treat models as generic text generators, stripping away the interface context that actively shapes user interaction. To address this limitation, we present ShareChat, a large-scale, cross-platform corpus comprising 142,808 conversations and over 660,000 turns collected from publicly shared URLs across five major platforms: ChatGPT, Claude, Gemini, Perplexity, and Grok. ShareChat distinguishes itself by preserving native platform affordances often lost in standard logs, including reasoning traces, source links, and code artifacts, while spanning 101 languages over the period from April 2023 to October 2025. Furthermore, ShareChat offers substantially longer context windows and greater interaction depth than prior datasets. We demonstrate the dataset's multifaceted utility through three representative analyses: (1) analyzing conversation completeness to measure user intent satisfaction; (2) evaluating source citation behaviors in content generation; and (3) conducting temporal analysis to track evolving usage patterns. This work provides the community with a vital and timely resource for understanding authentic user-LLM chatbot interactions in the wild.","authors":["Yueru Yan","Tuc Nguyen","Bo Su","Melissa Lieffers","Thai Le"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2507.22219v3","updated":"2025-12-19T17:35:31Z","published":"2025-07-29T20:35:35Z","title":"RL from Teacher-Model Refinement: Gradual Imitation Learning for Machine Translation","summary":"Preference-learning methods for machine translation (MT), such as Direct Preference Optimization (DPO), have shown strong gains but typically rely on large, carefully curated preference triplets and often struggle to generalize beyond their tuning domains. We propose Reinforcement Learning from Teacher-Model Refinement (RLfR), which replaces static triplets with on-policy, actor-conditioned refinements produced by a frozen teacher. At each step, the actor samples candidate translations, the teacher performs a minimal local edit of each draft, and the actor is reinforced to close the gap using a composite reward that combines scaled negative edit distance for lexical and structural fidelity with COMET for semantic adequacy. This formulation yields a stable, model-aware learning signal without requiring explicit preference datasets. Experiments on FLORES-200 (English to German, Spanish, Chinese, Korean, and Japanese) show that RLfR consistently outperforms strong MT-SFT, DPO, and fixed-reference RL baselines, improving semantic quality and entity preservation, and also achieves superior performance under LLM-based judge evaluations.","authors":["Dongyub Jude Lee","Zhenyi Ye","Pengcheng He"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2506.10892v3","updated":"2025-12-19T17:14:07Z","published":"2025-06-12T16:55:35Z","title":"The Diffusion Duality","summary":"Uniform-state discrete diffusion models hold the promise of fast text generation due to their inherent ability to self-correct. However, they are typically outperformed by autoregressive models and masked diffusion models. In this work, we narrow this performance gap by leveraging a key insight: Uniform-state diffusion processes naturally emerge from an underlying Gaussian diffusion. Our method, Duo, transfers powerful techniques from Gaussian diffusion to improve both training and sampling. First, we introduce a curriculum learning strategy guided by the Gaussian process, doubling training speed by reducing variance. Models trained with curriculum learning surpass autoregressive models in zero-shot perplexity on 3 of 7 benchmarks. Second, we present Discrete Consistency Distillation, which adapts consistency distillation from the continuous to the discrete setting. This algorithm unlocks few-step generation in diffusion language models by accelerating sampling by two orders of magnitude. We provide the code, model checkpoints, and video tutorials on the project page: http://s-sahoo.github.io/duo","authors":["Subham Sekhar Sahoo","Justin Deschenaux","Aaron Gokaslan","Guanghan Wang","Justin Chiu","Volodymyr Kuleshov"],"pdf_url":"","comment":"ICML 2025. We provide the code at: https://github.com/s-sahoo/duo [v3] includes improved theory, clearer presentation, and a new future work section"},{"id":"http://arxiv.org/abs/2512.17776v1","updated":"2025-12-19T16:46:20Z","published":"2025-12-19T16:46:20Z","title":"DEER: A Comprehensive and Reliable Benchmark for Deep-Research Expert Reports","summary":"As large language models (LLMs) advance, deep research systems can generate expert-level reports via multi-step reasoning and evidence-based synthesis, but evaluating such reports remains challenging. Existing benchmarks often lack systematic criteria for expert reporting, evaluations that rely heavily on LLM judges can fail to capture issues that require expert judgment, and source verification typically covers only a limited subset of explicitly cited statements rather than report-wide factual reliability. We introduce DEER, a benchmark for evaluating expert-level deep research reports. DEER comprises 50 report-writing tasks spanning 13 domains and an expert-grounded evaluation taxonomy (7 dimensions, 25 sub-dimension) operationalized into 130 fine-grained rubric items. DEER further provides task-specific expert guidance to help LLM judges assess expert-level report quality more consistently. Complementing rubric-based assessment, we propose a document-level fact-checking architecture that extracts and verifies all claims across the entire report, including both cited and uncited ones, and quantifies external-evidence quality. DEER correlates closely with human expert judgments and yields interpretable diagnostics of system strengths and weaknesses.","authors":["Janghoon Han","Heegyu Kim","Changho Lee","Dahm Lee","Min Hyung Park","Hosung Song","Stanley Jungkyu Choi","Moontae Lee","Honglak Lee"],"pdf_url":"","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2512.17769v1","updated":"2025-12-19T16:41:16Z","published":"2025-12-19T16:41:16Z","title":"Bangla MedER: Multi-BERT Ensemble Approach for the Recognition of Bangla Medical Entity","summary":"Medical Entity Recognition (MedER) is an essential NLP task for extracting meaningful entities from the medical corpus. Nowadays, MedER-based research outcomes can remarkably contribute to the development of automated systems in the medical sector, ultimately enhancing patient care and outcomes. While extensive research has been conducted on MedER in English, low-resource languages like Bangla remain underexplored. Our work aims to bridge this gap. For Bangla medical entity recognition, this study first examined a number of transformer models, including BERT, DistilBERT, ELECTRA, and RoBERTa. We also propose a novel Multi-BERT Ensemble approach that outperformed all baseline models with the highest accuracy of 89.58%. Notably, it provides an 11.80% accuracy improvement over the single-layer BERT model, demonstrating its effectiveness for this task. A major challenge in MedER for low-resource languages is the lack of annotated datasets. To address this issue, we developed a high-quality dataset tailored for the Bangla MedER task. The dataset was used to evaluate the effectiveness of our model through multiple performance metrics, demonstrating its robustness and applicability. Our findings highlight the potential of Multi-BERT Ensemble models in improving MedER for Bangla and set the foundation for further advancements in low-resource medical NLP.","authors":["Tanjim Taharat Aurpa","Farzana Akter","Md. Mehedi Hasan","Shakil Ahmed","Shifat Ara Rafiq","Fatema Khan"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2509.00496v2","updated":"2025-12-19T16:38:02Z","published":"2025-08-30T13:37:28Z","title":"ResearchQA: Evaluating Scholarly Question Answering at Scale Across 75 Fields with Survey-Mined Questions and Rubrics","summary":"Evaluating long-form responses to research queries heavily relies on expert annotators, restricting attention to areas like AI where researchers can conveniently enlist colleagues. Yet, research expertise is abundant: survey articles consolidate knowledge spread across the literature. We introduce ResearchQA, a resource for evaluating LLM systems by distilling survey articles from 75 research fields into 21K queries and 160K rubric items. Queries and rubrics are jointly derived from survey sections, where rubric items list query-specific answer evaluation criteria, i.e., citing papers, making explanations, and describing limitations. 31 Ph.D. annotators in 8 fields judge that 90% of queries reflect Ph.D. information needs and 87% of rubric items warrant emphasis of a sentence or longer. We leverage ResearchQA to evaluate 18 systems in 7.6K head-to-heads. No parametric or retrieval-augmented system we evaluate exceeds 70% on covering rubric items, and the highest-ranking system shows 75% coverage. Error analysis reveals that the highest-ranking system fully addresses less than 11% of citation rubric items, 48% of limitation items, and 49% of comparison items. We release our data to facilitate more comprehensive multi-field evaluations.","authors":["Li S. Yifei","Allen Chang","Chaitanya Malaviya","Mark Yatskar"],"pdf_url":"","comment":"12 pages main, 40 pages total, 15 figures"},{"id":"http://arxiv.org/abs/2506.09147v4","updated":"2025-12-19T16:35:50Z","published":"2025-06-10T18:01:42Z","title":"LLM-as-a-qualitative-judge: automating error analysis in natural language generation","summary":"Prompting large language models (LLMs) to evaluate generated text, known as LLM-as-a-judge, has become a standard evaluation approach in natural language generation (NLG), but is primarily used as a quantitative tool, i.e. with numerical scores as main outputs. In this work, we propose LLM-as-a-qualitative-judge, an LLM-based evaluation approach with the main output being a structured report of common issue types in the NLG system outputs. Our approach is targeted at providing developers with meaningful insights on what improvements can be done to a given NLG system and consists of two main steps, namely open-ended per-instance issue analysis and clustering of the discovered issues using an intuitive cumulative algorithm. We also introduce a strategy for evaluating the proposed approach, coupled with ~300 annotations of issues in instances from 12 NLG datasets. Our results show that instance-specific issues output by LLM-as-a-qualitative-judge match those annotated by humans in 2/3 cases, and that LLM-as-a-qualitative-judge is capable of producing error type reports resembling the reports composed by human annotators. We also demonstrate in a case study how the use of LLM-as-a-qualitative-judge can substantially improve NLG systems performance. Our code and data are publicly available at https://github.com/tunde-ajayi/llm-as-a-qualitative-judge.","authors":["Nadezhda Chirkova","Tunde Oluwaseyi Ajayi","Seth Aycock","Zain Muhammad Mujahid","Vladana Perlić","Ekaterina Borisova","Markarit Vartampetian"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.10882v2","updated":"2025-12-19T16:35:45Z","published":"2025-12-11T18:11:46Z","title":"Computational emotion analysis with multimodal LLMs: Current evidence on an emerging methodological opportunity","summary":"Emotions are central to politics and analyzing their role in political communication has a long tradition. As research increasingly leverages audio-visual materials to analyze emotions, the emergence of multimodal generative Artificial Intelligence (AI) promises great advances. However, we lack evidence about the effectiveness of multimodal AI in analyzing emotions in political communication. This paper addresses this gap by evaluating current multimodal large language models (mLLMs) in the video-based analysis of emotional arousal, using two complementary datasets of human-labeled video recordings. It finds that under ideal circumstances, mLLMs' emotional arousal ratings are highly reliable and exhibit little to no demographic bias. However, in recordings of real-world parliamentary debates, mLLMs' arousal ratings fail to deliver on this promise with potential negative consequences for downstream statistical inferences. This study therefore underscores the need for continued, thorough evaluation of emerging generative AI methods in multimodal political analysis and contributes a suitable replicable framework.","authors":["Hauke Licht"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17756v1","updated":"2025-12-19T16:28:57Z","published":"2025-12-19T16:28:57Z","title":"AncientBench: Towards Comprehensive Evaluation on Excavated and Transmitted Chinese Corpora","summary":"Comprehension of ancient texts plays an important role in archaeology and understanding of Chinese history and civilization. The rapid development of large language models needs benchmarks that can evaluate their comprehension of ancient characters. Existing Chinese benchmarks are mostly targeted at modern Chinese and transmitted documents in ancient Chinese, but the part of excavated documents in ancient Chinese is not covered. To meet this need, we propose the AncientBench, which aims to evaluate the comprehension of ancient characters, especially in the scenario of excavated documents. The AncientBench is divided into four dimensions, which correspond to the four competencies of ancient character comprehension: glyph comprehension, pronunciation comprehension, meaning comprehension, and contextual comprehension. The benchmark also contains ten tasks, including radical, phonetic radical, homophone, cloze, translation, and more, providing a comprehensive framework for evaluation. We convened archaeological researchers to conduct experimental evaluations, proposed an ancient model as baseline, and conducted extensive experiments on the currently best-performing large language models. The experimental results reveal the great potential of large language models in ancient textual scenarios as well as the gap with humans. Our research aims to promote the development and application of large language models in the field of archaeology and ancient Chinese language.","authors":["Zhihan Zhou","Daqian Shi","Rui Song","Lida Shi","Xiaolei Diao","Hao Xu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17752v1","updated":"2025-12-19T16:26:21Z","published":"2025-12-19T16:26:21Z","title":"Affect, Body, Cognition, Demographics, and Emotion: The ABCDE of Text Features for Computational Affective Science","summary":"Work in Computational Affective Science and Computational Social Science explores a wide variety of research questions about people, emotions, behavior, and health. Such work often relies on language data that is first labeled with relevant information, such as the use of emotion words or the age of the speaker. Although many resources and algorithms exist to enable this type of labeling, discovering, accessing, and using them remains a substantial impediment, particularly for practitioners outside of computer science. Here, we present the ABCDE dataset (Affect, Body, Cognition, Demographics, and Emotion), a large-scale collection of over 400 million text utterances drawn from social media, blogs, books, and AI-generated sources. The dataset is annotated with a wide range of features relevant to computational affective and social science. ABCDE facilitates interdisciplinary research across numerous fields, including affective science, cognitive science, the digital humanities, sociology, political science, and computational linguistics.","authors":["Jan Philip Wahle","Krishnapriya Vishnubhotla","Bela Gipp","Saif M. Mohammad"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2410.21041v2","updated":"2025-12-19T16:18:06Z","published":"2024-10-28T13:58:04Z","title":"Clean Up the Mess: Addressing Data Pollution in Cryptocurrency Abuse Reporting Services","summary":"Cryptocurrency abuse reporting services are a valuable data source about abusive blockchain addresses, prevalent types of cryptocurrency abuse, and their financial impact on victims. However, they may suffer data pollution due to their crowd-sourced nature. This work analyzes the extent and impact of data pollution in cryptocurrency abuse reporting services and proposes a novel LLM-based defense to address the pollution. We collect 289K abuse reports submitted over 6 years to two popular services and use them to answer three research questions. RQ1 analyzes the extent and impact of pollution. We show that spam reports will eventually flood unchecked abuse reporting services, with BitcoinAbuse receiving 75% of spam before stopping operations. We build a public dataset of 19,443 abuse reports labeled with 19 popular abuse types and use it to reveal the inaccuracy of user-reported abuse types. We identified 91 (0.1%) benign addresses reported, responsible for 60% of all the received funds. RQ2 examines whether we can automate identifying valid reports and their classification into abuse types. We propose an unsupervised LLM-based classifier that achieves an F1 score of 0.95 when classifying reports, an F1 of 0.89 when classifying out-of-distribution data, and an F1 of 0.99 when identifying spam reports. Our unsupervised LLM-based classifier clearly outperforms two baselines: a supervised classifier and a naive usage of the LLM. Finally, RQ3 demonstrates the usefulness of our LLM-based classifier for quantifying the financial impact of different cryptocurrency abuse types. We show that victim-reported losses heavily underestimate cybercriminal revenue by estimating a 29 times higher revenue from deposit transactions. We identified that investment scams have the highest financial impact and that extortions have lower conversion rates but compensate for them with massive email campaigns.","authors":["Gibran Gomez","Kevin van Liebergen","Davide Sanvito","Giuseppe Siracusano","Roberto Gonzalez","Juan Caballero"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2509.12508v4","updated":"2025-12-19T16:18:03Z","published":"2025-09-15T23:19:36Z","title":"Fun-ASR Technical Report","summary":"In recent years, automatic speech recognition (ASR) has witnessed transformative advancements driven by three complementary paradigms: data scaling, model size scaling, and deep integration with large language models (LLMs). However, LLMs are prone to hallucination, which can significantly degrade user experience in real-world ASR applications. In this paper, we present Fun-ASR, a large-scale, LLM-based ASR system that synergistically combines massive data, large model capacity, LLM integration, and reinforcement learning to achieve state-of-the-art performance across diverse and complex speech recognition scenarios. Moreover, Fun-ASR is specifically optimized for practical deployment, with enhancements in streaming capability, noise robustness, code-switching, hotword customization, and satisfying other real-world application requirements. Experimental results show that while most LLM-based ASR systems achieve strong performance on open-source benchmarks, they often underperform on real industry evaluation sets. Thanks to production-oriented optimizations, Fun-ASR achieves state-of-the-art performance on real application datasets, demonstrating its effectiveness and robustness in practical settings. The code and models are accessible at https://github.com/FunAudioLLM/Fun-ASR .","authors":["Keyu An","Yanni Chen","Zhigao Chen","Chong Deng","Zhihao Du","Changfeng Gao","Zhifu Gao","Bo Gong","Xiangang Li","Yabin Li","Ying Liu","Xiang Lv","Yunjie Ji","Yiheng Jiang","Bin Ma","Haoneng Luo","Chongjia Ni","Zexu Pan","Yiping Peng","Zhendong Peng","Peiyao Wang","Hao Wang","Haoxu Wang","Wen Wang","Wupeng Wang","Yuzhong Wu","Biao Tian","Zhentao Tan","Nan Yang","Bin Yuan","Jieping Ye","Jixing Yu","Qinglin Zhang","Kun Zou","Han Zhao","Shengkui Zhao","Jingren Zhou","Yanqiao Zhu"],"pdf_url":"","comment":"Authors are listed in alphabetical order. Work in progress"},{"id":"http://arxiv.org/abs/2512.17738v1","updated":"2025-12-19T16:17:23Z","published":"2025-12-19T16:17:23Z","title":"When the Gold Standard isn't Necessarily Standard: Challenges of Evaluating the Translation of User-Generated Content","summary":"User-generated content (UGC) is characterised by frequent use of non-standard language, from spelling errors to expressive choices such as slang, character repetitions, and emojis. This makes evaluating UGC translation particularly challenging: what counts as a \"good\" translation depends on the level of standardness desired in the output. To explore this, we examine the human translation guidelines of four UGC datasets, and derive a taxonomy of twelve non-standard phenomena and five translation actions (NORMALISE, COPY, TRANSFER, OMIT, CENSOR). Our analysis reveals notable differences in how UGC is treated, resulting in a spectrum of standardness in reference translations. Through a case study on large language models (LLMs), we show that translation scores are highly sensitive to prompts with explicit translation instructions for UGC, and that they improve when these align with the dataset's guidelines. We argue that when preserving UGC style is important, fair evaluation requires both models and metrics to be aware of translation guidelines. Finally, we call for clear guidelines during dataset creation and for the development of controllable, guideline-aware evaluation frameworks for UGC translation.","authors":["Lydia Nishimwe","Benoît Sagot","Rachel Bawden"],"pdf_url":"","comment":"10 pages, 19 pages with references and appendices"},{"id":"http://arxiv.org/abs/2412.17669v2","updated":"2025-12-19T15:52:00Z","published":"2024-12-23T15:54:15Z","title":"Generating Completions for Broca's Aphasic Sentences Using Large Language Models","summary":"Broca's aphasia is a type of aphasia characterized by non-fluent, effortful and agrammatic speech production with relatively good comprehension. Since traditional aphasia treatment methods are often time-consuming, labour-intensive, and do not reflect real-world conversations, applying natural language processing based approaches such as Large Language Models (LLMs) could potentially contribute to improving existing treatment approaches. To address this issue, we explore the use of sequence-to-sequence LLMs for completing Broca's aphasic sentences. We first generate synthetic Broca's aphasic data using a rule-based system designed to mirror the linguistic characteristics of Broca's aphasic speech. Using this synthetic data (without authentic aphasic samples), we then fine-tune four pre-trained LLMs on the task of completing agrammatic sentences. We evaluate our fine-tuned models on both synthetic and authentic Broca's aphasic data. We demonstrate LLMs' capability for reconstructing agrammatic sentences, with the models showing improved performance with longer input utterances. Our result highlights the LLMs' potential in advancing communication aids for individuals with Broca's aphasia and possibly other clinical populations.","authors":["Sijbren van Vaals","Yevgen Matusevych","Frank Tsiwah"],"pdf_url":"","comment":"in IEEE Journal of Biomedical and Health Informatics"},{"id":"http://arxiv.org/abs/2512.17677v1","updated":"2025-12-19T15:17:19Z","published":"2025-12-19T15:17:19Z","title":"Toward Ethical AI Through Bayesian Uncertainty in Neural Question Answering","summary":"We explore Bayesian reasoning as a means to quantify uncertainty in neural networks for question answering. Starting with a multilayer perceptron on the Iris dataset, we show how posterior inference conveys confidence in predictions. We then extend this to language models, applying Bayesian inference first to a frozen head and finally to LoRA-adapted transformers, evaluated on the CommonsenseQA benchmark. Rather than aiming for state-of-the-art accuracy, we compare Laplace approximations against maximum a posteriori (MAP) estimates to highlight uncertainty calibration and selective prediction. This allows models to abstain when confidence is low. An ``I don't know'' response not only improves interpretability but also illustrates how Bayesian methods can contribute to more responsible and ethical deployment of neural question-answering systems.","authors":["Riccardo Di Sipio"],"pdf_url":"","comment":"14 pages, 8 figures,"},{"id":"http://arxiv.org/abs/2512.17657v1","updated":"2025-12-19T14:56:28Z","published":"2025-12-19T14:56:28Z","title":"Peeking Into The Future For Contextual Biasing","summary":"While end-to-end (E2E) automatic speech recognition (ASR) models excel at general transcription, they struggle to recognize rare or unseen named entities (e.g., contact names, locations), which are critical for downstream applications like virtual assistants. In this paper, we propose a contextual biasing method for attention based encoder decoder (AED) models using a list of candidate named entities. Instead of predicting only the next token, we simultaneously predict multiple future tokens, enabling the model to \"peek into the future\" and score potential candidate entities in the entity list. Moreover, our approach leverages the multi-token prediction logits directly without requiring additional entity encoders or cross-attention layers, significantly reducing architectural complexity. Experiments on Librispeech demonstrate that our approach achieves up to 50.34% relative improvement in named entity word error rate compared to the baseline AED model.","authors":["Ramaneswaran Selvakumar","Cindy Tseng","Eesung Kim","Vijendra Raj Apsingekar","Yun Tang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17648v1","updated":"2025-12-19T14:48:59Z","published":"2025-12-19T14:48:59Z","title":"Simulstream: Open-Source Toolkit for Evaluation and Demonstration of Streaming Speech-to-Text Translation Systems","summary":"Streaming Speech-to-Text Translation (StreamST) requires producing translations concurrently with incoming speech, imposing strict latency constraints and demanding models that balance partial-information decision-making with high translation quality. Research efforts on the topic have so far relied on the SimulEval repository, which is no longer maintained and does not support systems that revise their outputs. In addition, it has been designed for simulating the processing of short segments, rather than long-form audio streams, and it does not provide an easy method to showcase systems in a demo. As a solution, we introduce simulstream, the first open-source framework dedicated to unified evaluation and demonstration of StreamST systems. Designed for long-form speech processing, it supports not only incremental decoding approaches, but also re-translation methods, enabling for their comparison within the same framework both in terms of quality and latency. In addition, it also offers an interactive web interface to demo any system built within the tool.","authors":["Marco Gaido","Sara Papi","Mauro Cettolo","Matteo Negri","Luisa Bentivogli"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17639v1","updated":"2025-12-19T14:41:09Z","published":"2025-12-19T14:41:09Z","title":"Linear Personality Probing and Steering in LLMs: A Big Five Study","summary":"Large language models (LLMs) exhibit distinct and consistent personalities that greatly impact trust and engagement. While this means that personality frameworks would be highly valuable tools to characterize and control LLMs' behavior, current approaches remain either costly (post-training) or brittle (prompt engineering). Probing and steering via linear directions has recently emerged as a cheap and efficient alternative. In this paper, we investigate whether linear directions aligned with the Big Five personality traits can be used for probing and steering model behavior. Using Llama 3.3 70B, we generate descriptions of 406 fictional characters and their Big Five trait scores. We then prompt the model with these descriptions and questions from the Alpaca questionnaire, allowing us to sample hidden activations that vary along personality traits in known, quantifiable ways. Using linear regression, we learn a set of per-layer directions in activation space, and test their effectiveness for probing and steering model behavior. Our results suggest that linear directions aligned with trait-scores are effective probes for personality detection, while their steering capabilities strongly depend on context, producing reliable effects in forced-choice tasks but limited influence in open-ended generation or when additional context is present in the prompt.","authors":["Michel Frising","Daniel Balcells"],"pdf_url":"","comment":"29 pages, 6 figures"},{"id":"http://arxiv.org/abs/2512.17630v1","updated":"2025-12-19T14:33:14Z","published":"2025-12-19T14:33:14Z","title":"Confidence-Credibility Aware Weighted Ensembles of Small LLMs Outperform Large LLMs in Emotion Detection","summary":"This paper introduces a confidence-weighted, credibility-aware ensemble framework for text-based emotion detection, inspired by Condorcet's Jury Theorem (CJT). Unlike conventional ensembles that often rely on homogeneous architectures, our approach combines architecturally diverse small transformer-based large language models (sLLMs) - BERT, RoBERTa, DistilBERT, DeBERTa, and ELECTRA, each fully fine-tuned for emotion classification. To preserve error diversity, we minimize parameter convergence while taking advantage of the unique biases of each model. A dual-weighted voting mechanism integrates both global credibility (validation F1 score) and local confidence (instance-level probability) to dynamically weight model contributions. Experiments on the DAIR-AI dataset demonstrate that our credibility-confidence ensemble achieves a macro F1 score of 93.5 percent, surpassing state-of-the-art benchmarks and significantly outperforming large-scale LLMs, including Falcon, Mistral, Qwen, and Phi, even after task-specific Low-Rank Adaptation (LoRA). With only 595M parameters in total, our small LLMs ensemble proves more parameter-efficient and robust than models up to 7B parameters, establishing that carefully designed ensembles of small, fine-tuned models can outperform much larger LLMs in specialized natural language processing (NLP) tasks such as emotion detection.","authors":["Menna Elgabry","Ali Hamdi"],"pdf_url":"","comment":"Accepted at IRICT 2025"},{"id":"http://arxiv.org/abs/2507.06261v6","updated":"2025-12-19T14:25:46Z","published":"2025-07-07T17:36:04Z","title":"Gemini 2.5: Pushing the Frontier with Advanced Reasoning, Multimodality, Long Context, and Next Generation Agentic Capabilities","summary":"In this report, we introduce the Gemini 2.X model family: Gemini 2.5 Pro and Gemini 2.5 Flash, as well as our earlier Gemini 2.0 Flash and Flash-Lite models. Gemini 2.5 Pro is our most capable model yet, achieving SoTA performance on frontier coding and reasoning benchmarks. In addition to its incredible coding and reasoning skills, Gemini 2.5 Pro is a thinking model that excels at multimodal understanding and it is now able to process up to 3 hours of video content. Its unique combination of long context, multimodal and reasoning capabilities can be combined to unlock new agentic workflows. Gemini 2.5 Flash provides excellent reasoning abilities at a fraction of the compute and latency requirements and Gemini 2.0 Flash and Flash-Lite provide high performance at low latency and cost. Taken together, the Gemini 2.X model generation spans the full Pareto frontier of model capability vs cost, allowing users to explore the boundaries of what is possible with complex agentic problem solving.","authors":["Gheorghe Comanici","Eric Bieber","Mike Schaekermann","Ice Pasupat","Noveen Sachdeva","Inderjit Dhillon","Marcel Blistein","Ori Ram","Dan Zhang","Evan Rosen","Luke Marris","Sam Petulla","Colin Gaffney","Asaf Aharoni","Nathan Lintz","Tiago Cardal Pais","Henrik Jacobsson","Idan Szpektor","Nan-Jiang Jiang","Krishna Haridasan","Ahmed Omran","Nikunj Saunshi","Dara Bahri","Gaurav Mishra","Eric Chu","Toby Boyd","Brad Hekman","Aaron Parisi","Chaoyi Zhang","Kornraphop Kawintiranon","Tania Bedrax-Weiss","Oliver Wang","Ya Xu","Ollie Purkiss","Uri Mendlovic","Ilaï Deutel","Nam Nguyen","Adam Langley","Flip Korn","Lucia Rossazza","Alexandre Ramé","Sagar Waghmare","Helen Miller","Nathan Byrd","Ashrith Sheshan","Raia Hadsell","Sangnie Bhardwaj","Pawel Janus","Tero Rissa","Dan Horgan","Alvin Abdagic","Lior Belenki","James Allingham","Anima Singh","Theo Guidroz","Srivatsan Srinivasan","Herman Schmit","Kristen Chiafullo","Andre Elisseeff","Nilpa Jha","Prateek Kolhar","Leonard Berrada","Frank Ding","Xiance Si","Shrestha Basu Mallick","Franz Och","Sofia Erell","Eric Ni","Tejasi Latkar","Sherry Yang","Petar Sirkovic","Ziqiang Feng","Robert Leland","Rachel Hornung","Gang Wu","Charles Blundell","Hamidreza Alvari","Po-Sen Huang","Cathy Yip","Sanja Deur","Li Liu","Gabriela Surita","Pablo Duque","Dima Damen","Johnson Jia","Arthur Guez","Markus Mircea","Animesh Sinha","Alberto Magni","Paweł Stradomski","Tal Marian","Vlado Galić","Wenhu Chen","Hisham Husain","Achintya Singhal","Dominik Grewe","François-Xavier Aubet","Shuang Song","Lorenzo Blanco","Leland Rechis","Lewis Ho","Rich Munoz","Kelvin Zheng","Jessica Hamrick","Kevin Mather","Hagai Taitelbaum","Eliza Rutherford","Yun Lei","Kuangyuan Chen","Anand Shukla","Erica Moreira","Eric Doi","Berivan Isik","Nir Shabat","Dominika Rogozińska","Kashyap Kolipaka","Jason Chang","Eugen Vušak","Srinivasan Venkatachary","Shadi Noghabi","Tarun Bharti","Younghoon Jun","Aleksandr Zaks","Simon Green","Jeshwanth Challagundla","William Wong","Muqthar Mohammad","Dean Hirsch","Yong Cheng","Iftekhar Naim","Lev Proleev","Damien Vincent","Aayush Singh","Maxim Krikun","Dilip Krishnan","Zoubin Ghahramani","Aviel Atias","Rajeev Aggarwal","Christo Kirov","Dimitrios Vytiniotis","Christy Koh","Alexandra Chronopoulou","Pawan Dogra","Vlad-Doru Ion","Gladys Tyen","Jason Lee","Felix Weissenberger","Trevor Strohman","Ashwin Balakrishna","Jack Rae","Marko Velic","Raoul de Liedekerke","Oded Elyada","Wentao Yuan","Canoee Liu","Lior Shani","Sergey Kishchenko","Bea Alessio","Yandong Li","Richard Song","Sam Kwei","Orion Jankowski","Aneesh Pappu","Youhei Namiki","Yenai Ma","Nilesh Tripuraneni","Colin Cherry","Marissa Ikonomidis","Yu-Cheng Ling","Colin Ji","Beka Westberg","Auriel Wright","Da Yu","David Parkinson","Swaroop Ramaswamy","Jerome Connor","Soheil Hassas Yeganeh","Snchit Grover","George Kenwright","Lubo Litchev","Chris Apps","Alex Tomala","Felix Halim","Alex Castro-Ros","Zefei Li","Anudhyan Boral","Pauline Sho","Michal Yarom","Eric Malmi","David Klinghoffer","Rebecca Lin","Alan Ansell","Pradeep Kumar S","Shubin Zhao","Siqi Zuo","Adam Santoro","Heng-Tze Cheng","Solomon Demmessie","Yuchi Liu","Nicole Brichtova","Allie Culp","Nathaniel Braun","Dan Graur","Will Ng","Nikhil Mehta","Aaron Phillips","Patrik Sundberg","Varun Godbole","Fangyu Liu","Yash Katariya","David Rim","Mojtaba Seyedhosseini","Sean Ammirati","Jonas Valfridsson","Mahan Malihi","Timothy Knight","Andeep Toor","Thomas Lampe","Abe Ittycheriah","Lewis Chiang","Chak Yeung","Alexandre Fréchette","Jinmeng Rao","Huisheng Wang","Himanshu Srivastava","Richard Zhang","Rocky Rhodes","Ariel Brand","Dean Weesner","Ilya Figotin","Felix Gimeno","Rachana Fellinger","Pierre Marcenac","José Leal","Eyal Marcus","Victor Cotruta","Rodrigo Cabrera","Sheryl Luo","Dan Garrette","Vera Axelrod","Sorin Baltateanu","David Barker","Dongkai Chen","Horia Toma","Ben Ingram","Jason Riesa","Chinmay Kulkarni","Yujing Zhang","Hongbin Liu","Chao Wang","Martin Polacek","Will Wu","Kai Hui","Adrian N Reyes","Yi Su","Megan Barnes","Ishaan Malhi","Anfal Siddiqui","Qixuan Feng","Mihai Damaschin","Daniele Pighin","Andreas Steiner","Samuel Yang","Ramya Sree Boppana","Simeon Ivanov","Arun Kandoor","Aditya Shah","Asier Mujika","Da Huang","Christopher A. Choquette-Choo","Mohak Patel","Tianhe Yu","Toni Creswell"," Jerry"," Liu","Catarina Barros","Yasaman Razeghi","Aurko Roy","Phil Culliton","Binbin Xiong","Jiaqi Pan","Thomas Strohmann","Tolly Powell","Babi Seal","Doug DeCarlo","Pranav Shyam","Kaan Katircioglu","Xuezhi Wang","Cassidy Hardin","Immanuel Odisho","Josef Broder","Oscar Chang","Arun Nair","Artem Shtefan","Maura O'Brien","Manu Agarwal","Sahitya Potluri","Siddharth Goyal","Amit Jhindal","Saksham Thakur","Yury Stuken","James Lyon","Kristina Toutanova","Fangxiaoyu Feng","Austin Wu","Ben Horn","Alek Wang","Alex Cullum","Gabe Taubman","Disha Shrivastava","Chongyang Shi","Hamish Tomlinson","Roma Patel","Tao Tu","Ada Maksutaj Oflazer","Francesco Pongetti","Mingyao Yang","Adrien Ali Taïga","Vincent Perot","Nuo Wang Pierse","Feng Han","Yoel Drori","Iñaki Iturrate","Ayan Chakrabarti","Legg Yeung","Dave Dopson","Yi-ting Chen","Apoorv Kulshreshtha","Tongfei Guo","Philip Pham","Tal Schuster","Junquan Chen","Alex Polozov","Jinwei Xing","Huanjie Zhou","Praneeth Kacham","Doron Kukliansky","Antoine Miech","Sergey Yaroshenko","Ed Chi","Sholto Douglas","Hongliang Fei","Mathieu Blondel","Preethi Myla","Lior Madmoni","Xing Wu","Daniel Keysers","Kristian Kjems","Isabela Albuquerque","Lijun Yu","Joel D'sa","Michelle Plantan","Vlad Ionescu","Jaume Sanchez Elias","Abhirut Gupta","Manish Reddy Vuyyuru","Fred Alcober","Tong Zhou","Kaiyang Ji","Florian Hartmann","Subha Puttagunta","Hugo Song","Ehsan Amid","Anca Stefanoiu","Andrew Lee","Paul Pucciarelli","Emma Wang","Amit Raul","Slav Petrov","Isaac Tian","Valentin Anklin","Nana Nti","Victor Gomes","Max Schumacher","Grace Vesom","Alex Panagopoulos","Konstantinos Bousmalis","Daniel Andor","Josh Jacob","Yuan Zhang","Bill Rosgen","Matija Kecman","Matthew Tung","Alexandra Belias","Noah Goodman","Paul Covington","Brian Wieder","Nikita Saxena","Elnaz Davoodi","Muhuan Huang","Sharath Maddineni","Vincent Roulet","Folawiyo Campbell-Ajala","Pier Giuseppe Sessa"," Xintian"," Wu","Guangda Lai","Paul Collins","Alex Haig","Vytenis Sakenas","Xiaowei Xu","Marissa Giustina","Laurent El Shafey","Pichi Charoenpanit","Shefali Garg","Joshua Ainslie","Boone Severson","Montse Gonzalez Arenas","Shreya Pathak","Sujee Rajayogam","Jie Feng","Michiel Bakker","Sheng Li","Nevan Wichers","Jamie Rogers","Xinyang Geng","Yeqing Li","Rolf Jagerman","Chao Jia","Nadav Olmert","David Sharon","Matthew Mauger","Sandeep Mariserla","Hongxu Ma","Megha Mohabey","Kyuyeun Kim","Alek Andreev","Scott Pollom","Juliette Love","Vihan Jain","Priyanka Agrawal","Yannick Schroecker","Alisa Fortin","Manfred Warmuth","Ji Liu","Andrew Leach","Irina Blok","Ganesh Poomal Girirajan","Roee Aharoni","Benigno Uria","Andrei Sozanschi","Dan Goldberg","Lucian Ionita","Marco Tulio Ribeiro","Martin Zlocha","Vighnesh Birodkar","Sami Lachgar","Liangzhe Yuan","Himadri Choudhury","Matt Ginsberg","Fei Zheng","Gregory Dibb","Emily Graves","Swachhand Lokhande","Gabriel Rasskin","George-Cristian Muraru","Corbin Quick","Sandeep Tata","Pierre Sermanet","Aditya Chawla","Itay Karo","Yan Wang","Susan Zhang","Orgad Keller","Anca Dragan","Guolong Su","Ian Chou","Xi Liu","Yiqing Tao","Shruthi Prabhakara","Marc Wilson","Ruibo Liu","Shibo Wang","Georgie Evans","David Du","Alfonso Castaño","Gautam Prasad","Mona El Mahdy","Sebastian Gerlach","Machel Reid","Jarrod Kahn","Amir Zait","Thanumalayan Sankaranarayana Pillai","Thatcher Ulrich","Guanyu Wang","Jan Wassenberg","Efrat Farkash","Kiran Yalasangi","Congchao Wang","Maria Bauza","Simon Bucher","Ting Liu","Jun Yan","Gary Leung","Vikas Sindhwani","Parker Barnes","Avi Singh","Ivan Jurin","Jichuan Chang","Niket Kumar Bhumihar","Sivan Eiger","Gui Citovsky","Ben Withbroe","Zhang Li","Siyang Xue","Niccolò Dal Santo","Georgi Stoyanov","Yves Raimond","Steven Zheng","Yilin Gao","Vít Listík","Sławek Kwasiborski","Rachel Saputro","Adnan Ozturel","Ganesh Mallya","Kushal Majmundar","Ross West","Paul Caron","Jinliang Wei","Lluis Castrejon","Sharad Vikram","Deepak Ramachandran","Nikhil Dhawan","Jiho Park","Sara Smoot","George van den Driessche","Yochai Blau","Chase Malik","Wei Liang","Roy Hirsch","Cicero Nogueira dos Santos","Eugene Weinstein","Aäron van den Oord","Sid Lall","Nicholas FitzGerald","Zixuan Jiang","Xuan Yang","Dale Webster","Ali Elqursh","Aedan Pope","Georges Rotival","David Raposo","Wanzheng Zhu","Jeff Dean","Sami Alabed","Dustin Tran","Arushi Gupta","Zach Gleicher","Jessica Austin","Edouard Rosseel","Megh Umekar","Dipanjan Das","Yinghao Sun","Kai Chen","Karolis Misiunas","Xiang Zhou","Yixian Di","Alyssa Loo","Josh Newlan","Bo Li","Vinay Ramasesh","Ying Xu","Alex Chen","Sudeep Gandhe","Radu Soricut","Nikita Gupta","Shuguang Hu","Seliem El-Sayed","Xavier Garcia","Idan Brusilovsky","Pu-Chin Chen","Andrew Bolt","Lu Huang","Alex Gurney","Zhiying Zhang","Alexander Pritzel","Jarek Wilkiewicz","Bryan Seybold","Bhargav Kanagal Shamanna","Felix Fischer","Josef Dean","Karan Gill","Ross Mcilroy","Abhishek Bhowmick","Jeremy Selier","Antoine Yang","Derek Cheng","Vladimir Magay","Jie Tan","Dhriti Varma","Christian Walder","Tomas Kocisky","Ryo Nakashima","Paul Natsev","Mike Kwong","Ionel Gog","Chiyuan Zhang","Sander Dieleman","Thomas Jimma","Andrey Ryabtsev","Siddhartha Brahma","David Steiner","Dayou Du","Ante Žužul","Mislav Žanić","Mukund Raghavachari","Willi Gierke","Zeyu Zheng","Dessie Petrova","Yann Dauphin","Yuchuan Liu","Ido Kessler","Steven Hand","Chris Duvarney","Seokhwan Kim","Hyo Lee","Léonard Hussenot","Jeffrey Hui","Josh Smith","Deepali Jain","Jiawei Xia","Gaurav Singh Tomar","Keyvan Amiri","Du Phan","Fabian Fuchs","Tobias Weyand","Nenad Tomasev","Alexandra Cordell","Xin Liu","Jonathan Mallinson","Pankaj Joshi","Andy Crawford","Arun Suggala","Steve Chien","Nick Fernando","Mariella Sanchez-Vargas","Duncan Williams","Phil Crone","Xiyang Luo","Igor Karpov","Jyn Shan","Terry Thurk","Robin Strudel","Paul Voigtlaender","Piyush Patil","Tim Dozat","Ali Khodaei","Sahil Singla","Piotr Ambroszczyk","Qiyin Wu","Yifan Chang","Brian Roark","Chaitra Hegde","Tianli Ding","Angelos Filos","Zhongru Wu","André Susano Pinto","Shuang Liu","Saarthak Khanna","Aditya Pandey","Siobhan Mcloughlin","Qiujia Li","Sam Haves","Allan Zhou","Elena Buchatskaya","Isabel Leal","Peter de Boursac","Nami Akazawa","Nina Anderson","Terry Chen","Krishna Somandepalli","Chen Liang","Sheela Goenka","Stephanie Winkler","Alexander Grushetsky","Yifan Ding","Jamie Smith","Fan Ye","Jordi Pont-Tuset","Eric Li","Ruichao Li","Tomer Golany","Dawid Wegner","Tao Jiang","Omer Barak","Yuan Shangguan","Eszter Vértes","Renee Wong","Jörg Bornschein","Alex Tudor","Michele Bevilacqua","Tom Schaul","Ankit Singh Rawat","Yang Zhao","Kyriakos Axiotis","Lei Meng","Cory McLean","Jonathan Lai","Jennifer Beattie","Nate Kushman","Yaxin Liu","Blair Kutzman","Fiona Lang","Jingchen Ye","Praneeth Netrapalli","Pushkar Mishra","Myriam Khan","Megha Goel","Rob Willoughby","David Tian","Honglei Zhuang","JD Chen","Zak Tsai","Tasos Kementsietsidis","Arjun Khare","James Keeling","Keyang Xu","Nathan Waters","Florent Altché","Ashok Popat","Bhavishya Mittal","David Saxton","Dalia El Badawy","Michael Mathieu","Zheng Zheng","Hao Zhou","Nishant Ranka","Richard Shin","Qingnan Duan","Tim Salimans","Ioana Mihailescu","Uri Shaham","Ming-Wei Chang","Yannis Assael","Nishanth Dikkala","Martin Izzard","Vincent Cohen-Addad","Cat Graves","Vlad Feinberg","Grace Chung","DJ Strouse","Danny Karmon","Sahand Sharifzadeh","Zoe Ashwood","Khiem Pham","Jon Blanton","Alex Vasiloff","Jarred Barber","Mark Geller","Aurick Zhou","Fedir Zubach","Tzu-Kuo Huang","Lei Zhang","Himanshu Gupta","Matt Young","Julia Proskurnia","Ronny Votel","Valentin Gabeur","Gabriel Barcik","Aditya Tripathi","Hongkun Yu","Geng Yan","Beer Changpinyo","Filip Pavetić","Amy Coyle","Yasuhisa Fujii","Jorge Gonzalez Mendez","Tianhao Zhou","Harish Rajamani","Blake Hechtman","Eddie Cao","Da-Cheng Juan","Yi-Xuan Tan","Valentin Dalibard","Yilun Du","Natalie Clay","Kaisheng Yao","Wenhao Jia","Dimple Vijaykumar","Yuxiang Zhou","Xinyi Bai","Wei-Chih Hung","Steven Pecht","Georgi Todorov","Nikhil Khadke","Pramod Gupta","Preethi Lahoti","Arnaud Autef","Karthik Duddu","James Lee-Thorp","Alexander Bykovsky","Tautvydas Misiunas","Sebastian Flennerhag","Santhosh Thangaraj","Jed McGiffin","Zack Nado","Markus Kunesch","Andreas Noever","Amir Hertz","Marco Liang","Victor Stone","Evan Palmer","Samira Daruki","Arijit Pramanik","Siim Põder","Austin Kyker","Mina Khan","Evgeny Sluzhaev","Marvin Ritter","Avraham Ruderman","Wenlei Zhou","Chirag Nagpal","Kiran Vodrahalli","George Necula","Paul Barham","Ellie Pavlick","Jay Hartford","Izhak Shafran","Long Zhao","Maciej Mikuła","Tom Eccles","Hidetoshi Shimokawa","Kanav Garg","Luke Vilnis","Hanwen Chen","Ilia Shumailov","Kuang-Huei Lee","Abdelrahman Abdelhamed","Meiyan Xie","Vered Cohen","Ester Hlavnova","Dan Malkin","Chawin Sitawarin","James Lottes","Pauline Coquinot","Tianli Yu","Sandeep Kumar","Jingwei Zhang","Aroma Mahendru","Zafarali Ahmed","James Martens","Tao Chen","Aviel Boag","Daiyi Peng","Coline Devin","Arseniy Klimovskiy","Mary Phuong","Danny Vainstein","Jin Xie","Bhuvana Ramabhadran","Nathan Howard","Xinxin Yu","Gitartha Goswami","Jingyu Cui","Sam Shleifer","Mario Pinto","Chih-Kuan Yeh","Ming-Hsuan Yang","Sara Javanmardi","Dan Ethier","Chace Lee","Jordi Orbay","Suyog Kotecha","Carla Bromberg","Pete Shaw","James Thornton","Adi Gerzi Rosenthal","Shane Gu","Matt Thomas","Ian Gemp","Aditya Ayyar","Asahi Ushio","Aarush Selvan","Joel Wee","Chenxi Liu","Maryam Majzoubi","Weiren Yu","Jake Abernethy","Tyler Liechty","Renke Pan","Hoang Nguyen"," Qiong"," Hu","Sarah Perrin","Abhinav Arora","Emily Pitler","Weiyi Wang","Kaushik Shivakumar","Flavien Prost","Ben Limonchik","Jing Wang","Yi Gao","Timothee Cour","Shyamal Buch","Huan Gui","Maria Ivanova","Philipp Neubeck","Kelvin Chan","Lucy Kim","Huizhong Chen","Naman Goyal","Da-Woon Chung","Lu Liu","Yao Su","Anastasia Petrushkina","Jiajun Shen","Armand Joulin","Yuanzhong Xu","Stein Xudong Lin","Yana Kulizhskaya","Ciprian Chelba","Shobha Vasudevan","Eli Collins","Vasilisa Bashlovkina","Tony Lu","Doug Fritz","Jongbin Park","Yanqi Zhou","Chen Su","Richard Tanburn","Mikhail Sushkov","Mitchelle Rasquinha","Jinning Li","Jennifer Prendki","Yiming Li","Pallavi LV","Shriya Sharma","Hen Fitoussi","Hui Huang","Andrew Dai","Phuong Dao","Mike Burrows","Henry Prior","Danfeng Qin","Golan Pundak","Lars Lowe Sjoesund","Art Khurshudov","Zhenkai Zhu","Albert Webson","Elizabeth Kemp","Tat Tan","Saurabh Agrawal","Susie Sargsyan","Liqun Cheng","Jim Stephan","Tom Kwiatkowski","David Reid","Arunkumar Byravan","Assaf Hurwitz Michaely","Nicolas Heess","Luowei Zhou","Sonam Goenka","Viral Carpenter","Anselm Levskaya","Bo Wang","Reed Roberts","Rémi Leblond","Sharat Chikkerur","Stav Ginzburg","Max Chang","Robert Riachi"," Chuqiao"," Xu","Zalán Borsos","Michael Pliskin","Julia Pawar","Morgane Lustman","Hannah Kirkwood","Ankit Anand","Aditi Chaudhary","Norbert Kalb","Kieran Milan","Sean Augenstein","Anna Goldie","Laurel Prince","Karthik Raman","Yanhua Sun","Vivian Xia","Aaron Cohen","Zhouyuan Huo","Josh Camp","Seher Ellis","Lukas Zilka","David Vilar Torres","Lisa Patel","Sho Arora","Betty Chan","Jonas Adler","Kareem Ayoub","Jacky Liang","Fayaz Jamil","Jiepu Jiang","Simon Baumgartner","Haitian Sun","Yael Karov","Yaroslav Akulov","Hui Zheng","Irene Cai","Claudio Fantacci","James Rubin","Alex Rav Acha","Mengchao Wang","Nina D'Souza","Rohit Sathyanarayana","Shengyang Dai","Simon Rowe","Andrey Simanovsky","Omer Goldman","Yuheng Kuang","Xiaoyue Pan","Andrew Rosenberg","Tania Rojas-Esponda","Praneet Dutta","Amy Zeng","Irina Jurenka","Greg Farquhar","Yamini Bansal","Shariq Iqbal","Becca Roelofs","Ga-Young Joung","Parker Beak","Changwan Ryu","Ryan Poplin","Yan Wu","Jean-Baptiste Alayrac","Senaka Buthpitiya","Olaf Ronneberger","Caleb Habtegebriel","Wei Li","Paul Cavallaro","Aurora Wei","Guy Bensky","Timo Denk","Harish Ganapathy","Jeff Stanway","Pratik Joshi","Francesco Bertolini","Jessica Lo","Olivia Ma","Zachary Charles","Geta Sampemane","Himanshu Sahni","Xu Chen","Harry Askham","David Gaddy","Peter Young","Jiewen Tan","Matan Eyal","Arthur Bražinskas","Li Zhong","Zhichun Wu","Mark Epstein","Kai Bailey","Andrew Hard","Kamyu Lee","Sasha Goldshtein","Alex Ruiz","Mohammed Badawi","Matthias Lochbrunner","JK Kearns","Ashley Brown","Fabio Pardo","Theophane Weber","Haichuan Yang","Pan-Pan Jiang","Berkin Akin","Zhao Fu","Marcus Wainwright","Chi Zou","Meenu Gaba","Pierre-Antoine Manzagol","Wendy Kan","Yang Song","Karina Zainullina","Rui Lin","Jeongwoo Ko","Salil Deshmukh","Apoorv Jindal","James Svensson","Divya Tyam","Heri Zhao","Christine Kaeser-Chen","Scott Baird","Pooya Moradi","Jamie Hall","Qiuchen Guo","Vincent Tsang","Bowen Liang","Fernando Pereira","Suhas Ganesh","Ivan Korotkov","Jakub Adamek","Sridhar Thiagarajan","Vinh Tran","Charles Chen","Chris Tar","Sanil Jain","Ishita Dasgupta","Taylan Bilal","David Reitter","Kai Zhao","Giulia Vezzani","Yasmin Gehman","Pulkit Mehta","Lauren Beltrone","Xerxes Dotiwalla","Sergio Guadarrama","Zaheer Abbas","Stefani Karp","Petko Georgiev","Chun-Sung Ferng","Marc Brockschmidt","Liqian Peng","Christoph Hirnschall","Vikas Verma","Yingying Bi","Ying Xiao","Avigail Dabush","Kelvin Xu","Phil Wallis","Randall Parker","Qifei Wang","Yang Xu","Ilkin Safarli","Dinesh Tewari","Yin Zhang","Seungyeon Kim","Andrea Gesmundo","Mackenzie Thomas","Sergey Levi","Ahmed Chowdhury","Kanishka Rao","Peter Garst","Sam Conway-Rahman","Helen Ran","Kay McKinney","Zhisheng Xiao","Wenhao Yu","Rohan Agrawal","Axel Stjerngren","Catalin Ionescu","Jingjing Chen","Vivek Sharma","Justin Chiu","Fei Liu","Ken Franko","Clayton Sanford","Xingyu Cai","Paul Michel","Sanjay Ganapathy","Jane Labanowski","Zachary Garrett","Ben Vargas","Sean Sun","Bryan Gale","Thomas Buschmann","Guillaume Desjardins","Nimesh Ghelani","Palak Jain","Mudit Verma","Chulayuth Asawaroengchai","Julian Eisenschlos","Jitendra Harlalka","Hideto Kazawa","Don Metzler","Joshua Howland","Ying Jian","Jake Ades","Viral Shah","Tynan Gangwani","Seungji Lee","Roman Ring","Steven M. Hernandez","Dean Reich","Amer Sinha","Ashutosh Sathe","Joe Kovac","Ashleah Gill","Ajay Kannan","Andrea D'olimpio","Martin Sevenich","Jay Whang","Been Kim","Khe Chai Sim","Jilin Chen","Jiageng Zhang","Shuba Lall","Yossi Matias","Bill Jia","Abe Friesen","Sara Nasso","Ashish Thapliyal","Bryan Perozzi","Ting Yu","Anna Shekhawat","Safeen Huda","Peter Grabowski","Eric Wang","Ashwin Sreevatsa","Hilal Dib","Mehadi Hassen","Parker Schuh","Vedrana Milutinovic","Chris Welty","Michael Quinn","Ali Shah","Bangju Wang","Gabe Barth-Maron","Justin Frye","Natalie Axelsson","Tao Zhu","Yukun Ma","Irene Giannoumis","Hanie Sedghi","Chang Ye","Yi Luan","Kevin Aydin","Bilva Chandra","Vivek Sampathkumar","Ronny Huang","Victor Lavrenko","Ahmed Eleryan","Zhi Hong","Steven Hansen","Sara Mc Carthy","Bidisha Samanta","Domagoj Ćevid","Xin Wang","Fangtao Li","Michael Voznesensky","Matt Hoffman","Andreas Terzis","Vikash Sehwag","Gil Fidel","Luheng He","Mu Cai","Yanzhang He","Alex Feng","Martin Nikoltchev","Samrat Phatale","Jason Chase","Rory Lawton","Ming Zhang","Tom Ouyang","Manuel Tragut","Mehdi Hafezi Manshadi","Arjun Narayanan","Jiaming Shen","Xu Gao","Tolga Bolukbasi","Nick Roy","Xin Li","Daniel Golovin","Liviu Panait","Zhen Qin","Guangxing Han","Thomas Anthony","Sneha Kudugunta","Viorica Patraucean","Aniket Ray","Xinyun Chen","Xiaochen Yang","Tanuj Bhatia","Pranav Talluri","Alex Morris","Andrija Ražnatović","Bethanie Brownfield","James An","Sheng Peng","Patrick Kane","Ce Zheng","Nico Duduta","Joshua Kessinger","James Noraky","Siqi Liu","Keran Rong","Petar Veličković","Keith Rush","Alex Goldin","Fanny Wei","Shiva Mohan Reddy Garlapati","Caroline Pantofaru","Okwan Kwon","Jianmo Ni","Eric Noland","Julia Di Trapani","Françoise Beaufays","Abhijit Guha Roy","Yinlam Chow","Aybuke Turker","Geoffrey Cideron","Lantao Mei","Jon Clark","Qingyun Dou","Matko Bošnjak","Ralph Leith","Yuqing Du","Amir Yazdanbakhsh","Milad Nasr","Chester Kwak","Suraj Satishkumar Sheth","Alex Kaskasoli","Ankesh Anand","Balaji Lakshminarayanan","Sammy Jerome","David Bieber","Chun-Te Chu","Alexandre Senges","Tianxiao Shen","Mukund Sridhar","Ndaba Ndebele","Benjamin Beyret","Shakir Mohamed","Mia Chen","Markus Freitag","Jiaxian Guo","Luyang Liu","Paul Roit","Heng Chen","Shen Yan","Tom Stone","JD Co-Reyes","Jeremy Cole","Salvatore Scellato","Shekoofeh Azizi","Hadi Hashemi","Alicia Jin","Anand Iyer","Marcella Valentine","András György","Arun Ahuja","Daniel Hernandez Diaz","Chen-Yu Lee","Nathan Clement","Weize Kong","Drew Garmon","Ishaan Watts","Kush Bhatia","Khyatti Gupta","Matt Miecnikowski","Hugo Vallet","Ankur Taly","Edward Loper","Saket Joshi","James Atwood","Jo Chick","Mark Collier","Fotis Iliopoulos","Ryan Trostle","Beliz Gunel","Ramiro Leal-Cavazos","Arnar Mar Hrafnkelsson","Michael Guzman","Xiaoen Ju","Andy Forbes","Jesse Emond","Kushal Chauhan","Ben Caine","Li Xiao","Wenjun Zeng","Alexandre Moufarek","Daniel Murphy","Maya Meng","Nitish Gupta","Felix Riedel","Anil Das","Elijah Lawal","Shashi Narayan","Tiberiu Sosea","James Swirhun","Linda Friso","Behnam Neyshabur","Jing Lu","Sertan Girgin","Michael Wunder","Edouard Yvinec","Aroonalok Pyne","Victor Carbune","Shruti Rijhwani","Yang Guo","Tulsee Doshi","Anton Briukhov","Max Bain","Ayal Hitron","Xuanhui Wang","Ashish Gupta","Ke Chen","Cosmo Du","Weiyang Zhang","Dhruv Shah","Arjun Akula","Max Dylla","Ashyana Kachra","Weicheng Kuo","Tingting Zou","Lily Wang","Luyao Xu","Jifan Zhu","Justin Snyder","Sachit Menon","Orhan Firat","Igor Mordatch","Yuan Yuan","Natalia Ponomareva","Rory Blevins","Lawrence Moore","Weijun Wang","Phil Chen","Martin Scholz","Artur Dwornik","Jason Lin","Sicheng Li","Diego Antognini","Te I","Xiaodan Song","Matt Miller","Uday Kalra","Adam Raveret","Oscar Akerlund","Felix Wu","Andrew Nystrom","Namrata Godbole","Tianqi Liu","Hannah DeBalsi","Jewel Zhao","Buhuang Liu","Avi Caciularu","Lauren Lax","Urvashi Khandelwal","Victoria Langston","Eric Bailey","Silvio Lattanzi","Yufei Wang","Neel Kovelamudi","Sneha Mondal","Guru Guruganesh","Nan Hua","Ofir Roval","Paweł Wesołowski","Rishikesh Ingale","Jonathan Halcrow","Tim Sohn","Christof Angermueller","Bahram Raad","Eli Stickgold","Eva Lu","Alec Kosik","Jing Xie","Timothy Lillicrap","Austin Huang","Lydia Lihui Zhang","Dominik Paulus","Clement Farabet","Alex Wertheim","Bing Wang","Rishabh Joshi","Chu-ling Ko","Yonghui Wu","Shubham Agrawal","Lily Lin","XiangHai Sheng","Peter Sung","Tyler Breland-King","Christina Butterfield","Swapnil Gawde","Sumeet Singh","Qiao Zhang","Raj Apte","Shilpa Shetty","Adrian Hutter","Tao Li","Elizabeth Salesky","Federico Lebron","Jonni Kanerva","Michela Paganini","Arthur Nguyen","Rohith Vallu","Jan-Thorsten Peter","Sarmishta Velury","David Kao","Jay Hoover","Anna Bortsova","Colton Bishop","Shoshana Jakobovits","Alessandro Agostini","Alekh Agarwal","Chang Liu","Charles Kwong","Sasan Tavakkol","Ioana Bica","Alex Greve","Anirudh GP","Jake Marcus","Le Hou","Tom Duerig","Rivka Moroshko","Dave Lacey","Andy Davis","Julien Amelot","Guohui Wang","Frank Kim","Theofilos Strinopoulos","Hui Wan","Charline Le Lan","Shankar Krishnan","Haotian Tang","Peter Humphreys","Junwen Bai","Idan Heimlich Shtacher","Diego Machado","Chenxi Pang","Ken Burke","Dangyi Liu","Renga Aravamudhan","Yue Song","Ed Hirst","Abhimanyu Singh","Brendan Jou","Liang Bai","Francesco Piccinno","Chuyuan Kelly Fu","Robin Alazard","Barak Meiri","Daniel Winter","Charlie Chen","Mingda Zhang","Jens Heitkaemper","John Lambert","Jinhyuk Lee","Alexander Frömmgen","Sergey Rogulenko","Pranav Nair","Paul Niemczyk","Anton Bulyenov","Bibo Xu","Hadar Shemtov","Morteza Zadimoghaddam","Serge Toropov","Mateo Wirth","Hanjun Dai","Sreenivas Gollapudi","Daniel Zheng","Alex Kurakin","Chansoo Lee","Kalesha Bullard","Nicolas Serrano","Ivana Balazevic","Yang Li","Johan Schalkwyk","Mark Murphy","Mingyang Zhang","Kevin Sequeira","Romina Datta","Nishant Agrawal","Charles Sutton","Nithya Attaluri","Mencher Chiang","Wael Farhan","Gregory Thornton","Kate Lin","Travis Choma","Hung Nguyen","Kingshuk Dasgupta","Dirk Robinson","Iulia Comşa","Michael Riley","Arjun Pillai","Basil Mustafa","Ben Golan","Amir Zandieh","Jean-Baptiste Lespiau","Billy Porter","David Ross","Sujeevan Rajayogam","Mohit Agarwal","Subhashini Venugopalan","Bobak Shahriari","Qiqi Yan","Hao Xu","Taylor Tobin","Pavel Dubov","Hongzhi Shi","Adrià Recasens","Anton Kovsharov","Sebastian Borgeaud","Lucio Dery","Shanthal Vasanth","Elena Gribovskaya","Linhai Qiu","Mahdis Mahdieh","Wojtek Skut","Elizabeth Nielsen","CJ Zheng","Adams Yu","Carrie Grimes Bostock","Shaleen Gupta","Aaron Archer","Chris Rawles","Elinor Davies","Alexey Svyatkovskiy","Tomy Tsai","Yoni Halpern","Christian Reisswig","Bartek Wydrowski","Bo Chang","Joan Puigcerver","Mor Hazan Taege","Jian Li","Eva Schnider","Xinjian Li","Dragos Dena","Yunhan Xu","Umesh Telang","Tianze Shi","Heiga Zen","Kyle Kastner","Yeongil Ko","Neesha Subramaniam","Aviral Kumar","Pete Blois","Zhuyun Dai","John Wieting","Yifeng Lu","Yoel Zeldes","Tian Xie","Anja Hauth","Alexandru Ţifrea","Yuqi Li","Sam El-Husseini","Dan Abolafia","Howard Zhou","Wen Ding","Sahra Ghalebikesabi","Carlos Guía","Andrii Maksai","Ágoston Weisz","Sercan Arik","Nick Sukhanov","Aga Świetlik","Xuhui Jia","Luo Yu","Weiyue Wang","Mark Brand","Dawn Bloxwich","Sean Kirmani","Zhe Chen","Alec Go","Pablo Sprechmann","Nithish Kannen","Alen Carin","Paramjit Sandhu","Isabel Edkins","Leslie Nooteboom","Jai Gupta","Loren Maggiore","Javad Azizi","Yael Pritch","Pengcheng Yin","Mansi Gupta","Danny Tarlow","Duncan Smith","Desi Ivanov","Mohammad Babaeizadeh","Ankita Goel","Satish Kambala","Grace Chu","Matej Kastelic","Michelle Liu","Hagen Soltau","Austin Stone","Shivani Agrawal","Min Kim","Kedar Soparkar","Srinivas Tadepalli","Oskar Bunyan","Rachel Soh","Arvind Kannan","DY Kim","Blake JianHang Chen","Afief Halumi","Sudeshna Roy","Yulong Wang","Olcan Sercinoglu","Gena Gibson","Sijal Bhatnagar","Motoki Sano","Daniel von Dincklage","Qingchun Ren","Blagoj Mitrevski","Mirek Olšák","Jennifer She","Carl Doersch"," Jilei"," Wang","Bingyuan Liu","Qijun Tan","Tamar Yakar","Tris Warkentin","Alex Ramirez","Carl Lebsack","Josh Dillon","Rajiv Mathews","Tom Cobley","Zelin Wu","Zhuoyuan Chen","Jon Simon","Swaroop Nath","Tara Sainath","Alexei Bendebury","Ryan Julian","Bharath Mankalale","Daria Ćurko","Paulo Zacchello","Adam R. Brown","Kiranbir Sodhia","Heidi Howard","Sergi Caelles","Abhinav Gupta","Gareth Evans","Anna Bulanova","Lesley Katzen","Roman Goldenberg","Anton Tsitsulin","Joe Stanton","Benoit Schillings","Vitaly Kovalev","Corey Fry","Rushin Shah","Kuo Lin","Shyam Upadhyay","Cheng Li","Soroush Radpour","Marcello Maggioni","Jing Xiong","Lukas Haas","Jenny Brennan","Aishwarya Kamath","Nikolay Savinov","Arsha Nagrani","Trevor Yacovone","Ryan Kappedal","Kostas Andriopoulos","Li Lao","YaGuang Li","Grigory Rozhdestvenskiy","Kazuma Hashimoto","Andrew Audibert","Sophia Austin","Daniel Rodriguez","Anian Ruoss","Garrett Honke","Deep Karkhanis","Xi Xiong","Qing Wei","James Huang","Zhaoqi Leng","Vittal Premachandran","Stan Bileschi","Georgios Evangelopoulos","Thomas Mensink","Jay Pavagadhi","Denis Teplyashin","Paul Chang","Linting Xue","Garrett Tanzer","Sally Goldman","Kaushal Patel","Shixin Li","Jeremy Wiesner","Ivy Zheng","Ian Stewart-Binks","Jie Han","Zhi Li","Liangchen Luo","Karel Lenc","Mario Lučić","Fuzhao Xue","Ryan Mullins","Alexey Guseynov","Chung-Ching Chang","Isaac Galatzer-Levy","Adam Zhang","Garrett Bingham","Grace Hu","Ale Hartman","Yue Ma","Jordan Griffith","Alex Irpan","Carey Radebaugh","Summer Yue","Lijie Fan","Victor Ungureanu","Christina Sorokin","Hannah Teufel","Peiran Li","Rohan Anil","Dimitris Paparas","Todd Wang","Chu-Cheng Lin","Hui Peng","Megan Shum","Goran Petrovic","Demetra Brady","Richard Nguyen","Klaus Macherey","Zhihao Li","Harman Singh","Madhavi Yenugula","Mariko Iinuma","Xinyi Chen","Kavya Kopparapu","Alexey Stern","Shachi Dave","Chandu Thekkath","Florence Perot","Anurag Kumar","Fangda Li","Yang Xiao","Matthew Bilotti","Mohammad Hossein Bateni","Isaac Noble","Lisa Lee","Amelio Vázquez-Reina","Julian Salazar","Xiaomeng Yang","Boyu Wang","Ela Gruzewska","Anand Rao","Sindhu Raghuram","Zheng Xu","Eyal Ben-David","Jieru Mei","Sid Dalmia","Zhaoyi Zhang","Yuchen Liu","Gagan Bansal","Helena Pankov","Steven Schwarcz","Andrea Burns","Christine Chan","Sumit Sanghai","Ricky Liang","Ethan Liang","Antoine He","Amy Stuart","Arun Narayanan","Yukun Zhu","Christian Frank","Bahar Fatemi","Amit Sabne","Oran Lang","Indro Bhattacharya","Shane Settle","Maria Wang","Brendan McMahan","Andrea Tacchetti","Livio Baldini Soares","Majid Hadian","Serkan Cabi","Timothy Chung","Nikita Putikhin","Gang Li","Jeremy Chen","Austin Tarango","Henryk Michalewski","Mehran Kazemi","Hussain Masoom","Hila Sheftel","Rakesh Shivanna","Archita Vadali","Ramona Comanescu","Doug Reid","Joss Moore","Arvind Neelakantan","Michaël Sander","Jonathan Herzig","Aviv Rosenberg","Mostafa Dehghani","JD Choi","Michael Fink","Reid Hayes","Eric Ge","Shitao Weng","Chia-Hua Ho","John Karro","Kalpesh Krishna","Lam Nguyen Thiet","Amy Skerry-Ryan","Daniel Eppens","Marco Andreetto","Navin Sarma","Silvano Bonacina","Burcu Karagol Ayan","Megha Nawhal","Zhihao Shan","Mike Dusenberry","Shantanu Thakoor","Sagar Gubbi","Duc Dung Nguyen","Reut Tsarfaty","Samuel Albanie","Jovana Mitrović","Meet Gandhi","Bo-Juen Chen","Alessandro Epasto","Georgi Stephanov","Ye Jin","Samuel Gehman","Aida Amini","Jack Weber","Feryal Behbahani","Shawn Xu","Miltos Allamanis","Xi Chen","Myle Ott","Claire Sha","Michal Jastrzebski","Hang Qi","David Greene","Xinyi Wu","Abodunrinwa Toki","Daniel Vlasic","Jane Shapiro","Ragha Kotikalapudi","Zhe Shen","Takaaki Saeki","Sirui Xie","Albin Cassirer","Shikhar Bharadwaj","Tatsuya Kiyono","Srinadh Bhojanapalli","Elan Rosenfeld","Sam Ritter","Jieming Mao","João Gabriel Oliveira","Zoltan Egyed","Bernd Bandemer","Emilio Parisotto","Keisuke Kinoshita","Juliette Pluto","Petros Maniatis","Steve Li","Yaohui Guo","Golnaz Ghiasi","Jean Tarbouriech","Srimon Chatterjee","Julie Jin"," Katrina"," Xu","Jennimaria Palomaki","Séb Arnold","Madhavi Sewak","Federico Piccinini","Mohit Sharma","Ben Albrecht","Sean Purser-haskell","Ashwin Vaswani","Chongyan Chen","Matheus Wisniewski","Qin Cao","John Aslanides","Nguyet Minh Phu","Maximilian Sieb","Lauren Agubuzu","Anne Zheng","Daniel Sohn","Marco Selvi","Anders Andreassen","Krishan Subudhi","Prem Eruvbetine","Oliver Woodman","Tomas Mery","Sebastian Krause","Xiaoqi Ren","Xiao Ma","Jincheng Luo","Dawn Chen","Wei Fan","Henry Griffiths","Christian Schuler","Alice Li","Shujian Zhang","Jean-Michel Sarr","Shixin Luo","Riccardo Patana","Matthew Watson","Dani Naboulsi","Michael Collins","Sailesh Sidhwani","Emiel Hoogeboom","Sharon Silver","Emily Caveness","Xiaokai Zhao","Mikel Rodriguez","Maxine Deines","Libin Bai","Patrick Griffin","Marco Tagliasacchi","Emily Xue","Spandana Raj Babbula","Bo Pang","Nan Ding","Gloria Shen","Elijah Peake","Remi Crocker","Shubha Srinivas Raghvendra","Danny Swisher","Woohyun Han","Richa Singh","Ling Wu","Vladimir Pchelin","Tsendsuren Munkhdalai","Dana Alon","Geoff Bacon","Efren Robles","Jannis Bulian","Melvin Johnson","George Powell","Felipe Tiengo Ferreira","Yaoyiran Li","Frederik Benzing","Mihajlo Velimirović","Hubert Soyer","William Kong"," Tony"," Nguyên","Zhen Yang","Jeremiah Liu","Joost van Amersfoort","Daniel Gillick","Baochen Sun","Nathalie Rauschmayr","Katie Zhang","Serena Zhan","Tao Zhou","Alexey Frolov","Chengrun Yang","Denis Vnukov","Louis Rouillard","Hongji Li","Amol Mandhane","Nova Fallen","Rajesh Venkataraman","Clara Huiyi Hu","Jennifer Brennan","Jenny Lee","Jerry Chang","Martin Sundermeyer","Zhufeng Pan","Rosemary Ke","Simon Tong","Alex Fabrikant","William Bono","Jindong Gu","Ryan Foley","Yiran Mao","Manolis Delakis","Dhruva Bhaswar","Roy Frostig","Nick Li","Avital Zipori","Cath Hope","Olga Kozlova","Swaroop Mishra","Josip Djolonga","Craig Schiff","Majd Al Merey","Eleftheria Briakou","Peter Morgan","Andy Wan","Avinatan Hassidim","RJ Skerry-Ryan","Kuntal Sengupta","Mary Jasarevic","Praveen Kallakuri","Paige Kunkle","Hannah Brennan","Tom Lieber","Hassan Mansoor","Julian Walker","Bing Zhang","Annie Xie","Goran Žužić","Adaeze Chukwuka","Alex Druinsky","Donghyun Cho","Rui Yao","Ferjad Naeem","Shiraz Butt","Eunyoung Kim","Zhipeng Jia","Mandy Jordan","Adam Lelkes","Mark Kurzeja","Sophie Wang","James Zhao","Andrew Over","Abhishek Chakladar","Marcel Prasetya","Neha Jha","Sriram Ganapathy","Yale Cong","Prakash Shroff","Carl Saroufim","Sobhan Miryoosefi","Mohamed Hammad","Tajwar Nasir","Weijuan Xi","Yang Gao","Young Maeng","Ben Hora","Chin-Yi Cheng","Parisa Haghani","Yoad Lewenberg","Caden Lu","Martin Matysiak","Naina Raisinghani","Huiyu Wang","Lexi Baugher","Rahul Sukthankar","Minh Giang","John Schultz","Noah Fiedel","Minmin Chen","Cheng-Chun Lee","Tapomay Dey","Hao Zheng","Shachi Paul","Celine Smith","Andy Ly","Yicheng Wang","Rishabh Bansal","Bartek Perz","Susanna Ricco","Stasha Blank","Vaishakh Keshava","Deepak Sharma","Marvin Chow","Kunal Lad","Komal Jalan","Simon Osindero","Craig Swanson","Jacob Scott","Anastasija Ilić","Xiaowei Li","Siddhartha Reddy Jonnalagadda","Afzal Shama Soudagar","Yan Xiong","Bat-Orgil Batsaikhan","Daniel Jarrett","Naveen Kumar","Maulik Shah","Matt Lawlor","Austin Waters","Mark Graham","Rhys May","Sabela Ramos","Sandra Lefdal","Zeynep Cankara","Nacho Cano","Brendan O'Donoghue","Jed Borovik","Frederick Liu","Jordan Grimstad","Mahmoud Alnahlawi","Katerina Tsihlas","Tom Hudson","Nikolai Grigorev","Yiling Jia","Terry Huang","Tobenna Peter Igwe","Sergei Lebedev","Xiaodan Tang","Igor Krivokon","Frankie Garcia","Melissa Tan","Eric Jia","Peter Stys","Shikhar Vashishth","Yu Liang","Balaji Venkatraman","Chenjie Gu","Anastasios Kementsietsidis","Chen Zhu","Junehyuk Jung","Yunfei Bai","Mohammad Javad Hosseini","Faruk Ahmed","Aditya Gupta","Xin Yuan","Shereen Ashraf","Shitij Nigam","Gautam Vasudevan","Pranjal Awasthi","Adi Mayrav Gilady","Zelda Mariet","Ramy Eskander","Haiguang Li","Hexiang Hu","Guillermo Garrido","Philippe Schlattner","George Zhang","Rohun Saxena","Petar Dević","Kritika Muralidharan","Ashwin Murthy","Yiqian Zhou","Min Choi","Arissa Wongpanich","Zhengdong Wang","Premal Shah","Yuntao Xu","Yiling Huang","Stephen Spencer","Alice Chen","James Cohan","Junjie Wang","Jonathan Tompson","Junru Wu","Ruba Haroun","Haiqiong Li","Blanca Huergo","Fan Yang","Tongxin Yin","James Wendt","Michael Bendersky","Rahma Chaabouni","Javier Snaider","Johan Ferret","Abhishek Jindal","Tara Thompson","Andrew Xue","Will Bishop","Shubham Milind Phal","Archit Sharma","Yunhsuan Sung","Prabakar Radhakrishnan","Mo Shomrat","Reeve Ingle","Roopali Vij","Justin Gilmer","Mihai Dorin Istin","Sam Sobell","Yang Lu","Emily Nottage","Dorsa Sadigh","Jeremiah Willcock","Tingnan Zhang","Steve Xu","Sasha Brown","Katherine Lee","Gary Wang","Yun Zhu","Yi Tay","Cheolmin Kim","Audrey Gutierrez","Abhanshu Sharma","Yongqin Xian","Sungyong Seo","Claire Cui","Elena Pochernina","Cip Baetu","Krzysztof Jastrzębski","Mimi Ly","Mohamed Elhawaty","Dan Suh","Eren Sezener","Pidong Wang","Nancy Yuen","George Tucker","Jiahao Cai","Zuguang Yang","Cindy Wang","Alex Muzio","Hai Qian","Jae Yoo","Derek Lockhart","Kevin R. McKee","Mandy Guo","Malika Mehrotra","Artur Mendonça","Sanket Vaibhav Mehta","Sherry Ben","Chetan Tekur","Jiaqi Mu","Muye Zhu","Victoria Krakovna","Hongrae Lee","AJ Maschinot","Sébastien Cevey","HyunJeong Choe","Aijun Bai","Hansa Srinivasan","Derek Gasaway","Nick Young","Patrick Siegler","Dan Holtmann-Rice","Vihari Piratla","Kate Baumli","Roey Yogev","Alex Hofer","Hado van Hasselt","Svetlana Grant","Yuri Chervonyi","David Silver","Andrew Hogue","Ayushi Agarwal","Kathie Wang","Preeti Singh","Four Flynn","Josh Lipschultz","Robert David","Lizzetth Bellot","Yao-Yuan Yang","Long Le","Filippo Graziano","Kate Olszewska","Kevin Hui","Akanksha Maurya","Nikos Parotsidis","Weijie Chen","Tayo Oguntebi","Joe Kelley","Anirudh Baddepudi","Johannes Mauerer","Gregory Shaw","Alex Siegman","Lin Yang","Shravya Shetty","Subhrajit Roy","Yunting Song","Wojciech Stokowiec","Ryan Burnell","Omkar Savant","Robert Busa-Fekete","Jin Miao","Samrat Ghosh","Liam MacDermed","Phillip Lippe","Mikhail Dektiarev","Zach Behrman","Fabian Mentzer","Kelvin Nguyen","Meng Wei","Siddharth Verma","Chris Knutsen","Sudeep Dasari","Zhipeng Yan","Petr Mitrichev","Xingyu Wang","Virat Shejwalkar","Jacob Austin","Srinivas Sunkara","Navneet Potti","Yan Virin","Christian Wright","Gaël Liu","Oriana Riva","Etienne Pot","Greg Kochanski","Quoc Le","Gargi Balasubramaniam","Arka Dhar","Yuguo Liao","Adam Bloniarz","Divyansh Shukla","Elizabeth Cole","Jong Lee","Sheng Zhang","Sushant Kafle","Siddharth Vashishtha","Parsa Mahmoudieh","Grace Chen","Raphael Hoffmann","Pranesh Srinivasan","Agustin Dal Lago","Yoav Ben Shalom","Zi Wang","Michael Elabd","Anuj Sharma","Junhyuk Oh","Suraj Kothawade","Maigo Le","Marianne Monteiro","Shentao Yang","Kaiz Alarakyia","Robert Geirhos","Diana Mincu","Håvard Garnes","Hayato Kobayashi","Soroosh Mariooryad","Kacper Krasowiak"," Zhixin"," Lai","Shibl Mourad","Mingqiu Wang","Fan Bu","Ophir Aharoni","Guanjie Chen","Abhimanyu Goyal","Vadim Zubov","Ankur Bapna","Elahe Dabir","Nisarg Kothari","Kay Lamerigts","Nicola De Cao","Jeremy Shar","Christopher Yew","Nitish Kulkarni","Dre Mahaarachchi","Mandar Joshi","Zhenhai Zhu","Jared Lichtarge","Yichao Zhou","Hannah Muckenhirn","Vittorio Selo","Oriol Vinyals","Peter Chen","Anthony Brohan","Vaibhav Mehta","Sarah Cogan","Ruth Wang","Ty Geri","Wei-Jen Ko","Wei Chen","Fabio Viola","Keshav Shivam","Lisa Wang","Madeleine Clare Elish","Raluca Ada Popa","Sébastien Pereira","Jianqiao Liu","Raphael Koster","Donnie Kim","Gufeng Zhang","Sayna Ebrahimi","Partha Talukdar","Yanyan Zheng","Petra Poklukar","Ales Mikhalap","Dale Johnson","Anitha Vijayakumar","Mark Omernick","Matt Dibb","Ayush Dubey","Qiong Hu","Apurv Suman","Vaibhav Aggarwal","Ilya Kornakov","Fei Xia","Wing Lowe","Alexey Kolganov","Ted Xiao","Vitaly Nikolaev","Steven Hemingray","Bonnie Li","Joana Iljazi","Mikołaj Rybiński","Ballie Sandhu","Peggy Lu","Thang Luong","Rodolphe Jenatton","Vineetha Govindaraj"," Hui"," Li","Gabriel Dulac-Arnold","Wonpyo Park","Henry Wang","Abhinit Modi","Jean Pouget-Abadie","Kristina Greller","Rahul Gupta","Robert Berry","Prajit Ramachandran","Jinyu Xie","Liam McCafferty","Jianling Wang","Kilol Gupta","Hyeontaek Lim","Blaž Bratanič","Andy Brock","Ilia Akolzin","Jim Sproch","Dan Karliner","Duhyeon Kim","Adrian Goedeckemeyer","Noam Shazeer","Cordelia Schmid","Daniele Calandriello","Parul Bhatia","Krzysztof Choromanski","Ceslee Montgomery","Dheeru Dua","Ana Ramalho","Helen King","Yue Gao","Lynn Nguyen","David Lindner","Divya Pitta","Oleaser Johnson","Khalid Salama","Diego Ardila","Michael Han","Erin Farnese","Seth Odoom","Ziyue Wang","Xiangzhuo Ding","Norman Rink","Ray Smith","Harshal Tushar Lehri","Eden Cohen","Neera Vats","Tong He","Parthasarathy Gopavarapu","Adam Paszke","Miteyan Patel","Wouter Van Gansbeke","Lucia Loher","Luis Castro","Maria Voitovich","Tamara von Glehn","Nelson George","Simon Niklaus","Zach Eaton-Rosen","Nemanja Rakićević","Erik Jue","Sagi Perel","Carrie Zhang","Yuval Bahat","Angéline Pouget","Zhi Xing","Fantine Huot","Ashish Shenoy","Taylor Bos","Vincent Coriou","Bryan Richter","Natasha Noy","Yaqing Wang","Santiago Ontanon","Siyang Qin","Gleb Makarchuk","Demis Hassabis","Zhuowan Li","Mandar Sharma","Kumaran Venkatesan","Iurii Kemaev","Roxanne Daniel","Shiyu Huang","Saloni Shah","Octavio Ponce"," Warren"," Chen","Manaal Faruqui","Jialin Wu","Slavica Andačić","Szabolcs Payrits","Daniel McDuff","Tom Hume","Yuan Cao","MH Tessler","Qingze Wang","Yinan Wang","Ivor Rendulic","Eirikur Agustsson","Matthew Johnson","Tanya Lando","Andrew Howard","Sri Gayatri Sundara Padmanabhan","Mayank Daswani","Andrea Banino","Michael Kilgore","Jonathan Heek","Ziwei Ji","Alvaro Caceres","Conglong Li","Nora Kassner","Alexey Vlaskin","Zeyu Liu","Alex Grills","Yanhan Hou","Roykrong Sukkerd","Gowoon Cheon","Nishita Shetty","Larisa Markeeva","Piotr Stanczyk","Tejas Iyer","Yuan Gong","Shawn Gao","Keerthana Gopalakrishnan","Tim Blyth","Malcolm Reynolds","Avishkar Bhoopchand","Misha Bilenko","Dero Gharibian","Vicky Zayats","Aleksandra Faust","Abhinav Singh","Min Ma","Hongyang Jiao","Sudheendra Vijayanarasimhan","Lora Aroyo","Vikas Yadav","Sarah Chakera","Ashwin Kakarla","Vilobh Meshram","Karol Gregor","Gabriela Botea","Evan Senter","Dawei Jia","Geza Kovacs","Neha Sharma","Sebastien Baur","Kai Kang","Yifan He","Lin Zhuo","Marija Kostelac","Itay Laish","Songyou Peng","Louis O'Bryan","Daniel Kasenberg","Girish Ramchandra Rao","Edouard Leurent","Biao Zhang","Sage Stevens","Ana Salazar","Ye Zhang","Ivan Lobov","Jake Walker","Allen Porter","Morgan Redshaw","Han Ke","Abhishek Rao","Alex Lee","Hoi Lam","Michael Moffitt","Jaeyoun Kim","Siyuan Qiao","Terry Koo","Robert Dadashi","Xinying Song","Mukund Sundararajan","Peng Xu","Chizu Kawamoto","Yan Zhong","Clara Barbu","Apoorv Reddy","Mauro Verzetti","Leon Li","George Papamakarios","Hanna Klimczak-Plucińska","Mary Cassin","Koray Kavukcuoglu","Rigel Swavely","Alain Vaucher","Jeffrey Zhao","Ross Hemsley","Michael Tschannen","Heming Ge","Gaurav Menghani","Yang Yu","Natalie Ha","Wei He","Xiao Wu","Maggie Song","Rachel Sterneck","Stefan Zinke","Dan A. Calian","Annie Marsden","Alejandro Cruzado Ruiz","Matteo Hessel","Almog Gueta","Benjamin Lee","Brian Farris","Manish Gupta","Yunjie Li","Mohammad Saleh","Vedant Misra","Kefan Xiao","Piermaria Mendolicchio","Gavin Buttimore","Varvara Krayvanova","Nigamaa Nayakanti","Matthew Wiethoff","Yash Pande","Azalia Mirhoseini","Ni Lao","Jasmine Liu","Yiqing Hua","Angie Chen","Yury Malkov","Dmitry Kalashnikov","Shubham Gupta","Kartik Audhkhasi","Yuexiang Zhai","Sudhindra Kopalle","Prateek Jain","Eran Ofek","Clemens Meyer","Khuslen Baatarsukh","Hana Strejček","Jun Qian","James Freedman","Ricardo Figueira","Michal Sokolik","Olivier Bachem","Raymond Lin","Dia Kharrat","Chris Hidey","Pingmei Xu","Dennis Duan","Yin Li","Muge Ersoy","Richard Everett","Kevin Cen","Rebeca Santamaria-Fernandez","Amir Taubenfeld","Ian Mackinnon","Linda Deng","Polina Zablotskaia","Shashank Viswanadha","Shivanker Goel","Damion Yates","Yunxiao Deng","Peter Choy","Mingqing Chen","Abhishek Sinha","Alex Mossin","Yiming Wang","Arthur Szlam","Susan Hao","Paul Kishan Rubenstein","Metin Toksoz-Exley","Miranda Aperghis","Yin Zhong","Junwhan Ahn","Michael Isard","Olivier Lacombe","Florian Luisier","Chrysovalantis Anastasiou","Yogesh Kalley","Utsav Prabhu","Emma Dunleavy","Shaan Bijwadia","Justin Mao-Jones","Kelly Chen","Rama Pasumarthi","Emily Wood","Adil Dostmohamed","Nate Hurley","Jiri Simsa","Alicia Parrish","Mantas Pajarskas","Matt Harvey","Ondrej Skopek","Yony Kochinski","Javier Rey","Verena Rieser","Denny Zhou","Sun Jae Lee","Trilok Acharya","Guowang Li","Joe Jiang","Xiaofan Zhang","Bryant Gipson","Ethan Mahintorabi","Marco Gelmi","Nima Khajehnouri","Angel Yeh","Kayi Lee","Loic Matthey","Leslie Baker","Trang Pham","Han Fu","Alex Pak","Prakhar Gupta","Cristina Vasconcelos","Adam Sadovsky","Brian Walker","Sissie Hsiao","Patrik Zochbauer","Andreea Marzoca","Noam Velan","Junhao Zeng","Gilles Baechler","Danny Driess","Divya Jain","Yanping Huang","Lizzie Tao","John Maggs","Nir Levine","Jon Schneider","Erika Gemzer","Samuel Petit","Shan Han","Zach Fisher","Dustin Zelle","Courtney Biles","Eugene Ie","Asya Fadeeva","Casper Liu","Juliana Vicente Franco","Adrian Collister","Hao Zhang","Renshen Wang","Ruizhe Zhao","Leandro Kieliger","Kurt Shuster","Rui Zhu","Boqing Gong","Lawrence Chan","Ruoxi Sun","Sujoy Basu","Roland Zimmermann","Jamie Hayes","Abhishek Bapna","Jasper Snoek","Weel Yang","Puranjay Datta","Jad Al Abdallah","Kevin Kilgour","Lu Li","SQ Mah","Yennie Jun","Morgane Rivière","Abhijit Karmarkar","Tammo Spalink","Tao Huang","Lucas Gonzalez","Duc-Hieu Tran","Averi Nowak","John Palowitch","Martin Chadwick","Ellie Talius","Harsh Mehta","Thibault Sellam","Philipp Fränken","Massimo Nicosia","Kyle He","Aditya Kini","David Amos","Sugato Basu","Harrison Jobe","Eleni Shaw","Qiantong Xu","Colin Evans","Daisuke Ikeda","Chaochao Yan","Larry Jin","Lun Wang","Sachin Yadav","Ilia Labzovsky","Ramesh Sampath","Ada Ma","Candice Schumann","Aditya Siddhant","Rohin Shah","John Youssef","Rishabh Agarwal","Natalie Dabney","Alessio Tonioni","Moran Ambar","Jing Li","Isabelle Guyon","Benny Li","David Soergel","Boya Fang","Georgi Karadzhov","Cristian Udrescu","Trieu Trinh","Vikas Raunak","Seb Noury","Dee Guo","Sonal Gupta","Mara Finkelstein","Denis Petek","Lihao Liang","Greg Billock","Pei Sun","David Wood","Yiwen Song","Xiaobin Yu","Tatiana Matejovicova","Regev Cohen","Kalyan Andra","David D'Ambrosio","Zhiwei Deng","Vincent Nallatamby","Ebrahim Songhori","Rumen Dangovski","Andrew Lampinen","Pankil Botadra","Adam Hillier","Jiawei Cao","Nagabhushan Baddi","Adhi Kuncoro","Toshihiro Yoshino","Ankit Bhagatwala","Marcáurelio Ranzato","Rylan Schaeffer","Tianlin Liu","Shuai Ye","Obaid Sarvana","John Nham","Chenkai Kuang","Isabel Gao","Jinoo Baek","Shubham Mittal","Ayzaan Wahid","Anita Gergely","Bin Ni","Josh Feldman","Carrie Muir","Pascal Lamblin","Wolfgang Macherey","Ethan Dyer","Logan Kilpatrick","Víctor Campos","Mukul Bhutani","Stanislav Fort","Yanif Ahmad","Aliaksei Severyn","Kleopatra Chatziprimou","Oleksandr Ferludin","Mason Dimarco","Aditya Kusupati","Joe Heyward","Dan Bahir","Kevin Villela","Katie Millican","Dror Marcus","Sanaz Bahargam","Caglar Unlu","Nicholas Roth","Zichuan Wei","Siddharth Gopal","Deepanway Ghoshal","Edward Lee","Sharon Lin","Jennie Lees","Dayeong Lee","Anahita Hosseini","Connie Fan","Seth Neel","Marcus Wu","Yasemin Altun","Honglong Cai","Enrique Piqueras","Josh Woodward","Alessandro Bissacco","Salem Haykal","Mahyar Bordbar","Prasha Sundaram","Sarah Hodkinson","Daniel Toyama","George Polovets","Austin Myers","Anu Sinha","Tomer Levinboim","Kashyap Krishnakumar","Rachita Chhaparia","Tatiana Sholokhova","Nitesh Bharadwaj Gundavarapu","Ganesh Jawahar","Haroon Qureshi","Jieru Hu","Nikola Momchev","Matthew Rahtz","Renjie Wu","Aishwarya P S","Kedar Dhamdhere","Meiqi Guo","Umang Gupta","Ali Eslami","Mariano Schain","Michiel Blokzijl","David Welling","Dave Orr","Levent Bolelli","Nicolas Perez-Nieves","Mikhail Sirotenko","Aman Prasad","Arjun Kar","Borja De Balle Pigem","Tayfun Terzi","Gellért Weisz","Dipankar Ghosh","Aditi Mavalankar","Dhruv Madeka","Kaspar Daugaard","Hartwig Adam","Viraj Shah","Dana Berman","Maggie Tran","Steven Baker","Ewa Andrejczuk","Grishma Chole","Ganna Raboshchuk","Mahdi Mirzazadeh","Thais Kagohara","Shimu Wu","Christian Schallhart","Bernett Orlando","Chen Wang","Alban Rrustemi","Hao Xiong","Hao Liu","Arpi Vezer","Nolan Ramsden","Shuo-yiin Chang","Sidharth Mudgal","Yan Li","Nino Vieillard","Yedid Hoshen","Farooq Ahmad","Ambrose Slone","Amy Hua","Natan Potikha","Mirko Rossini","Jon Stritar","Sushant Prakash","Zifeng Wang","Xuanyi Dong","Alireza Nazari","Efrat Nehoran","Kaan Tekelioglu","Yinxiao Li","Kartikeya Badola","Tom Funkhouser","Yuanzhen Li","Varun Yerram","Ramya Ganeshan","Daniel Formoso","Karol Langner","Tian Shi","Huijian Li","Yumeya Yamamori","Amayika Panda","Alaa Saade","Angelo Scorza Scarpati","Chris Breaux","CJ Carey","Zongwei Zhou","Cho-Jui Hsieh","Sophie Bridgers","Alena Butryna","Nishesh Gupta","Vaibhav Tulsyan","Sanghyun Woo","Evgenii Eltyshev","Will Grathwohl","Chanel Parks","Seth Benjamin","Rina Panigrahy","Shenil Dodhia","Daniel De Freitas","Chris Sauer","Will Song","Ferran Alet","Jackson Tolins","Cosmin Paduraru","Xingyi Zhou","Brian Albert","Zizhao Zhang","Lei Shu","Mudit Bansal","Sarah Nguyen","Amir Globerson","Owen Xiao","James Manyika","Tom Hennigan","Rong Rong","Josip Matak","Anton Bakalov","Ankur Sharma","Danila Sinopalnikov","Andrew Pierson","Stephen Roller","Geoff Brown","Mingcen Gao","Toshiyuki Fukuzawa","Amin Ghafouri","Kenny Vassigh","Iain Barr","Zhicheng Wang","Anna Korsun","Rajesh Jayaram","Lijie Ren","Tim Zaman","Samira Khan","Yana Lunts","Dan Deutsch","Dave Uthus","Nitzan Katz","Masha Samsikova","Amr Khalifa","Nikhil Sethi","Jiao Sun","Luming Tang","Uri Alon","Xianghong Luo","Dian Yu","Abhishek Nayyar","Bryce Petrini","Will Truong","Vincent Hellendoorn","Nikolai Chinaev","Chris Alberti","Wei Wang","Jingcao Hu","Vahab Mirrokni","Ananth Balashankar","Avia Aharon","Aahil Mehta","Ahmet Iscen","Joseph Kready","Lucas Manning","Anhad Mohananey","Yuankai Chen","Anshuman Tripathi","Allen Wu","Igor Petrovski","Dawsen Hwang","Martin Baeuml","Shreyas Chandrakaladharan","Yuan Liu","Rey Coaguila","Maxwell Chen","Sally Ma","Pouya Tafti","Susheel Tatineni","Terry Spitz","Jiayu Ye","Paul Vicol","Mihaela Rosca","Adrià Puigdomènech","Zohar Yahav","Sanjay Ghemawat","Hanzhao Lin","Phoebe Kirk","Zaid Nabulsi","Sergey Brin","Bernd Bohnet","Ken Caluwaerts","Aditya Srikanth Veerubhotla","Dan Zheng","Zihang Dai","Petre Petrov","Yichong Xu","Ramin Mehran","Zhuo Xu","Luisa Zintgraf","Jiho Choi","Spurthi Amba Hombaiah","Romal Thoppilan","Sashank Reddi","Lukasz Lew","Li Li","Kellie Webster","KP Sawhney","Lampros Lamprou","Siamak Shakeri","Mayank Lunayach","Jianmin Chen","Sumit Bagri","Alex Salcianu","Ying Chen","Yani Donchev","Charlotte Magister","Signe Nørly","Vitor Rodrigues","Tomas Izo","Hila Noga","Joe Zou","Thomas Köppe","Wenxuan Zhou","Kenton Lee","Xiangzhu Long","Danielle Eisenbud","Anthony Chen","Connor Schenck","Chi Ming To","Peilin Zhong","Emanuel Taropa","Minh Truong","Omer Levy","Danilo Martins","Zhiyuan Zhang","Christopher Semturs","Kelvin Zhang","Alex Yakubovich","Pol Moreno","Lara McConnaughey","Di Lu","Sam Redmond","Lotte Weerts","Yonatan Bitton","Tiziana Refice","Nicolas Lacasse","Arthur Conmy","Corentin Tallec","Julian Odell","Hannah Forbes-Pollard","Arkadiusz Socala","Jonathan Hoech","Pushmeet Kohli","Alanna Walton","Rui Wang","Mikita Sazanovich","Kexin Zhu","Andrei Kapishnikov","Rich Galt","Matthew Denton","Ben Murdoch","Caitlin Sikora","Kareem Mohamed","Wei Wei","Uri First","Tim McConnell","Luis C. Cobo","James Qin","Thi Avrahami","Daniel Balle","Yu Watanabe","Annie Louis","Adam Kraft","Setareh Ariafar","Yiming Gu","Eugénie Rives","Charles Yoon","Andrei Rusu","James Cobon-Kerr","Chris Hahn","Jiaming Luo"," Yuvein"," Zhu","Niharika Ahuja","Rodrigo Benenson","Raphaël Lopez Kaufman","Honglin Yu","Lloyd Hightower","Junlin Zhang","Darren Ni","Lisa Anne Hendricks","Gabby Wang","Gal Yona","Lalit Jain","Pablo Barrio","Surya Bhupatiraju","Siva Velusamy","Allan Dafoe","Sebastian Riedel","Tara Thomas","Zhe Yuan","Mathias Bellaiche","Sheena Panthaplackel","Klemen Kloboves","Sarthak Jauhari","Canfer Akbulut","Todor Davchev","Evgeny Gladchenko","David Madras","Aleksandr Chuklin","Tyrone Hill","Quan Yuan","Mukundan Madhavan","Luke Leonhard","Dylan Scandinaro","Qihang Chen","Ning Niu","Arthur Douillard","Bogdan Damoc","Yasumasa Onoe","Fabian Pedregosa","Fred Bertsch","Chas Leichner","Joseph Pagadora","Jonathan Malmaud","Sameera Ponda","Andy Twigg","Oleksii Duzhyi","Jingwei Shen","Miaosen Wang","Roopal Garg","Jing Chen","Utku Evci","Jonathan Lee","Leon Liu","Koji Kojima","Masa Yamaguchi","Arunkumar Rajendran","AJ Piergiovanni","Vinodh Kumar Rajendran","Marco Fornoni","Gabriel Ibagon","Harry Ragan","Sadh MNM Khan","John Blitzer","Andrew Bunner","Guan Sun","Takahiro Kosakai","Scott Lundberg","Ndidi Elue","Kelvin Guu","SK Park","Jane Park","Arunachalam Narayanaswamy","Chengda Wu","Jayaram Mudigonda","Trevor Cohn","Hairong Mu","Ravi Kumar","Laura Graesser","Yichi Zhang","Richard Killam","Vincent Zhuang","Mai Giménez","Wael Al Jishi","Ruy Ley-Wild","Alex Zhai","Kazuki Osawa","Diego Cedillo","Jialu Liu","Mayank Upadhyay","Marcin Sieniek","Roshan Sharma","Tom Paine","Anelia Angelova","Sravanti Addepalli","Carolina Parada","Kingshuk Majumder","Avery Lamp","Sanjiv Kumar","Xiang Deng","Artiom Myaskovsky","Tea Sabolić","Jeffrey Dudek","Sarah York","Félix de Chaumont Quitry","Jiazhong Nie","Dee Cattle","Alok Gunjan","Bilal Piot","Waleed Khawaja","Seojin Bang","Simon Wang","Siavash Khodadadeh","Raghavender R","Praynaa Rawlani","Richard Powell","Kevin Lee","Johannes Griesser","GS Oh","Cesar Magalhaes","Yujia Li","Simon Tokumine","Hadas Natalie Vogel","Dennis Hsu","Arturo BC","Disha Jindal","Matan Cohen","Zi Yang","Junwei Yuan","Dario de Cesare","Tony Bruguier","Jun Xu","Monica Roy","Alon Jacovi","Dan Belov","Rahul Arya","Phoenix Meadowlark","Shlomi Cohen-Ganor","Wenting Ye","Patrick Morris-Suzuki","Praseem Banzal","Gan Song","Pranavaraj Ponnuramu","Fred Zhang","George Scrivener","Salah Zaiem","Alif Raditya Rochman","Kehang Han","Badih Ghazi","Kate Lee","Shahar Drath","Daniel Suo","Antonious Girgis","Pradeep Shenoy","Duy Nguyen","Douglas Eck","Somit Gupta","Le Yan","Joao Carreira","Anmol Gulati","Ruoxin Sang","Daniil Mirylenka","Emma Cooney","Edward Chou","Mingyang Ling","Cindy Fan","Ben Coleman","Guilherme Tubone","Ravin Kumar","Jason Baldridge","Felix Hernandez-Campos","Angeliki Lazaridou","James Besley","Itay Yona","Neslihan Bulut","Quentin Wellens","AJ Pierigiovanni","Jasmine George","Richard Green","Pu Han","Connie Tao","Geoff Clark","Chong You","Abbas Abdolmaleki","Justin Fu","Tongzhou Chen","Ashwin Chaugule","Angad Chandorkar","Altaf Rahman","Will Thompson","Penporn Koanantakool","Mike Bernico","Jie Ren","Andrey Vlasov","Sergei Vassilvitskii","Maciej Kula","Yizhong Liang","Dahun Kim","Yangsibo Huang","Chengxi Ye","Dmitry Lepikhin","Wesley Helmholz"],"pdf_url":"","comment":"72 pages, 17 figures"},{"id":"http://arxiv.org/abs/2512.17525v1","updated":"2025-12-19T12:50:39Z","published":"2025-12-19T12:50:39Z","title":"Computational analysis reveals historical trajectory of East-Polynesian lunar calendars","summary":"We investigate a type of lunar calendar known as lists of the 'nights of the moon', found throughout East Polynesia, including Rapa Nui (Easter Island). Using computational methods, we analyzed the lexical and structural divergence of 49 calendric lists from all major archipelagos, each containing about 30 night names. Our results, presented as a rooted phylogenetic tree, show a clear split into two main groups: one including lists from Rapa Nui, Mangareva, and the Marquesas; the other comprising lists from New Zealand, Hawaii, the Cook Islands, the Austral Islands, Tahiti, and the Tuamotu. This pattern aligns with a recent alternative classification of East Polynesian languages into 'Distal' (Marquesan, Mangarevan, Rapanui) and 'Proximal' (Maori, Hawaiian, Tahitian, etc.) subgroups. Since both language and lunar calendars are symbolic systems passed down and changed within communities - and given the geographic isolation of many archipelagos - we interpret this correspondence as evidence that the early divergence of East Polynesian lunar calendars mirrors early population movements and language splits in the region.","authors":["Miguel Valério","Fabio Tamburini","Michele Corazza"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.10787v2","updated":"2025-12-19T12:41:35Z","published":"2025-12-11T16:31:29Z","title":"Replace, Don't Expand: Mitigating Context Dilution in Multi-Hop RAG via Fixed-Budget Evidence Assembly","summary":"Retrieval-Augmented Generation (RAG) systems often fail on multi-hop queries when the initial retrieval misses a bridge fact. Prior corrective approaches, such as Self-RAG, CRAG, and Adaptive-$k$, typically address this by \\textit{adding} more context or pruning existing lists. However, simply expanding the context window often leads to \\textbf{context dilution}, where distractors crowd out relevant information. We propose \\textbf{SEAL-RAG}, a training-free controller that adopts a \\textbf{``replace, don't expand''} strategy to fight context dilution under a fixed retrieval depth $k$. SEAL executes a (\\textbf{S}earch $\\rightarrow$ \\textbf{E}xtract $\\rightarrow$ \\textbf{A}ssess $\\rightarrow$ \\textbf{L}oop) cycle: it performs on-the-fly, entity-anchored extraction to build a live \\textit{gap specification} (missing entities/relations), triggers targeted micro-queries, and uses \\textit{entity-first ranking} to actively swap out distractors for gap-closing evidence. We evaluate SEAL-RAG against faithful re-implementations of Basic RAG, CRAG, Self-RAG, and Adaptive-$k$ in a shared environment on \\textbf{HotpotQA} and \\textbf{2WikiMultiHopQA}. On HotpotQA ($k=3$), SEAL improves answer correctness by \\textbf{+3--13 pp} and evidence precision by \\textbf{+12--18 pp} over Self-RAG. On 2WikiMultiHopQA ($k=5$), it outperforms Adaptive-$k$ by \\textbf{+8.0 pp} in accuracy and maintains \\textbf{96\\%} evidence precision compared to 22\\% for CRAG. These gains are statistically significant ($p<0.001$). By enforcing fixed-$k$ replacement, SEAL yields a predictable cost profile while ensuring the top-$k$ slots are optimized for precision rather than mere breadth. We release our code and data at https://github.com/mosherino/SEAL-RAG.","authors":["Moshe Lahmy","Roi Yozevitch"],"pdf_url":"","comment":"24 pages, 2 figures"},{"id":"http://arxiv.org/abs/2512.01037v2","updated":"2025-12-19T11:00:50Z","published":"2025-11-30T19:11:45Z","title":"When Safety Blocks Sense: Measuring Semantic Confusion in LLM Refusals","summary":"Safety-aligned language models often refuse prompts that are actually harmless. Current evaluations mostly report global rates such as false rejection or compliance. These scores treat each prompt alone and miss local inconsistency, where a model accepts one phrasing of an intent but rejects a close paraphrase. This gap limits diagnosis and tuning. We introduce \"semantic confusion,\" a failure mode that captures such local inconsistency, and a framework to measure it. We build ParaGuard, a 10k-prompt corpus of controlled paraphrase clusters that hold intent fixed while varying surface form. We then propose three model-agnostic metrics at the token level: Confusion Index, Confusion Rate, and Confusion Depth. These metrics compare each refusal to its nearest accepted neighbors and use token embeddings, next-token probabilities, and perplexity signals. Experiments across diverse model families and deployment guards show that global false-rejection rate hides critical structure. Our metrics reveal globally unstable boundaries in some settings, localized pockets of inconsistency in others, and cases where stricter refusal does not increase inconsistency. We also show how confusion-aware auditing separates how often a system refuses from how sensibly it refuses. This gives developers a practical signal to reduce false refusals while preserving safety.","authors":["Riad Ahmed Anonto","Md Labid Al Nahiyan","Md Tanvir Hassan"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2506.22376v4","updated":"2025-12-19T10:17:53Z","published":"2025-06-27T16:44:11Z","title":"OptScale: Probabilistic Optimality for Inference-time Scaling","summary":"Inference-time scaling has emerged as a powerful technique for enhancing the reasoning performance of Large Language Models (LLMs). However, existing approaches often rely on heuristic strategies for parallel sampling, lacking a principled foundation. To address this gap, we propose a probabilistic framework that formalizes the optimality of inference-time scaling under the assumption that parallel samples are independently and identically distributed (i.i.d.), and where the Best-of-$N$ selection strategy follows a probability distribution that can be estimated. Within this framework, we derive a theoretical lower bound on the required number of samples to achieve a target performance level, providing the first principled guidance for compute-efficient scaling. Leveraging this insight, we develop \\textsc{OptScale}, a practical algorithm that dynamically determines the optimal number of sampled responses. \\textsc{OptScale} employs a language model-based predictor to estimate probabilistic prior parameters, enabling the decision of the minimal number of samples needed that satisfy predefined performance thresholds and confidence levels. Extensive experiments on representative reasoning benchmarks (including MATH-500, GSM8K, AIME, and AMC) demonstrate that \\textsc{OptScale} significantly reduces sampling overhead while remaining better or on par with state-of-the-art reasoning performance. Our work offers both a theoretical foundation and a practical solution for principled inference-time scaling, addressing a critical gap in the efficient deployment of LLMs for complex reasoning.","authors":["Youkang Wang","Jian Wang","Rubing Chen","Xiao-Yong Wei"],"pdf_url":"","comment":"Accepted by AAAI-2026"},{"id":"http://arxiv.org/abs/2512.17419v1","updated":"2025-12-19T10:16:51Z","published":"2025-12-19T10:16:51Z","title":"SWE-Bench++: A Framework for the Scalable Generation of Software Engineering Benchmarks from Open-Source Repositories","summary":"Benchmarks like SWE-bench have standardized the evaluation of Large Language Models (LLMs) on repository-level software engineering tasks. However, these efforts remain limited by manual curation, static datasets, and a focus on Python-based bug fixes. We introduce SWE-Bench++, an automated framework that generates repository-level coding tasks from open-source GitHub projects. Unlike synthetic approaches, our pipeline harvests live pull requests to cover both bug fixes and feature requests across 11 languages. SWE-Bench++ turns GitHub pull requests (PRs) into reproducible, execution-based tasks via four stages: programmatic sourcing, environment synthesis, test oracle extraction, and quality assurance. A final hint-guided trajectory synthesis step converts instances that strong models fail on into training trajectories. Our initial benchmark consists of 11,133 instances from 3,971 repositories across 11 languages. On a subset of 1,782 instances of this benchmark, today's strongest models perform as follows: claude-sonnet-4.5 achieves 36.20% pass@10, gpt-5-2025-08-07 34.57%, gemini/gemini-2.5-pro 24.92%, and gpt-4o 16.89%. We further demonstrate the utility of our dataset by showing that fine-tuning on SWE-Bench++ instances yields measurable improvements on the SWE-bench Multilingual benchmark. SWE-Bench++ provides a scalable, multilingual benchmark for evaluating and improving repository-level code generation.","authors":["Lilin Wang","Lucas Ramalho","Alan Celestino","Phuc Anthony Pham","Yu Liu","Umang Kumar Sinha","Andres Portillo","Onassis Osunwa","Gabriel Maduekwe"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17396v1","updated":"2025-12-19T09:47:54Z","published":"2025-12-19T09:47:54Z","title":"RadImageNet-VQA: A Large-Scale CT and MRI Dataset for Radiologic Visual Question Answering","summary":"In this work, we introduce RadImageNet-VQA, a large-scale dataset designed to advance radiologic visual question answering (VQA) on CT and MRI exams. Existing medical VQA datasets are limited in scale, dominated by X-ray imaging or biomedical illustrations, and often prone to text-based shortcuts. RadImageNet-VQA is built from expert-curated annotations and provides 750K images paired with 7.5M question-answer samples. It covers three key tasks - abnormality detection, anatomy recognition, and pathology identification - spanning eight anatomical regions and 97 pathology categories, and supports open-ended, closed-ended, and multiple-choice questions. Extensive experiments show that state-of-the-art vision-language models still struggle with fine-grained pathology identification, particularly in open-ended settings and even after fine-tuning. Text-only analysis further reveals that model performance collapses to near-random without image inputs, confirming that RadImageNet-VQA is free from linguistic shortcuts. The full dataset and benchmark are publicly available at https://huggingface.co/datasets/raidium/RadImageNet-VQA.","authors":["Léo Butsanets","Charles Corbière","Julien Khlaut","Pierre Manceron","Corentin Dancette"],"pdf_url":"","comment":"Preprint, 23 pages, 12 figures, 7 tables"},{"id":"http://arxiv.org/abs/2512.17394v1","updated":"2025-12-19T09:47:38Z","published":"2025-12-19T09:47:38Z","title":"Are Vision Language Models Cross-Cultural Theory of Mind Reasoners?","summary":"Theory of Mind (ToM) -- the ability to attribute beliefs, desires, and emotions to others -- is fundamental for human social intelligence, yet remains a major challenge for artificial agents. Existing Vision-Language Models (VLMs) are increasingly applied in socially grounded tasks, but their capacity for cross-cultural ToM reasoning is largely unexplored. In this work, we introduce CulturalToM-VQA, a new evaluation benchmark containing 5095 questions designed to probe ToM reasoning across diverse cultural contexts through visual question answering. The dataset captures culturally grounded cues such as rituals, attire, gestures, and interpersonal dynamics, enabling systematic evaluation of ToM reasoning beyond Western-centric benchmarks. Our dataset is built through a VLM-assisted human-in-the-loop pipeline, where human experts first curate culturally rich images across traditions, rituals, and social interactions; a VLM then assist in generating structured ToM-focused scene descriptions, which are refined into question-answer pairs spanning a taxonomy of six ToM tasks and four graded complexity levels. The resulting dataset covers diverse theory of mind facets such as mental state attribution, false belief reasoning, non-literal communication, social norm violations, perspective coordination, and multi-agent reasoning.","authors":["Zabir Al Nazi","G M Shahariar","Abrar Hossain","Wei Peng"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17387v1","updated":"2025-12-19T09:43:20Z","published":"2025-12-19T09:43:20Z","title":"CIFE: Code Instruction-Following Evaluation","summary":"Large Language Models (LLMs) are increasingly applied to real-world code generation, where functional correctness alone is insufficient for reliable deployment, developers also expect adherence to explicit requirements for robustness, formatting, and security. Existing benchmarks primarily assess correctness through test-case execution, offering limited insight into how reliably models follow such constraints. We introduce a benchmark of 1,000 Python tasks, each paired with an average of 7 developer-specified constraints spanning 13 categories. Constraints are curated through a four-stage human-LLM pipeline to ensure they are atomic, relevant, and objective. We evaluate 14 open- and closed-source models using complementary adherence metrics and propose the C2A Score, a composite measure that jointly captures correctness and constraint compliance. Results reveal a substantial gap between partial and strict satisfaction, while strong models achieve over 90% partial adherence, strict adherence remains between 39-66%. These findings highlight that trustworthy code generation requires not only correctness but also consistent adherence to developer intent.","authors":["Sravani Gunnu","Shanmukha Guttula","Hima Patel"],"pdf_url":"","comment":"20 pages, 22 figures, 2 tables"},{"id":"http://arxiv.org/abs/2512.17385v1","updated":"2025-12-19T09:42:04Z","published":"2025-12-19T09:42:04Z","title":"UCoder: Unsupervised Code Generation by Internal Probing of Large Language Models","summary":"Large language models (LLMs) have demonstrated remarkable capabilities in code generation tasks. However, their effectiveness heavily relies on supervised training with extensive labeled (e.g., question-answering pairs) or unlabeled datasets (e.g., code snippets), which are often expensive and difficult to obtain at scale. To address this limitation, this paper introduces a method IPC, an unsupervised framework that leverages Internal Probing of LLMs for Code generation without any external corpus, even unlabeled code snippets. We introduce the problem space probing, test understanding probing, solution space probing, and knowledge consolidation and reinforcement to probe the internal knowledge and confidence patterns existing in LLMs. Further, IPC identifies reliable code candidates through self-consistency mechanisms and representation-based quality estimation to train UCoder (coder with unsupervised learning). We validate the proposed approach across multiple code benchmarks, demonstrating that unsupervised methods can achieve competitive performance compared to supervised approaches while significantly reducing the dependency on labeled data and computational resources. Analytic experiments reveal that internal model states contain rich signals about code quality and correctness, and that properly harnessing these signals enables effective unsupervised learning for code generation tasks, opening new directions for training code LLMs in resource-constrained scenarios.","authors":["Jiajun Wu","Jian Yang","Wei Zhang","Lin Jing","Yuqing Ma","Ensheng Shi","Yuchi Ma","Zhoujun Li","Xianglong Liu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17375v1","updated":"2025-12-19T09:22:11Z","published":"2025-12-19T09:22:11Z","title":"AdvJudge-Zero: Binary Decision Flips in LLM-as-a-Judge via Adversarial Control Tokens","summary":"Reward models and LLM-as-a-Judge systems are central to modern post-training pipelines such as RLHF, DPO, and RLAIF, where they provide scalar feedback and binary decisions that guide model selection and RL-based fine-tuning. We show that these judge systems exhibit a recurring vulnerability: short sequences of low-perplexity control tokens can flip many binary evaluations from correct ``No'' judgments to incorrect ``Yes'' judgments by steering the last-layer logit gap. These control tokens are patterns that a policy model could plausibly generate during post-training, and thus represent realistic reward-hacking risks rather than worst-case adversarial strings. Our method, AdvJudge-Zero, uses the model's next-token distribution and beam-search exploration to discover diverse control-token sequences from scratch, and our analysis shows that the induced hidden-state perturbations concentrate in a low-rank ``soft mode'' that is anti-aligned with the judge's refusal direction. Empirically, these tokens cause very high false positive rates when large open-weight and specialized judge models score incorrect answers on math and reasoning benchmarks. Finally, we show that LoRA-based adversarial training on small sets of control-token-augmented examples can markedly reduce these false positives while preserving evaluation quality.","authors":["Tung-Ling Li","Yuhao Wu","Hongliang Liu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2509.25123v3","updated":"2025-12-19T09:09:46Z","published":"2025-09-29T17:44:27Z","title":"From $f(x)$ and $g(x)$ to $f(g(x))$: LLMs Learn New Skills in RL by Composing Old Ones","summary":"Does RL teach LLMs genuinely new skills, or does it merely activate existing ones? This question lies at the core of ongoing debates about the role of RL in LLM post-training. On one side, strong empirical results can be achieved with RL even without preceding supervised finetuning; on the other, critics argue that RL contributes little beyond reweighting existing reasoning strategies. This work provides concrete evidence that LLMs can acquire genuinely new skills during RL by composing existing ones, mirroring one of the central mechanisms by which humans acquire new cognitive skills. To mitigate data contamination and other confounding factors, and to allow precise control over task complexity, we develop a synthetic framework for our investigation. Specifically, we define a skill as the ability to infer the output of a string transformation function f(x) given x. When an LLM has already learned f and g prior to RL, our experiments reveal that RL enables it to learn unseen compositions of them h(x)=g(f(x)). Further, this compositional ability generalizes to more difficult problems such as compositions of >2 functions unseen during RL training. Surprisingly, our experiments show that compositional skill acquired on a source task transfers to a different target task. This transfer happens even without compositional training on the target, requiring only prior knowledge of the target's atomic skills. Our qualitative analysis shows that RL fundamentally changes the reasoning behaviors of the models. In contrast, next-token training with the same data yields none of these findings. Our systematic experiments provide fresh insights into LLM learning, suggesting the value of first building base models with basic skills, then using RL to incentivize advanced, generalizable skills for complex problems.","authors":["Lifan Yuan","Weize Chen","Yuchen Zhang","Ganqu Cui","Hanbin Wang","Ziming You","Ning Ding","Zhiyuan Liu","Maosong Sun","Hao Peng"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2507.23358v2","updated":"2025-12-19T08:52:20Z","published":"2025-07-31T09:08:59Z","title":"Text-to-SQL Task-oriented Dialogue Ontology Construction","summary":"Large language models (LLMs) are widely used as general-purpose knowledge sources, but they rely on parametric knowledge, limiting explainability and trustworthiness. In task-oriented dialogue (TOD) systems, this separation is explicit, using an external database structured by an explicit ontology to ensure explainability and controllability. However, building such ontologies requires manual labels or supervised training. We introduce TeQoDO: a Text-to-SQL task-oriented Dialogue Ontology construction method. Here, an LLM autonomously builds a TOD ontology from scratch using only its inherent SQL programming capabilities combined with concepts from modular TOD systems provided in the prompt. We show that TeQoDO outperforms transfer learning approaches, and its constructed ontology is competitive on a downstream dialogue state tracking task. Ablation studies demonstrate the key role of modular TOD system concepts. TeQoDO also scales to allow construction of much larger ontologies, which we investigate on a Wikipedia and arXiv dataset. We view this as a step towards broader application of ontologies.","authors":["Renato Vukovic","Carel van Niekerk","Michael Heck","Benjamin Ruppik","Hsien-Chin Lin","Shutong Feng","Nurul Lubis","Milica Gasic"],"pdf_url":"","comment":"Accepted to Transactions of the Association for Computational Linguistics"},{"id":"http://arxiv.org/abs/2512.17351v1","updated":"2025-12-19T08:47:28Z","published":"2025-12-19T08:47:28Z","title":"Physics of Language Models: Part 4.1, Architecture Design and the Magic of Canon Layers","summary":"Understanding architectural differences in language models is challenging, especially at academic-scale pretraining (e.g., 1.3B parameters, 100B tokens), where results are often dominated by noise and randomness. To overcome this, we introduce controlled synthetic pretraining tasks that isolate and evaluate core model capabilities. Within this framework, we discover CANON LAYERS: lightweight architectural components -- named after the musical term \"canon\" -- that promote horizontal information flow across neighboring tokens. Canon layers compute weighted sums of nearby token representations and integrate seamlessly into Transformers, linear attention, state-space models, or any sequence architecture.\n  We present 12 key results. This includes how Canon layers enhance reasoning depth (e.g., by $2\\times$), reasoning breadth, knowledge manipulation, etc. They lift weak architectures like NoPE to match RoPE, and linear attention to rival SOTA linear models like Mamba2/GDN -- validated both through synthetic tasks and real-world academic-scale pretraining. This synthetic playground offers an economical, principled path to isolate core model capabilities often obscured at academic scales. Equipped with infinite high-quality data, it may even PREDICT how future architectures will behave as training pipelines improve -- e.g., through better data curation or RL-based post-training -- unlocking deeper reasoning and hierarchical inference.","authors":["Zeyuan Allen-Zhu"],"pdf_url":"","comment":"V1.1 appeared in NeurIPS 2025 main conference; V2 adds GDN experiments, tightens some experiments (for a stronger, fairer comparison), and re-organizes sections"},{"id":"http://arxiv.org/abs/2512.17347v1","updated":"2025-12-19T08:38:28Z","published":"2025-12-19T08:38:28Z","title":"Stakeholder Suite: A Unified AI Framework for Mapping Actors, Topics and Arguments in Public Debates","summary":"Public debates surrounding infrastructure and energy projects involve complex networks of stakeholders, arguments, and evolving narratives. Understanding these dynamics is crucial for anticipating controversies and informing engagement strategies, yet existing tools in media intelligence largely rely on descriptive analytics with limited transparency. This paper presents Stakeholder Suite, a framework deployed in operational contexts for mapping actors, topics, and arguments within public debates. The system combines actor detection, topic modeling, argument extraction and stance classification in a unified pipeline. Tested on multiple energy infrastructure projects as a case study, the approach delivers fine-grained, source-grounded insights while remaining adaptable to diverse domains. The framework achieves strong retrieval precision and stance accuracy, producing arguments judged relevant in 75% of pilot use cases. Beyond quantitative metrics, the tool has proven effective for operational use: helping project teams visualize networks of influence, identify emerging controversies, and support evidence-based decision-making.","authors":["Mohamed Chenene","Jeanne Rouhier","Jean Daniélou","Mihir Sarkar","Elena Cabrio"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17344v1","updated":"2025-12-19T08:35:51Z","published":"2025-12-19T08:35:51Z","title":"Governance-Aware Hybrid Fine-Tuning for Multilingual Large Language Models","summary":"We present a governance-aware hybrid fine-tuning framework for multilingual, low-resource adaptation of large language models. The core algorithm combines gradient-aligned low-rank updates with structured orthogonal transformations through layer-wise mixing and introduces unitary constraints in selected sub-layers to stabilize deep optimization. In tandem with lightweight, label-free data governance steps, including language identification, near-duplicate removal, and quality filtering, the framework targets accuracy, calibration, and cross-language parity under tight compute budgets. Across XNLI and FLORES, the hybrid approach delivers consistent gains over strong PEFT baselines while maintaining directional balance and improving probability calibration, as shown in Tables II and III. It is more resilient to lightweight orthographic variants, as shown in Table IV, and benefits additively from simple governance steps, as shown in Table V. Training footprint measurements indicate modest overhead and a favorable cost-quality frontier, as shown in Table VI and Figure 2. Together, these results show that hybrid and unitary PEFT provide a stable and accessible path to resource-efficient multilingual adaptation when paired with practical data governance.","authors":["Haomin Qi","Chengbo Huang","Zihan Dai","Yunkai Gao"],"pdf_url":"","comment":"11 pages, 4 figures, 6 tables. arXiv admin note: substantial text overlap with arXiv:2507.18076"},{"id":"http://arxiv.org/abs/2503.19041v4","updated":"2025-12-19T08:17:42Z","published":"2025-03-24T18:11:42Z","title":"LookAhead Tuning: Safer Language Models via Partial Answer Previews","summary":"Fine-tuning enables large language models (LLMs) to adapt to specific domains, but often compromises their previously established safety alignment. To mitigate the degradation of model safety during fine-tuning, we introduce LookAhead Tuning, a lightweight and effective data-driven approach that preserves safety during fine-tuning. The method introduces two simple strategies that modify training data by previewing partial answer prefixes, thereby minimizing perturbations to the model's initial token distributions and maintaining its built-in safety mechanisms. Comprehensive experiments demonstrate that LookAhead Tuning effectively maintains model safety without sacrificing robust performance on downstream tasks. Our findings position LookAhead Tuning as a reliable and efficient solution for the safe and effective adaptation of LLMs.","authors":["Kangwei Liu","Mengru Wang","Yujie Luo","Lin Yuan","Mengshu Sun","Lei Liang","Zhiqiang Zhang","Jun Zhou","Bryan Hooi","Shumin Deng"],"pdf_url":"","comment":"WSDM 2026 short"},{"id":"http://arxiv.org/abs/2502.01436v3","updated":"2025-12-19T08:17:09Z","published":"2025-02-03T15:19:28Z","title":"Towards Safer Chatbots: Automated Policy Compliance Evaluation of Custom GPTs","summary":"User-configured chatbots built on top of large language models are increasingly available through centralized marketplaces such as OpenAI's GPT Store. While these platforms enforce usage policies intended to prevent harmful or inappropriate behavior, the scale and opacity of customized chatbots make systematic policy enforcement challenging. As a result, policy-violating chatbots continue to remain publicly accessible despite existing review processes. This paper presents a fully automated method for evaluating the compliance of Custom GPTs with its marketplace usage policy using black-box interaction. The method combines large-scale GPT discovery, policy-driven red-teaming prompts, and automated compliance assessment using an LLM-as-a-judge. We focus on three policy-relevant domains explicitly addressed in OpenAI's usage policies: Romantic, Cybersecurity, and Academic GPTs. We validate our compliance assessment component against a human-annotated ground-truth dataset, achieving an F1 score of 0.975 for binary policy violation detection. We then apply the method in a large-scale empirical study of 782 Custom GPTs retrieved from the GPT Store. The results show that 58.7% of the evaluated GPTs exhibit at least one policy-violating response, with substantial variation across policy domains. A comparison with the base models (GPT-4 and GPT-4o) indicates that most violations originate from model-level behavior, while customization tends to amplify these tendencies rather than create new failure modes. Our findings reveal limitations in current review mechanisms for user-configured chatbots and demonstrate the feasibility of scalable, behavior-based policy compliance evaluation.","authors":["David Rodriguez","William Seymour","Jose M. Del Alamo","Jose Such"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.12218v2","updated":"2025-12-19T08:16:24Z","published":"2025-12-13T07:04:42Z","title":"Journey Before Destination: On the importance of Visual Faithfulness in Slow Thinking","summary":"Reasoning-augmented vision language models (VLMs) generate explicit chains of thought that promise greater capability and transparency but also introduce new failure modes: models may reach correct answers via visually unfaithful intermediate steps, or reason faithfully yet fail on the final prediction. Standard evaluations that only measure final-answer accuracy cannot distinguish these behaviors. We introduce the visual faithfulness of reasoning chains as a distinct evaluation dimension, focusing on whether the perception steps of a reasoning chain are grounded in the image. We propose a training- and reference-free framework that decomposes chains into perception versus reasoning steps and uses off-the-shelf VLM judges for step-level faithfulness, additionally verifying this approach through a human meta-evaluation. Building on this metric, we present a lightweight self-reflection procedure that detects and locally regenerates unfaithful perception steps without any training. Across multiple reasoning-trained VLMs and perception-heavy benchmarks, our method reduces Unfaithful Perception Rate while preserving final-answer accuracy, improving the reliability of multimodal reasoning.","authors":["Rheeya Uppaal","Phu Mon Htut","Min Bai","Nikolaos Pappas","Zheng Qi","Sandesh Swamy"],"pdf_url":"","comment":"Preprint"},{"id":"http://arxiv.org/abs/2512.17325v1","updated":"2025-12-19T08:14:21Z","published":"2025-12-19T08:14:21Z","title":"Task Schema and Binding: A Double Dissociation Study of In-Context Learning","summary":"We provide causal mechanistic validation that in-context learning (ICL) decomposes into two separable mechanisms: Task Schema (abstract task type recognition) and Binding (specific input-output associations). Through activation patching experiments across 9 models from 7 Transformer families plus Mamba (370M-13B parameters), we establish three key findings:\n  1. Double dissociation: Task Schema transfers at 100% via late MLP patching; Binding transfers at 62% via residual stream patching -- proving separable mechanisms\n  2. Prior-Schema trade-off: Schema reliance inversely correlates with prior knowledge (Spearman rho = -0.596, p < 0.001, N=28 task-model pairs)\n  3. Architecture generality: The mechanism operates across all tested architectures including the non-Transformer Mamba\n  These findings offer a mechanistic account of the ICL puzzle that contrasts with prior views treating ICL as a monolithic mechanism (whether retrieval-based, gradient descent-like, or purely Bayesian). By establishing that Schema and Binding are neurally dissociable -- not merely behavioral modes -- we provide causal evidence for dual-process theories of ICL. Models rely on Task Schema when prior knowledge is absent, but prior knowledge interferes through attentional mis-routing (72.7% recency bias) rather than direct output competition (0%). This explains why arbitrary mappings succeed (zero prior leads to full Schema reliance) while factual overrides fail -- and reveals that the true bottleneck is attentional, not output-level. Practical implications: Understanding these dual mechanisms enables more efficient prompt engineering -- reliable schema transfer reduces required demonstrations for novel tasks, while prior-aware design can mitigate the 38% binding failure rate in high-prior scenarios, improving ICL system reliability in production deployments.","authors":["Chaeha Kim"],"pdf_url":"","comment":"20pages, 2figures"},{"id":"http://arxiv.org/abs/2507.18076v2","updated":"2025-12-19T08:12:27Z","published":"2025-07-24T04:00:02Z","title":"Hybrid and Unitary PEFT for Resource-Efficient Large Language Models","summary":"Fine-tuning large language models (LLMs) remains a computational bottleneck due to their scale and memory demands. This paper presents a comprehensive evaluation of parameter-efficient fine-tuning (PEFT) techniques, including LoRA, BOFT, LoRA-GA, and uRNN, and introduces a novel hybrid strategy that dynamically integrates BOFT's orthogonal stability with LoRA-GA's gradient-aligned rapid convergence. By computing per-layer adaptive updates guided by gradient norms, the hybrid method achieves superior convergence efficiency and generalization across diverse tasks. We also explore, for the first time, the adaptation of unitary RNN (uRNN) principles to Transformer-based LLMs, enhancing gradient stability through structured unitary constraints. Across GLUE, GSM8K, MT-Bench, and HumanEval, using models ranging from 7B to 405B parameters, the hybrid approach yields consistent gains across three independent runs per task and model, approaching the quality of full fine-tuning while reducing training time by approximately 2.1 times and peak memory usage by nearly 50 percent, indicating practical significance under resource constraints. A compact multilingual and low-resource study on XNLI and FLORES, using 32 examples per language, further demonstrates consistent gains under the same budget with a small and stable footprint. These results indicate a practical and scalable path toward accessible LLM fine-tuning under resource constraints.","authors":["Haomin Qi","Zihan Dai","Chengbo Huang"],"pdf_url":"","comment":"11 pages, 2 figures and 7 table"},{"id":"http://arxiv.org/abs/2512.17308v1","updated":"2025-12-19T07:46:29Z","published":"2025-12-19T07:46:29Z","title":"Large Language Models as Pokémon Battle Agents: Strategic Play and Content Generation","summary":"Strategic decision-making in Pokémon battles presents a unique testbed for evaluating large language models. Pokémon battles demand reasoning about type matchups, statistical trade-offs, and risk assessment, skills that mirror human strategic thinking. This work examines whether Large Language Models (LLMs) can serve as competent battle agents, capable of both making tactically sound decisions and generating novel, balanced game content. We developed a turn-based Pokémon battle system where LLMs select moves based on battle state rather than pre-programmed logic. The framework captures essential Pokémon mechanics: type effectiveness multipliers, stat-based damage calculations, and multi-Pokémon team management. Through systematic evaluation across multiple model architectures we measured win rates, decision latency, type-alignment accuracy, and token efficiency. These results suggest LLMs can function as dynamic game opponents without domain-specific training, offering a practical alternative to reinforcement learning for turn-based strategic games. The dual capability of tactical reasoning and content creation, positions LLMs as both players and designers, with implications for procedural generation and adaptive difficulty systems in interactive entertainment.","authors":["Daksh Jain","Aarya Jain","Ashutosh Desai","Avyakt Verma","Ishan Bhanuka","Pratik Narang","Dhruv Kumar"],"pdf_url":"","comment":"Under Review"},{"id":"http://arxiv.org/abs/2506.09707v4","updated":"2025-12-19T07:32:25Z","published":"2025-06-11T13:21:06Z","title":"Fine-Tuning Large Audio-Language Models with LoRA for Precise Temporal Localization of Prolonged Exposure Therapy Elements","summary":"Prolonged Exposure (PE) therapy is an effective treatment for post-traumatic stress disorder (PTSD), but evaluating therapist fidelity remains labor-intensive due to the need for manual review of session recordings. We present a method for the automatic temporal localization of key PE fidelity elements, identifying their start and stop times, directly from session audio and transcripts. Our approach fine-tunes a large pre-trained audio-language model, Qwen2-Audio, using Low-Rank Adaptation (LoRA) to process focused 30-second windows of audio-transcript input. Fidelity labels for three core protocol phases, therapist orientation (P1), imaginal exposure (P2), and post-imaginal processing (P3), are generated via LLM-based prompting and verified by trained raters. The model is trained to predict normalized boundary offsets using soft supervision guided by task-specific prompts. On a dataset of 308 real PE sessions, our best configuration (LoRA rank 8, 30s windows) achieves a mean absolute error (MAE) of 5.3s across tasks, within typical rater tolerance for timestamp review, enabling practical fidelity QC. We further analyze the effects of window size and LoRA rank, highlighting the importance of context granularity and model adaptation. This work introduces a privacy-preserving, scalable framework for fidelity tracking in PE therapy, with potential to support clinician training, supervision, and quality assurance.","authors":["Suhas BN","Andrew M. Sherrill","Jyoti Alaparthi","Dominik Mattioli","Rosa I. Arriaga","Chris W. Wiese","Saeed Abdullah"],"pdf_url":"","comment":"5 pages, 2 figures"},{"id":"http://arxiv.org/abs/2512.13478v4","updated":"2025-12-19T07:21:39Z","published":"2025-12-15T16:14:32Z","title":"Non-Resolution Reasoning (NRR): A Computational Framework for Contextual Identity and Ambiguity Preservation","summary":"Current artificial intelligence systems, despite remarkable capabilities in text generation and pattern recognition, exhibit a fundamental architectural limitation: they resolve ambiguity prematurely. This premature semantic collapse -- the tendency to collapse multiple valid interpretations into a single output -- stems from classical identity assumptions embedded in standard neural architectures. We propose Non-Resolution Reasoning (NRR), a computational framework that treats ambiguity retention as a valid reasoning mode rather than a defect to be eliminated. NRR introduces three core principles: (1) Non-Identity ($A \\neq A$) -- the same symbol refers to different entities across contexts; (2) Approximate Identity ($A \\approx A$) -- entities share partial structural overlap without being identical; and (3) Non-Resolution -- conflicting interpretations can coexist without forced convergence. We formalize these principles through three architectural components: Multi-Vector Embeddings for context-dependent representation, Non-Collapsing Attention for parallel interpretation retention, and Contextual Identity Tracking (CIT) for maintaining $A \\neq A$ across inference. We demonstrate NRR's advantages through case studies in paradox handling, creative generation, and context-dependent reasoning. Crucially, we provide a minimal empirical validation on a synthetic context-shift task where an NRR-lite model achieves 90.9% out-of-distribution accuracy compared to 9.1% for standard architectures, demonstrating that ambiguity preservation enables structural generalization. NRR challenges the assumption that meaning must collapse to be useful, offering a foundation for AI systems capable of sophisticated ambiguity handling and creative reasoning. The question is not whether AI should resolve ambiguity, but when, how, and under whose control.","authors":["Kei Saito"],"pdf_url":"","comment":"16 pages, 1 figure. Updated version with corrected references and aligned acknowledgments"},{"id":"http://arxiv.org/abs/2510.16882v2","updated":"2025-12-19T07:13:05Z","published":"2025-10-19T15:32:01Z","title":"Utility-Diversity Aware Online Batch Selection for LLM Supervised Fine-tuning","summary":"Supervised fine-tuning (SFT) is a commonly used technique to adapt large language models (LLMs) to downstream tasks. In practice, SFT on a full dataset is computationally expensive and sometimes suffers from overfitting or bias amplification. This facilitates the rise of data curation in SFT, which prioritizes the most valuable data to optimze. This work studies the online batch selection family that dynamically scores and filters samples during the training process. However, existing popular methods often (i) rely merely on the utility of data to select a subset while neglecting other crucial factors like diversity, (ii) rely on external resources such as reference models or validation sets, and (iii) incur extra training time over full-dataset training. To address these limitations, this work develops \\textbf{UDS (Utility-Diversity Sampling)}, a framework for efficient online batch selection in SFT. UDS leverages the nuclear norm of the logits matrix to capture both data utility and intra-sample diversity, while estimating inter-sample diversity through efficient low-dimensional embedding comparisons with a lightweight memory buffer of historical samples. Such a design eliminates the need for external resources and unnecessary backpropagation, securing computational efficiency. Experiments on multiple benchmarks demonstrate that UDS consistently outperforms state-of-the-art online batch selection methods under varying data budgets, and significantly reduces training time compared to full-dataset fine-tuning. Code is available at https://github.com/gfyddha/UDS.","authors":["Heming Zou","Yixiu Mao","Yun Qu","Qi Wang","Xiangyang Ji"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17289v1","updated":"2025-12-19T07:11:50Z","published":"2025-12-19T07:11:50Z","title":"Subjective Question Generation and Answer Evaluation using NLP","summary":"Natural Language Processing (NLP) is one of the most revolutionary technologies today. It uses artificial intelligence to understand human text and spoken words. It is used for text summarization, grammar checking, sentiment analysis, and advanced chatbots and has many more potential use cases. Furthermore, it has also made its mark on the education sector. Much research and advancements have already been conducted on objective question generation; however, automated subjective question generation and answer evaluation are still in progress. An automated system to generate subjective questions and evaluate the answers can help teachers assess student work and enhance the student's learning experience by allowing them to self-assess their understanding after reading an article or a chapter of a book. This research aims to improve current NLP models or make a novel one for automated subjective question generation and answer evaluation from text input.","authors":["G. M. Refatul Islam","Safwan Shaheer","Yaseen Nur","Mohammad Rafid Hamid"],"pdf_url":"","comment":"5 pages, 5 figures, 2 tables, conference paper"},{"id":"http://arxiv.org/abs/2405.15877v4","updated":"2025-12-19T06:49:59Z","published":"2024-05-24T18:40:20Z","title":"Basis Selection: Low-Rank Decomposition of Pretrained Large Language Models for Target Applications","summary":"Large language models (LLMs) significantly enhance the performance of various applications, but they are computationally intensive and energy-demanding. This makes it challenging to deploy them on devices with limited resources, such as personal computers and mobile/wearable devices, and results in substantial inference costs in resource-rich environments like cloud servers. To extend the use of LLMs, we introduce a low-rank decomposition approach to effectively compress these models, tailored to the requirements of specific applications. We observe that LLMs pretrained on general datasets contain many redundant components not needed for particular applications. Our method focuses on identifying and removing these redundant parts, retaining only the necessary elements for the target applications. Specifically, we represent the weight matrices of LLMs as a linear combination of base components. We then prune the irrelevant bases and enhance the model with new bases beneficial for specific applications. Deep compression results on the Llama 2-7b and -13B models, conducted on target applications including mathematical reasoning and code generation, show that our method significantly reduces model size while maintaining comparable accuracy to state-of-the-art low-rank compression techniques.","authors":["Yang Li","Daniel Agyei Asante","Changsheng Zhao","Ernie Chang","Yangyang Shi","Vikas Chandra"],"pdf_url":"","comment":"Transactions on Machine Learning Research (TMLR), 2025"},{"id":"http://arxiv.org/abs/2511.11571v2","updated":"2025-12-19T06:48:56Z","published":"2025-11-14T18:59:59Z","title":"Optimizing Mixture of Block Attention","summary":"Mixture of Block Attention (MoBA) (Lu et al., 2025) is a promising building block for efficiently processing long contexts in LLMs by enabling queries to sparsely attend to a small subset of key-value blocks, drastically reducing computational cost. However, the design principles governing MoBA's performance are poorly understood, and it lacks an efficient GPU implementation, hindering its practical adoption. In this paper, we first develop a statistical model to analyze MoBA's underlying mechanics. Our model reveals that performance critically depends on the router's ability to accurately distinguish relevant from irrelevant blocks based on query-key affinities. We derive a signal-to-noise ratio that formally connects architectural parameters to this retrieval accuracy. Guided by our analysis, we identify two key pathways for improvement: using smaller block sizes and applying a short convolution on keys to cluster relevant signals, which enhances routing accuracy. While theoretically better, small block sizes are inefficient on GPUs. To bridge this gap, we introduce FlashMoBA, a hardware-aware CUDA kernel that enables efficient MoBA execution even with the small block sizes our theory recommends. We validate our insights by training LLMs from scratch, showing that our improved MoBA models match the performance of dense attention baselines. FlashMoBA achieves up to 14.7x speedup over FlashAttention-2 for small blocks, making our theoretically-grounded improvements practical. Code is available at: https://github.com/mit-han-lab/flash-moba.","authors":["Guangxuan Xiao","Junxian Guo","Kasra Mazaheri","Song Han"],"pdf_url":"","comment":"The first two authors contributed equally to this work"},{"id":"http://arxiv.org/abs/2512.17270v1","updated":"2025-12-19T06:37:44Z","published":"2025-12-19T06:37:44Z","title":"Understanding Generalization in Role-Playing Models via Information Theory","summary":"Role-playing models (RPMs) are widely used in real-world applications but underperform when deployed in the wild. This degradation can be attributed to distribution shifts, including user, character, and dialogue compositional shifts. Existing methods like LLM-as-a-judge fall short in providing a fine-grained diagnosis of how these shifts affect RPM generalization, and thus there lack formal frameworks to characterize RPM generalization behaviors. To bridge these gaps, we introduce an information-theoretic metric, named reasoning-based effective mutual information difference (R-EMID), to measure RPM performance degradation in an interpretable way. We also derive an upper bound on R-EMID to predict the worst-case generalization performance of RPMs and theoretically reveal how various shifts contribute to the RPM performance degradation. Moreover, we propose a co-evolving reinforcement learning framework to adaptively model the connection among user, character, and dialogue context and thus enhance the estimation of dialogue response generation probability, which is critical for calculating R-EMID. Finally, we evaluate the generalization performance of various RPMs using R-EMID, finding that user shift poses the highest risk among all shifts and reinforcement learning is the most effective approach for enhancing RPM generalization.","authors":["Yongqi Li","Hao Lang","Fei Huang","Tieyun Qian","Yongbin Li"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.16189v2","updated":"2025-12-19T06:34:23Z","published":"2025-12-18T05:23:47Z","title":"Mitigating Hallucinations in Healthcare LLMs with Granular Fact-Checking and Domain-Specific Adaptation","summary":"In healthcare, it is essential for any LLM-generated output to be reliable and accurate, particularly in cases involving decision-making and patient safety. However, the outputs are often unreliable in such critical areas due to the risk of hallucinated outputs from the LLMs. To address this issue, we propose a fact-checking module that operates independently of any LLM, along with a domain-specific summarization model designed to minimize hallucination rates. Our model is fine-tuned using Low-Rank Adaptation (LoRa) on the MIMIC III dataset and is paired with the fact-checking module, which uses numerical tests for correctness and logical checks at a granular level through discrete logic in natural language processing (NLP) to validate facts against electronic health records (EHRs). We trained the LLM model on the full MIMIC-III dataset. For evaluation of the fact-checking module, we sampled 104 summaries, extracted them into 3,786 propositions, and used these as facts. The fact-checking module achieves a precision of 0.8904, a recall of 0.8234, and an F1-score of 0.8556. Additionally, the LLM summary model achieves a ROUGE-1 score of 0.5797 and a BERTScore of 0.9120 for summary quality.","authors":["Musarrat Zeba","Abdullah Al Mamun","Kishoar Jahan Tithee","Debopom Sutradhar","Mohaimenul Azam Khan Raiaan","Saddam Mukta","Reem E. Mohamed","Md Rafiqul Islam","Yakub Sebastian","Mukhtar Hussain","Sami Azam"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17267v1","updated":"2025-12-19T06:32:46Z","published":"2025-12-19T06:32:46Z","title":"AutoMetrics: Approximate Human Judgements with Automatically Generated Evaluators","summary":"Evaluating user-facing AI applications remains a central challenge, especially in open-ended domains such as travel planning, clinical note generation, or dialogue. The gold standard is user feedback (e.g., thumbs up/down) or behavioral signals (e.g., retention), but these are often scarce in prototypes and research projects, or too-slow to use for system optimization. We present AutoMetrics, a framework for synthesizing evaluation metrics under low-data constraints. AutoMetrics combines retrieval from MetricBank, a collection of 48 metrics we curate, with automatically generated LLM-as-a-Judge criteria informed by lightweight human feedback. These metrics are composed via regression to maximize correlation with human signal. AutoMetrics takes you from expensive measures to interpretable automatic metrics. Across 5 diverse tasks, AutoMetrics improves Kendall correlation with human ratings by up to 33.4% over LLM-as-a-Judge while requiring fewer than 100 feedback points. We show that AutoMetrics can be used as a proxy reward to equal effect as a verifiable reward. We release the full AutoMetrics toolkit and MetricBank to accelerate adaptive evaluation of LLM applications.","authors":["Michael J. Ryan","Yanzhe Zhang","Amol Salunkhe","Yi Chu","Di Xu","Diyi Yang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2505.20112v3","updated":"2025-12-19T06:23:19Z","published":"2025-05-26T15:14:54Z","title":"ResSVD: Residual Compensated SVD for Large Language Model Compression","summary":"Large language models (LLMs) have demonstrated impressive capabilities in a wide range of downstream natural language processing tasks. Nevertheless, their considerable sizes and memory demands hinder practical deployment, underscoring the importance of developing efficient compression strategies. Singular value decomposition (SVD) decomposes a matrix into orthogonal components, enabling efficient low-rank approximation. This is particularly suitable for LLM compression, where weight matrices often exhibit significant redundancy. However, current SVD-based methods neglect the residual matrix from truncation, resulting in significant truncation loss. Additionally, compressing all layers of the model results in severe performance degradation. To overcome these limitations, we propose ResSVD, a new post-training SVD-based LLM compression method. Specifically, we leverage the residual matrix generated during the truncation process to reduce truncation loss. Moreover, under a fixed overall compression ratio, we selectively compress the last few layers of the model, which mitigates error propagation and significantly improves the performance of compressed models. Comprehensive evaluations of ResSVD on diverse LLM families and multiple benchmark datasets indicate that ResSVD consistently achieves superior performance over existing counterpart methods, demonstrating its practical effectiveness.","authors":["Haolei Bai","Siyong Jian","Tuo Liang","Yu Yin","Huan Wang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17260v1","updated":"2025-12-19T06:19:55Z","published":"2025-12-19T06:19:55Z","title":"Seed-Prover 1.5: Mastering Undergraduate-Level Theorem Proving via Learning from Experience","summary":"Large language models have recently made significant progress to generate rigorous mathematical proofs. In contrast, utilizing LLMs for theorem proving in formal languages (such as Lean) remains challenging and computationally expensive, particularly when addressing problems at the undergraduate level and beyond. In this work, we present \\textbf{Seed-Prover 1.5}, a formal theorem-proving model trained via large-scale agentic reinforcement learning, alongside an efficient test-time scaling (TTS) workflow. Through extensive interactions with Lean and other tools, the model continuously accumulates experience during the RL process, substantially enhancing the capability and efficiency of formal theorem proving. Furthermore, leveraging recent advancements in natural language proving, our TTS workflow efficiently bridges the gap between natural and formal languages. Compared to state-of-the-art methods, Seed-Prover 1.5 achieves superior performance with a smaller compute budget. It solves \\textbf{88\\% of PutnamBench} (undergraduate-level), \\textbf{80\\% of Fate-H} (graduate-level), and \\textbf{33\\% of Fate-X} (PhD-level) problems. Notably, using our system, we solved \\textbf{11 out of 12 problems} from Putnam 2025 within 9 hours. Our findings suggest that scaling learning from experience, driven by high-quality formal feedback, holds immense potential for the future of formal mathematical reasoning.","authors":["Jiangjie Chen","Wenxiang Chen","Jiacheng Du","Jinyi Hu","Zhicheng Jiang","Allan Jie","Xiaoran Jin","Xing Jin","Chenggang Li","Wenlei Shi","Zhihong Wang","Mingxuan Wang","Chenrui Wei","Shufa Wei","Huajian Xin","Fan Yang","Weihao Gao","Zheng Yuan","Tianyang Zhan","Zeyu Zheng","Tianxi Zhou","Thomas Hanwen Zhu"],"pdf_url":"","comment":"21 pages"},{"id":"http://arxiv.org/abs/2512.16248v2","updated":"2025-12-19T05:44:04Z","published":"2025-12-18T06:57:42Z","title":"Sigma-MoE-Tiny Technical Report","summary":"Mixture-of-Experts (MoE) has emerged as a promising paradigm for foundation models due to its efficient and powerful scalability. In this work, we present Sigma-MoE-Tiny, an MoE language model that achieves the highest sparsity compared to existing open-source models. Sigma-MoE-Tiny employs fine-grained expert segmentation with up to 96 experts per layer, while activating only one expert for each token, resulting in 20B total parameters with just 0.5B activated. The major challenge introduced by such extreme sparsity lies in expert load balancing. We find that the widely-used load balancing loss tends to become ineffective in the lower layers under this setting. To address this issue, we propose a progressive sparsification schedule aiming to balance expert utilization and training stability. Sigma-MoE-Tiny is pre-trained on a diverse and high-quality corpus, followed by post-training to further unlock its capabilities. The entire training process remains remarkably stable, with no occurrence of irrecoverable loss spikes. Comprehensive evaluations reveal that, despite activating only 0.5B parameters, Sigma-MoE-Tiny achieves top-tier performance among counterparts of comparable or significantly larger scale. In addition, we provide an in-depth discussion of load balancing in highly sparse MoE models, offering insights for advancing sparsity in future MoE architectures.\n  Project page: https://qghuxmu.github.io/Sigma-MoE-Tiny\n  Code: https://github.com/microsoft/ltp-megatron-lm","authors":["Qingguo Hu","Zhenghao Lin","Ziyue Yang","Yucheng Ding","Xiao Liu","Yuting Jiang","Ruizhe Wang","Tianyu Chen","Zhongxin Guo","Yifan Xiong","Rui Gao","Lei Qu","Jinsong Su","Peng Cheng","Yeyun Gong"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17247v1","updated":"2025-12-19T05:26:50Z","published":"2025-12-19T05:26:50Z","title":"Incorporating Error Level Noise Embedding for Improving LLM-Assisted Robustness in Persian Speech Recognition","summary":"Automatic Speech Recognition (ASR) systems suffer significant performance degradation in noisy environments, a challenge that is especially severe for low-resource languages such as Persian. Even state-of-the-art models such as Whisper struggle to maintain accuracy under varying signal-to-noise ratios (SNRs). This study presents a robust noise-sensitive ASR error correction framework that combines multiple hypotheses and noise-aware modeling. Using noisy Persian speech, we generate 5-best hypotheses from a modified Whisper-large decoder. Error Level Noise (ELN) is introduced as a representation that captures semantic- and token-level disagreement across hypotheses, quantifying the linguistic distortions caused by noise. ELN thus provides a direct measure of noise-induced uncertainty, enabling the LLM to reason about the reliability of each hypothesis during correction. Three models are evaluated: (1) a base LLaMA-2-7B model without fine-tuning, (2) a fine-tuned variant trained on text-only hypotheses, and (3) a noise-conditioned model integrating ELN embeddings at both sentence and word levels. Experimental results demonstrate that the ELN-conditioned model achieves substantial reductions in Word Error Rate (WER). Specifically, on the challenging Mixed Noise test set, the proposed Fine-tuned + ELN (Ours) model reduces the WER from a baseline of 31.10\\% (Raw Whisper) to 24.84\\%, significantly surpassing the Fine-tuned (No ELN) text-only baseline of 30.79\\%, whereas the original LLaMA-2-7B model increased the WER to 64.58\\%, demonstrating that it is unable to correct Persian errors on its own. This confirms the effectiveness of combining multiple hypotheses with noise-aware embeddings for robust Persian ASR in noisy real-world scenarios.","authors":["Zahra Rahmani","Hossein Sameti"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2408.09030v4","updated":"2025-12-19T05:10:32Z","published":"2024-08-16T21:57:23Z","title":"Studying the Effects of Collaboration in Interactive Theme Discovery Systems","summary":"NLP-assisted solutions have gained considerable traction to support qualitative data analysis. However, there does not exist a unified evaluation framework that can account for the many different settings in which qualitative researchers may employ them. In this paper, we take a first step in this direction by proposing an evaluation framework to study the way in which different tools may result in different outcomes depending on the collaboration strategy employed. Specifically, we study the impact of synchronous vs. asynchronous collaboration using two different NLP-assisted qualitative research tools and present a comprehensive analysis of significant differences in the consistency, cohesiveness, and correctness of their outputs.","authors":["Alvin Po-Chun Chen","Rohan Das","Dananjay Srinivas","Alexandra Barry","Maksim Seniw","Maria Leonor Pacheco"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.07540v2","updated":"2025-12-19T04:16:55Z","published":"2025-12-08T13:21:44Z","title":"Minimum Bayes Risk Decoding for Error Span Detection in Reference-Free Automatic Machine Translation Evaluation","summary":"Error Span Detection (ESD) extends automatic machine translation (MT) evaluation by localizing translation errors and labeling their severity. Current generative ESD methods typically use Maximum a Posteriori (MAP) decoding, assuming that the model-estimated probabilities are perfectly correlated with similarity to the human annotation, but we often observe higher likelihood assigned to an incorrect annotation than to the human one. We instead apply Minimum Bayes Risk (MBR) decoding to generative ESD. We use a sentence- or span-level similarity function for MBR decoding, which selects candidate hypotheses based on their approximate similarity to the human annotation. Experimental results on the WMT24 Metrics Shared Task show that MBR decoding significantly improves span-level performance and generally matches or outperforms MAP at the system and sentence levels. To reduce the computational cost of MBR decoding, we further distill its decisions into a model decoded via greedy search, removing the inference-time latency bottleneck.","authors":["Boxuan Lyu","Haiyue Song","Hidetaka Kamigaito","Chenchen Ding","Hideki Tanaka","Masao Utiyama","Kotaro Funakoshi","Manabu Okumura"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17220v1","updated":"2025-12-19T04:08:29Z","published":"2025-12-19T04:08:29Z","title":"Mindscape-Aware Retrieval Augmented Generation for Improved Long Context Understanding","summary":"Humans understand long and complex texts by relying on a holistic semantic representation of the content. This global view helps organize prior knowledge, interpret new information, and integrate evidence dispersed across a document, as revealed by the Mindscape-Aware Capability of humans in psychology. Current Retrieval-Augmented Generation (RAG) systems lack such guidance and therefore struggle with long-context tasks. In this paper, we propose Mindscape-Aware RAG (MiA-RAG), the first approach that equips LLM-based RAG systems with explicit global context awareness. MiA-RAG builds a mindscape through hierarchical summarization and conditions both retrieval and generation on this global semantic representation. This enables the retriever to form enriched query embeddings and the generator to reason over retrieved evidence within a coherent global context. We evaluate MiA-RAG across diverse long-context and bilingual benchmarks for evidence-based understanding and global sense-making. It consistently surpasses baselines, and further analysis shows that it aligns local details with a coherent global representation, enabling more human-like long-context retrieval and reasoning.","authors":["Yuqing Li","Jiangnan Li","Zheng Lin","Ziyan Zhou","Junjie Wu","Weiping Wang","Jie Zhou","Mo Yu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2503.10689v2","updated":"2025-12-19T03:49:52Z","published":"2025-03-12T01:33:40Z","title":"Learning to Contextualize Web Pages for Enhanced Decision Making by LLM Agents","summary":"Recent advances in large language models (LLMs) have led to a growing interest in developing LLM-based agents for automating web tasks. However, these agents often struggle with even simple tasks on real-world websites due to their limited capability to understand and process complex web page structures. In this work, we introduce LCoW, a framework for Learning language models to Contextualize complex Web pages into a more comprehensible form, thereby enhancing decision making by LLM agents. LCoW decouples web page understanding from decision making by training a separate contextualization module to transform complex web pages into comprehensible format, which are then utilized by the decision-making agent. We demonstrate that our contextualization module effectively integrates with LLM agents of various scales to significantly enhance their decision-making capabilities in web automation tasks. Notably, LCoW improves the success rates of closed-source LLMs (e.g., Gemini-1.5-flash, GPT-4o, Claude-3.5-Sonnet) by an average of 15.6%, and demonstrates a 23.7% average improvement in success rates for open-source LMs (e.g., Llama-3.1-8B, Llama-3.1-70B) on the WorkArena benchmark. Moreover, the Gemini-1.5-flash agent with LCoW achieves state-of-the-art results on the WebShop benchmark, outperforming human experts. The relevant code materials are available at our project page: https://lcowiclr2025.github.io.","authors":["Dongjun Lee","Juyong Lee","Kyuyoung Kim","Jihoon Tack","Jinwoo Shin","Yee Whye Teh","Kimin Lee"],"pdf_url":"","comment":"Accepted to ICLR 2025"},{"id":"http://arxiv.org/abs/2509.07414v3","updated":"2025-12-19T03:05:26Z","published":"2025-09-09T05:51:34Z","title":"Language Self-Play For Data-Free Training","summary":"Large language models (LLMs) have advanced rapidly in recent years, driven by scale, abundant high-quality training data, and reinforcement learning. Yet this progress faces a fundamental bottleneck: the need for ever more data from which models can continue to learn. In this work, we propose a reinforcement learning approach that removes this dependency by enabling models to improve without additional data. Our method leverages a game-theoretic framework of self-play, where a model's capabilities are cast as performance in a competitive game and stronger policies emerge by having the model play against itself-a process we call Language Self-Play (LSP). Experiments with Llama-3.2-3B-Instruct on instruction-following, mathematics, and coding benchmarks show that pretrained models can be effectively improved with self-play alone.","authors":["Jakub Grudzien Kuba","Mengting Gu","Qi Ma","Yuandong Tian","Vijai Mohan","Jason Chen"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17179v1","updated":"2025-12-19T02:37:30Z","published":"2025-12-19T02:37:30Z","title":"Enhancing Long Document Long Form Summarisation with Self-Planning","summary":"We introduce a novel approach for long context summarisation, highlight-guided generation, that leverages sentence-level information as a content plan to improve the traceability and faithfulness of generated summaries. Our framework applies self-planning methods to identify important content and then generates a summary conditioned on the plan. We explore both an end-to-end and two-stage variants of the approach, finding that the two-stage pipeline performs better on long and information-dense documents. Experiments on long-form summarisation datasets demonstrate that our method consistently improves factual consistency while preserving relevance and overall quality. On GovReport, our best approach has improved ROUGE-L by 4.1 points and achieves about 35% gains in SummaC scores. Qualitative analysis shows that highlight-guided summarisation helps preserve important details, leading to more accurate and insightful summaries across domains.","authors":["Xiaotang Du","Rohit Saxena","Laura Perez-Beltrachini","Pasquale Minervini","Ivan Titov"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.07461v2","updated":"2025-12-19T02:34:49Z","published":"2025-12-08T11:39:43Z","title":"Native Parallel Reasoner: Reasoning in Parallelism via Self-Distilled Reinforcement Learning","summary":"We introduce Native Parallel Reasoner (NPR), a teacher-free framework that enables Large Language Models (LLMs) to self-evolve genuine parallel reasoning capabilities. NPR transforms the model from sequential emulation to native parallel cognition through three key innovations: 1) a self-distilled progressive training paradigm that transitions from ``cold-start'' format discovery to strict topological constraints without external supervision; 2) a novel Parallel-Aware Policy Optimization (PAPO) algorithm that optimizes branching policies directly within the execution graph, allowing the model to learn adaptive decomposition via trial and error; and 3) a robust NPR Engine that refactors memory management and flow control of SGLang to enable stable, large-scale parallel RL training. Across eight reasoning benchmarks, NPR trained on Qwen3-4B achieves performance gains of up to 24.5% and inference speedups up to 4.6x. Unlike prior baselines that often fall back to autoregressive decoding, NPR demonstrates 100% genuine parallel execution, establishing a new standard for self-evolving, efficient, and scalable agentic reasoning.","authors":["Tong Wu","Yang Liu","Jun Bai","Zixia Jia","Shuyi Zhang","Ziyong Lin","Yanting Wang","Song-Chun Zhu","Zilong Zheng"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2511.06000v2","updated":"2025-12-19T02:07:06Z","published":"2025-11-08T13:12:36Z","title":"LLMs Do Not See Age: Assessing Demographic Bias in Automated Systematic Review Synthesis","summary":"Clinical interventions often hinge on age: medications and procedures safe for adults may be harmful to children or ineffective for older adults. However, as language models are increasingly integrated into biomedical evidence synthesis workflows, it remains uncertain whether these systems preserve such crucial demographic distinctions. To address this gap, we evaluate how well state-of-the-art language models retain age-related information when generating abstractive summaries of biomedical studies. We construct DemogSummary, a novel age-stratified dataset of systematic review primary studies, covering child, adult, and older adult populations. We evaluate three prominent summarisation-capable LLMs, Qwen (open-source), Longformer (open-source) and GPT-4.1 Nano (proprietary), using both standard metrics and a newly proposed Demographic Salience Score (DSS), which quantifies age-related entity retention and hallucination. Our results reveal systematic disparities across models and age groups: demographic fidelity is lowest for adult-focused summaries, and under-represented populations are more prone to hallucinations. These findings highlight the limitations of current LLMs in faithful and bias-free summarisation and point to the need for fairness-aware evaluation frameworks and summarisation pipelines in biomedical NLP.","authors":["Favour Yahdii Aghaebe","Tanefa Apekey","Elizabeth Williams","Nafise Sadat Moosavi"],"pdf_url":"","comment":"Accepted at AACL 2025 Version 2 Updated with Final version"},{"id":"http://arxiv.org/abs/2505.14886v2","updated":"2025-12-19T02:04:23Z","published":"2025-05-20T20:17:51Z","title":"Strategic Planning and Rationalizing on Trees Make LLMs Better Debaters","summary":"Winning competitive debates requires sophisticated reasoning and argument skills. There are unique challenges in the competitive debate: (1) The time constraints force debaters to make strategic choices about which points to pursue rather than covering all possible arguments; (2) The persuasiveness of the debate relies on the back-and-forth interaction between arguments, which a single final game status cannot evaluate. To address these challenges, we propose TreeDebater, a novel debate framework that excels in competitive debate. We introduce two tree structures: the Rehearsal Tree and Debate Flow Tree. The Rehearsal Tree anticipates the attack and defenses to evaluate the strength of the claim, while the Debate Flow Tree tracks the debate status to identify the active actions. TreeDebater allocates its time budget among candidate actions and uses the speech time controller and feedback from the simulated audience to revise its statement. The human evaluation on both the stage-level and the debate-level comparison shows that our TreeDebater outperforms the state-of-the-art multi-agent debate system, with a +15.6% improvement in stage-level persuasiveness with DeepSeek and +10% debate-level opinion shift win. Further investigation shows that TreeDebater shows better strategies in limiting time to important debate actions, aligning with the strategies of human debate experts.","authors":["Danqing Wang","Zhuorui Ye","Xinran Zhao","Fei Fang","Lei Li"],"pdf_url":"","comment":"9 main pages"},{"id":"http://arxiv.org/abs/2509.21791v3","updated":"2025-12-19T01:55:45Z","published":"2025-09-26T02:47:11Z","title":"Quantifying the Impact of Structured Output Format on Large Language Models through Causal Inference","summary":"Structured output from large language models (LLMs) has enhanced efficiency in processing generated information and is increasingly adopted in industrial applications. Prior studies have investigated the impact of structured output on LLMs' generation quality, often presenting one-way findings. Some suggest that structured format enhances completeness and factual accuracy, while others argue that it restricts the reasoning capacity of LLMs and leads to reductions in standard evaluation metrics. Potential limitations of these assessments include restricted testing scenarios, weakly controlled comparative settings, and reliance on coarse metrics. In this work, we present a refined analysis using causal inference. Based on one assumed and two guaranteed constraints, we derive five potential causal structures characterizing the influence of structured output on LLMs' generation: (1) collider without m-bias, (2) collider with m-bias, (3) single cause from instruction, (4) single cause from output format, and (5) independence. Across seven public and one developed reasoning tasks, we find that coarse metrics report positive, negative, or neutral effects of structured output on GPT-4o's generation. However, causal inference reveals no causal impact in 43 out of 48 scenarios. In the remaining 5, 3 involve multifaceted causal structures influenced by concrete instructions. Further experiments show that OpenAI-o3 are more resilient to output formats than general-purpose GPT-4o and GPT-4.1, highlighting an unaware advantage of reasoning models.","authors":["Han Yuan","Yue Zhao","Li Zhang","Wuqiong Luo","Zheng Ma"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2510.22732v2","updated":"2025-12-19T23:16:01Z","published":"2025-10-26T16:03:39Z","title":"WebATLAS: An LLM Agent with Experience-Driven Memory and Action Simulation","summary":"Large Language Model (LLM) web agents often struggle with long-horizon web navigation and web task completion in new websites, producing inefficient action sequences unless fine-tuned on environment-specific data. We show that experience-driven memory, combined with look-ahead action simulation, is sufficient for LLM agents to adapt to unseen web environments by remembering past failures and predicting the consequences of future actions. We introduce WebATLAS (Actor-Critic Task-completion with Look-ahead Action Simulation), a memory-augmented LLM web agent that learns a lightweight internal model of the environment from interaction experience and performs hypothetical action rollouts before acting in the real world. WebATLAS builds a persistent cognitive map via curiosity-driven exploration, stores interaction outcomes as experience-based memory, and evaluates candidate actions in cognitive space using a planner--simulator--critic loop. This enables the agent to reuse past experience, avoid previously unsuccessful behaviors, and generate more efficient plans. We evaluate WebATLAS on the WebArena-Lite benchmark for autonomous web navigation and demonstrate a success rate of 63%, outperforming the previous state-of-the-art at 53.9%. Unlike previous systems, our modular architecture requires no website-specific LLM fine-tuning. Ablation studies confirm that experience-driven memory, look-ahead action simulation, and hierarchical replanning play complementary roles in enabling robust, training-free web agents.","authors":["Jiali Cheng","Anjishnu Kumar","Roshan Lal","Rishi Rajasekaran","Hani Ramezani","Omar Zia Khan","Oleg Rokhlenko","Sunny Chiu-Webster","Gang Hua","Hadi Amiri"],"pdf_url":"","comment":"9 pages, NeurIPS 2025 Workshop on Language Agents and World Models"},{"id":"http://arxiv.org/abs/2512.18119v1","updated":"2025-12-19T22:56:57Z","published":"2025-12-19T22:56:57Z","title":"Distributed Asymmetric Allocation: A Topic Model for Large Imbalanced Corpora in Social Sciences","summary":"Social scientists employ latent Dirichlet allocation (LDA) to find highly specific topics in large corpora, but they often struggle in this task because (1) LDA, in general, takes a significant amount of time to fit on large corpora; (2) unsupervised LDA fragments topics into sub-topics in short documents; (3) semi-supervised LDA fails to identify specific topics defined using seed words. To solve these problems, I have developed a new topic model called distributed asymmetric allocation (DAA) that integrates multiple algorithms for efficiently identifying sentences about important topics in large corpora. I evaluate the ability of DAA to identify politically important topics by fitting it to the transcripts of speeches at the United Nations General Assembly between 1991 and 2017. The results show that DAA can classify sentences significantly more accurately and quickly than LDA thanks to the new algorithms. More generally, the results demonstrate that it is important for social scientists to optimize Dirichlet priors of LDA to perform content analysis accurately.","authors":["Kohei Watanabe"],"pdf_url":"","comment":"34 pages"},{"id":"http://arxiv.org/abs/2510.13901v2","updated":"2025-12-19T22:55:25Z","published":"2025-10-14T19:33:09Z","title":"RAID: Refusal-Aware and Integrated Decoding for Jailbreaking LLMs","summary":"Large language models (LLMs) achieve impressive performance across diverse tasks yet remain vulnerable to jailbreak attacks that bypass safety mechanisms. We present RAID (Refusal-Aware and Integrated Decoding), a framework that systematically probes these weaknesses by crafting adversarial suffixes that induce restricted content while preserving fluency. RAID relaxes discrete tokens into continuous embeddings and optimizes them with a joint objective that (i) encourages restricted responses, (ii) incorporates a refusal-aware regularizer to steer activations away from refusal directions in embedding space, and (iii) applies a coherence term to maintain semantic plausibility and non-redundancy. After optimization, a critic-guided decoding procedure maps embeddings back to tokens by balancing embedding affinity with language-model likelihood. This integration yields suffixes that are both effective in bypassing defenses and natural in form. Experiments on multiple open-source LLMs show that RAID achieves higher attack success rates with fewer queries and lower computational cost than recent white-box and black-box baselines. These findings highlight the importance of embedding-space regularization for understanding and mitigating LLM jailbreak vulnerabilities.","authors":["Tuan T. Nguyen","John Le","Thai T. Vu","Willy Susilo","Heath Cooper"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.18115v1","updated":"2025-12-19T22:43:12Z","published":"2025-12-19T22:43:12Z","title":"Layout-Aware Text Editing for Efficient Transformation of Academic PDFs to Markdown","summary":"Academic documents stored in PDF format can be transformed into plain text structured markup languages to enhance accessibility and enable scalable digital library workflows. Markup languages allow for easier updates and customization, making academic content more adaptable and accessible to diverse usage, such as linguistic corpus compilation. Such documents, typically delivered in PDF format, contain complex elements including mathematical formulas, figures, headers, and tables, as well as densely layouted text. Existing end-to-end decoder transformer models can transform screenshots of documents into markup language. However, these models exhibit significant inefficiencies; their token-by-token decoding from scratch wastes a lot of inference steps in regenerating dense text that could be directly copied from PDF files. To solve this problem, we introduce EditTrans, a hybrid editing-generation model whose features allow identifying a queue of to-be-edited text from a PDF before starting to generate markup language. EditTrans contains a lightweight classifier fine-tuned from a Document Layout Analysis model on 162,127 pages of documents from arXiv. In our evaluations, EditTrans reduced the transformation latency up to 44.5% compared to end-to-end decoder transformer models, while maintaining transformation quality. Our code and reproducible dataset production scripts are open-sourced.","authors":["Changxu Duan"],"pdf_url":"","comment":"Accepted ICDAR 2025"},{"id":"http://arxiv.org/abs/2505.14972v2","updated":"2025-12-19T22:14:03Z","published":"2025-05-20T23:20:38Z","title":"Multimodal Cultural Safety: Evaluation Framework and Alignment Strategies","summary":"Large vision-language models (LVLMs) are increasingly deployed in globally distributed applications, such as tourism assistants, yet their ability to produce culturally appropriate responses remains underexplored. Existing multimodal safety benchmarks primarily focus on physical safety and overlook violations rooted in cultural norms, which can result in symbolic harm. To address this gap, we introduce CROSS, a benchmark designed to assess the cultural safety reasoning capabilities of LVLMs. CROSS includes 1,284 multilingual visually grounded queries from 16 countries, three everyday domains, and 14 languages, where cultural norm violations emerge only when images are interpreted in context. We propose CROSS-Eval, an intercultural theory-based framework that measures four key dimensions: cultural awareness, norm education, compliance, and helpfulness. Using this framework, we evaluate 21 leading LVLMs, including mixture-of-experts models and reasoning models. Results reveal significant cultural safety gaps: the best-performing model achieves only 61.79% in awareness and 37.73% in compliance. While some open-source models reach GPT-4o-level performance, they still fall notably short of proprietary models. Our results further show that increasing reasoning capacity improves cultural alignment but does not fully resolve the issue. To improve model performance, we develop two enhancement strategies: supervised fine-tuning with culturally grounded, open-ended data and preference tuning with contrastive response pairs that highlight safe versus unsafe behaviors. These methods substantially improve GPT-4o's cultural awareness (+60.14%) and compliance (+55.2%), while preserving general multimodal capabilities with minimal performance reduction on general multimodal understanding benchmarks.","authors":["Haoyi Qiu","Kung-Hsiang Huang","Ruichen Zheng","Jiao Sun","Nanyun Peng"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2505.14918v2","updated":"2025-12-19T21:50:40Z","published":"2025-05-20T21:12:58Z","title":"Reliable Decision Support with LLMs: A Framework for Evaluating Consistency in Binary Text Classification Applications","summary":"This study introduces a framework for evaluating consistency in large language model (LLM) binary text classification, addressing the lack of established reliability assessment methods. Adapting psychometric principles, we determine sample size requirements, develop metrics for invalid responses, and evaluate intra- and inter-rater reliability. Our case study examines financial news sentiment classification across 14 LLMs (including claude-3-7-sonnet, gpt-4o, deepseek-r1, gemma3, llama3.2, phi4, and command-r-plus), with five replicates per model on 1,350 articles. Models demonstrated high intra-rater consistency, achieving perfect agreement on 90-98% of examples, with minimal differences between expensive and economical models from the same families. When validated against StockNewsAPI labels, models achieved strong performance (accuracy 0.76-0.88), with smaller models like gemma3:1B, llama3.2:3B, and claude-3-5-haiku outperforming larger counterparts. All models performed at chance when predicting actual market movements, indicating task constraints rather than model limitations. Our framework provides systematic guidance for LLM selection, sample size planning, and reliability assessment, enabling organizations to optimize resources for classification tasks.","authors":["Fadel M. Megahed","Ying-Ju Chen","L. Allision Jones-Farmer","Younghwa Lee","Jiawei Brooke Wang","Inez M. Zwetsloot"],"pdf_url":"","comment":"26 pages"},{"id":"http://arxiv.org/abs/2512.18072v1","updated":"2025-12-19T21:21:23Z","published":"2025-12-19T21:21:23Z","title":"Statistical laws and linguistics inform meaning in naturalistic and fictional conversation","summary":"Conversation is a cornerstone of social connection and is linked to well-being outcomes. Conversations vary widely in type with some portion generating complex, dynamic stories. One approach to studying how conversations unfold in time is through statistical patterns such as Heaps' law, which holds that vocabulary size scales with document length. Little work on Heaps's law has looked at conversation and considered how language features impact scaling. We measure Heaps' law for conversations recorded in two distinct mediums: 1. Strangers brought together on video chat and 2. Fictional characters in movies. We find that scaling of vocabulary size differs by parts of speech. We discuss these findings through behavioral and linguistic frameworks.","authors":["Ashley M. A. Fehr","Calla G. Beauregard","Julia Witte Zimmerman","Katie Ekström","Pablo Rosillo-Rodes","Christopher M. Danforth","Peter Sheridan Dodds"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2511.07044v2","updated":"2025-12-19T20:21:51Z","published":"2025-11-10T12:38:26Z","title":"Evaluating Large Language Models for Anxiety, Depression, and Stress Detection: Insights into Prompting Strategies and Synthetic Data","summary":"Mental health disorders affect over one-fifth of adults globally, yet detecting such conditions from text remains challenging due to the subtle and varied nature of symptom expression. This study evaluates multiple approaches for mental health detection, comparing Large Language Models (LLMs) such as Llama and GPT with classical machine learning and transformer-based architectures including BERT, XLNet, and Distil-RoBERTa. Using the DAIC-WOZ dataset of clinical interviews, we fine-tuned models for anxiety, depression, and stress classification and applied synthetic data generation to mitigate class imbalance. Results show that Distil-RoBERTa achieved the highest F1 score (0.883) for GAD-2, while XLNet outperformed others on PHQ tasks (F1 up to 0.891). For stress detection, a zero-shot synthetic approach (SD+Zero-Shot-Basic) reached an F1 of 0.884 and ROC AUC of 0.886. Findings demonstrate the effectiveness of transformer-based models and highlight the value of synthetic data in improving recall and generalization. However, careful calibration is required to prevent precision loss. Overall, this work emphasizes the potential of combining advanced language models and data augmentation to enhance automated mental health assessment from text.","authors":["Mihael Arcan","David-Paul Niland"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.18041v1","updated":"2025-12-19T20:14:44Z","published":"2025-12-19T20:14:44Z","title":"Narrative Consolidation: Formulating a New Task for Unifying Multi-Perspective Accounts","summary":"Processing overlapping narrative documents, such as legal testimonies or historical accounts, often aims not for compression but for a unified, coherent, and chronologically sound text. Standard Multi-Document Summarization (MDS), with its focus on conciseness, fails to preserve narrative flow. This paper formally defines this challenge as a new NLP task: Narrative Consolidation, where the central objectives are chronological integrity, completeness, and the fusion of complementary details. To demonstrate the critical role of temporal structure in this task, we introduce Temporal Alignment Event Graph (TAEG), a graph structure that explicitly models chronology and event alignment. By applying a standard centrality algorithm to TAEG, our method functions as a version selection mechanism, choosing the most central representation of each event in its correct temporal position. In a study on the four Biblical Gospels, this structure-focused approach guarantees perfect temporal ordering (Kendall's Tau of 1.000) by design and dramatically improves content metrics (e.g., +357.2% in ROUGE-L F1). The success of this baseline method validates the formulation of Narrative Consolidation as a relevant task and establishes that an explicit temporal backbone is a fundamental component for its resolution.","authors":["Roger A. Finger","Eduardo G. Cortes","Sandro J. Rigo","Gabriel de O. Ramos"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.18027v1","updated":"2025-12-19T19:47:33Z","published":"2025-12-19T19:47:33Z","title":"CoPE: A Small Language Model for Steerable and Scalable Content Labeling","summary":"This paper details the methodology behind CoPE, a policy-steerable small language model capable of fast and accurate content labeling. We present a novel training curricula called Contradictory Example Training that enables the model to learn policy interpretation rather than mere policy memorization. We also present a novel method for generating content policies, called Binocular Labeling, which enables rapid construction of unambiguous training datasets. When evaluated across seven different harm areas, CoPE exhibits equal or superior accuracy to frontier models at only 1% of their size. We openly release a 9 billion parameter version of the model that can be run on a single consumer-grade GPU. Models like CoPE represent a paradigm shift for classifier systems. By turning an ML task into a policy writing task, CoPE opens up new design possibilities for the governance of online platforms.","authors":["Samidh Chakrabarti","David Willner","Kevin Klyman","Tiffany Saade","Emily Capstick","Sabina Nong"],"pdf_url":"","comment":"21 pages, 2 figures, 7 tables"},{"id":"http://arxiv.org/abs/2512.18014v1","updated":"2025-12-19T19:13:41Z","published":"2025-12-19T19:13:41Z","title":"ReGal: A First Look at PPO-based Legal AI for Judgment Prediction and Summarization in India","summary":"This paper presents an early exploration of reinforcement learning methodologies for legal AI in the Indian context. We introduce Reinforcement Learning-based Legal Reasoning (ReGal), a framework that integrates Multi-Task Instruction Tuning with Reinforcement Learning from AI Feedback (RLAIF) using Proximal Policy Optimization (PPO). Our approach is evaluated across two critical legal tasks: (i) Court Judgment Prediction and Explanation (CJPE), and (ii) Legal Document Summarization. Although the framework underperforms on standard evaluation metrics compared to supervised and proprietary models, it provides valuable insights into the challenges of applying RL to legal texts. These challenges include reward model alignment, legal language complexity, and domain-specific adaptation. Through empirical and qualitative analysis, we demonstrate how RL can be repurposed for high-stakes, long-document tasks in law. Our findings establish a foundation for future work on optimizing legal reasoning pipelines using reinforcement learning, with broader implications for building interpretable and adaptive legal AI systems.","authors":["Shubham Kumar Nigam","Tanuj Tyagi","Siddharth Shukla","Aditya Kumar Guru","Balaramamahanthi Deepak Patnaik","Danush Khanna","Noel Shallum","Kripabandhu Ghosh","Arnab Bhattacharya"],"pdf_url":"","comment":"Accepted in AILaw @ AAAI 2026 conference"},{"id":"http://arxiv.org/abs/2512.18004v1","updated":"2025-12-19T19:06:14Z","published":"2025-12-19T19:06:14Z","title":"Seeing Justice Clearly: Handwritten Legal Document Translation with OCR and Vision-Language Models","summary":"Handwritten text recognition (HTR) and machine translation continue to pose significant challenges, particularly for low-resource languages like Marathi, which lack large digitized corpora and exhibit high variability in handwriting styles. The conventional approach to address this involves a two-stage pipeline: an OCR system extracts text from handwritten images, which is then translated into the target language using a machine translation model. In this work, we explore and compare the performance of traditional OCR-MT pipelines with Vision Large Language Models that aim to unify these stages and directly translate handwritten text images in a single, end-to-end step. Our motivation is grounded in the urgent need for scalable, accurate translation systems to digitize legal records such as FIRs, charge sheets, and witness statements in India's district and high courts. We evaluate both approaches on a curated dataset of handwritten Marathi legal documents, with the goal of enabling efficient legal document processing, even in low-resource environments. Our findings offer actionable insights toward building robust, edge-deployable solutions that enhance access to legal information for non-native speakers and legal professionals alike.","authors":["Shubham Kumar Nigam","Parjanya Aditya Shukla","Noel Shallum","Arnab Bhattacharya"],"pdf_url":"","comment":"Accepted in AILaw @ AAAI 2026 Conference"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2512.17909v1","updated":"2025-12-19T18:59:57Z","published":"2025-12-19T18:59:57Z","title":"Both Semantics and Reconstruction Matter: Making Representation Encoders Ready for Text-to-Image Generation and Editing","summary":"Modern Latent Diffusion Models (LDMs) typically operate in low-level Variational Autoencoder (VAE) latent spaces that are primarily optimized for pixel-level reconstruction. To unify vision generation and understanding, a burgeoning trend is to adopt high-dimensional features from representation encoders as generative latents. However, we empirically identify two fundamental obstacles in this paradigm: (1) the discriminative feature space lacks compact regularization, making diffusion models prone to off-manifold latents that lead to inaccurate object structures; and (2) the encoder's inherently weak pixel-level reconstruction hinders the generator from learning accurate fine-grained geometry and texture. In this paper, we propose a systematic framework to adapt understanding-oriented encoder features for generative tasks. We introduce a semantic-pixel reconstruction objective to regularize the latent space, enabling the compression of both semantic information and fine-grained details into a highly compact representation (96 channels with 16x16 spatial downsampling). This design ensures that the latent space remains semantically rich and achieves state-of-the-art image reconstruction, while remaining compact enough for accurate generation. Leveraging this representation, we design a unified Text-to-Image (T2I) and image editing model. Benchmarking against various feature spaces, we demonstrate that our approach achieves state-of-the-art reconstruction, faster convergence, and substantial performance gains in both T2I and editing tasks, validating that representation encoders can be effectively adapted into robust generative components.","authors":["Shilong Zhang","He Zhang","Zhifei Zhang","Chongjian Ge","Shuchen Xue","Shaoteng Liu","Mengwei Ren","Soo Ye Kim","Yuqian Zhou","Qing Liu","Daniil Pakhomov","Kai Zhang","Zhe Lin","Ping Luo"],"pdf_url":"","comment":"Project Page: https://jshilong.github.io/PS-VAE-PAGE/"},{"id":"http://arxiv.org/abs/2512.17908v1","updated":"2025-12-19T18:59:56Z","published":"2025-12-19T18:59:56Z","title":"Re-Depth Anything: Test-Time Depth Refinement via Self-Supervised Re-lighting","summary":"Monocular depth estimation remains challenging as recent foundation models, such as Depth Anything V2 (DA-V2), struggle with real-world images that are far from the training distribution. We introduce Re-Depth Anything, a test-time self-supervision framework that bridges this domain gap by fusing DA-V2 with the powerful priors of large-scale 2D diffusion models. Our method performs label-free refinement directly on the input image by re-lighting predicted depth maps and augmenting the input. This re-synthesis method replaces classical photometric reconstruction by leveraging shape from shading (SfS) cues in a new, generative context with Score Distillation Sampling (SDS). To prevent optimization collapse, our framework employs a targeted optimization strategy: rather than optimizing depth directly or fine-tuning the full model, we freeze the encoder and only update intermediate embeddings while also fine-tuning the decoder. Across diverse benchmarks, Re-Depth Anything yields substantial gains in depth accuracy and realism over the DA-V2, showcasing new avenues for self-supervision by augmenting geometric reasoning.","authors":["Ananta R. Bhattarai","Helge Rhodin"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17907v1","updated":"2025-12-19T18:59:51Z","published":"2025-12-19T18:59:51Z","title":"Dexterous World Models","summary":"Recent progress in 3D reconstruction has made it easy to create realistic digital twins from everyday environments. However, current digital twins remain largely static and are limited to navigation and view synthesis without embodied interactivity. To bridge this gap, we introduce Dexterous World Model (DWM), a scene-action-conditioned video diffusion framework that models how dexterous human actions induce dynamic changes in static 3D scenes.\n  Given a static 3D scene rendering and an egocentric hand motion sequence, DWM generates temporally coherent videos depicting plausible human-scene interactions. Our approach conditions video generation on (1) static scene renderings following a specified camera trajectory to ensure spatial consistency, and (2) egocentric hand mesh renderings that encode both geometry and motion cues to model action-conditioned dynamics directly. To train DWM, we construct a hybrid interaction video dataset. Synthetic egocentric interactions provide fully aligned supervision for joint locomotion and manipulation learning, while fixed-camera real-world videos contribute diverse and realistic object dynamics.\n  Experiments demonstrate that DWM enables realistic and physically plausible interactions, such as grasping, opening, and moving objects, while maintaining camera and scene consistency. This framework represents a first step toward video diffusion-based interactive digital twins and enables embodied simulation from egocentric actions.","authors":["Byungjun Kim","Taeksoo Kim","Junyoung Lee","Hanbyul Joo"],"pdf_url":"","comment":"Project Page: snuvclab.github.io/dwm"},{"id":"http://arxiv.org/abs/2512.17902v1","updated":"2025-12-19T18:59:16Z","published":"2025-12-19T18:59:16Z","title":"Adversarial Robustness of Vision in Open Foundation Models","summary":"With the increase in deep learning, it becomes increasingly difficult to understand the model in which AI systems can identify objects. Thus, an adversary could aim to modify an image by adding unseen elements, which will confuse the AI in its recognition of an entity. This paper thus investigates the adversarial robustness of LLaVA-1.5-13B and Meta's Llama 3.2 Vision-8B-2. These are tested for untargeted PGD (Projected Gradient Descent) against the visual input modality, and empirically evaluated on the Visual Question Answering (VQA) v2 dataset subset. The results of these adversarial attacks are then quantified using the standard VQA accuracy metric. This evaluation is then compared with the accuracy degradation (accuracy drop) of LLaVA and Llama 3.2 Vision. A key finding is that Llama 3.2 Vision, despite a lower baseline accuracy in this setup, exhibited a smaller drop in performance under attack compared to LLaVA, particularly at higher perturbation levels. Overall, the findings confirm that the vision modality represents a viable attack vector for degrading the performance of contemporary open-weight VLMs, including Meta's Llama 3.2 Vision. Furthermore, they highlight that adversarial robustness does not necessarily correlate directly with standard benchmark performance and may be influenced by underlying architectural and training factors.","authors":["Jonathon Fox","William J Buchanan","Pavlos Papadopoulos"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17900v1","updated":"2025-12-19T18:59:02Z","published":"2025-12-19T18:59:02Z","title":"Diffusion Forcing for Multi-Agent Interaction Sequence Modeling","summary":"Understanding and generating multi-person interactions is a fundamental challenge with broad implications for robotics and social computing. While humans naturally coordinate in groups, modeling such interactions remains difficult due to long temporal horizons, strong inter-agent dependencies, and variable group sizes. Existing motion generation methods are largely task-specific and do not generalize to flexible multi-agent generation. We introduce MAGNet (Multi-Agent Diffusion Forcing Transformer), a unified autoregressive diffusion framework for multi-agent motion generation that supports a wide range of interaction tasks through flexible conditioning and sampling. MAGNet performs dyadic prediction, partner inpainting, and full multi-agent motion generation within a single model, and can autoregressively generate ultra-long sequences spanning hundreds of v. Building on Diffusion Forcing, we introduce key modifications that explicitly model inter-agent coupling during autoregressive denoising, enabling coherent coordination across agents. As a result, MAGNet captures both tightly synchronized activities (e.g, dancing, boxing) and loosely structured social interactions. Our approach performs on par with specialized methods on dyadic benchmarks while naturally extending to polyadic scenarios involving three or more interacting people, enabled by a scalable architecture that is agnostic to the number of agents. We refer readers to the supplemental video, where the temporal dynamics and spatial coordination of generated interactions are best appreciated. Project page: https://von31.github.io/MAGNet/","authors":["Vongani H. Maluleke","Kie Horiuchi","Lea Wilken","Evonne Ng","Jitendra Malik","Angjoo Kanazawa"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17897v1","updated":"2025-12-19T18:57:33Z","published":"2025-12-19T18:57:33Z","title":"RadarGen: Automotive Radar Point Cloud Generation from Cameras","summary":"We present RadarGen, a diffusion model for synthesizing realistic automotive radar point clouds from multi-view camera imagery. RadarGen adapts efficient image-latent diffusion to the radar domain by representing radar measurements in bird's-eye-view form that encodes spatial structure together with radar cross section (RCS) and Doppler attributes. A lightweight recovery step reconstructs point clouds from the generated maps. To better align generation with the visual scene, RadarGen incorporates BEV-aligned depth, semantic, and motion cues extracted from pretrained foundation models, which guide the stochastic generation process toward physically plausible radar patterns. Conditioning on images makes the approach broadly compatible, in principle, with existing visual datasets and simulation frameworks, offering a scalable direction for multimodal generative simulation. Evaluations on large-scale driving data show that RadarGen captures characteristic radar measurement distributions and reduces the gap to perception models trained on real data, marking a step toward unified generative simulation across sensing modalities.","authors":["Tomer Borreda","Fangqiang Ding","Sanja Fidler","Shengyu Huang","Or Litany"],"pdf_url":"","comment":"Project page: https://radargen.github.io/"},{"id":"http://arxiv.org/abs/2512.17891v1","updated":"2025-12-19T18:47:04Z","published":"2025-12-19T18:47:04Z","title":"Keypoint Counting Classifiers: Turning Vision Transformers into Self-Explainable Models Without Training","summary":"Current approaches for designing self-explainable models (SEMs) require complicated training procedures and specific architectures which makes them impractical. With the advance of general purpose foundation models based on Vision Transformers (ViTs), this impracticability becomes even more problematic. Therefore, new methods are necessary to provide transparency and reliability to ViT-based foundation models. In this work, we present a new method for turning any well-trained ViT-based model into a SEM without retraining, which we call Keypoint Counting Classifiers (KCCs). Recent works have shown that ViTs can automatically identify matching keypoints between images with high precision, and we build on these results to create an easily interpretable decision process that is inherently visualizable in the input. We perform an extensive evaluation which show that KCCs improve the human-machine communication compared to recent baselines. We believe that KCCs constitute an important step towards making ViT-based foundation models more transparent and reliable.","authors":["Kristoffer Wickstrøm","Teresa Dorszewski","Siyan Chen","Michael Kampffmeyer","Elisabeth Wetzer","Robert Jenssen"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.15692v2","updated":"2025-12-19T18:30:30Z","published":"2025-12-17T18:47:31Z","title":"mimic-video: Video-Action Models for Generalizable Robot Control Beyond VLAs","summary":"Prevailing Vision-Language-Action Models (VLAs) for robotic manipulation are built upon vision-language backbones pretrained on large-scale, but disconnected static web data. As a result, despite improved semantic generalization, the policy must implicitly infer complex physical dynamics and temporal dependencies solely from robot trajectories. This reliance creates an unsustainable data burden, necessitating continuous, large-scale expert data collection to compensate for the lack of innate physical understanding. We contend that while vision-language pretraining effectively captures semantic priors, it remains blind to physical causality. A more effective paradigm leverages video to jointly capture semantics and visual dynamics during pretraining, thereby isolating the remaining task of low-level control. To this end, we introduce mimic-video, a novel Video-Action Model (VAM) that pairs a pretrained Internet-scale video model with a flow matching-based action decoder conditioned on its latent representations. The decoder serves as an Inverse Dynamics Model (IDM), generating low-level robot actions from the latent representation of video-space action plans. Our extensive evaluation shows that our approach achieves state-of-the-art performance on simulated and real-world robotic manipulation tasks, improving sample efficiency by 10x and convergence speed by 2x compared to traditional VLA architectures.","authors":["Jonas Pai","Liam Achenbach","Victoriano Montesinos","Benedek Forrai","Oier Mees","Elvis Nava"],"pdf_url":"","comment":"Revised Introduction, Related Work, and Appendix. Additional minor notational and grammatical fixes"},{"id":"http://arxiv.org/abs/2512.17875v1","updated":"2025-12-19T18:26:58Z","published":"2025-12-19T18:26:58Z","title":"Visually Prompted Benchmarks Are Surprisingly Fragile","summary":"A key challenge in evaluating VLMs is testing models' ability to analyze visual content independently from their textual priors. Recent benchmarks such as BLINK probe visual perception through visual prompting, where questions about visual content are paired with coordinates to which the question refers, with the coordinates explicitly marked in the image itself. While these benchmarks are an important part of VLM evaluation, we find that existing models are surprisingly fragile to seemingly irrelevant details of visual prompting: simply changing a visual marker from red to blue can completely change rankings among models on a leaderboard. By evaluating nine commonly-used open- and closed-source VLMs on two visually prompted tasks, we demonstrate how details in benchmark setup, including visual marker design and dataset size, have a significant influence on model performance and leaderboard rankings. These effects can even be exploited to lift weaker models above stronger ones; for instance, slightly increasing the size of the visual marker results in open-source InternVL3-8B ranking alongside or better than much larger proprietary models like Gemini 2.5 Pro. We further show that low-level inference choices that are often ignored in benchmarking, such as JPEG compression levels in API calls, can also cause model lineup changes. These details have substantially larger impacts on visually prompted benchmarks than on conventional semantic VLM evaluations. To mitigate this instability, we curate existing datasets to create VPBench, a larger visually prompted benchmark with 16 visual marker variants. VPBench and additional analysis tools are released at https://lisadunlap.github.io/vpbench/.","authors":["Haiwen Feng","Long Lian","Lisa Dunlap","Jiahao Shu","XuDong Wang","Renhao Wang","Trevor Darrell","Alane Suhr","Angjoo Kanazawa"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17873v1","updated":"2025-12-19T18:24:02Z","published":"2025-12-19T18:24:02Z","title":"InSPECT: Invariant Spectral Features Preservation of Diffusion Models","summary":"Modern diffusion models (DMs) have achieved state-of-the-art image generation. However, the fundamental design choice of diffusing data all the way to white noise and then reconstructing it leads to an extremely difficult and computationally intractable prediction task. To overcome this limitation, we propose InSPECT (Invariant Spectral Feature-Preserving Diffusion Model), a novel diffusion model that keeps invariant spectral features during both the forward and backward processes. At the end of the forward process, the Fourier coefficients smoothly converge to a specified random noise, enabling features preservation while maintaining diversity and randomness. By preserving invariant features, InSPECT demonstrates enhanced visual diversity, faster convergence rate, and a smoother diffusion process. Experiments on CIFAR-10, Celeb-A, and LSUN demonstrate that InSPECT achieves on average a 39.23% reduction in FID and 45.80% improvement in IS against DDPM for 10K iterations under specified parameter settings, which demonstrates the significant advantages of preserving invariant features: achieving superior generation quality and diversity, while enhancing computational efficiency and enabling faster convergence rate. To the best of our knowledge, this is the first attempt to analyze and preserve invariant spectral features in diffusion models.","authors":["Baohua Yan","Qingyuan Liu","Jennifer Kava","Xuan Di"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17864v1","updated":"2025-12-19T18:11:15Z","published":"2025-12-19T18:11:15Z","title":"Interpretable Plant Leaf Disease Detection Using Attention-Enhanced CNN","summary":"Plant diseases pose a significant threat to global food security, necessitating accurate and interpretable disease detection methods. This study introduces an interpretable attention-guided Convolutional Neural Network (CNN), CBAM-VGG16, for plant leaf disease detection. By integrating Convolution Block Attention Module (CBAM) at each convolutional stage, the model enhances feature extraction and disease localization. Trained on five diverse plant disease datasets, our approach outperforms recent techniques, achieving high accuracy (up to 98.87%) and demonstrating robust generalization. Here, we show the effectiveness of our method through comprehensive evaluation and interpretability analysis using CBAM attention maps, Grad-CAM, Grad-CAM++, and Layer-wise Relevance Propagation (LRP). This study advances the application of explainable AI in agricultural diagnostics, offering a transparent and reliable system for smart farming. The code of our proposed work is available at https://github.com/BS0111/PlantAttentionCBAM.","authors":["Balram Singh","Ram Prakash Sharma","Somnath Dey"],"pdf_url":"","comment":"27 pages, 12 figures"},{"id":"http://arxiv.org/abs/2512.17852v1","updated":"2025-12-19T17:54:57Z","published":"2025-12-19T17:54:57Z","title":"Simulation-Driven Deep Learning Framework for Raman Spectral Denoising Under Fluorescence-Dominant Conditions","summary":"Raman spectroscopy enables non-destructive, label-free molecular analysis with high specificity, making it a powerful tool for biomedical diagnostics. However, its application to biological tissues is challenged by inherently weak Raman scattering and strong fluorescence background, which significantly degrade signal quality. In this study, we present a simulation-driven denoising framework that combines a statistically grounded noise model with deep learning to enhance Raman spectra acquired under fluorescence-dominated conditions. We comprehensively modeled major noise sources. Based on this model, we generated biologically realistic Raman spectra and used them to train a cascaded deep neural network designed to jointly suppress stochastic detector noise and fluorescence baseline interference. To evaluate the performance of our approach, we simulated human skin spectra derived from real experimental data as a validation case study. Our results demonstrate the potential of physics-informed learning to improve spectral quality and enable faster, more accurate Raman-based tissue analysis.","authors":["Mengkun Chen","Sanidhya D. Tripathi","James W. Tunnell"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17851v1","updated":"2025-12-19T17:52:43Z","published":"2025-12-19T17:52:43Z","title":"InfSplign: Inference-Time Spatial Alignment of Text-to-Image Diffusion Models","summary":"Text-to-image (T2I) diffusion models generate high-quality images but often fail to capture the spatial relations specified in text prompts. This limitation can be traced to two factors: lack of fine-grained spatial supervision in training data and inability of text embeddings to encode spatial semantics. We introduce InfSplign, a training-free inference-time method that improves spatial alignment by adjusting the noise through a compound loss in every denoising step. Proposed loss leverages different levels of cross-attention maps extracted from the backbone decoder to enforce accurate object placement and a balanced object presence during sampling. The method is lightweight, plug-and-play, and compatible with any diffusion backbone. Our comprehensive evaluations on VISOR and T2I-CompBench show that InfSplign establishes a new state-of-the-art (to the best of our knowledge), achieving substantial performance gains over the strongest existing inference-time baselines and even outperforming the fine-tuning-based methods. Codebase is available at GitHub.","authors":["Sarah Rastegar","Violeta Chatalbasheva","Sieger Falkena","Anuj Singh","Yanbo Wang","Tejas Gokhale","Hamid Palangi","Hadi Jamali-Rad"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17838v1","updated":"2025-12-19T17:44:40Z","published":"2025-12-19T17:44:40Z","title":"ReX-MLE: The Autonomous Agent Benchmark for Medical Imaging Challenges","summary":"Autonomous coding agents built on large language models (LLMs) can now solve many general software and machine learning tasks, but they remain ineffective on complex, domain-specific scientific problems. Medical imaging is a particularly demanding domain, requiring long training cycles, high-dimensional data handling, and specialized preprocessing and validation pipelines, capabilities not fully measured in existing agent benchmarks. To address this gap, we introduce ReX-MLE, a benchmark of 20 challenges derived from high-impact medical imaging competitions spanning diverse modalities and task types. Unlike prior ML-agent benchmarks, ReX-MLE evaluates full end-to-end workflows, requiring agents to independently manage data preprocessing, model training, and submission under realistic compute and time constraints. Evaluating state-of-the-art agents (AIDE, ML-Master, R&D-Agent) with different LLM backends (GPT-5, Gemini, Claude), we observe a severe performance gap: most submissions rank in the 0th percentile compared to human experts. Failures stem from domain-knowledge and engineering limitations. ReX-MLE exposes these bottlenecks and provides a foundation for developing domain-aware autonomous AI systems.","authors":["Roshan Kenia","Xiaoman Zhang","Pranav Rajpurkar"],"pdf_url":"","comment":"https://github.com/rajpurkarlab/ReX-MLE"},{"id":"http://arxiv.org/abs/2511.03589v2","updated":"2025-12-19T17:42:14Z","published":"2025-11-05T16:10:02Z","title":"Human Mesh Modeling for Anny Body","summary":"Parametric body models provide the structural basis for many human-centric tasks, yet existing models often rely on costly 3D scans and learned shape spaces that are proprietary and demographically narrow. We introduce Anny, a simple, fully differentiable, and scan-free human body model grounded in anthropometric knowledge from the MakeHuman community. Anny defines a continuous, interpretable shape space, where phenotype parameters (e.g. gender, age, height, weight) control blendshapes spanning a wide range of human forms--across ages (from infants to elders), body types, and proportions. Calibrated using WHO population statistics, it provides realistic and demographically grounded human shape variation within a single unified model. Thanks to its openness and semantic control, Anny serves as a versatile foundation for 3D human modeling--supporting millimeter-accurate scan fitting, controlled synthetic data generation, and Human Mesh Recovery (HMR). We further introduce Anny-One, a collection of 800k photorealistic images generated with Anny, showing that despite its simplicity, HMR models trained with Anny can match the performance of those trained with scan-based body models. The Anny body model and its code are released under the Apache 2.0 license, making Anny an accessible foundation for human-centric 3D modeling.","authors":["Romain Brégier","Guénolé Fiche","Laura Bravo-Sánchez","Thomas Lucas","Matthieu Armando","Philippe Weinzaepfel","Grégory Rogez","Fabien Baradel"],"pdf_url":"","comment":"We release our model and code at https://github.com/naver/anny"},{"id":"http://arxiv.org/abs/2512.15431v2","updated":"2025-12-19T17:36:21Z","published":"2025-12-17T13:26:30Z","title":"Step-GUI Technical Report","summary":"Recent advances in multimodal large language models unlock unprecedented opportunities for GUI automation. However, a fundamental challenge remains: how to efficiently acquire high-quality training data while maintaining annotation reliability? We introduce a self-evolving training pipeline powered by the Calibrated Step Reward System, which converts model-generated trajectories into reliable training signals through trajectory-level calibration, achieving >90% annotation accuracy with 10-100x lower cost. Leveraging this pipeline, we introduce Step-GUI, a family of models (4B/8B) that achieves state-of-the-art GUI performance (8B: 80.2% AndroidWorld, 48.5% OSWorld, 62.6% ScreenShot-Pro) while maintaining robust general capabilities. As GUI agent capabilities improve, practical deployment demands standardized interfaces across heterogeneous devices while protecting user privacy. To this end, we propose GUI-MCP, the first Model Context Protocol for GUI automation with hierarchical architecture that combines low-level atomic operations and high-level task delegation to local specialist models, enabling high-privacy execution where sensitive data stays on-device. Finally, to assess whether agents can handle authentic everyday usage, we introduce AndroidDaily, a benchmark grounded in real-world mobile usage patterns with 3146 static actions and 235 end-to-end tasks across high-frequency daily scenarios (8B: static 89.91%, end-to-end 52.50%). Our work advances the development of practical GUI agents and demonstrates strong potential for real-world deployment in everyday digital interactions.","authors":["Haolong Yan","Jia Wang","Xin Huang","Yeqing Shen","Ziyang Meng","Zhimin Fan","Kaijun Tan","Jin Gao","Lieyu Shi","Mi Yang","Shiliang Yang","Zhirui Wang","Brian Li","Kang An","Chenyang Li","Lei Lei","Mengmeng Duan","Danxun Liang","Guodong Liu","Hang Cheng","Hao Wu","Jie Dong","Junhao Huang","Mei Chen","Renjie Yu","Shunshan Li","Xu Zhou","Yiting Dai","Yineng Deng","Yingdan Liang","Zelin Chen","Wen Sun","Chengxu Yan","Chunqin Xu","Dong Li","Fengqiong Xiao","Guanghao Fan","Guopeng Li","Guozhen Peng","Hongbing Li","Hang Li","Hongming Chen","Jingjing Xie","Jianyong Li","Jingyang Zhang","Jiaju Ren","Jiayu Yuan","Jianpeng Yin","Kai Cao","Liang Zhao","Liguo Tan","Liying Shi","Mengqiang Ren","Min Xu","Manjiao Liu","Mao Luo","Mingxin Wan","Na Wang","Nan Wu","Ning Wang","Peiyao Ma","Qingzhou Zhang","Qiao Wang","Qinlin Zeng","Qiong Gao","Qiongyao Li","Shangwu Zhong","Shuli Gao","Shaofan Liu","Shisi Gao","Shuang Luo","Xingbin Liu","Xiaojia Liu","Xiaojie Hou","Xin Liu","Xuanti Feng","Xuedan Cai","Xuan Wen","Xianwei Zhu","Xin Liang","Xin Liu","Xin Zhou","Yifan Sui","Yingxiu Zhao","Yukang Shi","Yunfang Xu","Yuqing Zeng","Yixun Zhang","Zejia Weng","Zhonghao Yan","Zhiguo Huang","Zhuoyu Wang","Zihan Yan","Zheng Ge","Jing Li","Yibo Zhu","Binxing Jiao","Xiangyu Zhang","Daxin Jiang"],"pdf_url":"","comment":"41 pages, 26 figures"},{"id":"http://arxiv.org/abs/2512.17817v1","updated":"2025-12-19T17:22:35Z","published":"2025-12-19T17:22:35Z","title":"Chorus: Multi-Teacher Pretraining for Holistic 3D Gaussian Scene Encoding","summary":"While 3DGS has emerged as a high-fidelity scene representation, encoding rich, general-purpose features directly from its primitives remains under-explored. We address this gap by introducing Chorus, a multi-teacher pretraining framework that learns a holistic feed-forward 3D Gaussian Splatting (3DGS) scene encoder by distilling complementary signals from 2D foundation models. Chorus employs a shared 3D encoder and teacher-specific projectors to learn from language-aligned, generalist, and object-aware teachers, encouraging a shared embedding space that captures signals from high-level semantics to fine-grained structure.\n  We evaluate Chorus on a wide range of tasks: open-vocabulary semantic and instance segmentation, linear and decoder probing, as well as data-efficient supervision. Besides 3DGS, we also test Chorus on several benchmarks that only support point clouds by pretraining a variant using only Gaussians' centers, colors, estimated normals as inputs. Interestingly, this encoder shows strong transfer and outperforms the point clouds baseline while using 39.9 times fewer training scenes. Finally, we propose a render-and-distill adaptation that facilitates out-of-domain finetuning. Our code and model will be released upon publication.","authors":["Yue Li","Qi Ma","Runyi Yang","Mengjiao Ma","Bin Ren","Nikola Popovic","Nicu Sebe","Theo Gevers","Luc Van Gool","Danda Pani Paudel","Martin R. Oswald"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17784v1","updated":"2025-12-19T16:54:43Z","published":"2025-12-19T16:54:43Z","title":"Long-Range depth estimation using learning based Hybrid Distortion Model for CCTV cameras","summary":"Accurate camera models are essential for photogrammetry applications such as 3D mapping and object localization, particularly for long distances. Various stereo-camera based 3D localization methods are available but are limited to few hundreds of meters' range. This is majorly due to the limitation of the distortion models assumed for the non-linearities present in the camera lens. This paper presents a framework for modeling a suitable distortion model that can be used for localizing the objects at longer distances. It is well known that neural networks can be a better alternative to model a highly complex non-linear lens distortion function; on contrary, it is observed that a direct application of neural networks to distortion models fails to converge to estimate the camera parameters. To resolve this, a hybrid approach is presented in this paper where the conventional distortion models are initially extended to incorporate higher-order terms and then enhanced using neural network based residual correction model. This hybrid approach has substantially improved long-range localization performance and is capable of estimating the 3D position of objects at distances up to 5 kilometres. The estimated 3D coordinates are transformed to GIS coordinates and are plotted on a GIS map for visualization. Experimental validation demonstrates the robustness and effectiveness of proposed framework, offering a practical solution to calibrate CCTV cameras for long-range photogrammetry applications.","authors":["Ami Pandat","Punna Rajasekhar","G. Aravamuthan","Gopika Vinod","Rohit Shukla"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17782v1","updated":"2025-12-19T16:51:29Z","published":"2025-12-19T16:51:29Z","title":"UrbanDIFF: A Denoising Diffusion Model for Spatial Gap Filling of Urban Land Surface Temperature Under Dense Cloud Cover","summary":"Satellite-derived Land Surface Temperature (LST) products are central to surface urban heat island (SUHI) monitoring due to their consistent grid-based coverage over large metropolitan regions. However, cloud contamination frequently obscures LST observations, limiting their usability for continuous SUHI analysis. Most existing LST reconstruction methods rely on multitemporal information or multisensor data fusion, requiring auxiliary observations that may be unavailable or unreliable under persistent cloud cover. Purely spatial gap-filling approaches offer an alternative, but traditional statistical methods degrade under large or spatially contiguous gaps, while many deep learning based spatial models deteriorate rapidly with increasing missingness.\n  Recent advances in denoising diffusion based image inpainting models have demonstrated improved robustness under high missingness, motivating their adoption for spatial LST reconstruction. In this work, we introduce UrbanDIFF, a purely spatial denoising diffusion model for reconstructing cloud contaminated urban LST imagery. The model is conditioned on static urban structure information, including built-up surface data and a digital elevation model, and enforces strict consistency with revealed cloud free pixels through a supervised pixel guided refinement step during inference.\n  UrbanDIFF is trained and evaluated using NASA MODIS Terra LST data from seven major United States metropolitan areas spanning 2002 to 2025. Experiments using synthetic cloud masks with 20 to 85 percent coverage show that UrbanDIFF consistently outperforms an interpolation baseline, particularly under dense cloud occlusion, achieving SSIM of 0.89, RMSE of 1.2 K, and R2 of 0.84 at 85 percent cloud coverage, while exhibiting slower performance degradation as cloud density increases.","authors":["Arya Chavoshi","Hassan Dashtian","Naveen Sudharsan","Dev Niyogi"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17781v1","updated":"2025-12-19T16:50:52Z","published":"2025-12-19T16:50:52Z","title":"LiteGE: Lightweight Geodesic Embedding for Efficient Geodesics Computation and Non-Isometric Shape Correspondence","summary":"Computing geodesic distances on 3D surfaces is fundamental to many tasks in 3D vision and geometry processing, with deep connections to tasks such as shape correspondence. Recent learning-based methods achieve strong performance but rely on large 3D backbones, leading to high memory usage and latency, which limit their use in interactive or resource-constrained settings. We introduce LiteGE, a lightweight approach that constructs compact, category-aware shape descriptors by applying PCA to unsigned distance field (UDFs) samples at informative voxels. This descriptor is efficient to compute and removes the need for high-capacity networks. LiteGE remains robust on sparse point clouds, supporting inputs with as few as 300 points, where prior methods fail. Extensive experiments show that LiteGE reduces memory usage and inference time by up to 300$\\times$ compared to existing neural approaches. In addition, by exploiting the intrinsic relationship between geodesic distance and shape correspondence, LiteGE enables fast and accurate shape matching. Our method achieves up to 1000$\\times$ speedup over state-of-the-art mesh-based approaches while maintaining comparable accuracy on non-isometric shape pairs, including evaluations on point-cloud inputs.","authors":["Yohanes Yudhi Adikusuma","Qixing Huang","Ying He"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2508.09814v2","updated":"2025-12-19T16:47:41Z","published":"2025-08-13T13:47:34Z","title":"On the dynamic evolution of CLIP texture-shape bias and its relationship to human alignment and model robustness","summary":"Contrastive language-image models such as CLIP have demonstrated remarkable generalization capabilities. However, how their internal visual representations evolve during training and how this evolution relates to human perception remains poorly understood. Most existing analysis characterize fully trained models, leaving the dynamics of representational biases and perceptual alignment largely unexplored. In this work, we present an epoch-by-epoch analysis of CLIP models throughout training, focusing on the evolution of texture-shape bias, alignment with human perceptual judgements, and sensitivity to image noise. Using multiple perceptual benchmarks spanning low-level image quality assessment, mid-level perceptual similarity, saliency correspondence, and noisy robustness, we identify a consistent, training-stage-dependent representational transition. Early training stages exhibit strong texture bias, elevated alignment with low-level human perceptual measures, and increased sensitivity to Gaussian noise perturbations. As training progresses, this texture bias gradually diminishes in favor of more shape-based representations, coinciding with improved robustness to noise and a decline in low-level perceptual alignment. Importantly, these dynamics are consistently observed across multiple CLIP model scales, indicating that the phenomenon is not specific to a particular architecture size. Our findings provide an empirical characterization of how perceptual alignment, feature bias, and robustness co-evolve during multimodal model training. This work reveals a systematic trade-off between early low-level perceptual alignment and later robustness, offering new insights into the representational dynamics of vision-language models and their relationship to human visual processing.","authors":["Pablo Hernández-Cámara","Jose Manuel Jaén-Lorites","Alexandra Gómez-Villa","Jorge Vila-Tomás","Valero Laparra","Jesus Malo"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17774v1","updated":"2025-12-19T16:45:23Z","published":"2025-12-19T16:45:23Z","title":"MedNeXt-v2: Scaling 3D ConvNeXts for Large-Scale Supervised Representation Learning in Medical Image Segmentation","summary":"Large-scale supervised pretraining is rapidly reshaping 3D medical image segmentation. However, existing efforts focus primarily on increasing dataset size and overlook the question of whether the backbone network is an effective representation learner at scale. In this work, we address this gap by revisiting ConvNeXt-based architectures for volumetric segmentation and introducing MedNeXt-v2, a compound-scaled 3D ConvNeXt that leverages improved micro-architecture and data scaling to deliver state-of-the-art performance. First, we show that routinely used backbones in large-scale pretraining pipelines are often suboptimal. Subsequently, we use comprehensive backbone benchmarking prior to scaling and demonstrate that stronger from scratch performance reliably predicts stronger downstream performance after pretraining. Guided by these findings, we incorporate a 3D Global Response Normalization module and use depth, width, and context scaling to improve our architecture for effective representation learning. We pretrain MedNeXt-v2 on 18k CT volumes and demonstrate state-of-the-art performance when fine-tuning across six challenging CT and MR benchmarks (144 structures), showing consistent gains over seven publicly released pretrained models. Beyond improvements, our benchmarking of these models also reveals that stronger backbones yield better results on similar data, representation scaling disproportionately benefits pathological segmentation, and that modality-specific pretraining offers negligible benefit once full finetuning is applied. In conclusion, our results establish MedNeXt-v2 as a strong backbone for large-scale supervised representation learning in 3D Medical Image Segmentation. Our code and pretrained models are made available with the official nnUNet repository at: https://www.github.com/MIC-DKFZ/nnUNet","authors":["Saikat Roy","Yannick Kirchhoff","Constantin Ulrich","Maximillian Rokuss","Tassilo Wald","Fabian Isensee","Klaus Maier-Hein"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17773v1","updated":"2025-12-19T16:44:32Z","published":"2025-12-19T16:44:32Z","title":"Pix2NPHM: Learning to Regress NPHM Reconstructions From a Single Image","summary":"Neural Parametric Head Models (NPHMs) are a recent advancement over mesh-based 3d morphable models (3DMMs) to facilitate high-fidelity geometric detail. However, fitting NPHMs to visual inputs is notoriously challenging due to the expressive nature of their underlying latent space. To this end, we propose Pix2NPHM, a vision transformer (ViT) network that directly regresses NPHM parameters, given a single image as input. Compared to existing approaches, the neural parametric space allows our method to reconstruct more recognizable facial geometry and accurate facial expressions. For broad generalization, we exploit domain-specific ViTs as backbones, which are pretrained on geometric prediction tasks. We train Pix2NPHM on a mixture of 3D data, including a total of over 100K NPHM registrations that enable direct supervision in SDF space, and large-scale 2D video datasets, for which normal estimates serve as pseudo ground truth geometry. Pix2NPHM not only allows for 3D reconstructions at interactive frame rates, it is also possible to improve geometric fidelity by a subsequent inference-time optimization against estimated surface normals and canonical point maps. As a result, we achieve unprecedented face reconstruction quality that can run at scale on in-the-wild data.","authors":["Simon Giebenhain","Tobias Kirschstein","Liam Schoneveld","Davide Davoli","Zhe Chen","Matthias Nießner"],"pdf_url":"","comment":"Project website: https://simongiebenhain.github.io/Pix2NPHM/ , Video: https://www.youtube.com/watch?v=MgpEJC5p1Ts"},{"id":"http://arxiv.org/abs/2512.17759v1","updated":"2025-12-19T16:32:31Z","published":"2025-12-19T16:32:31Z","title":"Breast Cancer Neoadjuvant Chemotherapy Treatment Response Prediction Using Aligned Longitudinal MRI and Clinical Data","summary":"Aim: This study investigates treatment response prediction to neoadjuvant chemotherapy (NACT) in breast cancer patients, using longitudinal contrast-enhanced magnetic resonance images (CE-MRI) and clinical data. The goal is to develop machine learning (ML) models to predict pathologic complete response (PCR binary classification) and 5-year relapse-free survival status (RFS binary classification). Method: The proposed framework includes tumour segmentation, image registration, feature extraction, and predictive modelling. Using the image registration method, MRI image features can be extracted and compared from the original tumour site at different time points, therefore monitoring the intratumor changes during NACT process. Four feature extractors, including one radiomics and three deep learning-based (MedicalNet, Segformer3D, SAM-Med3D) were implemented and compared. In combination with three feature selection methods and four ML models, predictive models are built and compared. Results: The proposed image registration-based feature extraction consistently improves the predictive models. In the PCR and RFS classification tasks logistic regression model trained on radiomic features performed the best with an AUC of 0.88 and classification accuracy of 0.85 for PCR classification, and AUC of 0.78 and classification accuracy of 0.72 for RFS classification. Conclusions: It is evidenced that the image registration method has significantly improved performance in longitudinal feature learning in predicting PCR and RFS. The radiomics feature extractor is more effective than the pre-trained deep learning feature extractors, with higher performance and better interpretability.","authors":["Rahul Ravi","Ruizhe Li","Tarek Abdelfatah","Stephen Chan","Xin Chen"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2510.24830v2","updated":"2025-12-19T16:21:05Z","published":"2025-10-28T16:42:53Z","title":"The Generation Phases of Flow Matching: a Denoising Perspective","summary":"Flow matching has achieved remarkable success, yet the factors influencing the quality of its generation process remain poorly understood. In this work, we adopt a denoising perspective and design a framework to empirically probe the generation process. Laying down the formal connections between flow matching models and denoisers, we provide a common ground to compare their performances on generation and denoising. This enables the design of principled and controlled perturbations to influence sample generation: noise and drift. This leads to new insights on the distinct dynamical phases of the generative process, enabling us to precisely characterize at which stage of the generative process denoisers succeed or fail and why this matters.","authors":["Anne Gagneux","Ségolène Martin","Rémi Gribonval","Mathurin Massias"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2508.21052v2","updated":"2025-12-19T16:10:13Z","published":"2025-08-28T17:55:14Z","title":"FakeParts: a New Family of AI-Generated DeepFakes","summary":"We introduce FakeParts, a new class of deepfakes characterized by subtle, localized manipulations to specific spatial regions or temporal segments of otherwise authentic videos. Unlike fully synthetic content, these partial manipulations - ranging from altered facial expressions to object substitutions and background modifications - blend seamlessly with real elements, making them particularly deceptive and difficult to detect. To address the critical gap in detection, we present FakePartsBench, the first large-scale benchmark specifically designed to capture the full spectrum of partial deepfakes. Comprising over 81K (including 44K FakeParts) videos with pixel- and frame-level manipulation annotations, our dataset enables comprehensive evaluation of detection methods. Our user studies demonstrate that FakeParts reduces human detection accuracy by up to 26% compared to traditional deepfakes, with similar performance degradation observed in state-of-the-art detection models. This work identifies an urgent vulnerability in current detectors and provides the necessary resources to develop methods robust to partial manipulations.","authors":["Ziyi Liu","Firas Gabetni","Awais Hussain Sani","Xi Wang","Soobash Daiboo","Gaetan Brison","Gianni Franchi","Vicky Kalogeiton"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17730v1","updated":"2025-12-19T16:06:03Z","published":"2025-12-19T16:06:03Z","title":"AdaptPrompt: Parameter-Efficient Adaptation of VLMs for Generalizable Deepfake Detection","summary":"Recent advances in image generation have led to the widespread availability of highly realistic synthetic media, increasing the difficulty of reliable deepfake detection. A key challenge is generalization, as detectors trained on a narrow class of generators often fail when confronted with unseen models. In this work, we address the pressing need for generalizable detection by leveraging large vision-language models, specifically CLIP, to identify synthetic content across diverse generative techniques. First, we introduce Diff-Gen, a large-scale benchmark dataset comprising 100k diffusion-generated fakes that capture broad spectral artifacts unlike traditional GAN datasets. Models trained on Diff-Gen demonstrate stronger cross-domain generalization, particularly on previously unseen image generators. Second, we propose AdaptPrompt, a parameter-efficient transfer learning framework that jointly learns task-specific textual prompts and visual adapters while keeping the CLIP backbone frozen. We further show via layer ablation that pruning the final transformer block of the vision encoder enhances the retention of high-frequency generative artifacts, significantly boosting detection accuracy. Our evaluation spans 25 challenging test sets, covering synthetic content generated by GANs, diffusion models, and commercial tools, establishing a new state-of-the-art in both standard and cross-domain scenarios. We further demonstrate the framework's versatility through few-shot generalization (using as few as 320 images) and source attribution, enabling the precise identification of generator architectures in closed-set settings.","authors":["Yichen Jiang","Mohammed Talha Alam","Sohail Ahmed Khan","Duc-Tien Dang-Nguyen","Fakhri Karray"],"pdf_url":"","comment":"Under Review"},{"id":"http://arxiv.org/abs/2508.15216v3","updated":"2025-12-19T16:05:16Z","published":"2025-08-21T04:02:22Z","title":"STAGNet: A Spatio-Temporal Graph and LSTM Framework for Accident Anticipation","summary":"Accident prediction and timely preventive actions improve road safety by reducing the risk of injury to road users and minimizing property damage. Hence, they are critical components of advanced driver assistance systems (ADAS) and autonomous vehicles. While many existing systems depend on multiple sensors such as LiDAR, radar, and GPS, relying solely on dash-cam videos presents a more challenging, yet more cost-effective and easily deployable solution. In this work, we incorporate improved spatio-temporal features and aggregate them through a recurrent network to enhance state-of-the-art graph neural networks for predicting accidents from dash-cam videos. Experiments using three publicly available datasets (DAD, DoTA and DADA) show that our proposed STAGNet model achieves higher average precision and mean time-to-accident scores than previous methods, both when cross-validated on a given dataset and when trained and tested on different datasets.","authors":["Vipooshan Vipulananthan","Kumudu Mohottala","Kavindu Chinthana","Nimsara Paramulla","Charith D Chitraranjan"],"pdf_url":"","comment":"Published in IEEE Access"},{"id":"http://arxiv.org/abs/2512.17726v1","updated":"2025-12-19T16:01:14Z","published":"2025-12-19T16:01:14Z","title":"MambaMIL+: Modeling Long-Term Contextual Patterns for Gigapixel Whole Slide Image","summary":"Whole-slide images (WSIs) are an important data modality in computational pathology, yet their gigapixel resolution and lack of fine-grained annotations challenge conventional deep learning models. Multiple instance learning (MIL) offers a solution by treating each WSI as a bag of patch-level instances, but effectively modeling ultra-long sequences with rich spatial context remains difficult. Recently, Mamba has emerged as a promising alternative for long sequence learning, scaling linearly to thousands of tokens. However, despite its efficiency, it still suffers from limited spatial context modeling and memory decay, constraining its effectiveness to WSI analysis. To address these limitations, we propose MambaMIL+, a new MIL framework that explicitly integrates spatial context while maintaining long-range dependency modeling without memory forgetting. Specifically, MambaMIL+ introduces 1) overlapping scanning, which restructures the patch sequence to embed spatial continuity and instance correlations; 2) a selective stripe position encoder (S2PE) that encodes positional information while mitigating the biases of fixed scanning orders; and 3) a contextual token selection (CTS) mechanism, which leverages supervisory knowledge to dynamically enlarge the contextual memory for stable long-range modeling. Extensive experiments on 20 benchmarks across diagnostic classification, molecular prediction, and survival analysis demonstrate that MambaMIL+ consistently achieves state-of-the-art performance under three feature extractors (ResNet-50, PLIP, and CONCH), highlighting its effectiveness and robustness for large-scale computational pathology","authors":["Qian Zeng","Yihui Wang","Shu Yang","Yingxue Xu","Fengtao Zhou","Jiabo Ma","Dejia Cai","Zhengyu Zhang","Lijuan Qu","Yu Wang","Li Liang","Hao Chen"],"pdf_url":"","comment":"18 pages, 11 figures, 10 tables"},{"id":"http://arxiv.org/abs/2512.17724v1","updated":"2025-12-19T15:58:52Z","published":"2025-12-19T15:58:52Z","title":"SAVeD: A First-Person Social Media Video Dataset for ADAS-equipped vehicle Near-Miss and Crash Event Analyses","summary":"The advancement of safety-critical research in driving behavior in ADAS-equipped vehicles require real-world datasets that not only include diverse traffic scenarios but also capture high-risk edge cases such as near-miss events and system failures. However, existing datasets are largely limited to either simulated environments or human-driven vehicle data, lacking authentic ADAS (Advanced Driver Assistance System) vehicle behavior under risk conditions. To address this gap, this paper introduces SAVeD, a large-scale video dataset curated from publicly available social media content, explicitly focused on ADAS vehicle-related crashes, near-miss incidents, and disengagements. SAVeD features 2,119 first-person videos, capturing ADAS vehicle operations in diverse locations, lighting conditions, and weather scenarios. The dataset includes video frame-level annotations for collisions, evasive maneuvers, and disengagements, enabling analysis of both perception and decision-making failures. We demonstrate SAVeD's utility through multiple analyses and contributions: (1) We propose a novel framework integrating semantic segmentation and monocular depth estimation to compute real-time Time-to-Collision (TTC) for dynamic objects. (2) We utilize the Generalized Extreme Value (GEV) distribution to model and quantify the extreme risk in crash and near-miss events across different roadway types. (3) We establish benchmarks for state-of-the-art VLLMs (VideoLLaMA2 and InternVL2.5 HiCo R16), showing that SAVeD's detailed annotations significantly enhance model performance through domain adaptation in complex near-miss scenarios.","authors":["Shaoyan Zhai","Mohamed Abdel-Aty","Chenzhu Wang","Rodrigo Vena Garcia"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17717v1","updated":"2025-12-19T15:51:44Z","published":"2025-12-19T15:51:44Z","title":"FlexAvatar: Flexible Large Reconstruction Model for Animatable Gaussian Head Avatars with Detailed Deformation","summary":"We present FlexAvatar, a flexible large reconstruction model for high-fidelity 3D head avatars with detailed dynamic deformation from single or sparse images, without requiring camera poses or expression labels. It leverages a transformer-based reconstruction model with structured head query tokens as canonical anchor to aggregate flexible input-number-agnostic, camera-pose-free and expression-free inputs into a robust canonical 3D representation. For detailed dynamic deformation, we introduce a lightweight UNet decoder conditioned on UV-space position maps, which can produce detailed expression-dependent deformations in real time. To better capture rare but critical expressions like wrinkles and bared teeth, we also adopt a data distribution adjustment strategy during training to balance the distribution of these expressions in the training set. Moreover, a lightweight 10-second refinement can further enhances identity-specific details in extreme identities without affecting deformation quality. Extensive experiments demonstrate that our FlexAvatar achieves superior 3D consistency, detailed dynamic realism compared with previous methods, providing a practical solution for animatable 3D avatar creation.","authors":["Cheng Peng","Zhuo Su","Liao Wang","Chen Guo","Zhaohu Li","Chengjiang Long","Zheng Lv","Jingxiang Sun","Chenyangguang Zhang","Yebin Liu"],"pdf_url":"","comment":"Project page: https://pengc02.github.io/flexavatar"},{"id":"http://arxiv.org/abs/2505.04006v2","updated":"2025-12-19T15:48:32Z","published":"2025-05-06T22:35:54Z","title":"The Eye as a Window to Systemic Health: A Survey of Retinal Imaging from Classical Techniques to Oculomics","summary":"The unique vascularized anatomy of the human eye, encased in the retina, provides an opportunity to act as a window for human health. The retinal structure assists in assessing the early detection, monitoring of disease progression and intervention for both ocular and non-ocular diseases. The advancement in imaging technology leveraging Artificial Intelligence has seized this opportunity to bridge the gap between the eye and human health. This track paves the way for unveiling systemic health insight from the ocular system and surrogating non-invasive markers for timely intervention and identification. The new frontiers of oculomics in ophthalmology cover both ocular and systemic diseases, and getting more attention to explore them. In this survey paper, we explore the evolution of retinal imaging techniques, the dire need for the integration of AI-driven analysis, and the shift of retinal imaging from classical techniques to oculomics. We also discuss some hurdles that may be faced in the progression of oculomics, highlighting the research gaps and future directions.","authors":[" Inamullah","Imran Razzak","Shoaib Jameel"],"pdf_url":"","comment":"Review Article"},{"id":"http://arxiv.org/abs/2509.23339v2","updated":"2025-12-19T15:48:12Z","published":"2025-09-27T14:42:29Z","title":"Enhancing Blind Face Restoration through Online Reinforcement Learning","summary":"Blind Face Restoration (BFR) encounters inherent challenges in exploring its large solution space, leading to common artifacts like missing details and identity ambiguity in the restored images. To tackle these challenges, we propose a Likelihood-Regularized Policy Optimization (LRPO) framework, the first to apply online reinforcement learning (RL) to the BFR task. LRPO leverages rewards from sampled candidates to refine the policy network, increasing the likelihood of high-quality outputs while improving restoration performance on low-quality inputs. However, directly applying RL to BFR creates incompatibility issues, producing restoration results that deviate significantly from the ground truth. To balance perceptual quality and fidelity, we propose three key strategies: 1) a composite reward function tailored for face restoration assessment, 2) ground-truth guided likelihood regularization, and 3) noise-level advantage assignment. Extensive experiments demonstrate that our proposed LRPO significantly improves the face restoration quality over baseline methods and achieves state-of-the-art performance.","authors":["Bin Wu","Yahui Liu","Chi Zhang","Yao Zhao","Wei Wang"],"pdf_url":"","comment":"8 figures, 4 tables"},{"id":"http://arxiv.org/abs/2501.15151v4","updated":"2025-12-19T15:40:55Z","published":"2025-01-25T09:24:14Z","title":"SpikeDet: Better Firing Patterns for Accurate and Energy-Efficient Object Detection with Spiking Neural Networks","summary":"Spiking Neural Networks (SNNs) are the third generation of neural networks. They have gained widespread attention in object detection due to their low power consumption and biological interpretability. However, existing SNN-based object detection methods suffer from local firing saturation, where adjacent neurons concurrently reach maximum firing rates, especially in object-centric regions. This abnormal neuron firing pattern reduces the feature discrimination capability and detection accuracy, while also increasing the firing rates that prevent SNNs from achieving their potential energy efficiency. To address this problem, we propose SpikeDet, a novel spiking object detector that optimizes firing patterns for accurate and energy-efficient detection. Specifically, we design a spiking backbone network, MDSNet, which effectively adjusts the membrane synaptic input distribution at each layer, achieving better neuron firing patterns during spiking feature extraction. For the neck, to better utilize and preserve these high-quality backbone features, we introduce the Spiking Multi-direction Fusion Module (SMFM), which realizes multi-direction fusion of spiking features, enhancing the multi-scale detection capability of the model. Furthermore, we propose the Local Firing Saturation Index (LFSI) to quantitatively measure local firing saturation. Experimental results validate the effectiveness of our method, with SpikeDet achieving superior performance. On the COCO 2017 dataset, it achieves 52.2% AP, outperforming previous SNN-based methods by 3.3% AP while requiring only half the power consumption. On object detection sub-tasks, including event-based GEN1, underwater URPC 2019, low-light ExDARK, and dense scene CrowdHuman datasets, SpikeDet also achieves the best performance.","authors":["Yimeng Fan","Changsong Liu","Mingyang Li","Dongze Liu","Yanyan Liu","Wei Zhang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17675v1","updated":"2025-12-19T15:17:12Z","published":"2025-12-19T15:17:12Z","title":"An Empirical Study of Sampling Hyperparameters in Diffusion-Based Super-Resolution","summary":"Diffusion models have shown strong potential for solving inverse problems such as single-image super-resolution, where a high-resolution image is recovered from a low-resolution observation using a pretrained unconditional prior. Conditioning methods, including Diffusion Posterior Sampling (DPS) and Manifold Constrained Gradient (MCG), can substantially improve reconstruction quality, but they introduce additional hyperparameters that require careful tuning. In this work, we conduct an empirical ablation study on FFHQ super-resolution to identify the dominant factors affecting performance when applying conditioning to pretrained diffusion models, and show that the conditioning step size has a significantly greater impact than the diffusion step count, with step sizes in the range of [2.0, 3.0] yielding the best overall performance in our experiments.","authors":["Yudhistira Arief Wibowo"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17673v1","updated":"2025-12-19T15:15:58Z","published":"2025-12-19T15:15:58Z","title":"Learning Spatio-Temporal Feature Representations for Video-Based Gaze Estimation","summary":"Video-based gaze estimation methods aim to capture the inherently temporal dynamics of human eye gaze from multiple image frames. However, since models must capture both spatial and temporal relationships, performance is limited by the feature representations within a frame but also between multiple frames. We propose the Spatio-Temporal Gaze Network (ST-Gaze), a model that combines a CNN backbone with dedicated channel attention and self-attention modules to fuse eye and face features optimally. The fused features are then treated as a spatial sequence, allowing for the capture of an intra-frame context, which is then propagated through time to model inter-frame dynamics. We evaluated our method on the EVE dataset and show that ST-Gaze achieves state-of-the-art performance both with and without person-specific adaptation. Additionally, our ablation study provides further insights into the model performance, showing that preserving and modelling intra-frame spatial context with our spatio-temporal recurrence is fundamentally superior to premature spatial pooling. As such, our results pave the way towards more robust video-based gaze estimation using commonly available cameras.","authors":["Alexandre Personnic","Mihai Bâce"],"pdf_url":"","comment":"12 pages, 5 figures, the code repository is available at https://gitlab.kuleuven.be/u0172623/ST-Gaze"},{"id":"http://arxiv.org/abs/2512.04890v4","updated":"2025-12-19T15:07:24Z","published":"2025-12-04T15:15:55Z","title":"Equivariant symmetry-aware head pose estimation for fetal MRI","summary":"We present E(3)-Pose, a novel fast pose estimation method that jointly and explicitly models rotation equivariance and object symmetry. Our work is motivated by the challenging problem of accounting for fetal head motion during a diagnostic MRI scan. We aim to enable automatic adaptive prescription of 2D diagnostic MRI slices with 6-DoF head pose estimation, supported by 3D MRI volumes rapidly acquired before each 2D slice. Existing methods struggle to generalize to clinical volumes, due to pose ambiguities induced by inherent anatomical symmetries, as well as low resolution, noise, and artifacts. In contrast, E(3)-Pose captures anatomical symmetries and rigid pose equivariance by construction, and yields robust estimates of the fetal head pose. Our experiments on publicly available and representative clinical fetal MRI datasets demonstrate the superior robustness and generalization of our method across domains. Crucially, E(3)-Pose achieves state-of-the-art accuracy on clinical MRI volumes, paving the way for clinical translation. Our implementation is available at github.com/ramyamut/E3-Pose.","authors":["Ramya Muthukrishnan","Borjan Gagoski","Aryn Lee","P. Ellen Grant","Elfar Adalsteinsson","Polina Golland","Benjamin Billot"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17655v1","updated":"2025-12-19T14:53:42Z","published":"2025-12-19T14:53:42Z","title":"Bitbox: Behavioral Imaging Toolbox for Computational Analysis of Behavior from Videos","summary":"Computational measurement of human behavior from video has recently become feasible due to major advances in AI. These advances now enable granular and precise quantification of facial expression, head movement, body action, and other behavioral modalities and are increasingly used in psychology, psychiatry, neuroscience, and mental health research. However, mainstream adoption remains slow. Most existing methods and software are developed for engineering audiences, require specialized software stacks, and fail to provide behavioral measurements at a level directly useful for hypothesis-driven research. As a result, there is a large barrier to entry for researchers who wish to use modern, AI-based tools in their work. We introduce Bitbox, an open-source toolkit designed to remove this barrier and make advanced computational analysis directly usable by behavioral scientists and clinical researchers. Bitbox is guided by principles of reproducibility, modularity, and interpretability. It provides a standardized interface for extracting high-level behavioral measurements from video, leveraging multiple face, head, and body processors. The core modules have been tested and validated on clinical samples and are designed so that new measures can be added with minimal effort. Bitbox is intended to serve both sides of the translational gap. It gives behavioral researchers access to robust, high-level behavioral metrics without requiring engineering expertise, and it provides computer scientists a practical mechanism for disseminating methods to domains where their impact is most needed. We expect that Bitbox will accelerate integration of computational behavioral measurement into behavioral, clinical, and mental health research. Bitbox has been designed from the beginning as a community-driven effort that will evolve through contributions from both method developers and domain scientists.","authors":["Evangelos Sariyanidi","Gokul Nair","Lisa Yankowitz","Casey J. Zampella","Mohan Kashyap Pargi","Aashvi Manakiwala","Maya McNealis","John D. Herrington","Jeffrey Cohn","Robert T. Schultz","Birkan Tunc"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17650v1","updated":"2025-12-19T14:49:30Z","published":"2025-12-19T14:49:30Z","title":"Region-Constraint In-Context Generation for Instructional Video Editing","summary":"The In-context generation paradigm recently has demonstrated strong power in instructional image editing with both data efficiency and synthesis quality. Nevertheless, shaping such in-context learning for instruction-based video editing is not trivial. Without specifying editing regions, the results can suffer from the problem of inaccurate editing regions and the token interference between editing and non-editing areas during denoising. To address these, we present ReCo, a new instructional video editing paradigm that novelly delves into constraint modeling between editing and non-editing regions during in-context generation. Technically, ReCo width-wise concatenates source and target video for joint denoising. To calibrate video diffusion learning, ReCo capitalizes on two regularization terms, i.e., latent and attention regularization, conducting on one-step backward denoised latents and attention maps, respectively. The former increases the latent discrepancy of the editing region between source and target videos while reducing that of non-editing areas, emphasizing the modification on editing area and alleviating outside unexpected content generation. The latter suppresses the attention of tokens in the editing region to the tokens in counterpart of the source video, thereby mitigating their interference during novel object generation in target video. Furthermore, we propose a large-scale, high-quality video editing dataset, i.e., ReCo-Data, comprising 500K instruction-video pairs to benefit model training. Extensive experiments conducted on four major instruction-based video editing tasks demonstrate the superiority of our proposal.","authors":["Zhongwei Zhang","Fuchen Long","Wei Li","Zhaofan Qiu","Wu Liu","Ting Yao","Tao Mei"],"pdf_url":"","comment":"Project page: https://zhw-zhang.github.io/ReCo-page/"},{"id":"http://arxiv.org/abs/2512.08557v2","updated":"2025-12-19T14:45:54Z","published":"2025-12-09T12:58:10Z","title":"SSCATeR: Sparse Scatter-Based Convolution Algorithm with Temporal Data Recycling for Real-Time 3D Object Detection in LiDAR Point Clouds","summary":"This work leverages the continuous sweeping motion of LiDAR scanning to concentrate object detection efforts on specific regions that receive a change in point data from one frame to another. We achieve this by using a sliding time window with short strides and consider the temporal dimension by storing convolution results between passes. This allows us to ignore unchanged regions, significantly reducing the number of convolution operations per forward pass without sacrificing accuracy. This data reuse scheme introduces extreme sparsity to detection data. To exploit this sparsity, we extend our previous work on scatter-based convolutions to allow for data reuse, and as such propose Sparse Scatter-Based Convolution Algorithm with Temporal Data Recycling (SSCATeR). This operation treats incoming LiDAR data as a continuous stream and acts only on the changing parts of the point cloud. By doing so, we achieve the same results with as much as a 6.61-fold reduction in processing time. Our test results show that the feature maps output by our method are identical to those produced by traditional sparse convolution techniques, whilst greatly increasing the computational efficiency of the network.","authors":["Alexander Dow","Manduhu Manduhu","Matheus Santos","Ben Bartlett","Gerard Dooly","James Riordan"],"pdf_url":"","comment":"23 Pages, 27 Figures, This work has been submitted to the IEEE Sensors Journal for possible publication"},{"id":"http://arxiv.org/abs/2512.17640v1","updated":"2025-12-19T14:41:50Z","published":"2025-12-19T14:41:50Z","title":"Generative Human-Object Interaction Detection via Differentiable Cognitive Steering of Multi-modal LLMs","summary":"Human-object interaction (HOI) detection aims to localize human-object pairs and the interactions between them. Existing methods operate under a closed-world assumption, treating the task as a classification problem over a small, predefined verb set, which struggles to generalize to the long-tail of unseen or ambiguous interactions in the wild. While recent multi-modal large language models (MLLMs) possess the rich world knowledge required for open-vocabulary understanding, they remain decoupled from existing HOI detectors since fine-tuning them is computationally prohibitive. To address these constraints, we propose \\GRASP-HO}, a novel Generative Reasoning And Steerable Perception framework that reformulates HOI detection from the closed-set classification task to the open-vocabulary generation problem. To bridge the vision and cognitive, we first extract hybrid interaction representations, then design a lightweight learnable cognitive steering conduit (CSC) module to inject the fine-grained visual evidence into a frozen MLLM for effective reasoning. To address the supervision mismatch between classification-based HOI datasets and open-vocabulary generative models, we introduce a hybrid guidance strategy that coupling the language modeling loss and auxiliary classification loss, enabling discriminative grounding without sacrificing generative flexibility. Experiments demonstrate state-of-the-art closed-set performance and strong zero-shot generalization, achieving a unified paradigm that seamlessly bridges discriminative perception and generative reasoning for open-world HOI detection.","authors":["Zhaolin Cai","Huiyu Duan","Zitong Xu","Fan Li","Zhi Liu","Jing Liu","Wei Shen","Xiongkuo Min","Guangtao Zhai"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2510.17330v2","updated":"2025-12-19T14:39:03Z","published":"2025-10-20T09:23:29Z","title":"CharDiff-LP: A Diffusion Model with Character-Level Guidance for License Plate Image Restoration","summary":"License plate image restoration is important not only as a preprocessing step for license plate recognition but also for enhancing evidential value, improving visual clarity, and enabling broader reuse of license plate images. We propose a novel diffusion-based framework with character-level guidance, CharDiff-LP, which effectively restores and recognizes severely degraded license plate images captured under realistic conditions. CharDiff-LP leverages fine-grained character-level priors extracted through external segmentation and Optical Character Recognition (OCR) modules tailored for low-quality license plate images. For precise and focused guidance, CharDiff-LP incorporates a novel Character-guided Attention through Region-wise Masking (CHARM) module, which ensures that each character's guidance is restricted to its own region, thereby avoiding interference with other regions. In experiments, CharDiff-LP significantly outperformed baseline restoration models in both restoration quality and recognition accuracy, achieving a 28.3% relative reduction in character error rate (CER) on the Roboflow-LP dataset compared with the best-performing baseline.","authors":["Kihyun Na","Gyuhwan Park","Injung Kim"],"pdf_url":"","comment":"15 pages, 6 figures, 4 tables"},{"id":"http://arxiv.org/abs/2512.06345v2","updated":"2025-12-19T14:36:14Z","published":"2025-12-06T08:26:36Z","title":"CLUENet: Cluster Attention Makes Neural Networks Have Eyes","summary":"Despite the success of convolution- and attention-based models in vision tasks, their rigid receptive fields and complex architectures limit their ability to model irregular spatial patterns and hinder interpretability, therefore posing challenges for tasks requiring high model transparency. Clustering paradigms offer promising interpretability and flexible semantic modeling, but suffer from limited accuracy, low efficiency, and gradient vanishing during training. To address these issues, we propose CLUster attEntion Network (CLUENet), an transparent deep architecture for visual semantic understanding. We propose three key innovations include (i) a Global Soft Aggregation and Hard Assignment with a Temperature-Scaled Cosin Attention and gated residual connections for enhanced local modeling, (ii) inter-block Hard and Shared Feature Dispatching, and (iii) an improved cluster pooling strategy. These enhancements significantly improve both classification performance and visual interpretability. Experiments on CIFAR-100 and Mini-ImageNet demonstrate that CLUENet outperforms existing clustering methods and mainstream visual models, offering a compelling balance of accuracy, efficiency, and transparency.","authors":["Xiangshuai Song","Jun-Jie Huang","Tianrui Liu","Ke Liang","Chang Tang"],"pdf_url":"","comment":"10 pages, 6 figures, 2026 Association for the Advancement of Artificial Intelligence"},{"id":"http://arxiv.org/abs/2509.05144v2","updated":"2025-12-19T14:32:19Z","published":"2025-09-05T14:37:31Z","title":"SGS-3D: High-Fidelity 3D Instance Segmentation via Reliable Semantic Mask Splitting and Growing","summary":"Accurate 3D instance segmentation is crucial for high-quality scene understanding in the 3D vision domain. However, 3D instance segmentation based on 2D-to-3D lifting approaches struggle to produce precise instance-level segmentation, due to accumulated errors introduced during the lifting process from ambiguous semantic guidance and insufficient depth constraints. To tackle these challenges, we propose splitting and growing reliable semantic mask for high-fidelity 3D instance segmentation (SGS-3D), a novel \"split-then-grow\" framework that first purifies and splits ambiguous lifted masks using geometric primitives, and then grows them into complete instances within the scene. Unlike existing approaches that directly rely on raw lifted masks and sacrifice segmentation accuracy, SGS-3D serves as a training-free refinement method that jointly fuses semantic and geometric information, enabling effective cooperation between the two levels of representation. Specifically, for semantic guidance, we introduce a mask filtering strategy that leverages the co-occurrence of 3D geometry primitives to identify and remove ambiguous masks, thereby ensuring more reliable semantic consistency with the 3D object instances. For the geometric refinement, we construct fine-grained object instances by exploiting both spatial continuity and high-level features, particularly in the case of semantic ambiguity between distinct objects. Experimental results on ScanNet200, ScanNet++, and KITTI-360 demonstrate that SGS-3D substantially improves segmentation accuracy and robustness against inaccurate masks from pre-trained models, yielding high-fidelity object instances while maintaining strong generalization across diverse indoor and outdoor environments. Code is available at https://github.com/wangchaolei7/SGS-3D.","authors":["Chaolei Wang","Yang Luo","Jing Du","Siyu Chen","Yiping Chen","Ting Han"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17621v1","updated":"2025-12-19T14:26:50Z","published":"2025-12-19T14:26:50Z","title":"PathFLIP: Fine-grained Language-Image Pretraining for Versatile Computational Pathology","summary":"While Vision-Language Models (VLMs) have achieved notable progress in computational pathology (CPath), the gigapixel scale and spatial heterogeneity of Whole Slide Images (WSIs) continue to pose challenges for multimodal understanding. Existing alignment methods struggle to capture fine-grained correspondences between textual descriptions and visual cues across thousands of patches from a slide, compromising their performance on downstream tasks. In this paper, we propose PathFLIP (Pathology Fine-grained Language-Image Pretraining), a novel framework for holistic WSI interpretation. PathFLIP decomposes slide-level captions into region-level subcaptions and generates text-conditioned region embeddings to facilitate precise visual-language grounding. By harnessing Large Language Models (LLMs), PathFLIP can seamlessly follow diverse clinical instructions and adapt to varied diagnostic contexts. Furthermore, it exhibits versatile capabilities across multiple paradigms, efficiently handling slide-level classification and retrieval, fine-grained lesion localization, and instruction following. Extensive experiments demonstrate that PathFLIP outperforms existing large-scale pathological VLMs on four representative benchmarks while requiring significantly less training data, paving the way for fine-grained, instruction-aware WSI interpretation in clinical practice.","authors":["Fengchun Liu","Songhan Jiang","Linghan Cai","Ziyue Wang","Yongbing Zhang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17620v1","updated":"2025-12-19T14:25:46Z","published":"2025-12-19T14:25:46Z","title":"StereoMV2D: A Sparse Temporal Stereo-Enhanced Framework for Robust Multi-View 3D Object Detection","summary":"Multi-view 3D object detection is a fundamental task in autonomous driving perception, where achieving a balance between detection accuracy and computational efficiency remains crucial. Sparse query-based 3D detectors efficiently aggregate object-relevant features from multi-view images through a set of learnable queries, offering a concise and end-to-end detection paradigm. Building on this foundation, MV2D leverages 2D detection results to provide high-quality object priors for query initialization, enabling higher precision and recall. However, the inherent depth ambiguity in single-frame 2D detections still limits the accuracy of 3D query generation. To address this issue, we propose StereoMV2D, a unified framework that integrates temporal stereo modeling into the 2D detection-guided multi-view 3D detector. By exploiting cross-temporal disparities of the same object across adjacent frames, StereoMV2D enhances depth perception and refines the query priors, while performing all computations efficiently within 2D regions of interest (RoIs). Furthermore, a dynamic confidence gating mechanism adaptively evaluates the reliability of temporal stereo cues through learning statistical patterns derived from the inter-frame matching matrix together with appearance consistency, ensuring robust detection under object appearance and occlusion. Extensive experiments on the nuScenes and Argoverse 2 datasets demonstrate that StereoMV2D achieves superior detection performance without incurring significant computational overhead. Code will be available at https://github.com/Uddd821/StereoMV2D.","authors":["Di Wu","Feng Yang","Wenhui Zhao","Jinwen Yu","Pan Liao","Benlian Xu","Dingwen Zhang"],"pdf_url":"","comment":"12 pages, 4 figures. This work has been submitted to the IEEE for possible publication"},{"id":"http://arxiv.org/abs/2510.16442v2","updated":"2025-12-19T14:22:03Z","published":"2025-10-18T10:34:05Z","title":"EDVD-LLaMA: Explainable Deepfake Video Detection via Multimodal Large Language Model Reasoning","summary":"The rapid development of deepfake video technology has not only facilitated artistic creation but also made it easier to spread misinformation. Traditional deepfake video detection (DVD) methods face issues such as a lack of transparency in their principles and insufficient generalization capabilities to cope with evolving forgery techniques. This highlights an urgent need for detectors that can identify forged content and provide verifiable reasoning explanations. This paper proposes the explainable deepfake video detection (EDVD) task and designs the EDVD-LLaMA multimodal, a large language model (MLLM) reasoning framework, which provides traceable reasoning processes alongside accurate detection results and trustworthy explanations. Our approach first incorporates a Spatio-Temporal Subtle Information Tokenization (ST-SIT) to extract and fuse global and local cross-frame deepfake features, providing rich spatio-temporal semantic information input for MLLM reasoning. Second, we construct a Fine-grained Multimodal Chain-of-Thought (Fg-MCoT) mechanism, which introduces facial feature data as hard constraints during the reasoning process to achieve pixel-level spatio-temporal video localization, suppress hallucinated outputs, and enhance the reliability of the chain of thought. In addition, we build an Explainable Reasoning FF++ dataset (ER-FF++set), leveraging structured data to annotate videos and ensure quality control, thereby supporting dual supervision for reasoning and detection. Extensive experiments demonstrate that EDVD-LLaMA achieves outstanding performance and robustness in terms of detection accuracy, explainability, and its ability to handle cross-forgery methods and cross-dataset scenarios. Compared to previous DVD methods, it provides a more explainable and superior solution. The project page is available at: https://11ouo1.github.io/edvd-llama/.","authors":["Haoran Sun","Chen Cai","Huiping Zhuang","Kong Aik Lee","Lap-Pui Chau","Yi Wang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17612v1","updated":"2025-12-19T14:15:31Z","published":"2025-12-19T14:15:31Z","title":"Self-Supervised Weighted Image Guided Quantitative MRI Super-Resolution","summary":"High-resolution (HR) quantitative MRI (qMRI) relaxometry provides objective tissue characterization but remains clinically underutilized due to lengthy acquisition times. We propose a physics-informed, self-supervised framework for qMRI super-resolution that uses routinely acquired HR weighted MRI (wMRI) scans as guidance, thus, removing the necessity for HR qMRI ground truth during training. We formulate super-resolution as Bayesian maximum a posteriori inference, minimizing two discrepancies: (1) between HR images synthesized from super-resolved qMRI maps and acquired wMRI guides via forward signal models, and (2) between acquired LR qMRI and downsampled predictions. This physics-informed objective allows the models to learn from clinical wMRI without HR qMRI supervision. To validate the concept, we generate training data by synthesizing wMRI guides from HR qMRI using signal equations, then degrading qMRI resolution via k-space truncation. A deep neural network learns the super-resolution mapping. Ablation experiments demonstrate that T1-weighted images primarily enhance T1 maps, T2-weighted images improve T2 maps, and combined guidance optimally enhances all parameters simultaneously. Validation on independently acquired in-vivo data from a different qMRI sequence confirms cross-qMRI sequence generalizability. Models trained on synthetic data can produce super-resolved maps from a 1-minute acquisition with quality comparable to a 5-minute reference scan, leveraging the scanner-independent nature of relaxometry parameters. By decoupling training from HR qMRI requirement, our framework enables fast qMRI acquisitions enhanced via routine clinical images, offering a practical pathway for integrating quantitative relaxometry into clinical workflows with acceptable additional scan time.","authors":["Alireza Samadifardheris","Dirk H. J. Poot","Florian Wiesinger","Stefan Klein","Juan A. Hernandez-Tamames"],"pdf_url":"","comment":"This work has been submitted to IEEE TMI for possible publication"},{"id":"http://arxiv.org/abs/2512.17610v1","updated":"2025-12-19T14:14:41Z","published":"2025-12-19T14:14:41Z","title":"Semi-Supervised 3D Segmentation for Type-B Aortic Dissection with Slim UNETR","summary":"Convolutional neural networks (CNN) for multi-class segmentation of medical images are widely used today. Especially models with multiple outputs that can separately predict segmentation classes (regions) without relying on a probabilistic formulation of the segmentation of regions. These models allow for more precise segmentation by tailoring the network's components to each class (region). They have a common encoder part of the architecture but branch out at the output layers, leading to improved accuracy.\n  These methods are used to diagnose type B aortic dissection (TBAD), which requires accurate segmentation of aortic structures based on the ImageTBDA dataset, which contains 100 3D computed tomography angiography (CTA) images. These images identify three key classes: true lumen (TL), false lumen (FL), and false lumen thrombus (FLT) of the aorta, which is critical for diagnosis and treatment decisions. In the dataset, 68 examples have a false lumen, while the remaining 32 do not, creating additional complexity for pathology detection.\n  However, implementing these CNN methods requires a large amount of high-quality labeled data. Obtaining accurate labels for the regions of interest can be an expensive and time-consuming process, particularly for 3D data. Semi-supervised learning methods allow models to be trained by using both labeled and unlabeled data, which is a promising approach for overcoming the challenge of obtaining accurate labels. However, these learning methods are not well understood for models with multiple outputs.\n  This paper presents a semi-supervised learning method for models with multiple outputs. The method is based on the additional rotations and flipping, and does not assume the probabilistic nature of the model's responses. This makes it a universal approach, which is especially important for architectures that involve separate segmentation.","authors":["Denis Mikhailapov","Vladimir Berikov"],"pdf_url":"","comment":"7 pages, 5 figures, 1 listing"},{"id":"http://arxiv.org/abs/2512.17605v1","updated":"2025-12-19T14:10:36Z","published":"2025-12-19T14:10:36Z","title":"MGRegBench: A Novel Benchmark Dataset with Anatomical Landmarks for Mammography Image Registration","summary":"Robust mammography registration is essential for clinical applications like tracking disease progression and monitoring longitudinal changes in breast tissue. However, progress has been limited by the absence of public datasets and standardized benchmarks. Existing studies are often not directly comparable, as they use private data and inconsistent evaluation frameworks. To address this, we present MGRegBench, a public benchmark dataset for mammogram registration. It comprises over 5,000 image pairs, with 100 containing manual anatomical landmarks and segmentation masks for rigorous evaluation. This makes MGRegBench one of the largest public 2D registration datasets with manual annotations. Using this resource, we benchmarked diverse registration methods including classical (ANTs), learning-based (VoxelMorph, TransMorph), implicit neural representation (IDIR), a classic mammography-specific approach, and a recent state-of-the-art deep learning method MammoRegNet. The implementations were adapted to this modality from the authors' implementations or re-implemented from scratch. Our contributions are: (1) the first public dataset of this scale with manual landmarks and masks for mammography registration; (2) the first like-for-like comparison of diverse methods on this modality; and (3) an extensive analysis of deep learning-based registration. We publicly release our code and data to establish a foundational resource for fair comparisons and catalyze future research. The source code and data are at https://github.com/KourtKardash/MGRegBench.","authors":["Svetlana Krasnova","Emiliya Starikova","Ilia Naletov","Andrey Krylov","Dmitry Sorokin"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2508.16295v2","updated":"2025-12-19T14:10:20Z","published":"2025-08-22T10:57:27Z","title":"Edge-Native Digitization of Handwritten Marksheets: A Hybrid Heuristic-Deep Learning Framework","summary":"The digitization of structured handwritten documents, such as academic marksheets, remains a significant challenge due to the dual complexity of irregular table structures and diverse handwriting styles. While recent Transformer-based approaches like TableNet and TrOCR achieve state-of-the-art accuracy, their high computational cost renders them unsuitable for resource-constrained edge deployments. This paper introduces a resource-efficient hybrid framework that integrates a heuristic OpenCV-based pipeline for rapid table structure detection with a modified lightweight YOLOv8 architecture for handwritten character recognition. By strategically removing the SPPF and deep C2f layers from the standard YOLOv8 backbone, we reduce computational overhead while maintaining high recognition fidelity. Experimental results on the EMNIST digit benchmark demonstrate that our Modified YOLOv8 model achieves 97.5% accuracy. Furthermore, we provide a comprehensive efficiency analysis showing that our framework offers a 95 times inference speedup over standard OCR pipelines and massive efficiency gains over emerging Large Multimodal Models (LMMs) like Qwen2.5-VL, achieving real-time performance 29 FPS on standard CPU hardware. A qualitative and quantitative evaluation on the AMES dataset, a challenging subset of real-world marksheets, confirms the system's robustness in handling mixed alphanumeric content, bridging the gap between high-performance deep learning and practical, scalable document automation.","authors":["Md. Irtiza Hossain","Junaid Ahmed Sifat","Abir Chowdhury"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17601v1","updated":"2025-12-19T14:07:34Z","published":"2025-12-19T14:07:34Z","title":"HeadHunt-VAD: Hunting Robust Anomaly-Sensitive Heads in MLLM for Tuning-Free Video Anomaly Detection","summary":"Video Anomaly Detection (VAD) aims to locate events that deviate from normal patterns in videos. Traditional approaches often rely on extensive labeled data and incur high computational costs. Recent tuning-free methods based on Multimodal Large Language Models (MLLMs) offer a promising alternative by leveraging their rich world knowledge. However, these methods typically rely on textual outputs, which introduces information loss, exhibits normalcy bias, and suffers from prompt sensitivity, making them insufficient for capturing subtle anomalous cues. To address these constraints, we propose HeadHunt-VAD, a novel tuning-free VAD paradigm that bypasses textual generation by directly hunting robust anomaly-sensitive internal attention heads within the frozen MLLM. Central to our method is a Robust Head Identification module that systematically evaluates all attention heads using a multi-criteria analysis of saliency and stability, identifying a sparse subset of heads that are consistently discriminative across diverse prompts. Features from these expert heads are then fed into a lightweight anomaly scorer and a temporal locator, enabling efficient and accurate anomaly detection with interpretable outputs. Extensive experiments show that HeadHunt-VAD achieves state-of-the-art performance among tuning-free methods on two major VAD benchmarks while maintaining high efficiency, validating head-level probing in MLLMs as a powerful and practical solution for real-world anomaly detection.","authors":["Zhaolin Cai","Fan Li","Ziwei Zheng","Haixia Bi","Lijun He"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17594v1","updated":"2025-12-19T14:02:37Z","published":"2025-12-19T14:02:37Z","title":"MAD-OOD: A Deep Learning Cluster-Driven Framework for an Out-of-Distribution Malware Detection and Classification","summary":"Out of distribution (OOD) detection remains a critical challenge in malware classification due to the substantial intra family variability introduced by polymorphic and metamorphic malware variants. Most existing deep learning based malware detectors rely on closed world assumptions and fail to adequately model this intra class variation, resulting in degraded performance when confronted with previously unseen malware families. This paper presents MADOOD, a novel two stage, cluster driven deep learning framework for robust OOD malware detection and classification. In the first stage, malware family embeddings are modeled using class conditional spherical decision boundaries derived from Gaussian Discriminant Analysis (GDA), enabling statistically grounded separation of indistribution and OOD samples without requiring OOD data during training. Z score based distance analysis across multiple class centroids is employed to reliably identify anomalous samples in the latent space. In the second stage, a deep neural network integrates cluster based predictions, refined embeddings, and supervised classifier outputs to enhance final classification accuracy. Extensive evaluations on benchmark malware datasets comprising 25 known families and multiple novel OOD variants demonstrate that MADOOD significantly outperforms state of the art OOD detection methods, achieving an AUC of up to 0.911 on unseen malware families. The proposed framework provides a scalable, interpretable, and statistically principled solution for real world malware detection and anomaly identification in evolving cybersecurity environments.","authors":["Tosin Ige","Christopher Kiekintveld","Aritran Piplai","Asif Rahman","Olukunle Kolade","Sasidhar Kunapuli"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2412.12667v2","updated":"2025-12-19T13:54:51Z","published":"2024-12-17T08:36:47Z","title":"Embedding-Driven Data Distillation for 360-Degree IQA With Residual-Aware Refinement","summary":"This article identifies and addresses a fundamental bottleneck in data-driven 360-degree image quality assessment (IQA): the lack of intelligent, sample-level data selection. Hence, we propose a novel framework that introduces a critical refinement step between patches sampling and model training. The core of our contribution is an embedding similarity-based selection algorithm that distills an initial, potentially redundant set of patches into a compact, maximally informative subset. This is formulated as a regularized optimization problem that preserves intrinsic perceptual relationships in a low-dimensional space, using residual analysis to explicitly filter out irrelevant or redundant samples. Extensive experiments on three benchmark datasets (CVIQ, OIQA, MVAQD) demonstrate that our selection enables a baseline model to match or exceed the performance of using all sampled data while keeping only 40-50% of patches. Particularly, we demonstrate the universal applicability of our approach by integrating it with several state-of-the-art IQA models, incleasy to deploy. Most significantly, its value as a generic,uding CNN- and transformer-based architectures, consistently enabling them to maintain or improve performance with 20-40\\% reduced computational load. This work establishes that adaptive, post-sampling data refinement is a powerful and widely applicable strategy for achieving efficient and robust 360-degree IQA.","authors":["Abderrezzaq Sendjasni","Seif-Eddine Benkabou","Mohamed-Chaker Larabi"],"pdf_url":"","comment":"Submitted to IEEE Transactions on Image Processing"},{"id":"http://arxiv.org/abs/2512.17585v1","updated":"2025-12-19T13:52:11Z","published":"2025-12-19T13:52:11Z","title":"SkinGenBench: Generative Model and Preprocessing Effects for Synthetic Dermoscopic Augmentation in Melanoma Diagnosis","summary":"This work introduces SkinGenBench, a systematic biomedical imaging benchmark that investigates how preprocessing complexity interacts with generative model choice for synthetic dermoscopic image augmentation and downstream melanoma diagnosis. Using a curated dataset of 14,116 dermoscopic images from HAM10000 and MILK10K across five lesion classes, we evaluate the two representative generative paradigms: StyleGAN2-ADA and Denoising Diffusion Probabilistic Models (DDPMs) under basic geometric augmentation and advanced artifact removal pipelines. Synthetic melanoma images are assessed using established perceptual and distributional metrics (FID, KID, IS), feature space analysis, and their impact on diagnostic performance across five downstream classifiers. Experimental results demonstrate that generative architecture choice has a stronger influence on both image fidelity and diagnostic utility than preprocessing complexity. StyleGAN2-ADA consistently produced synthetic images more closely aligned with real data distributions, achieving the lowest FID (~65.5) and KID (~0.05), while diffusion models generated higher variance samples at the cost of reduces perceptual fidelity and class anchoring. Advanced artifact removal yielded only marginal improvements in generative metrics and provided limited downstream diagnostic gains, suggesting possible suppression of clinically relevant texture cues. In contrast, synthetic data augmentation substantially improved melanoma detection with 8-15% absolute gains in melanoma F1-score, and ViT-B/16 achieving F1~0.88 and ROC-AUC~0.98, representing an improvement of approximately 14% over non-augmented baselines. Our code can be found at https://github.com/adarsh-crafts/SkinGenBench","authors":["N. A. Adarsh Pritam","Jeba Shiney O","Sanyam Jain"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17581v1","updated":"2025-12-19T13:48:10Z","published":"2025-12-19T13:48:10Z","title":"Medical Imaging AI Competitions Lack Fairness","summary":"Benchmarking competitions are central to the development of artificial intelligence (AI) in medical imaging, defining performance standards and shaping methodological progress. However, it remains unclear whether these benchmarks provide data that are sufficiently representative, accessible, and reusable to support clinically meaningful AI. In this work, we assess fairness along two complementary dimensions: (1) whether challenge datasets are representative of real-world clinical diversity, and (2) whether they are accessible and legally reusable in line with the FAIR principles. To address this question, we conducted a large-scale systematic study of 241 biomedical image analysis challenges comprising 458 tasks across 19 imaging modalities. Our findings show substantial biases in dataset composition, including geographic location, modality-, and problem type-related biases, indicating that current benchmarks do not adequately reflect real-world clinical diversity. Despite their widespread influence, challenge datasets were frequently constrained by restrictive or ambiguous access conditions, inconsistent or non-compliant licensing practices, and incomplete documentation, limiting reproducibility and long-term reuse. Together, these shortcomings expose foundational fairness limitations in our benchmarking ecosystem and highlight a disconnect between leaderboard success and clinical relevance.","authors":["Annika Reinke","Evangelia Christodoulou","Sthuthi Sadananda","A. Emre Kavur","Khrystyna Faryna","Daan Schouten","Bennett A. Landman","Carole Sudre","Olivier Colliot","Nick Heller","Sophie Loizillon","Martin Maška","Maëlys Solal","Arya Yazdan-Panah","Vilma Bozgo","Ömer Sümer","Siem de Jong","Sophie Fischer","Michal Kozubek","Tim Rädsch","Nadim Hammoud","Fruzsina Molnár-Gábor","Steven Hicks","Michael A. Riegler","Anindo Saha","Vajira Thambawita","Pal Halvorsen","Amelia Jiménez-Sánchez","Qingyang Yang","Veronika Cheplygina","Sabrina Bottazzi","Alexander Seitel","Spyridon Bakas","Alexandros Karargyris","Kiran Vaidhya Venkadesh","Bram van Ginneken","Lena Maier-Hein"],"pdf_url":"","comment":"Submitted to Nature BME"},{"id":"http://arxiv.org/abs/2512.17578v1","updated":"2025-12-19T13:44:36Z","published":"2025-12-19T13:44:36Z","title":"3One2: One-step Regression Plus One-step Diffusion for One-hot Modulation in Dual-path Video Snapshot Compressive Imaging","summary":"Video snapshot compressive imaging (SCI) captures dynamic scene sequences through a two-dimensional (2D) snapshot, fundamentally relying on optical modulation for hardware compression and the corresponding software reconstruction. While mainstream video SCI using random binary modulation has demonstrated success, it inevitably results in temporal aliasing during compression. One-hot modulation, activating only one sub-frame per pixel, provides a promising solution for achieving perfect temporal decoupling, thereby alleviating issues associated with aliasing. However, no algorithms currently exist to fully exploit this potential. To bridge this gap, we propose an algorithm specifically designed for one-hot masks. First, leveraging the decoupling properties of one-hot modulation, we transform the reconstruction task into a generative video inpainting problem and introduce a stochastic differential equation (SDE) of the forward process that aligns with the hardware compression process. Next, we identify limitations of the pure diffusion method for video SCI and propose a novel framework that combines one-step regression initialization with one-step diffusion refinement. Furthermore, to mitigate the spatial degradation caused by one-hot modulation, we implement a dual optical path at the hardware level, utilizing complementary information from another path to enhance the inpainted video. To our knowledge, this is the first work integrating diffusion into video SCI reconstruction. Experiments conducted on synthetic datasets and real scenes demonstrate the effectiveness of our method.","authors":["Ge Wang","Xing Liu","Xin Yuan"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.16625v2","updated":"2025-12-19T13:40:31Z","published":"2025-12-18T15:01:44Z","title":"DeContext as Defense: Safe Image Editing in Diffusion Transformers","summary":"In-context diffusion models allow users to modify images with remarkable ease and realism. However, the same power raises serious privacy concerns: personal images can be easily manipulated for identity impersonation, misinformation, or other malicious uses, all without the owner's consent. While prior work has explored input perturbations to protect against misuse in personalized text-to-image generation, the robustness of modern, large-scale in-context DiT-based models remains largely unexamined. In this paper, we propose DeContext, a new method to safeguard input images from unauthorized in-context editing. Our key insight is that contextual information from the source image propagates to the output primarily through multimodal attention layers. By injecting small, targeted perturbations that weaken these cross-attention pathways, DeContext breaks this flow, effectively decouples the link between input and output. This simple defense is both efficient and robust. We further show that early denoising steps and specific transformer blocks dominate context propagation, which allows us to concentrate perturbations where they matter most. Experiments on Flux Kontext and Step1X-Edit show that DeContext consistently blocks unwanted image edits while preserving visual quality. These results highlight the effectiveness of attention-based perturbations as a powerful defense against image manipulation. Code is available at https://github.com/LinghuiiShen/DeContext.","authors":["Linghui Shen","Mingyue Cui","Xingyi Yang"],"pdf_url":"","comment":"17 pages, 11 figures"},{"id":"http://arxiv.org/abs/2512.17573v1","updated":"2025-12-19T13:39:43Z","published":"2025-12-19T13:39:43Z","title":"RoomEditor++: A Parameter-Sharing Diffusion Architecture for High-Fidelity Furniture Synthesis","summary":"Virtual furniture synthesis, which seamlessly integrates reference objects into indoor scenes while maintaining geometric coherence and visual realism, holds substantial promise for home design and e-commerce applications. However, this field remains underexplored due to the scarcity of reproducible benchmarks and the limitations of existing image composition methods in achieving high-fidelity furniture synthesis while preserving background integrity. To overcome these challenges, we first present RoomBench++, a comprehensive and publicly available benchmark dataset tailored for this task. It consists of 112,851 training pairs and 1,832 testing pairs drawn from both real-world indoor videos and realistic home design renderings, thereby supporting robust training and evaluation under practical conditions. Then, we propose RoomEditor++, a versatile diffusion-based architecture featuring a parameter-sharing dual diffusion backbone, which is compatible with both U-Net and DiT architectures. This design unifies the feature extraction and inpainting processes for reference and background images. Our in-depth analysis reveals that the parameter-sharing mechanism enforces aligned feature representations, facilitating precise geometric transformations, texture preservation, and seamless integration. Extensive experiments validate that RoomEditor++ is superior over state-of-the-art approaches in terms of quantitative metrics, qualitative assessments, and human preference studies, while highlighting its strong generalization to unseen indoor scenes and general scenes without task-specific fine-tuning. The dataset and source code are available at \\url{https://github.com/stonecutter-21/roomeditor}.","authors":["Qilong Wang","Xiaofan Ming","Zhenyi Lin","Jinwen Li","Dongwei Ren","Wangmeng Zuo","Qinghua Hu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17566v1","updated":"2025-12-19T13:33:43Z","published":"2025-12-19T13:33:43Z","title":"A unified FLAIR hyperintensity segmentation model for various CNS tumor types and acquisition time points","summary":"T2-weighted fluid-attenuated inversion recovery (FLAIR) magnetic resonance imaging (MRI) scans are important for diagnosis, treatment planning and monitoring of brain tumors. Depending on the brain tumor type, the FLAIR hyperintensity volume is an important measure to asses the tumor volume or surrounding edema, and an automatic segmentation of this would be useful in the clinic. In this study, around 5000 FLAIR images of various tumors types and acquisition time points from different centers were used to train a unified FLAIR hyperintensity segmentation model using an Attention U-Net architecture. The performance was compared against dataset specific models, and was validated on different tumor types, acquisition time points and against BraTS. The unified model achieved an average Dice score of 88.65\\% for pre-operative meningiomas, 80.08% for pre-operative metastasis, 90.92% for pre-operative and 84.60% for post-operative gliomas from BraTS, and 84.47% for pre-operative and 61.27\\% for post-operative lower grade gliomas. In addition, the results showed that the unified model achieved comparable segmentation performance to the dataset specific models on their respective datasets, and enables generalization across tumor types and acquisition time points, which facilitates the deployment in a clinical setting. The model is integrated into Raidionics, an open-source software for CNS tumor analysis.","authors":["Mathilde Gajda Faanes","David Bouget","Asgeir S. Jakola","Timothy R. Smith","Vasileios K. Kavouridis","Francesco Latini","Margret Jensdottir","Peter Milos","Henrietta Nittby Redebrandt","Rickard L. Sjöberg","Rupavathana Mahesparan","Lars Kjelsberg Pedersen","Ole Solheim","Ingerid Reinertsen"],"pdf_url":"","comment":"13 pages, 4 figures"},{"id":"http://arxiv.org/abs/2512.17547v1","updated":"2025-12-19T13:11:55Z","published":"2025-12-19T13:11:55Z","title":"G3Splat: Geometrically Consistent Generalizable Gaussian Splatting","summary":"3D Gaussians have recently emerged as an effective scene representation for real-time splatting and accurate novel-view synthesis, motivating several works to adapt multi-view structure prediction networks to regress per-pixel 3D Gaussians from images. However, most prior work extends these networks to predict additional Gaussian parameters -- orientation, scale, opacity, and appearance -- while relying almost exclusively on view-synthesis supervision. We show that a view-synthesis loss alone is insufficient to recover geometrically meaningful splats in this setting. We analyze and address the ambiguities of learning 3D Gaussian splats under self-supervision for pose-free generalizable splatting, and introduce G3Splat, which enforces geometric priors to obtain geometrically consistent 3D scene representations. Trained on RE10K, our approach achieves state-of-the-art performance in (i) geometrically consistent reconstruction, (ii) relative pose estimation, and (iii) novel-view synthesis. We further demonstrate strong zero-shot generalization on ScanNet, substantially outperforming prior work in both geometry recovery and relative pose estimation. Code and pretrained models are released on our project page (https://m80hz.github.io/g3splat/).","authors":["Mehdi Hosseinzadeh","Shin-Fang Chng","Yi Xu","Simon Lucey","Ian Reid","Ravi Garg"],"pdf_url":"","comment":"Project page: https://m80hz.github.io/g3splat/"},{"id":"http://arxiv.org/abs/2512.16710v2","updated":"2025-12-19T13:10:35Z","published":"2025-12-18T16:13:34Z","title":"A multi-centre, multi-device benchmark dataset for landmark-based comprehensive fetal biometry","summary":"Accurate fetal growth assessment from ultrasound (US) relies on precise biometry measured by manually identifying anatomical landmarks in standard planes. Manual landmarking is time-consuming, operator-dependent, and sensitive to variability across scanners and sites, limiting the reproducibility of automated approaches. There is a need for multi-source annotated datasets to develop artificial intelligence-assisted fetal growth assessment methods. To address this bottleneck, we present an open, multi-centre, multi-device benchmark dataset of fetal US images with expert anatomical landmark annotations for clinically used fetal biometric measurements. These measurements include head bi-parietal and occipito-frontal diameters, abdominal transverse and antero-posterior diameters, and femoral length. The dataset comprises 4,513 de-identified US images from 1,904 subjects acquired at three clinical sites using seven different US devices. We provide standardised, subject-disjoint train/test splits, evaluation code, and baseline results to enable fair and reproducible comparison of methods. Using an automatic biometry model, we quantify domain shift and demonstrate that training and evaluation confined to a single centre substantially overestimate performance relative to multi-centre testing. To the best of our knowledge, this is the first publicly available multi-centre, multi-device, landmark-annotated dataset that covers all primary fetal biometry measures, providing a robust benchmark for domain adaptation and multi-centre generalisation in fetal biometry and enabling more reliable AI-assisted fetal growth assessment across centres. All data, annotations, training code, and evaluation pipelines are made publicly available.","authors":["Chiara Di Vece","Zhehua Mao","Netanell Avisdris","Brian Dromey","Raffaele Napolitano","Dafna Ben Bashat","Francisco Vasconcelos","Danail Stoyanov","Leo Joskowicz","Sophia Bano"],"pdf_url":"","comment":"11 pages, 5 figures, 3 tables"},{"id":"http://arxiv.org/abs/2512.17545v1","updated":"2025-12-19T13:10:31Z","published":"2025-12-19T13:10:31Z","title":"ClothHMR: 3D Mesh Recovery of Humans in Diverse Clothing from Single Image","summary":"With 3D data rapidly emerging as an important form of multimedia information, 3D human mesh recovery technology has also advanced accordingly. However, current methods mainly focus on handling humans wearing tight clothing and perform poorly when estimating body shapes and poses under diverse clothing, especially loose garments. To this end, we make two key insights: (1) tailoring clothing to fit the human body can mitigate the adverse impact of clothing on 3D human mesh recovery, and (2) utilizing human visual information from large foundational models can enhance the generalization ability of the estimation. Based on these insights, we propose ClothHMR, to accurately recover 3D meshes of humans in diverse clothing. ClothHMR primarily consists of two modules: clothing tailoring (CT) and FHVM-based mesh recovering (MR). The CT module employs body semantic estimation and body edge prediction to tailor the clothing, ensuring it fits the body silhouette. The MR module optimizes the initial parameters of the 3D human mesh by continuously aligning the intermediate representations of the 3D mesh with those inferred from the foundational human visual model (FHVM). ClothHMR can accurately recover 3D meshes of humans wearing diverse clothing, precisely estimating their body shapes and poses. Experimental results demonstrate that ClothHMR significantly outperforms existing state-of-the-art methods across benchmark datasets and in-the-wild images. Additionally, a web application for online fashion and shopping powered by ClothHMR is developed, illustrating that ClothHMR can effectively serve real-world usage scenarios. The code and model for ClothHMR are available at: \\url{https://github.com/starVisionTeam/ClothHMR}.","authors":["Yunqi Gao","Leyuan Liu","Yuhan Li","Changxin Gao","Yuanyuan Liu","Jingying Chen"],"pdf_url":"","comment":"15 pages,16 figures"},{"id":"http://arxiv.org/abs/2512.17541v1","updated":"2025-12-19T13:04:13Z","published":"2025-12-19T13:04:13Z","title":"FLEG: Feed-Forward Language Embedded Gaussian Splatting from Any Views","summary":"We present FLEG, a feed-forward network that reconstructs language-embedded 3D Gaussians from any views. Previous straightforward solutions combine feed-forward reconstruction with Gaussian heads but suffer from fixed input views and insufficient 3D training data. In contrast, we propose a 3D-annotation-free training framework for 2D-to-3D lifting from arbitrary uncalibrated and unposed multi-view images. Since the framework does not require 3D annotations, we can leverage large-scale video data with easily obtained 2D instance information to enrich semantic embedding. We also propose an instance-guided contrastive learning to align 2D semantics with the 3D representations. In addition, to mitigate the high memory and computational cost of dense views, we further propose a geometry-semantic hierarchical sparsification strategy. Our FLEG efficiently reconstructs language-embedded 3D Gaussian representation in a feed-forward manner from arbitrary sparse or dense views, jointly producing accurate geometry, high-fidelity appearance, and language-aligned semantics. Extensive experiments show that it outperforms existing methods on various related tasks. Project page: https://fangzhou2000.github.io/projects/fleg.","authors":["Qijian Tian","Xin Tan","Jiayu Ying","Xuhong Wang","Yuan Xie","Lizhuang Ma"],"pdf_url":"","comment":"Project page: https://fangzhou2000.github.io/projects/fleg"},{"id":"http://arxiv.org/abs/2512.17532v1","updated":"2025-12-19T12:56:17Z","published":"2025-12-19T12:56:17Z","title":"Robust-R1: Degradation-Aware Reasoning for Robust Visual Understanding","summary":"Multimodal Large Language Models struggle to maintain reliable performance under extreme real-world visual degradations, which impede their practical robustness. Existing robust MLLMs predominantly rely on implicit training/adaptation that focuses solely on visual encoder generalization, suffering from limited interpretability and isolated optimization. To overcome these limitations, we propose Robust-R1, a novel framework that explicitly models visual degradations through structured reasoning chains. Our approach integrates: (i) supervised fine-tuning for degradation-aware reasoning foundations, (ii) reward-driven alignment for accurately perceiving degradation parameters, and (iii) dynamic reasoning depth scaling adapted to degradation intensity. To facilitate this approach, we introduce a specialized 11K dataset featuring realistic degradations synthesized across four critical real-world visual processing stages, each annotated with structured chains connecting degradation parameters, perceptual influence, pristine semantic reasoning chain, and conclusion. Comprehensive evaluations demonstrate state-of-the-art robustness: Robust-R1 outperforms all general and robust baselines on the real-world degradation benchmark R-Bench, while maintaining superior anti-degradation performance under multi-intensity adversarial degradations on MMMB, MMStar, and RealWorldQA.","authors":["Jiaqi Tang","Jianmin Chen","Wei Wei","Xiaogang Xu","Runtao Liu","Xiangyu Wu","Qipeng Xie","Jiafei Wu","Lei Zhang","Qifeng Chen"],"pdf_url":"","comment":"Accepted by AAAI2026 Oral"},{"id":"http://arxiv.org/abs/2511.12346v2","updated":"2025-12-19T12:55:37Z","published":"2025-11-15T20:25:59Z","title":"CLAReSNet: When Convolution Meets Latent Attention for Hyperspectral Image Classification","summary":"Hyperspectral image (HSI) classification faces critical challenges, including high spectral dimensionality, complex spectral-spatial correlations, and limited training samples with severe class imbalance. While CNNs excel at local feature extraction and transformers capture long-range dependencies, their isolated application yields suboptimal results due to quadratic complexity and insufficient inductive biases. We propose CLAReSNet (Convolutional Latent Attention Residual Spectral Network), a hybrid architecture that integrates multi-scale convolutional extraction with transformer-style attention via an adaptive latent bottleneck. The model employs a multi-scale convolutional stem with deep residual blocks and an enhanced Convolutional Block Attention Module for hierarchical spatial features, followed by spectral encoder layers combining bidirectional RNNs (LSTM/GRU) with Multi-Scale Spectral Latent Attention (MSLA). MSLA reduces complexity from $\\mathcal{O}(T^2D)$ to $\\mathcal{O}(T\\log(T)D)$ by adaptive latent token allocation (8-64 tokens) that scales logarithmically with the sequence length. Hierarchical cross-attention fusion dynamically aggregates multi-level representations for robust classification. Experiments conducted on the Indian Pines and Salinas datasets show state-of-the-art performance, achieving overall accuracies of 99.71% and 99.96%, significantly surpassing HybridSN, SSRN, and SpectralFormer. The learned embeddings exhibit superior inter-class separability and compact intra-class clustering, validating CLAReSNet's effectiveness under severe class imbalance.","authors":["Asmit Bandyopadhyay","Anindita Das Bhattacharjee","Rakesh Das"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17517v1","updated":"2025-12-19T12:35:57Z","published":"2025-12-19T12:35:57Z","title":"PathBench-MIL: A Comprehensive AutoML and Benchmarking Framework for Multiple Instance Learning in Histopathology","summary":"We introduce PathBench-MIL, an open-source AutoML and benchmarking framework for multiple instance learning (MIL) in histopathology. The system automates end-to-end MIL pipeline construction, including preprocessing, feature extraction, and MIL-aggregation, and provides reproducible benchmarking of dozens of MIL models and feature extractors. PathBench-MIL integrates visualization tooling, a unified configuration system, and modular extensibility, enabling rapid experimentation and standardization across datasets and tasks. PathBench-MIL is publicly available at https://github.com/Sbrussee/PathBench-MIL","authors":["Siemen Brussee","Pieter A. Valkema","Jurre A. J. Weijer","Thom Doeleman","Anne M. R. Schrader","Jesper Kers"],"pdf_url":"","comment":"14 Pages, 3 Figures, 2 Appendices"},{"id":"http://arxiv.org/abs/2512.17514v1","updated":"2025-12-19T12:30:29Z","published":"2025-12-19T12:30:29Z","title":"Foundation Model Priors Enhance Object Focus in Feature Space for Source-Free Object Detection","summary":"Current state-of-the-art approaches in Source-Free Object Detection (SFOD) typically rely on Mean-Teacher self-labeling. However, domain shift often reduces the detector's ability to maintain strong object-focused representations, causing high-confidence activations over background clutter. This weak object focus results in unreliable pseudo-labels from the detection head. While prior works mainly refine these pseudo-labels, they overlook the underlying need to strengthen the feature space itself. We propose FALCON-SFOD (Foundation-Aligned Learning with Clutter suppression and Noise robustness), a framework designed to enhance object-focused adaptation under domain shift. It consists of two complementary components. SPAR (Spatial Prior-Aware Regularization) leverages the generalization strength of vision foundation models to regularize the detector's feature space. Using class-agnostic binary masks derived from OV-SAM, SPAR promotes structured and foreground-focused activations by guiding the network toward object regions. IRPL (Imbalance-aware Noise Robust Pseudo-Labeling) complements SPAR by promoting balanced and noise-tolerant learning under severe foreground-background imbalance. Guided by a theoretical analysis that connects these designs to tighter localization and classification error bounds, FALCON-SFOD achieves competitive performance across SFOD benchmarks.","authors":["Sairam VCR","Rishabh Lalla","Aveen Dayal","Tejal Kulkarni","Anuj Lalla","Vineeth N Balasubramanian","Muhammad Haris Khan"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.00324v2","updated":"2025-12-19T12:25:27Z","published":"2025-11-29T05:34:39Z","title":"MILE: A Mechanically Isomorphic Exoskeleton Data Collection System with Fingertip Visuotactile Sensing for Dexterous Manipulation","summary":"Imitation learning provides a promising approach to dexterous hand manipulation, but its effectiveness is limited by the lack of large-scale, high-fidelity data. Existing data-collection pipelines suffer from inaccurate motion retargeting, low data-collection efficiency, and missing high-resolution fingertip tactile sensing. We address this gap with MILE, a mechanically isomorphic teleoperation and data-collection system co-designed from human hand to exoskeleton to robotic hand. The exoskeleton is anthropometrically derived from the human hand, and the robotic hand preserves one-to-one joint-position isomorphism, eliminating nonlinear retargeting and enabling precise, natural control. The exoskeleton achieves a multi-joint mean absolute angular error below one degree, while the robotic hand integrates compact fingertip visuotactile modules that provide high-resolution tactile observations. Built on this retargeting-free interface, we teleoperate complex, contact-rich in-hand manipulation and efficiently collect a multimodal dataset comprising high-resolution fingertip visuotactile signals, RGB-D images, and joint positions. The teleoperation pipeline achieves a mean success rate improvement of 64%. Incorporating fingertip tactile observations further increases the success rate by an average of 25% over the vision-only baseline, validating the fidelity and utility of the dataset. Further details are available at: https://sites.google.com/view/mile-system.","authors":["Jinda Du","Jieji Ren","Qiaojun Yu","Ningbin Zhang","Yu Deng","Xingyu Wei","Yufei Liu","Guoying Gu","Xiangyang Zhu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17505v1","updated":"2025-12-19T12:14:37Z","published":"2025-12-19T12:14:37Z","title":"Adaptive Covariance and Quaternion-Focused Hybrid Error-State EKF/UKF for Visual-Inertial Odometry","summary":"This study presents an innovative hybrid Visual-Inertial Odometry (VIO) method for Unmanned Aerial Vehicles (UAVs) that is resilient to environmental challenges and capable of dynamically assessing sensor reliability. Built upon a loosely coupled sensor fusion architecture, the system utilizes a novel hybrid Quaternion-focused Error-State EKF/UKF (Qf-ES-EKF/UKF) architecture to process inertial measurement unit (IMU) data. This architecture first propagates the entire state using an Error-State Extended Kalman Filter (ESKF) and then applies a targeted Scaled Unscented Kalman Filter (SUKF) step to refine only the orientation. This sequential process blends the accuracy of SUKF in quaternion estimation with the overall computational efficiency of ESKF. The reliability of visual measurements is assessed via a dynamic sensor confidence score based on metrics, such as image entropy, intensity variation, motion blur, and inference quality, adapting the measurement noise covariance to ensure stable pose estimation even under challenging conditions. Comprehensive experimental analyses on the EuRoC MAV dataset demonstrate key advantages: an average improvement of 49% in position accuracy in challenging scenarios, an average of 57% in rotation accuracy over ESKF-based methods, and SUKF-comparable accuracy achieved with approximately 48% lower computational cost than a full SUKF implementation. These findings demonstrate that the presented approach strikes an effective balance between computational efficiency and estimation accuracy, and significantly enhances UAV pose estimation performance in complex environments with varying sensor reliability.","authors":["Ufuk Asil","Efendi Nasibov"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17504v1","updated":"2025-12-19T12:14:36Z","published":"2025-12-19T12:14:36Z","title":"InsertAnywhere: Bridging 4D Scene Geometry and Diffusion Models for Realistic Video Object Insertion","summary":"Recent advances in diffusion-based video generation have opened new possibilities for controllable video editing, yet realistic video object insertion (VOI) remains challenging due to limited 4D scene understanding and inadequate handling of occlusion and lighting effects. We present InsertAnywhere, a new VOI framework that achieves geometrically consistent object placement and appearance-faithful video synthesis. Our method begins with a 4D aware mask generation module that reconstructs the scene geometry and propagates user specified object placement across frames while maintaining temporal coherence and occlusion consistency. Building upon this spatial foundation, we extend a diffusion based video generation model to jointly synthesize the inserted object and its surrounding local variations such as illumination and shading. To enable supervised training, we introduce ROSE++, an illumination aware synthetic dataset constructed by transforming the ROSE object removal dataset into triplets of object removed video, object present video, and a VLM generated reference image. Through extensive experiments, we demonstrate that our framework produces geometrically plausible and visually coherent object insertions across diverse real world scenarios, significantly outperforming existing research and commercial models.","authors":["Hoiyeong Jin","Hyojin Jang","Jeongho Kim","Junha Hyung","Kinam Kim","Dongjin Kim","Huijin Choi","Hyeonji Kim","Jaegul Choo"],"pdf_url":"","comment":"16 pages, project page: https://myyzzzoooo.github.io/InsertAnywhere/"},{"id":"http://arxiv.org/abs/2512.17499v1","updated":"2025-12-19T12:08:28Z","published":"2025-12-19T12:08:28Z","title":"Validation of Diagnostic Artificial Intelligence Models for Prostate Pathology in a Middle Eastern Cohort","summary":"Background: Artificial intelligence (AI) is improving the efficiency and accuracy of cancer diagnostics. The performance of pathology AI systems has been almost exclusively evaluated on European and US cohorts from large centers. For global AI adoption in pathology, validation studies on currently under-represented populations - where the potential gains from AI support may also be greatest - are needed. We present the first study with an external validation cohort from the Middle East, focusing on AI-based diagnosis and Gleason grading of prostate cancer.\n  Methods: We collected and digitised 339 prostate biopsy specimens from the Kurdistan region, Iraq, representing a consecutive series of 185 patients spanning the period 2013-2024. We evaluated a task-specific end-to-end AI model and two foundation models in terms of their concordance with pathologists and consistency across samples digitised on three scanner models (Hamamatsu, Leica, and Grundium).\n  Findings: Grading concordance between AI and pathologists was similar to pathologist-pathologist concordance with Cohen's quadratically weighted kappa 0.801 vs. 0.799 (p=0.9824). Cross-scanner concordance was high (quadratically weighted kappa > 0.90) for all AI models and scanner pairs, including low-cost compact scanner.\n  Interpretation: AI models demonstrated pathologist-level performance in prostate histopathology assessment. Compact scanners can provide a route for validation studies in non-digitalised settings and enable cost-effective adoption of AI in laboratories with limited sample volumes. This first openly available digital pathology dataset from the Middle East supports further research into globally equitable AI pathology.\n  Funding: SciLifeLab and Wallenberg Data Driven Life Science Program, Instrumentarium Science Foundation, Karolinska Institutet Research Foundation.","authors":["Peshawa J. Muhammad Ali","Navin Vincent","Saman S. Abdulla","Han N. Mohammed Fadhl","Anders Blilie","Kelvin Szolnoky","Julia Anna Mielcarz","Xiaoyi Ji","Nita Mulliqi","Abdulbasit K. Al-Talabani","Kimmo Kartasalo"],"pdf_url":"","comment":"40 pages, 8 figures, 11 tables"},{"id":"http://arxiv.org/abs/2512.17495v1","updated":"2025-12-19T12:06:25Z","published":"2025-12-19T12:06:25Z","title":"GroundingME: Exposing the Visual Grounding Gap in MLLMs through Multi-Dimensional Evaluation","summary":"Visual grounding, localizing objects from natural language descriptions, represents a critical bridge between language and vision understanding. While multimodal large language models (MLLMs) achieve impressive scores on existing benchmarks, a fundamental question remains: can MLLMs truly ground language in vision with human-like sophistication, or are they merely pattern-matching on simplified datasets? Current benchmarks fail to capture real-world complexity where humans effortlessly navigate ambiguous references and recognize when grounding is impossible. To rigorously assess MLLMs' true capabilities, we introduce GroundingME, a benchmark that systematically challenges models across four critical dimensions: (1) Discriminative, distinguishing highly similar objects, (2) Spatial, understanding complex relational descriptions, (3) Limited, handling occlusions or tiny objects, and (4) Rejection, recognizing ungroundable queries. Through careful curation combining automated generation with human verification, we create 1,005 challenging examples mirroring real-world complexity. Evaluating 25 state-of-the-art MLLMs reveals a profound capability gap: the best model achieves only 45.1% accuracy, while most score 0% on rejection tasks, reflexively hallucinating objects rather than acknowledging their absence, raising critical safety concerns for deployment. We explore two strategies for improvements: (1) test-time scaling selects optimal response by thinking trajectory to improve complex grounding by up to 2.9%, and (2) data-mixture training teaches models to recognize ungroundable queries, boosting rejection accuracy from 0% to 27.9%. GroundingME thus serves as both a diagnostic tool revealing current limitations in MLLMs and a roadmap toward human-level visual grounding.","authors":["Rang Li","Lei Li","Shuhuai Ren","Hao Tian","Shuhao Gu","Shicheng Li","Zihao Yue","Yudong Wang","Wenhan Ma","Zhe Yang","Jingyuan Ma","Zhifang Sui","Fuli Luo"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17492v1","updated":"2025-12-19T12:03:05Z","published":"2025-12-19T12:03:05Z","title":"MMLANDMARKS: a Cross-View Instance-Level Benchmark for Geo-Spatial Understanding","summary":"Geo-spatial analysis of our world benefits from a multimodal approach, as every single geographic location can be described in numerous ways (images from various viewpoints, textual descriptions, and geographic coordinates). Current geo-spatial benchmarks have limited coverage across modalities, considerably restricting progress in the field, as current approaches cannot integrate all relevant modalities within a unified framework. We introduce the Multi-Modal Landmark dataset (MMLANDMARKS), a benchmark composed of four modalities: 197k highresolution aerial images, 329k ground-view images, textual information, and geographic coordinates for 18,557 distinct landmarks in the United States. The MMLANDMARKS dataset has a one-to-one correspondence across every modality, which enables training and benchmarking models for various geo-spatial tasks, including cross-view Ground-to-Satellite retrieval, ground and satellite geolocalization, Text-to-Image, and Text-to-GPS retrieval. We demonstrate broad generalization and competitive performance against off-the-shelf foundational models and specialized state-of-the-art models across different tasks by employing a simple CLIP-inspired baseline, illustrating the necessity for multimodal datasets to achieve broad geo-spatial understanding.","authors":["Oskar Kristoffersen","Alba R. Sánchez","Morten R. Hannemose","Anders B. Dahl","Dim P. Papadopoulos"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17489v1","updated":"2025-12-19T11:59:47Z","published":"2025-12-19T11:59:47Z","title":"LumiCtrl : Learning Illuminant Prompts for Lighting Control in Personalized Text-to-Image Models","summary":"Current text-to-image (T2I) models have demonstrated remarkable progress in creative image generation, yet they still lack precise control over scene illuminants, which is a crucial factor for content designers aiming to manipulate the mood, atmosphere, and visual aesthetics of generated images. In this paper, we present an illuminant personalization method named LumiCtrl that learns an illuminant prompt given a single image of an object. LumiCtrl consists of three basic components: given an image of the object, our method applies (a) physics-based illuminant augmentation along the Planckian locus to create fine-tuning variants under standard illuminants; (b) edge-guided prompt disentanglement using a frozen ControlNet to ensure prompts focus on illumination rather than structure; and (c) a masked reconstruction loss that focuses learning on the foreground object while allowing the background to adapt contextually, enabling what we call contextual light adaptation. We qualitatively and quantitatively compare LumiCtrl against other T2I customization methods. The results show that our method achieves significantly better illuminant fidelity, aesthetic quality, and scene coherence compared to existing personalization baselines. A human preference study further confirms strong user preference for LumiCtrl outputs. The code and data will be released upon publication.","authors":["Muhammad Atif Butt","Kai Wang","Javier Vazquez-Corral","Joost Van De Weijer"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17488v1","updated":"2025-12-19T11:59:41Z","published":"2025-12-19T11:59:41Z","title":"TwinSegNet: A Digital Twin-Enabled Federated Learning Framework for Brain Tumor Analysis","summary":"Brain tumor segmentation is critical in diagnosis and treatment planning for the disease. Yet, current deep learning methods rely on centralized data collection, which raises privacy concerns and limits generalization across diverse institutions. In this paper, we propose TwinSegNet, which is a privacy-preserving federated learning framework that integrates a hybrid ViT-UNet model with personalized digital twins for accurate and real-time brain tumor segmentation. Our architecture combines convolutional encoders with Vision Transformer bottlenecks to capture local and global context. Each institution fine-tunes the global model of private data to form its digital twin. Evaluated on nine heterogeneous MRI datasets, including BraTS 2019-2021 and custom tumor collections, TwinSegNet achieves high Dice scores (up to 0.90%) and sensitivity/specificity exceeding 90%, demonstrating robustness across non-independent and identically distributed (IID) client distributions. Comparative results against centralized models such as TumorVisNet highlight TwinSegNet's effectiveness in preserving privacy without sacrificing performance. Our approach enables scalable, personalized segmentation for multi-institutional clinical settings while adhering to strict data confidentiality requirements.","authors":["Almustapha A. Wakili","Adamu Hussaini","Abubakar A. Musa","Woosub Jung","Wei Yu"],"pdf_url":"","comment":"IEEE Virtual Conference on Communications. 4-6 November 2025"},{"id":"http://arxiv.org/abs/2507.17860v3","updated":"2025-12-19T11:48:41Z","published":"2025-07-23T18:33:27Z","title":"Towards Facilitated Fairness Assessment of AI-based Skin Lesion Classifiers Through GenAI-based Image Synthesis","summary":"Recent advances in deep learning and on-device inference could transform routine screening for skin cancers. Along with the anticipated benefits of this technology, potential dangers arise from unforeseen and inherent biases. A significant obstacle is building evaluation datasets that accurately reflect key demographics, including sex, age, and race, as well as other underrepresented groups. To address this, we train a state-of-the-art generative model to generate synthetic data in a controllable manner to assess the fairness of publicly available skin cancer classifiers. To evaluate whether synthetic images can be used as a fairness testing dataset, we prepare a real-image dataset (MILK10K) as a benchmark and compare the True Positive Rate result of three models (DeepGuide, MelaNet, and SkinLesionDensnet). As a result, the classification tendencies observed in each model when tested on real and generated images showed similar patterns across different attribute data sets. We confirm that highly realistic synthetic images facilitate model fairness verification.","authors":["Ko Watanabe","Stanislav Frolov","Aya Hassan","David Dembinsky","Adriano Lucieri","Andreas Dengel"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2507.00724v2","updated":"2025-12-19T11:29:15Z","published":"2025-06-24T15:40:11Z","title":"Holmes: Towards Effective and Harmless Model Ownership Verification to Personalized Large Vision Models via Decoupling Common Features","summary":"Large vision models (LVMs) achieve remarkable performance in various downstream tasks, primarily by personalizing pre-trained models through fine-tuning with private and valuable local data, which makes the personalized model a valuable intellectual property. Similar to the era of traditional DNNs, model stealing attacks also pose significant risks to LVMs. However, this paper reveals that most existing defense methods (developed for traditional DNNs), typically designed for models trained from scratch, either introduce additional security risks, are prone to misjudgment, or are even ineffective for fine-tuned models. To alleviate these problems, this paper proposes a harmless model ownership verification method for personalized LVMs by decoupling similar common features. In general, our method consists of three main stages. In the first stage, we create shadow models that retain common features of the victim model while disrupting dataset-specific features. We represent the dataset-specific features of the victim model by computing the output differences between the shadow and victim models, without altering the victim model or its training process. After that, a meta-classifier is trained to identify stolen models by determining whether suspicious models contain the dataset-specific features of the victim. In the third stage, we conduct model ownership verification by hypothesis test to mitigate randomness and enhance robustness. Extensive experiments on benchmark datasets verify the effectiveness of the proposed method in detecting different types of model stealing simultaneously. Our codes are available at https://github.com/zlh-thu/Holmes.","authors":["Linghui Zhu","Yiming Li","Haiqin Weng","Yan Liu","Tianwei Zhang","Shu-Tao Xia","Zhi Wang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17459v1","updated":"2025-12-19T11:20:52Z","published":"2025-12-19T11:20:52Z","title":"3D-RE-GEN: 3D Reconstruction of Indoor Scenes with a Generative Framework","summary":"Recent advances in 3D scene generation produce visually appealing output, but current representations hinder artists' workflows that require modifiable 3D textured mesh scenes for visual effects and game development. Despite significant advances, current textured mesh scene reconstruction methods are far from artist ready, suffering from incorrect object decomposition, inaccurate spatial relationships, and missing backgrounds. We present 3D-RE-GEN, a compositional framework that reconstructs a single image into textured 3D objects and a background. We show that combining state of the art models from specific domains achieves state of the art scene reconstruction performance, addressing artists' requirements.\n  Our reconstruction pipeline integrates models for asset detection, reconstruction, and placement, pushing certain models beyond their originally intended domains. Obtaining occluded objects is treated as an image editing task with generative models to infer and reconstruct with scene level reasoning under consistent lighting and geometry. Unlike current methods, 3D-RE-GEN generates a comprehensive background that spatially constrains objects during optimization and provides a foundation for realistic lighting and simulation tasks in visual effects and games. To obtain physically realistic layouts, we employ a novel 4-DoF differentiable optimization that aligns reconstructed objects with the estimated ground plane. 3D-RE-GEN~achieves state of the art performance in single image 3D scene reconstruction, producing coherent, modifiable scenes through compositional generation guided by precise camera recovery and spatial optimization.","authors":["Tobias Sautter","Jan-Niklas Dihlmann","Hendrik P. A. Lensch"],"pdf_url":"","comment":"Project Page: https://3dregen.jdihlmann.com/"},{"id":"http://arxiv.org/abs/2512.17450v1","updated":"2025-12-19T11:06:46Z","published":"2025-12-19T11:06:46Z","title":"MULTIAQUA: A multimodal maritime dataset and robust training strategies for multimodal semantic segmentation","summary":"Unmanned surface vehicles can encounter a number of varied visual circumstances during operation, some of which can be very difficult to interpret. While most cases can be solved only using color camera images, some weather and lighting conditions require additional information. To expand the available maritime data, we present a novel multimodal maritime dataset MULTIAQUA (Multimodal Aquatic Dataset). Our dataset contains synchronized, calibrated and annotated data captured by sensors of different modalities, such as RGB, thermal, IR, LIDAR, etc. The dataset is aimed at developing supervised methods that can extract useful information from these modalities in order to provide a high quality of scene interpretation regardless of potentially poor visibility conditions. To illustrate the benefits of the proposed dataset, we evaluate several multimodal methods on our difficult nighttime test set. We present training approaches that enable multimodal methods to be trained in a more robust way, thus enabling them to retain reliable performance even in near-complete darkness. Our approach allows for training a robust deep neural network only using daytime images, thus significantly simplifying data acquisition, annotation, and the training process.","authors":["Jon Muhovič","Janez Perš"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17445v1","updated":"2025-12-19T10:57:03Z","published":"2025-12-19T10:57:03Z","title":"LangDriveCTRL: Natural Language Controllable Driving Scene Editing with Multi-modal Agents","summary":"LangDriveCTRL is a natural-language-controllable framework for editing real-world driving videos to synthesize diverse traffic scenarios. It leverages explicit 3D scene decomposition to represent driving videos as a scene graph, containing static background and dynamic objects. To enable fine-grained editing and realism, it incorporates an agentic pipeline in which an Orchestrator transforms user instructions into execution graphs that coordinate specialized agents and tools. Specifically, an Object Grounding Agent establishes correspondence between free-form text descriptions and target object nodes in the scene graph; a Behavior Editing Agent generates multi-object trajectories from language instructions; and a Behavior Reviewer Agent iteratively reviews and refines the generated trajectories. The edited scene graph is rendered and then refined using a video diffusion tool to address artifacts introduced by object insertion and significant view changes. LangDriveCTRL supports both object node editing (removal, insertion and replacement) and multi-object behavior editing from a single natural-language instruction. Quantitatively, it achieves nearly $2\\times$ higher instruction alignment than the previous SoTA, with superior structural preservation, photorealism, and traffic realism. Project page is available at: https://yunhe24.github.io/langdrivectrl/.","authors":["Yun He","Francesco Pittaluga","Ziyu Jiang","Matthias Zwicker","Manmohan Chandraker","Zaid Tasneem"],"pdf_url":"","comment":"Project Page: https://yunhe24.github.io/langdrivectrl/"},{"id":"http://arxiv.org/abs/2512.15675v3","updated":"2025-12-19T10:55:54Z","published":"2025-12-17T18:28:04Z","title":"Stylized Synthetic Augmentation further improves Corruption Robustness","summary":"This paper proposes a training data augmentation pipeline that combines synthetic image data with neural style transfer in order to address the vulnerability of deep vision models to common corruptions. We show that although applying style transfer on synthetic images degrades their quality with respect to the common Frechet Inception Distance (FID) metric, these images are surprisingly beneficial for model training. We conduct a systematic empirical analysis of the effects of both augmentations and their key hyperparameters on the performance of image classifiers. Our results demonstrate that stylization and synthetic data complement each other well and can be combined with popular rule-based data augmentation techniques such as TrivialAugment, while not working with others. Our method achieves state-of-the-art corruption robustness on several small-scale image classification benchmarks, reaching 93.54%, 74.9% and 50.86% robust accuracy on CIFAR-10-C, CIFAR-100-C and TinyImageNet-C, respectively","authors":["Georg Siedel","Rojan Regmi","Abhirami Anand","Weijia Shao","Silvia Vock","Andrey Morozov"],"pdf_url":"","comment":"Accepted at VISAPP 2026 conference"},{"id":"http://arxiv.org/abs/2508.18236v3","updated":"2025-12-19T10:52:04Z","published":"2025-08-20T06:50:15Z","title":"Human-like Content Analysis for Generative AI with Language-Grounded Sparse Encoders","summary":"The rapid development of generative AI has transformed content creation, communication, and human development. However, this technology raises profound concerns in high-stakes domains, demanding rigorous methods to analyze and evaluate AI-generated content. While existing analytic methods often treat images as indivisible wholes, real-world AI failures generally manifest as specific visual patterns that can evade holistic detection and suit more granular and decomposed analysis. Here we introduce a content analysis tool, Language-Grounded Sparse Encoders (LanSE), which decompose images into interpretable visual patterns with natural language descriptions. Utilizing interpretability modules and large multimodal models, LanSE can automatically identify visual patterns within data modalities. Our method discovers more than 5,000 visual patterns with 93\\% human agreement, provides decomposed evaluation outperforming existing methods, establishes the first systematic evaluation of physical plausibility, and extends to medical imaging settings. Our method's capability to extract language-grounded patterns can be naturally adapted to numerous fields, including biology and geography, as well as other data modalities such as protein structures and time series, thereby advancing content analysis for generative AI.","authors":["Yiming Tang","Arash Lagzian","Srinivas Anumasa","Qiran Zou","Yingtao Zhu","Ye Zhang","Trang Nguyen","Yih-Chung Tham","Ehsan Adeli","Ching-Yu Cheng","Yilun Du","Dianbo Liu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17436v1","updated":"2025-12-19T10:43:37Z","published":"2025-12-19T10:43:37Z","title":"Xiaomi MiMo-VL-Miloco Technical Report","summary":"We open-source \\textbf{MiMo-VL-Miloco-7B} and its quantized variant \\textbf{MiMo-VL-Miloco-7B-GGUF}, a pair of home-centric vision-language models that achieve strong performance on both home-scenario understanding and general multimodal reasoning. Built on the MiMo-VL-7B backbone, MiMo-VL-Miloco-7B is specialized for smart-home environments, attaining leading F1 scores on gesture recognition and common home-scenario understanding, while also delivering consistent gains across video benchmarks such as Video-MME, Video-MMMU, and Charades-STA, as well as language understanding benchmarks including MMMU-Pro and MMLU-Pro. In our experiments, MiMo-VL-Miloco-7B outperforms strong closed-source and open-source baselines on home-scenario understanding and several multimodal reasoning benchmarks. To balance specialization and generality, we design a two-stage training pipeline that combines supervised fine-tuning with reinforcement learning based on Group Relative Policy Optimization, leveraging efficient multi-domain data. We further incorporate chain-of-thought supervision and token-budget-aware reasoning, enabling the model to learn knowledge in a data-efficient manner while also performing reasoning efficiently. Our analysis shows that targeted home-scenario training not only enhances activity and gesture understanding, but also improves text-only reasoning with only modest trade-offs on document-centric tasks. Model checkpoints, quantized GGUF weights, and our home-scenario evaluation toolkit are publicly available at \\href{https://github.com/XiaoMi/xiaomi-mimo-vl-miloco}{https://github.com/XiaoMi/xiaomi-mimo-vl-miloco} to support research and deployment in real-world smart-home applications.","authors":["Jiaze Li","Jingyang Chen","Yuxun Qu","Jianzhong Ju","Zhenbo Luo","Jian Luan","Shijie Xu","Zhenru Lin","Junyou Zhu","Boshen Xu","Wenhui Tan","Pei Fu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17432v1","updated":"2025-12-19T10:34:45Z","published":"2025-12-19T10:34:45Z","title":"AIFloodSense: A Global Aerial Imagery Dataset for Semantic Segmentation and Understanding of Flooded Environments","summary":"Accurate flood detection from visual data is a critical step toward improving disaster response and risk assessment, yet datasets for flood segmentation remain scarce due to the challenges of collecting and annotating large-scale imagery. Existing resources are often limited in geographic scope and annotation detail, hindering the development of robust, generalized computer vision methods. To bridge this gap, we introduce AIFloodSense, a comprehensive, publicly available aerial imagery dataset comprising 470 high-resolution images from 230 distinct flood events across 64 countries and six continents. Unlike prior benchmarks, AIFloodSense ensures global diversity and temporal relevance (2022-2024), supporting three complementary tasks: (i) Image Classification with novel sub-tasks for environment type, camera angle, and continent recognition; (ii) Semantic Segmentation providing precise pixel-level masks for flood, sky, and buildings; and (iii) Visual Question Answering (VQA) to enable natural language reasoning for disaster assessment. We establish baseline benchmarks for all tasks using state-of-the-art architectures, demonstrating the dataset's complexity and its value in advancing domain-generalized AI tools for climate resilience.","authors":["Georgios Simantiris","Konstantinos Bacharidis","Apostolos Papanikolaou","Petros Giannakakis","Costas Panagiotakis"],"pdf_url":"","comment":"36 pages, 19 figures, 8 tables"},{"id":"http://arxiv.org/abs/2512.17416v1","updated":"2025-12-19T10:13:43Z","published":"2025-12-19T10:13:43Z","title":"Beyond Occlusion: In Search for Near Real-Time Explainability of CNN-Based Prostate Cancer Classification","summary":"Deep neural networks are starting to show their worth in critical applications such as assisted cancer diagnosis. However, for their outputs to get accepted in practice, the results they provide should be explainable in a way easily understood by pathologists. A well-known and widely used explanation technique is occlusion, which, however, can take a long time to compute, thus slowing the development and interaction with pathologists. In this work, we set out to find a faster replacement for occlusion in a successful system for detecting prostate cancer. Since there is no established framework for comparing the performance of various explanation methods, we first identified suitable comparison criteria and selected corresponding metrics. Based on the results, we were able to choose a different explanation method, which cut the previously required explanation time at least by a factor of 10, without any negative impact on the quality of outputs. This speedup enables rapid iteration in model development and debugging and brings us closer to adopting AI-assisted prostate cancer detection in clinical settings. We propose that our approach to finding the replacement for occlusion can be used to evaluate candidate methods in other related applications.","authors":["Martin Krebs","Jan Obdržálek","Vít Musil","Tomáš Brázdil"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17396v1","updated":"2025-12-19T09:47:54Z","published":"2025-12-19T09:47:54Z","title":"RadImageNet-VQA: A Large-Scale CT and MRI Dataset for Radiologic Visual Question Answering","summary":"In this work, we introduce RadImageNet-VQA, a large-scale dataset designed to advance radiologic visual question answering (VQA) on CT and MRI exams. Existing medical VQA datasets are limited in scale, dominated by X-ray imaging or biomedical illustrations, and often prone to text-based shortcuts. RadImageNet-VQA is built from expert-curated annotations and provides 750K images paired with 7.5M question-answer samples. It covers three key tasks - abnormality detection, anatomy recognition, and pathology identification - spanning eight anatomical regions and 97 pathology categories, and supports open-ended, closed-ended, and multiple-choice questions. Extensive experiments show that state-of-the-art vision-language models still struggle with fine-grained pathology identification, particularly in open-ended settings and even after fine-tuning. Text-only analysis further reveals that model performance collapses to near-random without image inputs, confirming that RadImageNet-VQA is free from linguistic shortcuts. The full dataset and benchmark are publicly available at https://huggingface.co/datasets/raidium/RadImageNet-VQA.","authors":["Léo Butsanets","Charles Corbière","Julien Khlaut","Pierre Manceron","Corentin Dancette"],"pdf_url":"","comment":"Preprint, 23 pages, 12 figures, 7 tables"},{"id":"http://arxiv.org/abs/2512.17394v1","updated":"2025-12-19T09:47:38Z","published":"2025-12-19T09:47:38Z","title":"Are Vision Language Models Cross-Cultural Theory of Mind Reasoners?","summary":"Theory of Mind (ToM) -- the ability to attribute beliefs, desires, and emotions to others -- is fundamental for human social intelligence, yet remains a major challenge for artificial agents. Existing Vision-Language Models (VLMs) are increasingly applied in socially grounded tasks, but their capacity for cross-cultural ToM reasoning is largely unexplored. In this work, we introduce CulturalToM-VQA, a new evaluation benchmark containing 5095 questions designed to probe ToM reasoning across diverse cultural contexts through visual question answering. The dataset captures culturally grounded cues such as rituals, attire, gestures, and interpersonal dynamics, enabling systematic evaluation of ToM reasoning beyond Western-centric benchmarks. Our dataset is built through a VLM-assisted human-in-the-loop pipeline, where human experts first curate culturally rich images across traditions, rituals, and social interactions; a VLM then assist in generating structured ToM-focused scene descriptions, which are refined into question-answer pairs spanning a taxonomy of six ToM tasks and four graded complexity levels. The resulting dataset covers diverse theory of mind facets such as mental state attribution, false belief reasoning, non-literal communication, social norm violations, perspective coordination, and multi-agent reasoning.","authors":["Zabir Al Nazi","G M Shahariar","Abrar Hossain","Wei Peng"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2510.23117v3","updated":"2025-12-19T09:46:41Z","published":"2025-10-27T08:38:17Z","title":"Seeing Structural Failure Before it Happens: An Image-Based Physics-Informed Neural Network (PINN) for Spaghetti Bridge Load Prediction","summary":"Physics Informed Neural Networks (PINNs) are gaining attention for their ability to embed physical laws into deep learning models, which is particularly useful in structural engineering tasks with limited data. This paper aims to explore the use of PINNs to predict the weight of small scale spaghetti bridges, a task relevant to understanding load limits and potential failure modes in simplified structural models. Our proposed framework incorporates physics-based constraints to the prediction model for improved performance. In addition to standard PINNs, we introduce a novel architecture named Physics Informed Kolmogorov Arnold Network (PIKAN), which blends universal function approximation theory with physical insights. The structural parameters provided as input to the model are collected either manually or through computer vision methods. Our dataset includes 15 real bridges, augmented to 100 samples, and our best model achieves an $R^2$ score of 0.9603 and a mean absolute error (MAE) of 10.50 units. From applied perspective, we also provide a web based interface for parameter entry and prediction. These results show that PINNs can offer reliable estimates of structural weight, even with limited data, and may help inform early stage failure analysis in lightweight bridge designs.\n  The complete data and code are available at https://github.com/OmerJauhar/PINNS-For-Spaghetti-Bridges.","authors":["Omer Jauhar Khan","Sudais Khan","Hafeez Anwar","Shahzeb Khan","Shams Ul Arifeen"],"pdf_url":"","comment":"14 pages, 21 figures. Preprint"},{"id":"http://arxiv.org/abs/2308.02815v2","updated":"2025-12-19T09:43:02Z","published":"2023-08-05T08:11:26Z","title":"An AI-driven Assessment of Bone Density as a Biomarker Leading to the Aging Law","summary":"As global population aging intensifies, there is growing interest in the study of biological age. Bones have long been used to evaluate biological age, and the decline in bone density with age is a well-recognized phenomenon in adults. However, the pattern of this decline remains controversial, making it difficult to serve as a reliable indicator of the aging process. Here we present a novel AI-driven statistical method to assess the bone density, and a discovery that the bone mass distribution in trabecular bone of vertebrae follows a non-Gaussian, unimodal, and skewed distribution in CT images. The statistical mode of the distribution is defined as the measure of bone mass, which is a groundbreaking assessment of bone density, named Trabecular Bone Density (TBD). The dataset of CT images are collected from 1,719 patients who underwent PET/CT scans in three hospitals, in which a subset of the dataset is used for AI model training and generalization. Based upon the cases, we demonstrate that the pattern of bone density declining with aging exhibits a consistent trend of exponential decline across sexes and age groups using TBD assessment. The developed AI-driven statistical method blazes a trail in the field of AI for reliable quantitative computation and AI for medicine. The findings suggest that human aging is a gradual process, with the rate of decline slowing progressively over time, which will provide a valuable basis for scientific prediction of life expectancy.","authors":["Linmi Tao","Donglai Tao","Ruiyang Liu","Yu Cheng","Yuezhi Zhou","Li Huo","Zuoxiang He","Ti Jiang","Jingmao Cui","Yuanbiao Wang","Guilan Hu","Xiangsong Zhang","Yongwei Pan","Ye Yuan","Yun Liu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17376v1","updated":"2025-12-19T09:24:22Z","published":"2025-12-19T09:24:22Z","title":"Towards Deeper Emotional Reflection: Crafting Affective Image Filters with Generative Priors","summary":"Social media platforms enable users to express emotions by posting text with accompanying images. In this paper, we propose the Affective Image Filter (AIF) task, which aims to reflect visually-abstract emotions from text into visually-concrete images, thereby creating emotionally compelling results. We first introduce the AIF dataset and the formulation of the AIF models. Then, we present AIF-B as an initial attempt based on a multi-modal transformer architecture. After that, we propose AIF-D as an extension of AIF-B towards deeper emotional reflection, effectively leveraging generative priors from pre-trained large-scale diffusion models. Quantitative and qualitative experiments demonstrate that AIF models achieve superior performance for both content consistency and emotional fidelity compared to state-of-the-art methods. Extensive user study experiments demonstrate that AIF models are significantly more effective at evoking specific emotions. Based on the presented results, we comprehensively discuss the value and potential of AIF models.","authors":["Peixuan Zhang","Shuchen Weng","Jiajun Tang","Si Li","Boxin Shi"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.15411v2","updated":"2025-12-19T09:10:06Z","published":"2025-12-17T12:59:41Z","title":"MiVLA: Towards Generalizable Vision-Language-Action Model with Human-Robot Mutual Imitation Pre-training","summary":"While leveraging abundant human videos and simulated robot data poses a scalable solution to the scarcity of real-world robot data, the generalization capability of existing vision-language-action models (VLAs) remains limited by mismatches in camera views, visual appearance, and embodiment morphologies. To overcome this limitation, we propose MiVLA, a generalizable VLA empowered by human-robot mutual imitation pre-training, which leverages inherent behavioral similarity between human hands and robotic arms to build a foundation of strong behavioral priors for both human actions and robotic control. Specifically, our method utilizes kinematic rules with left/right hand coordinate systems for bidirectional alignment between human and robot action spaces. Given human or simulated robot demonstrations, MiVLA is trained to forecast behavior trajectories for one embodiment, and imitate behaviors for another one unseen in the demonstration. Based on this mutual imitation, it integrates the behavioral fidelity of real-world human data with the manipulative diversity of simulated robot data into a unified model, thereby enhancing the generalization capability for downstream tasks. Extensive experiments conducted on both simulation and real-world platforms with three robots (ARX, PiPer and LocoMan), demonstrate that MiVLA achieves strong improved generalization capability, outperforming state-of-the-art VLAs (e.g., $\\boldsymbolπ_{0}$, $\\boldsymbolπ_{0.5}$ and H-RDT) by 25% in simulation, and 14% in real-world robot control tasks.","authors":["Zhenhan Yin","Xuanhan Wang","Jiahao Jiang","Kaiyuan Deng","Pengqi Chen","Shuangle Li","Chong Liu","Xing Xu","Jingkuan Song","Lianli Gao","Heng Tao Shen"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2405.05814v2","updated":"2025-12-19T08:50:16Z","published":"2024-05-09T14:52:32Z","title":"MSDiff: Multi-Scale Diffusion Model for Ultra-Sparse View CT Reconstruction","summary":"Computed Tomography (CT) technology reduces radiation haz-ards to the human body through sparse sampling, but fewer sampling angles pose challenges for image reconstruction. Score-based generative models are widely used in sparse-view CT re-construction, performance diminishes significantly with a sharp reduction in projection angles. Therefore, we propose an ultra-sparse view CT reconstruction method utilizing multi-scale dif-fusion models (MSDiff), designed to concentrate on the global distribution of information and facilitate the reconstruction of sparse views with local image characteristics. Specifically, the proposed model ingeniously integrates information from both comprehensive sampling and selectively sparse sampling tech-niques. Through precise adjustments in diffusion model, it is capable of extracting diverse noise distribution, furthering the understanding of the overall structure of images, and aiding the fully sampled model in recovering image information more effec-tively. By leveraging the inherent correlations within the projec-tion data, we have designed an equidistant mask, enabling the model to focus its attention more effectively. Experimental re-sults demonstrated that the multi-scale model approach signifi-cantly improved the quality of image reconstruction under ultra-sparse angles, with good generalization across various datasets.","authors":["Junyan Zhang","Mengxiao Geng","Pinhuang Tan","Yi Liu","Zhili Liu","Bin Huang","Qiegen Liu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17350v1","updated":"2025-12-19T08:47:09Z","published":"2025-12-19T08:47:09Z","title":"Beyond Semantic Features: Pixel-level Mapping for Generalized AI-Generated Image Detection","summary":"The rapid evolution of generative technologies necessitates reliable methods for detecting AI-generated images. A critical limitation of current detectors is their failure to generalize to images from unseen generative models, as they often overfit to source-specific semantic cues rather than learning universal generative artifacts. To overcome this, we introduce a simple yet remarkably effective pixel-level mapping pre-processing step to disrupt the pixel value distribution of images and break the fragile, non-essential semantic patterns that detectors commonly exploit as shortcuts. This forces the detector to focus on more fundamental and generalizable high-frequency traces inherent to the image generation process. Through comprehensive experiments on GAN and diffusion-based generators, we show that our approach significantly boosts the cross-generator performance of state-of-the-art detectors. Extensive analysis further verifies our hypothesis that the disruption of semantic cues is the key to generalization.","authors":["Chenming Zhou","Jiaan Wang","Yu Li","Lei Li","Juan Cao","Sheng Tang"],"pdf_url":"","comment":"Accepted by AAAI 2026"},{"id":"http://arxiv.org/abs/2411.13929v3","updated":"2025-12-19T08:36:48Z","published":"2024-11-21T08:27:18Z","title":"From Engineering Diagrams to Graphs: Digitizing P&IDs with Transformers","summary":"Digitizing engineering diagrams like Piping and Instrumentation Diagrams (P&IDs) plays a vital role in maintainability and operational efficiency of process and hydraulic systems. Previous methods typically decompose the task into separate steps such as symbol detection and line detection, which can limit their ability to capture the structure in these diagrams. In this work, a transformer-based approach leveraging the Relationformer that addresses this limitation by jointly extracting symbols and their interconnections from P&IDs is introduced. To evaluate our approach and compare it to a modular digitization approach, we present the first publicly accessible benchmark dataset for P&ID digitization, annotated with graph-level ground truth. Experimental results on real-world diagrams show that our method significantly outperforms the modular baseline, achieving over 25% improvement in edge detection accuracy. This research contributes a reproducible evaluation framework and demonstrates the effectiveness of transformer models for structural understanding of complex engineering diagrams. The dataset is available under https://zenodo.org/records/14803338.","authors":["Jan Marius Stürmer","Marius Graumann","Tobias Koch"],"pdf_url":"","comment":"(c) 2025 IEEE. Published in the conference proceedings of the 2025 IEEE 12th International Conference on Data Science and Advanced Analytics (DSAA)"},{"id":"http://arxiv.org/abs/2512.17343v1","updated":"2025-12-19T08:35:08Z","published":"2025-12-19T08:35:08Z","title":"Multi-level distortion-aware deformable network for omnidirectional image super-resolution","summary":"As augmented reality and virtual reality applications gain popularity, image processing for OmniDirectional Images (ODIs) has attracted increasing attention. OmniDirectional Image Super-Resolution (ODISR) is a promising technique for enhancing the visual quality of ODIs. Before performing super-resolution, ODIs are typically projected from a spherical surface onto a plane using EquiRectangular Projection (ERP). This projection introduces latitude-dependent geometric distortion in ERP images: distortion is minimal near the equator but becomes severe toward the poles, where image content is stretched across a wider area. However, existing ODISR methods have limited sampling ranges and feature extraction capabilities, which hinder their ability to capture distorted patterns over large areas. To address this issue, we propose a novel Multi-level Distortion-aware Deformable Network (MDDN) for ODISR, designed to expand the sampling range and receptive field. Specifically, the feature extractor in MDDN comprises three parallel branches: a deformable attention mechanism (serving as the dilation=1 path) and two dilated deformable convolutions with dilation rates of 2 and 3. This architecture expands the sampling range to include more distorted patterns across wider areas, generating dense and comprehensive features that effectively capture geometric distortions in ERP images. The representations extracted from these deformable feature extractors are adaptively fused in a multi-level feature fusion module. Furthermore, to reduce computational cost, a low-rank decomposition strategy is applied to dilated deformable convolutions. Extensive experiments on publicly available datasets demonstrate that MDDN outperforms state-of-the-art methods, underscoring its effectiveness and superiority in ODISR.","authors":["Cuixin Yang","Rongkang Dong","Kin-Man Lam","Yuhang Zhang","Guoping Qiu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.11575v2","updated":"2025-12-19T08:22:55Z","published":"2025-12-12T14:03:40Z","title":"In-Context Learning for Seismic Data Processing","summary":"Seismic processing transforms raw data into subsurface images essential for geophysical applications. Traditional methods face challenges, such as noisy data, and manual parameter tuning, among others. Recently deep learning approaches have proposed alternative solutions to some of these problems. However, important challenges of existing deep learning approaches are spatially inconsistent results across neighboring seismic gathers and lack of user-control. We address these limitations by introducing ContextSeisNet, an in-context learning model, to seismic demultiple processing. Our approach conditions predictions on a support set of spatially related example pairs: neighboring common-depth point gathers from the same seismic line and their corresponding labels. This allows the model to learn task-specific processing behavior at inference time by observing how similar gathers should be processed, without any retraining. This method provides both flexibility through user-defined examples and improved lateral consistency across seismic lines. On synthetic data, ContextSeisNet outperforms a U-Net baseline quantitatively and demonstrates enhanced spatial coherence between neighboring gathers. On field data, our model achieves superior lateral consistency compared to both traditional Radon demultiple and the U-Net baseline. Relative to the U-Net, ContextSeisNet also delivers improved near-offset performance and more complete multiple removal. Notably, ContextSeisNet achieves comparable field data performance despite being trained on 90% less data, demonstrating substantial data efficiency. These results establish ContextSeisNet as a practical approach for spatially consistent seismic demultiple with potential applicability to other seismic processing tasks.","authors":["Fabian Fuchs","Mario Ruben Fernandez","Norman Ettrich","Janis Keuper"],"pdf_url":"","comment":"Source code available under https://codeberg.org/fuchsfa/in-context-learning-seismic. In submission to Geophysics"},{"id":"http://arxiv.org/abs/2512.17331v1","updated":"2025-12-19T08:21:23Z","published":"2025-12-19T08:21:23Z","title":"SynergyWarpNet: Attention-Guided Cooperative Warping for Neural Portrait Animation","summary":"Recent advances in neural portrait animation have demonstrated remarked potential for applications in virtual avatars, telepresence, and digital content creation. However, traditional explicit warping approaches often struggle with accurate motion transfer or recovering missing regions, while recent attention-based warping methods, though effective, frequently suffer from high complexity and weak geometric grounding. To address these issues, we propose SynergyWarpNet, an attention-guided cooperative warping framework designed for high-fidelity talking head synthesis. Given a source portrait, a driving image, and a set of reference images, our model progressively refines the animation in three stages. First, an explicit warping module performs coarse spatial alignment between the source and driving image using 3D dense optical flow. Next, a reference-augmented correction module leverages cross-attention across 3D keypoints and texture features from multiple reference images to semantically complete occluded or distorted regions. Finally, a confidence-guided fusion module integrates the warped outputs with spatially-adaptive fusing, using a learned confidence map to balance structural alignment and visual consistency. Comprehensive evaluations on benchmark datasets demonstrate state-of-the-art performance.","authors":["Shihang Li","Zhiqiang Gong","Minming Ye","Yue Gao","Wen Yao"],"pdf_url":"","comment":"Submitted to ICASSP 2026"},{"id":"http://arxiv.org/abs/2503.19041v4","updated":"2025-12-19T08:17:42Z","published":"2025-03-24T18:11:42Z","title":"LookAhead Tuning: Safer Language Models via Partial Answer Previews","summary":"Fine-tuning enables large language models (LLMs) to adapt to specific domains, but often compromises their previously established safety alignment. To mitigate the degradation of model safety during fine-tuning, we introduce LookAhead Tuning, a lightweight and effective data-driven approach that preserves safety during fine-tuning. The method introduces two simple strategies that modify training data by previewing partial answer prefixes, thereby minimizing perturbations to the model's initial token distributions and maintaining its built-in safety mechanisms. Comprehensive experiments demonstrate that LookAhead Tuning effectively maintains model safety without sacrificing robust performance on downstream tasks. Our findings position LookAhead Tuning as a reliable and efficient solution for the safe and effective adaptation of LLMs.","authors":["Kangwei Liu","Mengru Wang","Yujie Luo","Lin Yuan","Mengshu Sun","Lei Liang","Zhiqiang Zhang","Jun Zhou","Bryan Hooi","Shumin Deng"],"pdf_url":"","comment":"WSDM 2026 short"},{"id":"http://arxiv.org/abs/2512.12218v2","updated":"2025-12-19T08:16:24Z","published":"2025-12-13T07:04:42Z","title":"Journey Before Destination: On the importance of Visual Faithfulness in Slow Thinking","summary":"Reasoning-augmented vision language models (VLMs) generate explicit chains of thought that promise greater capability and transparency but also introduce new failure modes: models may reach correct answers via visually unfaithful intermediate steps, or reason faithfully yet fail on the final prediction. Standard evaluations that only measure final-answer accuracy cannot distinguish these behaviors. We introduce the visual faithfulness of reasoning chains as a distinct evaluation dimension, focusing on whether the perception steps of a reasoning chain are grounded in the image. We propose a training- and reference-free framework that decomposes chains into perception versus reasoning steps and uses off-the-shelf VLM judges for step-level faithfulness, additionally verifying this approach through a human meta-evaluation. Building on this metric, we present a lightweight self-reflection procedure that detects and locally regenerates unfaithful perception steps without any training. Across multiple reasoning-trained VLMs and perception-heavy benchmarks, our method reduces Unfaithful Perception Rate while preserving final-answer accuracy, improving the reliability of multimodal reasoning.","authors":["Rheeya Uppaal","Phu Mon Htut","Min Bai","Nikolaos Pappas","Zheng Qi","Sandesh Swamy"],"pdf_url":"","comment":"Preprint"},{"id":"http://arxiv.org/abs/2512.17326v1","updated":"2025-12-19T08:14:58Z","published":"2025-12-19T08:14:58Z","title":"Democratizing Pathology Co-Pilots: An Open Pipeline and Dataset for Whole-Slide Vision-Language Modelling","summary":"Vision-language models (VLMs) have the potential to become co-pilots for pathologists. However, most VLMs either focus on small regions of interest within whole-slide images, provide only static slide-level outputs, or rely on data that is not publicly available, limiting reproducibility. Furthermore, training data containing WSIs paired with detailed clinical reports is scarce, restricting progress toward transparent and generalisable VLMs. We address these limitations with three main contributions. First, we introduce Polysome, a standardised tool for synthetic instruction generation. Second, we apply Polysome to the public HISTAI dataset, generating HISTAI-Instruct, a large whole-slide instruction tuning dataset spanning 24,259 slides and over 1.1 million instruction-response pairs. Finally, we use HISTAI-Instruct to train ANTONI-α, a VLM capable of visual-question answering (VQA). We show that ANTONI-α outperforms MedGemma on WSI-level VQA tasks of tissue identification, neoplasm detection, and differential diagnosis. We also compare the performance of multiple incarnations of ANTONI-α trained with different amounts of data. All methods, data, and code are publicly available.","authors":["Sander Moonemans","Sebastiaan Ram","Frédérique Meeuwsen","Carlijn Lems","Jeroen van der Laak","Geert Litjens","Francesco Ciompi"],"pdf_url":"","comment":"10 pages, 4 figures"},{"id":"http://arxiv.org/abs/2508.13911v2","updated":"2025-12-19T08:13:38Z","published":"2025-08-19T15:10:30Z","title":"PhysGM: Large Physical Gaussian Model for Feed-Forward 4D Synthesis","summary":"Despite advances in physics-based 3D motion synthesis, current methods face key limitations: reliance on pre-reconstructed 3D Gaussian Splatting (3DGS) built from dense multi-view images with time-consuming per-scene optimization; physics integration via either inflexible, hand-specified attributes or unstable, optimization-heavy guidance from video models using Score Distillation Sampling (SDS); and naive concatenation of prebuilt 3DGS with physics modules, which ignores physical information embedded in appearance and yields suboptimal performance. To address these issues, we propose PhysGM, a feed-forward framework that jointly predicts 3D Gaussian representation and physical properties from a single image, enabling immediate simulation and high-fidelity 4D rendering. Unlike slow appearance-agnostic optimization methods, we first pre-train a physics-aware reconstruction model that directly infers both Gaussian and physical parameters. We further refine the model with Direct Preference Optimization (DPO), aligning simulations with the physically plausible reference videos and avoiding the high-cost SDS optimization. To address the absence of a supporting dataset for this task, we propose PhysAssets, a dataset of 50K+ 3D assets annotated with physical properties and corresponding reference videos. Experiments show that PhysGM produces high-fidelity 4D simulations from a single image in one minute, achieving a significant speedup over prior work while delivering realistic renderings.Our project page is at:https://hihixiaolv.github.io/PhysGM.github.io/","authors":["Chunji Lv","Zequn Chen","Donglin Di","Weinan Zhang","Hao Li","Wei Chen","Yinjie Lei","Changsheng Li"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17323v1","updated":"2025-12-19T08:12:20Z","published":"2025-12-19T08:12:20Z","title":"DESSERT: Diffusion-based Event-driven Single-frame Synthesis via Residual Training","summary":"Video frame prediction extrapolates future frames from previous frames, but suffers from prediction errors in dynamic scenes due to the lack of information about the next frame. Event cameras address this limitation by capturing per-pixel brightness changes asynchronously with high temporal resolution. Prior research on event-based video frame prediction has leveraged motion information from event data, often by predicting event-based optical flow and reconstructing frames via pixel warping. However, such approaches introduce holes and blurring when pixel displacement is inaccurate. To overcome this limitation, we propose DESSERT, a diffusion-based event-driven single-frame synthesis framework via residual training. Leveraging a pre-trained Stable Diffusion model, our method is trained on inter-frame residuals to ensure temporal consistency. The training pipeline consists of two stages: (1) an Event-to-Residual Alignment Variational Autoencoder (ER-VAE) that aligns the event frame between anchor and target frames with the corresponding residual, and (2) a diffusion model that denoises the residual latent conditioned on event data. Furthermore, we introduce Diverse-Length Temporal (DLT) augmentation, which improves robustness by training on frame segments of varying temporal lengths. Experimental results demonstrate that our method outperforms existing event-based reconstruction, image-based video frame prediction, event-based video frame prediction, and one-sided event-based video frame interpolation methods, producing sharper and more temporally consistent frame synthesis.","authors":["Jiyun Kong","Jun-Hyuk Kim","Jong-Seok Lee"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17322v1","updated":"2025-12-19T08:09:02Z","published":"2025-12-19T08:09:02Z","title":"Rotterdam artery-vein segmentation (RAV) dataset","summary":"Purpose: To provide a diverse, high-quality dataset of color fundus images (CFIs) with detailed artery-vein (A/V) segmentation annotations, supporting the development and evaluation of machine learning algorithms for vascular analysis in ophthalmology.\n  Methods: CFIs were sampled from the longitudinal Rotterdam Study (RS), encompassing a wide range of ages, devices, and capture conditions. Images were annotated using a custom interface that allowed graders to label arteries, veins, and unknown vessels on separate layers, starting from an initial vessel segmentation mask. Connectivity was explicitly verified and corrected using connected component visualization tools.\n  Results: The dataset includes 1024x1024-pixel PNG images in three modalities: original RGB fundus images, contrast-enhanced versions, and RGB-encoded A/V masks. Image quality varied widely, including challenging samples typically excluded by automated quality assessment systems, but judged to contain valuable vascular information.\n  Conclusion: This dataset offers a rich and heterogeneous source of CFIs with high-quality segmentations. It supports robust benchmarking and training of machine learning models under real-world variability in image quality and acquisition settings.\n  Translational Relevance: By including connectivity-validated A/V masks and diverse image conditions, this dataset enables the development of clinically applicable, generalizable machine learning tools for retinal vascular analysis, potentially improving automated screening and diagnosis of systemic and ocular diseases.","authors":["Jose Vargas Quiros","Bart Liefers","Karin van Garderen","Jeroen Vermeulen","Eyened Reading Center","Caroline Klaver"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17320v1","updated":"2025-12-19T08:08:19Z","published":"2025-12-19T08:08:19Z","title":"EMMA: Concept Erasure Benchmark with Comprehensive Semantic Metrics and Diverse Categories","summary":"The widespread adoption of text-to-image (T2I) generation has raised concerns about privacy, bias, and copyright violations. Concept erasure techniques offer a promising solution by selectively removing undesired concepts from pre-trained models without requiring full retraining. However, these methods are often evaluated on a limited set of concepts, relying on overly simplistic and direct prompts. To test the boundaries of concept erasure techniques, and assess whether they truly remove targeted concepts from model representations, we introduce EMMA, a benchmark that evaluates five key dimensions of concept erasure over 12 metrics. EMMA goes beyond standard metrics like image quality and time efficiency, testing robustness under challenging conditions, including indirect descriptions, visually similar non-target concepts, and potential gender and ethnicity bias, providing a socially aware analysis of method behavior. Using EMMA, we analyze five concept erasure methods across five domains (objects, celebrities, art styles, NSFW, and copyright). Our results show that existing methods struggle with implicit prompts (i.e., generating the erased concept when it is indirectly referenced) and visually similar non-target concepts (i.e., failing to generate non-targeted concepts resembling the erased one), while some amplify gender and ethnicity bias compared to the original model.","authors":["Lu Wei","Yuta Nakashima","Noa Garcia"],"pdf_url":"","comment":"Under review"},{"id":"http://arxiv.org/abs/2512.17319v1","updated":"2025-12-19T08:07:51Z","published":"2025-12-19T08:07:51Z","title":"A Benchmark for Ultra-High-Resolution Remote Sensing MLLMs","summary":"Multimodal large language models (MLLMs) demonstrate strong perception and reasoning performance on existing remote sensing (RS) benchmarks. However, most prior benchmarks rely on low-resolution imagery, and some high-resolution benchmarks suffer from flawed reasoning-task designs. We show that text-only LLMs can perform competitively with multimodal vision-language models on RS reasoning tasks without access to images, revealing a critical mismatch between current benchmarks and the intended evaluation of visual understanding. To enable faithful assessment, we introduce RSHR-Bench, a super-high-resolution benchmark for RS visual understanding and reasoning. RSHR-Bench contains 5,329 full-scene images with a long side of at least 4,000 pixels, with up to about 3 x 10^8 pixels per image, sourced from widely used RS corpora and UAV collections. We design four task families: multiple-choice VQA, open-ended VQA, image captioning, and single-image evaluation. These tasks cover nine perception categories and four reasoning types, supporting multi-turn and multi-image dialog. To reduce reliance on language priors, we apply adversarial filtering with strong LLMs followed by rigorous human verification. Overall, we construct 3,864 VQA tasks, 3,913 image captioning tasks, and 500 fully human-written or verified single-image evaluation VQA pairs. Evaluations across open-source, closed-source, and RS-specific VLMs reveal persistent performance gaps in super-high-resolution scenarios. Code: https://github.com/Yunkaidang/RSHR","authors":["Yunkai Dang","Meiyi Zhu","Donghao Wang","Yizhuo Zhang","Jiacheng Yang","Qi Fan","Yuekun Yang","Wenbin Li","Feng Miao","Yang Gao"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.12284v2","updated":"2025-12-19T08:02:44Z","published":"2025-12-13T11:02:04Z","title":"V-Rex: Real-Time Streaming Video LLM Acceleration via Dynamic KV Cache Retrieval","summary":"Streaming video large language models (LLMs) are increasingly used for real-time multimodal tasks such as video captioning, question answering, conversational agents, and augmented reality. However, these models face fundamental memory and computational challenges because their key-value (KV) caches grow substantially with continuous streaming video input. This process requires an iterative prefill stage, which is a unique feature of streaming video LLMs. Due to its iterative prefill stage, it suffers from significant limitations, including extensive computation, substantial data transfer, and degradation in accuracy. Crucially, this issue is exacerbated for edge deployment, which is the primary target for these models.\n  In this work, we propose V-Rex, the first software-hardware co-designed accelerator that comprehensively addresses both algorithmic and hardware bottlenecks in streaming video LLM inference. At its core, V-Rex introduces ReSV, a training-free dynamic KV cache retrieval algorithm. ReSV exploits temporal and spatial similarity-based token clustering to reduce excessive KV cache memory across video frames. To fully realize these algorithmic benefits, V-Rex offers a compact, low-latency hardware accelerator with a dynamic KV cache retrieval engine (DRE), featuring bit-level and early-exit based computing units. V-Rex achieves unprecedented real-time of 3.9-8.3 FPS and energy-efficient streaming video LLM inference on edge deployment with negligible accuracy loss. While DRE only accounts for 2.2% power and 2.0% area, the system delivers 1.9-19.7x speedup and 3.1-18.5x energy efficiency improvements over AGX Orin GPU. This work is the first to comprehensively tackle KV cache retrieval across algorithms and hardware, enabling real-time streaming video LLM inference on resource-constrained edge devices.","authors":["Donghyuk Kim","Sejeong Yang","Wonjin Shin","Joo-Young Kim"],"pdf_url":"","comment":"14 pages, 20 figures, conference"},{"id":"http://arxiv.org/abs/2512.17313v1","updated":"2025-12-19T07:52:25Z","published":"2025-12-19T07:52:25Z","title":"Auxiliary Descriptive Knowledge for Few-Shot Adaptation of Vision-Language Model","summary":"Despite the impressive zero-shot capabilities of Vision-Language Models (VLMs), they often struggle in downstream tasks with distribution shifts from the pre-training data. Few-Shot Adaptation (FSA-VLM) has emerged as a key solution, typically using Parameter-Efficient Fine-Tuning (PEFT) to adapt models with minimal data. However, these PEFT methods are constrained by their reliance on fixed, handcrafted prompts, which are often insufficient to understand the semantics of classes. While some studies have proposed leveraging image-induced prompts to provide additional clues for classification, they introduce prohibitive computational overhead at inference. Therefore, we introduce Auxiliary Descriptive Knowledge (ADK), a novel framework that efficiently enriches text representations without compromising efficiency. ADK first leverages a Large Language Model to generate a rich set of descriptive prompts for each class offline. These pre-computed features are then deployed in two ways: (1) as Compositional Knowledge, an averaged representation that provides rich semantics, especially beneficial when class names are ambiguous or unfamiliar to the VLM; and (2) as Instance-Specific Knowledge, where a lightweight, non-parametric attention mechanism dynamically selects the most relevant descriptions for a given image. This approach provides two additional types of knowledge alongside the handcrafted prompt, thereby facilitating category distinction across various domains. Also, ADK acts as a parameter-free, plug-and-play component that enhances existing PEFT methods. Extensive experiments demonstrate that ADK consistently boosts the performance of multiple PEFT baselines, setting a new state-of-the-art across various scenarios.","authors":["SuBeen Lee","GilHan Park","WonJun Moon","Hyun Seok Seong","Jae-Pil Heo"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17312v1","updated":"2025-12-19T07:52:23Z","published":"2025-12-19T07:52:23Z","title":"CodeDance: A Dynamic Tool-integrated MLLM for Executable Visual Reasoning","summary":"Recent releases such as o3 highlight human-like \"thinking with images\" reasoning that combines structured tool use with stepwise verification, yet most open-source approaches still rely on text-only chains, rigid visual schemas, or single-step pipelines, limiting flexibility, interpretability, and transferability on complex tasks. We introduce CodeDance, which explores executable code as a general solver for visual reasoning. Unlike fixed-schema calls (e.g., only predicting bounding-box coordinates), CodeDance defines, composes, and executes code to orchestrate multiple tools, compute intermediate results, and render visual artifacts (e.g., boxes, lines, plots) that support transparent, self-checkable reasoning. To guide this process, we introduce a reward for balanced and adaptive tool-call, which balances exploration with efficiency and mitigates tool overuse. Interestingly, beyond the expected capabilities taught by atomic supervision, we empirically observe novel emergent behaviors during RL training: CodeDance demonstrates novel tool invocations, unseen compositions, and cross-task transfer. These behaviors arise without task-specific fine-tuning, suggesting a general and scalable mechanism of executable visual reasoning. Extensive experiments across reasoning benchmarks (e.g., visual search, math, chart QA) show that CodeDance not only consistently outperforms schema-driven and text-only baselines, but also surpasses advanced closed models such as GPT-4o and larger open-source models.","authors":["Qi Song","Honglin Li","Yingchen Yu","Haoyi Zhou","Lin Yang","Song Bai","Qi She","Zilong Huang","Yunqing Zhao"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17306v1","updated":"2025-12-19T07:44:43Z","published":"2025-12-19T07:44:43Z","title":"Deep But Reliable: Advancing Multi-turn Reasoning for Thinking with Images","summary":"Recent advances in large Vision-Language Models (VLMs) have exhibited strong reasoning capabilities on complex visual tasks by thinking with images in their Chain-of-Thought (CoT), which is achieved by actively invoking tools to analyze visual inputs rather than merely perceiving them. However, existing models often struggle to reflect on and correct themselves when attempting incorrect reasoning trajectories. To address this limitation, we propose DRIM, a model that enables deep but reliable multi-turn reasoning when thinking with images in its multimodal CoT. Our pipeline comprises three stages: data construction, cold-start SFT and RL. Based on a high-resolution image dataset, we construct high-difficulty and verifiable visual question-answer pairs, where solving each task requires multi-turn tool calls to reach the correct answer. In the SFT stage, we collect tool trajectories as cold-start data, guiding a multi-turn reasoning pattern. In the RL stage, we introduce redundancy-penalized policy optimization, which incentivizes the model to develop a self-reflective reasoning pattern. The basic idea is to impose judgment on reasoning trajectories and penalize those that produce incorrect answers without sufficient multi-scale exploration. Extensive experiments demonstrate that DRIM achieves superior performance on visual understanding benchmarks.","authors":["Wenhao Yang","Yu Xia","Jinlong Huang","Shiyin Lu","Qing-Guo Chen","Zhao Xu","Weihua Luo","Kaifu Zhang","Yuanyu Wan","Lijun Zhang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17303v1","updated":"2025-12-19T07:36:07Z","published":"2025-12-19T07:36:07Z","title":"EMAG: Self-Rectifying Diffusion Sampling with Exponential Moving Average Guidance","summary":"In diffusion and flow-matching generative models, guidance techniques are widely used to improve sample quality and consistency. Classifier-free guidance (CFG) is the de facto choice in modern systems and achieves this by contrasting conditional and unconditional samples. Recent work explores contrasting negative samples at inference using a weaker model, via strong/weak model pairs, attention-based masking, stochastic block dropping, or perturbations to the self-attention energy landscape. While these strategies refine the generation quality, they still lack reliable control over the granularity or difficulty of the negative samples, and target-layer selection is often fixed. We propose Exponential Moving Average Guidance (EMAG), a training-free mechanism that modifies attention at inference time in diffusion transformers, with a statistics-based, adaptive layer-selection rule. Unlike prior methods, EMAG produces harder, semantically faithful negatives (fine-grained degradations), surfacing difficult failure modes, enabling the denoiser to refine subtle artifacts, boosting the quality and human preference score (HPS) by +0.46 over CFG. We further demonstrate that EMAG naturally composes with advanced guidance techniques, such as APG and CADS, further improving HPS.","authors":["Ankit Yadav","Ta Duc Huy","Lingqiao Liu"],"pdf_url":"","comment":"26 pages"},{"id":"http://arxiv.org/abs/2512.17302v1","updated":"2025-12-19T07:35:09Z","published":"2025-12-19T07:35:09Z","title":"MatLat: Material Latent Space for PBR Texture Generation","summary":"We propose a generative framework for producing high-quality PBR textures on a given 3D mesh. As large-scale PBR texture datasets are scarce, our approach focuses on effectively leveraging the embedding space and diffusion priors of pretrained latent image generative models while learning a material latent space, MatLat, through targeted fine-tuning. Unlike prior methods that freeze the embedding network and thus lead to distribution shifts when encoding additional PBR channels and hinder subsequent diffusion training, we fine-tune the pretrained VAE so that new material channels can be incorporated with minimal latent distribution deviation. We further show that correspondence-aware attention alone is insufficient for cross-view consistency unless the latent-to-image mapping preserves locality. To enforce this locality, we introduce a regularization in the VAE fine-tuning that crops latent patches, decodes them, and aligns the corresponding image regions to maintain strong pixel-latent spatial correspondence. Ablation studies and comparison with previous baselines demonstrate that our framework improves PBR texture fidelity and that each component is critical for achieving state-of-the-art performance.","authors":["Kyeongmin Yeo","Yunhong Min","Jaihoon Kim","Minhyuk Sung"],"pdf_url":"","comment":"Project page: https://matlat-proj.github.io"},{"id":"http://arxiv.org/abs/2510.11176v2","updated":"2025-12-19T07:32:01Z","published":"2025-10-13T09:08:59Z","title":"G2L:From Giga-Scale to Cancer-Specific Large-Scale Pathology Foundation Models via Knowledge Distillation","summary":"Recent studies in pathology foundation models have shown that scaling training data, diversifying cancer types, and increasing model size consistently improve their performance. However, giga-scale foundation models, which are trained on hundreds of thousands of slides covering tens of cancer types and contain billions of parameters, pose significant challenges for practical use due to their tremendous computational costs in both development and deployment. In this work, we present a novel strategy, named the G2L framework, to increase the performance of large-scale foundation models, which consist of only $15\\%$ of the parameters of giga-scale models, to a comparable performance level of giga-scale models in cancer-specific tasks. Our approach applies knowledge distillation, transferring the capabilities of a giga-scale model to a large-scale model, using just 1K pathology slides of a target cancer (e.g., breast, prostate, etc.). The resulting distilled model not only outperformed state-of-the-art models of the same size (i.e., large-scale) across several benchmarks but also, interestingly, surpassed the giga-scale teacher and huge-scale models in some benchmarks. In addition, the distilled model exhibited a higher robustness index, indicating improved resilience to image variations originating from multiple institutions. These findings suggest that the proposed distillation approach for a large-scale model is a data- and parameter-efficient way to achieve giga-scale-level performance for cancer-specific applications without prohibitive computational burden.","authors":["Yesung Cho","Sungmin Lee","Geongyu Lee","Minkyung Lee","Jongbae Park","Dongmyung Shin"],"pdf_url":"","comment":"Accepted in AAAI 2024 workshop in Health Intelligence Special Theme on Foundation Models and AI Agents"},{"id":"http://arxiv.org/abs/2512.17298v1","updated":"2025-12-19T07:27:19Z","published":"2025-12-19T07:27:19Z","title":"ProCache: Constraint-Aware Feature Caching with Selective Computation for Diffusion Transformer Acceleration","summary":"Diffusion Transformers (DiTs) have achieved state-of-the-art performance in generative modeling, yet their high computational cost hinders real-time deployment. While feature caching offers a promising training-free acceleration solution by exploiting temporal redundancy, existing methods suffer from two key limitations: (1) uniform caching intervals fail to align with the non-uniform temporal dynamics of DiT, and (2) naive feature reuse with excessively large caching intervals can lead to severe error accumulation. In this work, we analyze the evolution of DiT features during denoising and reveal that both feature changes and error propagation are highly time- and depth-varying. Motivated by this, we propose ProCache, a training-free dynamic feature caching framework that addresses these issues via two core components: (i) a constraint-aware caching pattern search module that generates non-uniform activation schedules through offline constrained sampling, tailored to the model's temporal characteristics; and (ii) a selective computation module that selectively computes within deep blocks and high-importance tokens for cached segments to mitigate error accumulation with minimal overhead. Extensive experiments on PixArt-alpha and DiT demonstrate that ProCache achieves up to 1.96x and 2.90x acceleration with negligible quality degradation, significantly outperforming prior caching-based methods.","authors":["Fanpu Cao","Yaofo Chen","Zeng You","Wei Luo","Cen Chen"],"pdf_url":"","comment":"Accepted for poster presentation at AAAI 2026"},{"id":"http://arxiv.org/abs/2512.17296v1","updated":"2025-12-19T07:25:13Z","published":"2025-12-19T07:25:13Z","title":"Towards Pixel-Wise Anomaly Location for High-Resolution PCBA \\\\ via Self-Supervised Image Reconstruction","summary":"Automated defect inspection of assembled Printed Circuit Board Assemblies (PCBA) is quite challenging due to the insufficient labeled data, micro-defects with just a few pixels in visually-complex and high-resolution images. To address these challenges, we present HiSIR-Net, a High resolution, Self-supervised Reconstruction framework for pixel-wise PCBA localization. Our design combines two lightweight modules that make this practical on real 4K-resolution boards: (i) a Selective Input-Reconstruction Gate (SIR-Gate) that lets the model decide where to trust reconstruction versus the original input, thereby reducing irrelevant reconstruction artifacts and false alarms; and (ii) a Region-level Optimized Patch Selection (ROPS) scheme with positional cues to select overlapping patch reconstructions coherently across arbitrary resolutions. Organically integrating these mechanisms yields clean, high-resolution anomaly maps with low false positive (FP) rate. To bridge the gap in high-resolution PCBA datasets, we further contribute a self-collected dataset named SIPCBA-500 of 500 images. We conduct extensive experiments on our SIPCBA-500 as well as public benchmarks, demonstrating the superior localization performance of our method while running at practical speed. Full code and dataset will be made available upon acceptance.","authors":["Wuyi Liu","Le Jin","Junxian Yang","Yuanchao Yu","Zishuo Peng","Jinfeng Xu","Xianzhi Li","Jun Zhou"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17292v1","updated":"2025-12-19T07:16:07Z","published":"2025-12-19T07:16:07Z","title":"Vision-Language Model Guided Image Restoration","summary":"Many image restoration (IR) tasks require both pixel-level fidelity and high-level semantic understanding to recover realistic photos with fine-grained details. However, previous approaches often struggle to effectively leverage both the visual and linguistic knowledge. Recent efforts have attempted to incorporate Vision-language models (VLMs), which excel at aligning visual and textual features, into universal IR. Nevertheless, these methods fail to utilize the linguistic priors to ensure semantic coherence during the restoration process. To address this issue, in this paper, we propose the Vision-Language Model Guided Image Restoration (VLMIR) framework, which leverages the rich vision-language priors of VLMs, such as CLIP, to enhance IR performance through improved visual perception and semantic understanding. Our approach consists of two stages: VLM-based feature extraction and diffusion-based image restoration. In the first stage, we extract complementary visual and linguistic representations of input images by condensing the visual perception and high-level semantic priors through VLMs. Specifically, we align the embeddings of captions from low-quality and high-quality images using a cosine similarity loss with LoRA fine-tuning, and employ a degradation predictor to decompose degradation and clean image content embeddings. These complementary visual and textual embeddings are then integrated into a diffusion-based model via cross-attention mechanisms for enhanced restoration. Extensive experiments and ablation studies demonstrate that VLMIR achieves superior performance across both universal and degradation-specific IR tasks, underscoring the critical role of integrated visual and linguistic knowledge from VLMs in advancing image restoration capabilities.","authors":["Cuixin Yang","Rongkang Dong","Kin-Man Lam"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17279v1","updated":"2025-12-19T06:54:30Z","published":"2025-12-19T06:54:30Z","title":"Diagnostic Performance of Universal-Learning Ultrasound AI Across Multiple Organs and Tasks: the UUSIC25 Challenge","summary":"IMPORTANCE: Current ultrasound AI remains fragmented into single-task tools, limiting clinical utility compared to versatile modern ultrasound systems.\n  OBJECTIVE: To evaluate the diagnostic accuracy and efficiency of single general-purpose deep learning models for multi-organ classification and segmentation.\n  DESIGN: The Universal UltraSound Image Challenge 2025 (UUSIC25) involved developing algorithms on 11,644 images (public/private). Evaluation used an independent, multi-center test set of 2,479 images, including data from a center completely unseen during training to assess generalization.\n  OUTCOMES: Diagnostic performance (Dice Similarity Coefficient [DSC]; Area Under the Receiver Operating Characteristic Curve [AUC]) and computational efficiency (inference time, GPU memory).\n  RESULTS: Of 15 valid algorithms, the top model (SMART) achieved a macro-averaged DSC of 0.854 across 5 segmentation tasks and AUC of 0.766 for binary classification. Models showed high capability in segmentation (e.g., fetal head DSC: 0.942) but variability in complex tasks subject to domain shift. Notably, in breast cancer molecular subtyping, the top model's performance dropped from AUC 0.571 (internal) to 0.508 (unseen external center), highlighting generalization challenges.\n  CONCLUSIONS: General-purpose AI models achieve high accuracy and efficiency across multiple tasks using a single architecture. However, performance degradation on unseen data suggests domain generalization is critical for future clinical deployment.","authors":["Zehui Lin","Luyi Han","Xin Wang","Ying Zhou","Yanming Zhang","Tianyu Zhang","Lingyun Bao","Shandong Wu","Dong Xu","Tao Tan","the UUSIC25 Challenge Consortium"],"pdf_url":"","comment":"8 pages, 2 figures. Summary of the UUSIC25 Challenge held at MICCAI 2025. Extensive Supplementary Material (containing original team reports) is available in the \"ancillary files\" section"},{"id":"http://arxiv.org/abs/2512.17278v1","updated":"2025-12-19T06:50:03Z","published":"2025-12-19T06:50:03Z","title":"WDFFU-Mamba: A Wavelet-guided Dual-attention Feature Fusion Mamba for Breast Tumor Segmentation in Ultrasound Images","summary":"Breast ultrasound (BUS) image segmentation plays a vital role in assisting clinical diagnosis and early tumor screening. However, challenges such as speckle noise, imaging artifacts, irregular lesion morphology, and blurred boundaries severely hinder accurate segmentation. To address these challenges, this work aims to design a robust and efficient model capable of automatically segmenting breast tumors in BUS images.We propose a novel segmentation network named WDFFU-Mamba, which integrates wavelet-guided enhancement and dual-attention feature fusion within a U-shaped Mamba architecture. A Wavelet-denoised High-Frequency-guided Feature (WHF) module is employed to enhance low-level representations through noise-suppressed high-frequency cues. A Dual Attention Feature Fusion (DAFF) module is also introduced to effectively merge skip-connected and semantic features, improving contextual consistency.Extensive experiments on two public BUS datasets demonstrate that WDFFU-Mamba achieves superior segmentation accuracy, significantly outperforming existing methods in terms of Dice coefficient and 95th percentile Hausdorff Distance (HD95).The combination of wavelet-domain enhancement and attention-based fusion greatly improves both the accuracy and robustness of BUS image segmentation, while maintaining computational efficiency.The proposed WDFFU-Mamba model not only delivers strong segmentation performance but also exhibits desirable generalization ability across datasets, making it a promising solution for real-world clinical applications in breast tumor ultrasound analysis.","authors":["Guoping Cai","Houjin Chen","Yanfeng Li","Jia Sun","Ziwei Chen","Qingzi Geng"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2506.07475v3","updated":"2025-12-19T06:49:01Z","published":"2025-06-09T06:50:15Z","title":"Text-guided multi-stage cross-perception network for medical image segmentation","summary":"Medical image segmentation plays a crucial role in clinical medicine, serving as a key tool for auxiliary diagnosis, treatment planning, and disease monitoring. However, traditional segmentation methods such as U-Net are often limited by weak semantic expression of target regions, which stems from insufficient generalization and a lack of interactivity. Incorporating text prompts offers a promising avenue to more accurately pinpoint lesion locations, yet existing text-guided methods are still hindered by insufficient cross-modal interaction and inadequate cross-modal feature representation. To address these challenges, we propose the Text-guided Multi-stage Cross-perception network (TMC). TMC incorporates a Multi-stage Cross-attention Module (MCM) to enhance the model's understanding of fine-grained semantic details and a Multi-stage Alignment Loss (MA Loss) to improve the consistency of cross-modal semantics across different feature levels. Experimental results on three public datasets (QaTa-COV19, MosMedData, and Duke-Breast-Cancer-MRI) demonstrate the superior performance of TMC, achieving Dice scores of 84.65\\%, 78.39\\%, and 88.09\\%, respectively, and consistently outperforming both U-Net-based networks and existing text-guided methods.","authors":["Gaoyu Chen","Haixia Pan"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.14121v2","updated":"2025-12-19T06:44:37Z","published":"2025-12-16T06:05:55Z","title":"SportsGPT: An LLM-driven Framework for Interpretable Sports Motion Assessment and Training Guidance","summary":"Existing intelligent sports analysis systems mainly focus on \"scoring and visualization,\" often lacking automatic performance diagnosis and interpretable training guidance. Recent advances in Large Language Models (LLMs) and motion analysis techniques provide new opportunities to address the above limitations. In this paper, we propose SportsGPT, an LLM-driven framework for interpretable sports motion assessment and training guidance, which establishes a closed loop from motion time-series input to professional training guidance. First, given a set of high-quality target models, we introduce MotionDTW, a two-stage time series alignment algorithm designed for accurate keyframe extraction from skeleton-based motion sequences. Subsequently, we design a Knowledge-based Interpretable Sports Motion Assessment Model (KISMAM) to obtain a set of interpretable assessment metrics (e.g., insufficient extension) by contrasting the keyframes with the target models. Finally, we propose SportsRAG, a RAG-based training guidance model built upon Qwen3. Leveraging a 6B-token knowledge base, it prompts the LLM to generate professional training guidance by retrieving domain-specific QA pairs. Experimental results demonstrate that MotionDTW significantly outperforms traditional methods with lower temporal error and higher IoU scores. Furthermore, ablation studies validate the KISMAM and SportsRAG, confirming that SportsGPT surpasses general LLMs in diagnostic accuracy and professionalism.","authors":["Wenbo Tian","Ruting Lin","Hongxian Zheng","Yaodong Yang","Geng Wu","Zihao Zhang","Zhang Zhang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.15126v2","updated":"2025-12-19T06:41:20Z","published":"2025-12-17T06:38:07Z","title":"3DProxyImg: Controllable 3D-Aware Animation Synthesis from Single Image via 2D-3D Aligned Proxy Embedding","summary":"3D animation is central to modern visual media, yet traditional production pipelines remain labor-intensive, expertise-demanding, and computationally expensive. Recent AIGC-based approaches partially automate asset creation and rigging, but they either inherit the heavy costs of full 3D pipelines or rely on video-synthesis paradigms that sacrifice 3D controllability and interactivity. We focus on single-image 3D animation generation and argue that progress is fundamentally constrained by a trade-off between rendering quality and 3D control.\n  To address this limitation, we propose a lightweight 3D animation framework that decouples geometric control from appearance synthesis. The core idea is a 2D-3D aligned proxy representation that uses a coarse 3D estimate as a structural carrier, while delegating high-fidelity appearance and view synthesis to learned image-space generative priors. This proxy formulation enables 3D-aware motion control and interaction comparable to classical pipelines, without requiring accurate geometry or expensive optimization, and naturally extends to coherent background animation. Extensive experiments demonstrate that our method achieves efficient animation generation on low-power platforms and outperforms video-based 3D animation generation in identity preservation, geometric and textural consistency, and the level of precise, interactive control it offers to users.","authors":["Yupeng Zhu","Xiongzhen Zhang","Ye Chen","Bingbing Ni"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17263v1","updated":"2025-12-19T06:27:14Z","published":"2025-12-19T06:27:14Z","title":"AnyCXR: Human Anatomy Segmentation of Chest X-ray at Any Acquisition Position using Multi-stage Domain Randomized Synthetic Data with Imperfect Annotations and Conditional Joint Annotation Regularization Learning","summary":"Robust anatomical segmentation of chest X-rays (CXRs) remains challenging due to the scarcity of comprehensive annotations and the substantial variability of real-world acquisition conditions. We propose AnyCXR, a unified framework that enables generalizable multi-organ segmentation across arbitrary CXR projection angles using only synthetic supervision. The method combines a Multi-stage Domain Randomization (MSDR) engine, which generates over 100,000 anatomically faithful and highly diverse synthetic radiographs from 3D CT volumes, with a Conditional Joint Annotation Regularization (CAR) learning strategy that leverages partial and imperfect labels by enforcing anatomical consistency in a latent space. Trained entirely on synthetic data, AnyCXR achieves strong zero-shot generalization on multiple real-world datasets, providing accurate delineation of 54 anatomical structures in PA, lateral, and oblique views. The resulting segmentation maps support downstream clinical tasks, including automated cardiothoracic ratio estimation, spine curvature assessment, and disease classification, where the incorporation of anatomical priors improves diagnostic performance. These results demonstrate that AnyCXR establishes a scalable and reliable foundation for anatomy-aware CXR analysis and offers a practical pathway toward reducing annotation burdens while improving robustness across diverse imaging conditions.","authors":["Dong Zifei","Wu Wenjie","Hao Jinkui","Chen Tianqi","Weng Ziqiao","Zhou Bo"],"pdf_url":"","comment":"20 pages, 12 figures, Preprint (under review at Medical Image Analysis)"},{"id":"http://arxiv.org/abs/2503.22182v2","updated":"2025-12-19T06:24:33Z","published":"2025-03-28T07:00:33Z","title":"Sell It Before You Make It: Revolutionizing E-Commerce with Personalized AI-Generated Items","summary":"E-commerce has revolutionized retail, yet its traditional workflows remain inefficient, with significant resource costs tied to product design and inventory. This paper introduces a novel system deployed at Alibaba that uses AI-generated items (AIGI) to address these challenges with personalized text-to-image generation for e-commerce product design. AIGI enables an innovative business mode called \"sell it before you make it\", where merchants can design fashion items and generate photorealistic images with digital models based on textual descriptions. Only when the items have received a certain number of orders, do the merchants start to produce them, which largely reduces reliance on physical prototypes and thus accelerates time to market. For such a promising application, we identify the underlying key scientific challenge, i.e., capturing users' group-level personalized preferences towards multiple generated images. To this end, we propose a Personalized Group-Level Preference Alignment Framework for Diffusion Models (PerFusion). We first design PerFusion Reward Model for user preference estimation with a feature-crossing-based personalized plug-in. Then we develop PerFusion with a personalized adaptive network to model diverse preferences across users, and meanwhile derive the group-level preference optimization objective to model comparative behaviors among multiple images. Both offline and online experiments demonstrate the effectiveness of our proposed algorithm. The AI-generated items achieve over 13% relative improvements for both click-through rate and conversion rate, as well as 7.9% decrease in return rate, compared to their human-designed counterparts, validating the transformative potential of AIGI for e-commerce platforms.","authors":["Jianghao Lin","Peng Du","Jiaqi Liu","Weite Li","Yong Yu","Weinan Zhang","Yang Cao"],"pdf_url":"","comment":"Accepted by KDD 2026 ADS Track"},{"id":"http://arxiv.org/abs/2512.17253v1","updated":"2025-12-19T05:52:15Z","published":"2025-12-19T05:52:15Z","title":"Mitty: Diffusion-based Human-to-Robot Video Generation","summary":"Learning directly from human demonstration videos is a key milestone toward scalable and generalizable robot learning. Yet existing methods rely on intermediate representations such as keypoints or trajectories, introducing information loss and cumulative errors that harm temporal and visual consistency. We present Mitty, a Diffusion Transformer that enables video In-Context Learning for end-to-end Human2Robot video generation. Built on a pretrained video diffusion model, Mitty leverages strong visual-temporal priors to translate human demonstrations into robot-execution videos without action labels or intermediate abstractions. Demonstration videos are compressed into condition tokens and fused with robot denoising tokens through bidirectional attention during diffusion. To mitigate paired-data scarcity, we also develop an automatic synthesis pipeline that produces high-quality human-robot pairs from large egocentric datasets. Experiments on Human2Robot and EPIC-Kitchens show that Mitty delivers state-of-the-art results, strong generalization to unseen environments, and new insights for scalable robot learning from human observations.","authors":["Yiren Song","Cheng Liu","Weijia Mao","Mike Zheng Shou"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2508.05899v2","updated":"2025-12-19T05:39:09Z","published":"2025-08-07T23:23:07Z","title":"HOLODECK 2.0: Vision-Language-Guided 3D World Generation with Editing","summary":"3D scene generation plays a crucial role in gaming, artistic creation, virtual reality, and many other domains. However, current 3D scene design still relies heavily on extensive manual effort from creators, and existing automated methods struggle to generate open-domain scenes or support flexible editing. To address those challenges, we introduce HOLODECK 2.0, an advanced vision-language-guided framework for 3D world generation with support for interactive scene editing based on human feedback. HOLODECK 2.0 can generate diverse and stylistically rich 3D scenes (e.g., realistic, cartoon, anime, and cyberpunk styles) that exhibit high semantic fidelity to fine-grained input descriptions, suitable for both indoor and open-domain environments. HOLODECK 2.0 leverages vision-language models (VLMs) to identify and parse the objects required in a scene and generates corresponding high-quality assets via state-of-the-art 3D generative models. Then, HOLODECK 2.0 iteratively applies spatial constraints derived from the VLMs to achieve semantically coherent and physically plausible layouts. Both human and model evaluations demonstrate that HOLODECK 2.0 effectively generates high-quality scenes closely aligned with detailed textual descriptions, consistently outperforming baselines across indoor and open-domain scenarios. Additionally, HOLODECK 2.0 provides editing capabilities that flexibly adapt to human feedback, supporting layout refinement and style-consistent object edits. Finally, we present a practical application of HOLODECK 2.0 in procedural game modeling to generate visually rich and immersive environments that can boost efficiency in game design.","authors":["Zixuan Bian","Ruohan Ren","Yue Yang","Chris Callison-Burch"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2509.09116v3","updated":"2025-12-19T05:20:46Z","published":"2025-09-11T02:53:58Z","title":"Zero-shot Hierarchical Plant Segmentation via Foundation Segmentation Models and Text-to-image Attention","summary":"Foundation segmentation models achieve reasonable leaf instance extraction from top-view crop images without training (i.e., zero-shot). However, segmenting entire plant individuals with each consisting of multiple overlapping leaves remains challenging. This problem is referred to as a hierarchical segmentation task, typically requiring annotated training datasets, which are often species-specific and require notable human labor. To address this, we introduce ZeroPlantSeg, a zero-shot segmentation for rosette-shaped plant individuals from top-view images. We integrate a foundation segmentation model, extracting leaf instances, and a vision-language model, reasoning about plants' structures to extract plant individuals without additional training. Evaluations on datasets with multiple plant species, growth stages, and shooting environments demonstrate that our method surpasses existing zero-shot methods and achieves better cross-domain performance than supervised methods. Implementations are available at https://github.com/JunhaoXing/ZeroPlantSeg.","authors":["Junhao Xing","Ryohei Miyakawa","Yang Yang","Xinpeng Liu","Risa Shinoda","Hiroaki Santo","Yosuke Toda","Fumio Okura"],"pdf_url":"","comment":"WACV 2026 Accepted"},{"id":"http://arxiv.org/abs/2508.07313v3","updated":"2025-12-19T05:13:52Z","published":"2025-08-10T12:03:45Z","title":"DocR1: Evidence Page-Guided GRPO for Multi-Page Document Understanding","summary":"Understanding multi-page documents poses a significant challenge for multimodal large language models (MLLMs), as it requires fine-grained visual comprehension and multi-hop reasoning across pages. While prior work has explored reinforcement learning (RL) for enhancing advanced reasoning in MLLMs, its application to multi-page document understanding remains underexplored. In this paper, we introduce DocR1, an MLLM trained with a novel RL framework, Evidence Page-Guided GRPO (EviGRPO). EviGRPO incorporates an evidence-aware reward mechanism that promotes a coarse-to-fine reasoning strategy, guiding the model to first retrieve relevant pages before generating answers. This training paradigm enables us to build high-quality models with limited supervision. To support this, we design a two-stage annotation pipeline and a curriculum learning strategy, based on which we construct two datasets: EviBench, a high-quality training set with 4.8k examples, and ArxivFullQA, an evaluation benchmark with 8.6k QA pairs based on scientific papers. Extensive experiments across a wide range of benchmarks demonstrate that DocR1 achieves state-of-the-art performance on multi-page tasks, while consistently maintaining strong results on single-page benchmarks.","authors":["Junyu Xiong","Yonghui Wang","Weichao Zhao","Chenyu Liu","Bing Yin","Wengang Zhou","Houqiang Li"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.15110v2","updated":"2025-12-19T05:02:31Z","published":"2025-12-17T06:02:25Z","title":"Is Nano Banana Pro a Low-Level Vision All-Rounder? A Comprehensive Evaluation on 14 Tasks and 40 Datasets","summary":"The rapid evolution of text-to-image generation models has revolutionized visual content creation. While commercial products like Nano Banana Pro have garnered significant attention, their potential as generalist solvers for traditional low-level vision challenges remains largely underexplored. In this study, we investigate the critical question: Is Nano Banana Pro a Low-Level Vision All-Rounder? We conducted a comprehensive zero-shot evaluation across 14 distinct low-level tasks spanning 40 diverse datasets. By utilizing simple textual prompts without fine-tuning, we benchmarked Nano Banana Pro against state-of-the-art specialist models. Our extensive analysis reveals a distinct performance dichotomy: while \\textbf{Nano Banana Pro demonstrates superior subjective visual quality}, often hallucinating plausible high-frequency details that surpass specialist models, it lags behind in traditional reference-based quantitative metrics. We attribute this discrepancy to the inherent stochasticity of generative models, which struggle to maintain the strict pixel-level consistency required by conventional metrics. This report identifies Nano Banana Pro as a capable zero-shot contender for low-level vision tasks, while highlighting that achieving the high fidelity of domain specialists remains a significant hurdle.","authors":["Jialong Zuo","Haoyou Deng","Hanyu Zhou","Jiaxin Zhu","Yicheng Zhang","Yiwei Zhang","Yongxin Yan","Kaixing Huang","Weisen Chen","Yongtai Deng","Rui Jin","Nong Sang","Changxin Gao"],"pdf_url":"","comment":"Technical Report; 65 Pages, 36 Figures, 17 Tables; Poject Page: https://lowlevelbanana.github.io/"},{"id":"http://arxiv.org/abs/2512.17229v1","updated":"2025-12-19T04:29:07Z","published":"2025-12-19T04:29:07Z","title":"Video Detective: Seek Critical Clues Recurrently to Answer Question from Long Videos","summary":"Long Video Question-Answering (LVQA) presents a significant challenge for Multi-modal Large Language Models (MLLMs) due to immense context and overloaded information, which could also lead to prohibitive memory consumption. While existing methods attempt to address these issues by reducing visual tokens or extending model's context length, they may miss useful information or take considerable computation. In fact, when answering given questions, only a small amount of crucial information is required. Therefore, we propose an efficient question-aware memory mechanism, enabling MLLMs to recurrently seek these critical clues. Our approach, named VideoDetective, simplifies this task by iteratively processing video sub-segments. For each sub-segment, a question-aware compression strategy is employed by introducing a few special memory tokens to achieve purposefully compression. This allows models to effectively seek critical clues while reducing visual tokens. Then, due to history context could have a significant impact, we recurrently aggregate and store these memory tokens to update history context, which would be reused for subsequent sub-segments. Furthermore, to more effectively measure model's long video understanding ability, we introduce GLVC (Grounding Long Video Clues), a long video question-answering dataset, which features grounding critical and concrete clues scattered throughout entire videos. Experimental results demonstrate our method enables MLLMs with limited context length of 32K to efficiently process 100K tokens (3600 frames, an hour-long video sampled at 1fps), requiring only 2 minutes and 37GB GPU memory usage. Evaluation results across multiple long video benchmarks illustrate our method can more effectively seek critical clues from massive information.","authors":["Henghui Du","Chang Zhou","Chunjie Zhang","Xi Chen","Di Hu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17227v1","updated":"2025-12-19T04:25:57Z","published":"2025-12-19T04:25:57Z","title":"Learning When to Look: A Disentangled Curriculum for Strategic Perception in Multimodal Reasoning","summary":"Multimodal Large Language Models (MLLMs) demonstrate significant potential but remain brittle in complex, long-chain visual reasoning tasks. A critical failure mode is \"visual forgetting\", where models progressively lose visual grounding as reasoning extends, a phenomenon aptly described as \"think longer, see less\". We posit this failure stems from current training paradigms prematurely entangling two distinct cognitive skills: (1) abstract logical reasoning \"how-to-think\") and (2) strategic visual perception (\"when-to-look\"). This creates a foundational cold-start deficiency -- weakening abstract reasoning -- and a strategic perception deficit, as models lack a policy for when to perceive. In this paper, we propose a novel curriculum-based framework to disentangle these skills. First, we introduce a disentangled Supervised Fine-Tuning (SFT) curriculum that builds a robust abstract reasoning backbone on text-only data before anchoring it to vision with a novel Perception-Grounded Chain-of-Thought (PG-CoT) paradigm. Second, we resolve the strategic perception deficit by formulating timing as a reinforcement learning problem. We design a Pivotal Perception Reward that teaches the model when to look by coupling perceptual actions to linguistic markers of cognitive uncertainty (e.g., \"wait\", \"verify\"), thereby learning an autonomous grounding policy. Our contributions include the formalization of these two deficiencies and the development of a principled, two-stage framework to address them, transforming the model from a heuristic-driven observer to a strategic, grounded reasoner. \\textbf{Code}: \\url{https://github.com/gaozilve-max/learning-when-to-look}.","authors":["Siqi Yang","Zilve Gao","Haibo Qiu","Fanfan Liu","Peng Shi","Zhixiong Zeng","Qingmin Liao","Lin Ma"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17226v1","updated":"2025-12-19T04:24:03Z","published":"2025-12-19T04:24:03Z","title":"Robust Scene Coordinate Regression via Geometrically-Consistent Global Descriptors","summary":"Recent learning-based visual localization methods use global descriptors to disambiguate visually similar places, but existing approaches often derive these descriptors from geometric cues alone (e.g., covisibility graphs), limiting their discriminative power and reducing robustness in the presence of noisy geometric constraints. We propose an aggregator module that learns global descriptors consistent with both geometrical structure and visual similarity, ensuring that images are close in descriptor space only when they are visually similar and spatially connected. This corrects erroneous associations caused by unreliable overlap scores. Using a batch-mining strategy based solely on the overlap scores and a modified contrastive loss, our method trains without manual place labels and generalizes across diverse environments. Experiments on challenging benchmarks show substantial localization gains in large-scale environments while preserving computational and memory efficiency. Code is available at \\href{https://github.com/sontung/robust\\_scr}{github.com/sontung/robust\\_scr}.","authors":["Son Tung Nguyen","Tobias Fischer","Alejandro Fontan","Michael Milford"],"pdf_url":"","comment":"WACV 2026 conference paper"},{"id":"http://arxiv.org/abs/2512.17224v1","updated":"2025-12-19T04:21:01Z","published":"2025-12-19T04:21:01Z","title":"Any-Optical-Model: A Universal Foundation Model for Optical Remote Sensing","summary":"Optical satellites, with their diverse band layouts and ground sampling distances, supply indispensable evidence for tasks ranging from ecosystem surveillance to emergency response. However, significant discrepancies in band composition and spatial resolution across different optical sensors present major challenges for existing Remote Sensing Foundation Models (RSFMs). These models are typically pretrained on fixed band configurations and resolutions, making them vulnerable to real world scenarios involving missing bands, cross sensor fusion, and unseen spatial scales, thereby limiting their generalization and practical deployment. To address these limitations, we propose Any Optical Model (AOM), a universal RSFM explicitly designed to accommodate arbitrary band compositions, sensor types, and resolution scales. To preserve distinctive spectral characteristics even when bands are missing or newly introduced, AOM introduces a spectrum-independent tokenizer that assigns each channel a dedicated band embedding, enabling explicit encoding of spectral identity. To effectively capture texture and contextual patterns from sub-meter to hundred-meter imagery, we design a multi-scale adaptive patch embedding mechanism that dynamically modulates the receptive field. Furthermore, to maintain global semantic consistency across varying resolutions, AOM incorporates a multi-scale semantic alignment mechanism alongside a channel-wise self-supervised masking and reconstruction pretraining strategy that jointly models spectral-spatial relationships. Extensive experiments on over 10 public datasets, including those from Sentinel-2, Landsat, and HLS, demonstrate that AOM consistently achieves state-of-the-art (SOTA) performance under challenging conditions such as band missing, cross sensor, and cross resolution settings.","authors":["Xuyang Li","Chenyu Li","Danfeng Hong"],"pdf_url":"","comment":"Accepted by AAAI2026"},{"id":"http://arxiv.org/abs/2512.17221v1","updated":"2025-12-19T04:09:24Z","published":"2025-12-19T04:09:24Z","title":"DAVE: A VLM Vision Encoder for Document Understanding and Web Agents","summary":"While Vision-language models (VLMs) have demonstrated remarkable performance across multi-modal tasks, their choice of vision encoders presents a fundamental weakness: their low-level features lack the robust structural and spatial information essential for document understanding and web agents. To bridge this gap, we introduce DAVE, a vision encoder purpose-built for VLMs and tailored for these tasks. Our training pipeline is designed to leverage abundant unlabeled data to bypass the need for costly large-scale annotations for document and web images. We begin with a self-supervised pretraining stage on unlabeled images, followed by a supervised autoregressive pretraining stage, where the model learns tasks like parsing and localization from limited, high-quality data. Within the supervised stage, we adopt two strategies to improve our encoder's alignment with both general visual knowledge and diverse document and web agentic tasks: (i) We introduce a novel model-merging scheme, combining encoders trained with different text decoders to ensure broad compatibility with different web agentic architectures. (ii) We use ensemble training to fuse features from pretrained generalist encoders (e.g., SigLIP2) with our own document and web-specific representations. Extensive experiments on classic document tasks, VQAs, web localization, and agent-based benchmarks validate the effectiveness of our approach, establishing DAVE as a strong vision encoder for document and web applications.","authors":["Brandon Huang","Hang Hua","Zhuoran Yu","Trevor Darrell","Rogerio Feris","Roei Herzig"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17213v1","updated":"2025-12-19T03:50:42Z","published":"2025-12-19T03:50:42Z","title":"CheXPO-v2: Preference Optimization for Chest X-ray VLMs with Knowledge Graph Consistency","summary":"Medical Vision-Language Models (VLMs) are prone to hallucinations, compromising clinical reliability. While reinforcement learning methods like Group Relative Policy Optimization (GRPO) offer a low-cost alignment solution, their reliance on sparse, outcome-based rewards inadvertently encourages models to \"overthink\" -- generating verbose, convoluted, and unverifiable Chain-of-Thought reasoning to justify answers. This focus on outcomes obscures factual errors and poses significant safety risks. To address this, we propose CheXPO-v2, a novel alignment framework that shifts from outcome to process supervision. Our core innovation is a Knowledge Graph Consistency Reward mechanism driven by Entity-Relation Matching. By explicitly parsing reasoning steps into structured \"Disease, Relation, Anatomy\" triplets, we provide fine-grained supervision that penalizes incoherent logic and hallucinations at the atomic level. Integrating this with a hard-example mining strategy, our approach significantly outperforms GRPO and state-of-the-art models on benchmarks like MIMIC-CXR-VQA. Crucially, CheXPO-v2 achieves new state-of-the-art accuracy using only 5k samples, demonstrating exceptional data efficiency while producing clinically sound and verifiable reasoning. The project source code is publicly available at: https://github.com/ecoxial2007/CheX-Phi4MM.","authors":["Xiao Liang","Yuxuan An","Di Wang","Jiawei Hu","Zhicheng Jiao","Bin Jing","Quan Wang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.07984v2","updated":"2025-12-19T03:34:12Z","published":"2025-12-08T19:15:08Z","title":"Restrictive Hierarchical Semantic Segmentation for Stratified Tooth Layer Detection","summary":"Accurate understanding of anatomical structures is essential for reliably staging certain dental diseases. A way of introducing this within semantic segmentation models is by utilising hierarchy-aware methodologies. However, existing hierarchy-aware segmentation methods largely encode anatomical structure through the loss functions, providing weak and indirect supervision. We introduce a general framework that embeds an explicit anatomical hierarchy into semantic segmentation by coupling a recurrent, level-wise prediction scheme with restrictive output heads and top-down feature conditioning. At each depth of the class tree, the backbone is re-run on the original image concatenated with logits from the previous level. Child class features are conditioned using Feature-wise Linear Modulation of their parent class probabilities, to modulate child feature spaces for fine grained detection. A probabilistic composition rule enforces consistency between parent and descendant classes. Hierarchical loss combines per-level class weighted Dice and cross entropy loss and a consistency term loss, ensuring parent predictions are the sum of their children. We validate our approach on our proposed dataset, TL-pano, containing 194 panoramic radiographs with dense instance and semantic segmentation annotations, of tooth layers and alveolar bone. Utilising UNet and HRNet as donor models across a 5-fold cross validation scheme, the hierarchical variants consistently increase IoU, Dice, and recall, particularly for fine-grained anatomies, and produce more anatomically coherent masks. However, hierarchical variants also demonstrated increased recall over precision, implying increased false positives. The results demonstrate that explicit hierarchical structuring improves both performance and clinical plausibility, especially in low data dental imaging regimes.","authors":["Ryan Banks","Camila Lindoni Azevedo","Hongying Tang","Yunpeng Li"],"pdf_url":"","comment":"13 pages, 7 figures, 3 tables"},{"id":"http://arxiv.org/abs/2512.17206v1","updated":"2025-12-19T03:32:53Z","published":"2025-12-19T03:32:53Z","title":"Reasoning Palette: Modulating Reasoning via Latent Contextualization for Controllable Exploration for (V)LMs","summary":"Exploration capacity shapes both inference-time performance and reinforcement learning (RL) training for large (vision-) language models, as stochastic sampling often yields redundant reasoning paths with little high-level diversity. This paper proposes Reasoning Palette, a novel latent-modulation framework that endows the model with a stochastic latent variable for strategic contextualization, guiding its internal planning prior to token generation. This latent context is inferred from the mean-pooled embedding of a question-answer pair via a variational autoencoder (VAE), where each sampled latent potentially encodes a distinct reasoning context. During inference, a sampled latent is decoded into learnable token prefixes and prepended to the input prompt, modulating the model's internal reasoning trajectory. In this way, the model performs internal sampling over reasoning strategies prior to output generation, which shapes the style and structure of the entire response sequence. A brief supervised fine-tuning (SFT) warm-up phase allows the model to adapt to this latent conditioning. Within RL optimization, Reasoning Palette facilitates structured exploration by enabling on-demand injection for diverse reasoning modes, significantly enhancing exploration efficiency and sustained learning capability. Experiments across multiple reasoning benchmarks demonstrate that our method enables interpretable and controllable control over the (vision-) language model's strategic behavior, thereby achieving consistent performance gains over standard RL methods.","authors":["Rujiao Long","Yang Li","Xingyao Zhang","Weixun Wang","Tianqianjin Lin","Xi Zhao","Yuchi Xu","Wenbo Su","Junchi Yan","Bo Zheng"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17202v1","updated":"2025-12-19T03:28:39Z","published":"2025-12-19T03:28:39Z","title":"Fose: Fusion of One-Step Diffusion and End-to-End Network for Pansharpening","summary":"Pansharpening is a significant image fusion task that fuses low-resolution multispectral images (LRMSI) and high-resolution panchromatic images (PAN) to obtain high-resolution multispectral images (HRMSI). The development of the diffusion models (DM) and the end-to-end models (E2E model) has greatly improved the frontier of pansharping. DM takes the multi-step diffusion to obtain an accurate estimation of the residual between LRMSI and HRMSI. However, the multi-step process takes large computational power and is time-consuming. As for E2E models, their performance is still limited by the lack of prior and simple structure. In this paper, we propose a novel four-stage training strategy to obtain a lightweight network Fose, which fuses one-step DM and an E2E model. We perform one-step distillation on an enhanced SOTA DM for pansharping to compress the inference process from 50 steps to only 1 step. Then we fuse the E2E model with one-step DM with lightweight ensemble blocks. Comprehensive experiments are conducted to demonstrate the significant improvement of the proposed Fose on three commonly used benchmarks. Moreover, we achieve a 7.42 speedup ratio compared to the baseline DM while achieving much better performance. The code and model are released at https://github.com/Kai-Liu001/Fose.","authors":["Kai Liu","Zeli Lin","Weibo Wang","Linghe Kong","Yulun Zhang"],"pdf_url":"","comment":"Code link: https://github.com/Kai-Liu001/Fose"},{"id":"http://arxiv.org/abs/2411.15355v3","updated":"2025-12-19T03:24:39Z","published":"2024-11-22T21:59:46Z","title":"UniGaussian: Driving Scene Reconstruction from Multiple Camera Models via Unified Gaussian Representations","summary":"Urban scene reconstruction is crucial for real-world autonomous driving simulators. Although existing methods have achieved photorealistic reconstruction, they mostly focus on pinhole cameras and neglect fisheye cameras. In fact, how to effectively simulate fisheye cameras in driving scene remains an unsolved problem. In this work, we propose UniGaussian, a novel approach that learns a unified 3D Gaussian representation from multiple camera models for urban scene reconstruction in autonomous driving. Our contributions are two-fold. First, we propose a new differentiable rendering method that distorts 3D Gaussians using a series of affine transformations tailored to fisheye camera models. This addresses the compatibility issue of 3D Gaussian splatting with fisheye cameras, which is hindered by light ray distortion caused by lenses or mirrors. Besides, our method maintains real-time rendering while ensuring differentiability. Second, built on the differentiable rendering method, we design a new framework that learns a unified Gaussian representation from multiple camera models. By applying affine transformations to adapt different camera models and regularizing the shared Gaussians with supervision from different modalities, our framework learns a unified 3D Gaussian representation with input data from multiple sources and achieves holistic driving scene understanding. As a result, our approach models multiple sensors (pinhole and fisheye cameras) and modalities (depth, semantic, normal and LiDAR point clouds). Our experiments show that our method achieves superior rendering quality and fast rendering speed for driving scene simulation.","authors":["Yuan Ren","Guile Wu","Runhao Li","Zheyuan Yang","Yibo Liu","Xingxin Chen","Tongtong Cao","Bingbing Liu"],"pdf_url":"","comment":"3DV 2026"},{"id":"http://arxiv.org/abs/2512.17189v1","updated":"2025-12-19T03:11:20Z","published":"2025-12-19T03:11:20Z","title":"Anatomical Region-Guided Contrastive Decoding: A Plug-and-Play Strategy for Mitigating Hallucinations in Medical VLMs","summary":"Medical Vision-Language Models (MedVLMs) show immense promise in clinical applicability. However, their reliability is hindered by hallucinations, where models often fail to derive answers from visual evidence, instead relying on learned textual priors. Existing mitigation strategies for MedVLMs have distinct limitations: training-based methods rely on costly expert annotations, limiting scalability, while training-free interventions like contrastive decoding, though data-efficient, apply a global, untargeted correction whose effects in complex real-world clinical settings can be unreliable. To address these challenges, we introduce Anatomical Region-Guided Contrastive Decoding (ARCD), a plug-and-play strategy that mitigates hallucinations by providing targeted, region-specific guidance. Our module leverages an anatomical mask to direct a three-tiered contrastive decoding process. By dynamically re-weighting at the token, attention, and logits levels, it verifiably steers the model's focus onto specified regions, reinforcing anatomical understanding and suppressing factually incorrect outputs. Extensive experiments across diverse datasets, including chest X-ray, CT, brain MRI, and ocular ultrasound, demonstrate our method's effectiveness in improving regional understanding, reducing hallucinations, and enhancing overall diagnostic accuracy.","authors":["Xiao Liang","Chenxi Liu","Zhi Ma","Di Wang","Bin Jing","Quan Wang","Yuanyuan Shi"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17188v1","updated":"2025-12-19T03:10:14Z","published":"2025-12-19T03:10:14Z","title":"Globally Optimal Solution to the Generalized Relative Pose Estimation Problem using Affine Correspondences","summary":"Mobile devices equipped with a multi-camera system and an inertial measurement unit (IMU) are widely used nowadays, such as self-driving cars. The task of relative pose estimation using visual and inertial information has important applications in various fields. To improve the accuracy of relative pose estimation of multi-camera systems, we propose a globally optimal solver using affine correspondences to estimate the generalized relative pose with a known vertical direction. First, a cost function about the relative rotation angle is established after decoupling the rotation matrix and translation vector, which minimizes the algebraic error of geometric constraints from affine correspondences. Then, the global optimization problem is converted into two polynomials with two unknowns based on the characteristic equation and its first derivative is zero. Finally, the relative rotation angle can be solved using the polynomial eigenvalue solver, and the translation vector can be obtained from the eigenvector. Besides, a new linear solution is proposed when the relative rotation is small. The proposed solver is evaluated on synthetic data and real-world datasets. The experiment results demonstrate that our method outperforms comparable state-of-the-art methods in accuracy.","authors":["Zhenbao Yu","Banglei Guan","Shunkun Liang","Zibin Liu","Yang Shang","Qifeng Yu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2502.01890v4","updated":"2025-12-19T03:06:57Z","published":"2025-02-03T23:47:45Z","title":"3D Cell Oversegmentation Correction via Geo-Wasserstein Divergence","summary":"3D cell segmentation methods are often hindered by \\emph{oversegmentation}, where a single cell is incorrectly split into multiple fragments. This degrades the final segmentation quality and is notoriously difficult to resolve, as oversegmentation errors often resemble natural gaps between adjacent cells. Our work makes two key contributions. First, for 3D cell segmentation, we are the first work to formulate oversegmentation as a concrete problem and propose a geometric framework to identify and correct these errors. Our approach builds a pre-trained classifier using both 2D geometric and 3D topological features extracted from flawed 3D segmentation results. Second, we introduce a novel metric, Geo-Wasserstein divergence, to quantify changes in 2D geometries. This captures the evolving trends of cell mask shape in a geometry-aware manner. We validate our method through extensive experiments on in-domain plant datasets, including both synthesized and real oversegmented cases, as well as on out-of-domain animal datasets to demonstrate transfer learning performance. An ablation study further highlights the contribution of the Geo-Wasserstein divergence. A clear pipeline is provided for end-users to build pre-trained models to any labeled dataset.","authors":["Peter Chen","Bryan Chang","Olivia A Creasey","Julie Beth Sneddon","Zev J Gartner","Yining Liu"],"pdf_url":"","comment":"Accepted to WACV 2026"},{"id":"http://arxiv.org/abs/2512.17186v1","updated":"2025-12-19T03:01:40Z","published":"2025-12-19T03:01:40Z","title":"It is not always greener on the other side: Greenery perception across demographics and personalities in multiple cities","summary":"Quantifying and assessing urban greenery is consequential for planning and development, reflecting the everlasting importance of green spaces for multiple climate and well-being dimensions of cities. Evaluation can be broadly grouped into objective (e.g., measuring the amount of greenery) and subjective (e.g., polling the perception of people) approaches, which may differ -- what people see and feel about how green a place is might not match the measurements of the actual amount of vegetation. In this work, we advance the state of the art by measuring such differences and explaining them through human, geographic, and spatial dimensions. The experiments rely on contextual information extracted from street view imagery and a comprehensive urban visual perception survey collected from 1,000 people across five countries with their extensive demographic and personality information. We analyze the discrepancies between objective measures (e.g., Green View Index (GVI)) and subjective scores (e.g., pairwise ratings), examining whether they can be explained by a variety of human and visual factors such as age group and spatial variation of greenery in the scene. The findings reveal that such discrepancies are comparable around the world and that demographics and personality do not play a significant role in perception. Further, while perceived and measured greenery correlate consistently across geographies (both where people and where imagery are from), where people live plays a significant role in explaining perceptual differences, with these two, as the top among seven, features that influences perceived greenery the most. This location influence suggests that cultural, environmental, and experiential factors substantially shape how individuals observe greenery in cities.","authors":["Matias Quintana","Fangqi Liu","Jussi Torkko","Youlong Gu","Xiucheng Liang","Yujun Hou","Koichi Ito","Yihan Zhu","Mahmoud Abdelrahman","Tuuli Toivonen","Yi Lu","Filip Biljecki"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2511.14014v3","updated":"2025-12-19T02:55:25Z","published":"2025-11-18T00:42:41Z","title":"CD-DPE: Dual-Prompt Expert Network Based on Convolutional Dictionary Feature Decoupling for Multi-Contrast MRI Super-Resolution","summary":"Multi-contrast magnetic resonance imaging (MRI) super-resolution intends to reconstruct high-resolution (HR) images from low-resolution (LR) scans by leveraging structural information present in HR reference images acquired with different contrasts. This technique enhances anatomical detail and soft tissue differentiation, which is vital for early diagnosis and clinical decision-making. However, inherent contrasts disparities between modalities pose fundamental challenges in effectively utilizing reference image textures to guide target image reconstruction, often resulting in suboptimal feature integration. To address this issue, we propose a dual-prompt expert network based on a convolutional dictionary feature decoupling (CD-DPE) strategy for multi-contrast MRI super-resolution. Specifically, we introduce an iterative convolutional dictionary feature decoupling module (CD-FDM) to separate features into cross-contrast and intra-contrast components, thereby reducing redundancy and interference. To fully integrate these features, a novel dual-prompt feature fusion expert module (DP-FFEM) is proposed. This module uses a frequency prompt to guide the selection of relevant reference features for incorporation into the target image, while an adaptive routing prompt determines the optimal method for fusing reference and target features to enhance reconstruction quality. Extensive experiments on public multi-contrast MRI datasets demonstrate that CD-DPE outperforms state-of-the-art methods in reconstructing fine details. Additionally, experiments on unseen datasets demonstrated that CD-DPE exhibits strong generalization capabilities.","authors":["Xianming Gu","Lihui Wang","Ying Cao","Zeyu Deng","Yingfeng Ou","Guodong Hu","Yi Chen"],"pdf_url":"","comment":"Accepted to AAAI-2026"},{"id":"http://arxiv.org/abs/2512.17178v1","updated":"2025-12-19T02:36:51Z","published":"2025-12-19T02:36:51Z","title":"ABE-CLIP: Training-Free Attribute Binding Enhancement for Compositional Image-Text Matching","summary":"Contrastive Language-Image Pretraining (CLIP) has achieved remarkable performance in various multimodal tasks. However, it still struggles with compositional image-text matching, particularly in accurately associating objects with their corresponding attributes, because its inherent global representation often overlooks fine-grained semantics for attribute binding. Existing methods often require additional training or extensive hard negative sampling, yet they frequently show limited generalization to novel compositional concepts and fail to fundamentally address the drawbacks of global representations. In this paper, we propose ABE-CLIP, a novel training-free Attribute Binding Enhancement method designed to strengthen attribute-object binding in CLIP-like models. Specifically, we employ a Semantic Refinement Mechanism to refine token embeddings for both object and attribute phrases in the text, thereby mitigating attribute confusion and improving semantic precision. We further introduce a Local Token-Patch Alignment strategy that computes similarity scores between refined textual tokens and their most relevant image patches. By aggregating localized similarity scores, ABE-CLIP computes the final image-text similarity. Experiments on multiple datasets demonstrate that ABE-CLIP significantly improves attribute-object binding performance, even surpassing methods that require extensive training.","authors":["Qi Zhang","Yuxu Chen","Lei Deng","Lili Shen"],"pdf_url":"","comment":"10 pages, 8 figures"},{"id":"http://arxiv.org/abs/2511.21029v2","updated":"2025-12-19T02:29:28Z","published":"2025-11-26T03:53:10Z","title":"FlowerDance: MeanFlow for Efficient and Refined 3D Dance Generation","summary":"Music-to-dance generation aims to translate auditory signals into expressive human motion, with broad applications in virtual reality, choreography, and digital entertainment. Despite promising progress, the limited generation efficiency of existing methods leaves insufficient computational headroom for high-fidelity 3D rendering, thereby constraining the expressiveness of 3D characters during real-world applications. Thus, we propose FlowerDance, which not only generates refined motion with physical plausibility and artistic expressiveness, but also achieves significant generation efficiency on inference speed and memory utilization. Specifically, FlowerDance combines MeanFlow with Physical Consistency Constraints, which enables high-quality motion generation with only a few sampling steps. Moreover, FlowerDance leverages a simple but efficient model architecture with BiMamba-based backbone and Channel-Level Cross-Modal Fusion, which generates dance with efficient non-autoregressive manner. Meanwhile, FlowerDance supports motion editing, enabling users to interactively refine dance sequences. Extensive experiments on AIST++ and FineDance show that FlowerDance achieves state-of-the-art results in both motion quality and generation efficiency. Code will be released upon acceptance. Project page: https://flowerdance25.github.io/ .","authors":["Kaixing Yang","Xulong Tang","Ziqiao Peng","Xiangyue Zhang","Puwei Wang","Jun He","Hongyan Liu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17160v1","updated":"2025-12-19T01:39:43Z","published":"2025-12-19T01:39:43Z","title":"Can Synthetic Images Serve as Effective and Efficient Class Prototypes?","summary":"Vision-Language Models (VLMs) have shown strong performance in zero-shot image classification tasks. However, existing methods, including Contrastive Language-Image Pre-training (CLIP), all rely on annotated text-to-image pairs for aligning visual and textual modalities. This dependency introduces substantial cost and accuracy requirement in preparing high-quality datasets. At the same time, processing data from two modes also requires dual-tower encoders for most models, which also hinders their lightweight. To address these limitations, we introduce a ``Contrastive Language-Image Pre-training via Large-Language-Model-based Generation (LGCLIP)\" framework. LGCLIP leverages a Large Language Model (LLM) to generate class-specific prompts that guide a diffusion model in synthesizing reference images. Afterwards these generated images serve as visual prototypes, and the visual features of real images are extracted and compared with the visual features of these prototypes to achieve comparative prediction. By optimizing prompt generation through the LLM and employing only a visual encoder, LGCLIP remains lightweight and efficient. Crucially, our framework requires only class labels as input during whole experimental procedure, eliminating the need for manually annotated image-text pairs and extra pre-processing. Experimental results validate the feasibility and efficiency of LGCLIP, demonstrating great performance in zero-shot classification tasks and establishing a novel paradigm for classification.","authors":["Dianxing Shi","Dingjie Fu","Yuqiao Liu","Jun Wang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2403.12019v3","updated":"2025-12-19T01:34:21Z","published":"2024-03-18T17:54:34Z","title":"LN3DIFF++: Scalable Latent Neural Fields Diffusion for Speedy 3D Generation","summary":"The field of neural rendering has witnessed significant progress with advancements in generative models and differentiable rendering techniques. Though 2D diffusion has achieved success, a unified 3D diffusion pipeline remains unsettled. This paper introduces a novel framework called LN3Diff++ to address this gap and enable fast, high-quality, and generic conditional 3D generation. Our approach harnesses a 3D-aware architecture and variational autoencoder (VAE) to encode the input image into a structured, compact, and 3D latent space. The latent is decoded by a transformer-based decoder into a high-capacity 3D neural field. Through training a diffusion model on this 3D-aware latent space, our method achieves state-of-the-art performance on ShapeNet for 3D generation and demonstrates superior performance in monocular 3D reconstruction and conditional 3D generation across various datasets. Moreover, it surpasses existing 3D diffusion methods in terms of inference speed, requiring no per-instance optimization. Our proposed LN3Diff presents a significant advancement in 3D generative modeling and holds promise for various applications in 3D vision and graphics tasks.","authors":["Yushi Lan","Fangzhou Hong","Shangchen Zhou","Shuai Yang","Xuyi Meng","Yongwei Chen","Zhaoyang Lyu","Bo Dai","Xingang Pan","Chen Change Loy"],"pdf_url":"","comment":"TPAMI 2025 version of LN3Diff. Project webpage: https://nirvanalan.github.io/projects/ln3diff/ Code: https://github.com/NIRVANALAN/LN3Diff"},{"id":"http://arxiv.org/abs/2512.17152v1","updated":"2025-12-19T01:16:40Z","published":"2025-12-19T01:16:40Z","title":"PhysFire-WM: A Physics-Informed World Model for Emulating Fire Spread Dynamics","summary":"Fine-grained fire prediction plays a crucial role in emergency response. Infrared images and fire masks provide complementary thermal and boundary information, yet current methods are predominantly limited to binary mask modeling with inherent signal sparsity, failing to capture the complex dynamics of fire. While world models show promise in video generation, their physical inconsistencies pose significant challenges for fire forecasting. This paper introduces PhysFire-WM, a Physics-informed World Model for emulating Fire spread dynamics. Our approach internalizes combustion dynamics by encoding structured priors from a Physical Simulator to rectify physical discrepancies, coupled with a Cross-task Collaborative Training strategy (CC-Train) that alleviates the issue of limited information in mask-based modeling. Through parameter sharing and gradient coordination, CC-Train effectively integrates thermal radiation dynamics and spatial boundary delineation, enhancing both physical realism and geometric accuracy. Extensive experiments on a fine-grained multimodal fire dataset demonstrate the superior accuracy of PhysFire-WM in fire spread prediction. Validation underscores the importance of physical priors and cross-task collaboration, providing new insights for applying physics-informed world models to disaster prediction.","authors":["Nan Zhou","Huandong Wang","Jiahao Li","Yang Li","Xiao-Ping Zhang","Yong Li","Xinlei Chen"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2502.09652v3","updated":"2025-12-19T01:13:42Z","published":"2025-02-11T20:22:00Z","title":"GraphCompNet: A Position-Aware Model for Predicting and Compensating Shape Deviations in 3D Printing","summary":"Shape deviation modeling and compensation in additive manufacturing are pivotal for achieving high geometric accuracy and enabling industrial-scale production. Critical challenges persist, including generalizability across complex geometries and adaptability to position-dependent variations in batch production. Traditional methods of controlling geometric deviations often rely on complex parameterized models and repetitive metrology, which can be time-consuming yet not applicable for batch production. In this paper, we present a novel, process-agnostic approach to address the challenge of ensuring geometric precision and accuracy in position-dependent AM production. The proposed GraphCompNet presents a novel computational framework integrating graph-based neural networks with a GAN inspired training paradigm. The framework leverages point cloud representations and dynamic graph convolutional neural networks (DGCNNs) to model intricate geometries while incorporating position-specific thermal and mechanical variations. A two-stage adversarial training process iteratively refines compensated designs using a compensator-predictor architecture, enabling real-time feedback and optimization. Experimental validation across various shapes and positions demonstrates the framework's ability to predict deviations in freeform geometries and adapt to position-dependent batch production conditions, significantly improving compensation accuracy (35 to 65 percent) across the entire printing space, addressing position-dependent variabilities within the print chamber. The proposed method advances the development of a Digital Twin for AM, offering scalable, real-time monitoring and compensation capabilities.","authors":["Juheon Lee"," Lei"," Chen","Juan Carlos Catana","Hui Wang","Jun Zeng"],"pdf_url":"","comment":"Accepted manuscript. Final version published in IEEE Transactions on Automation Science and Engineering"},{"id":"http://arxiv.org/abs/2512.17151v1","updated":"2025-12-19T01:10:24Z","published":"2025-12-19T01:10:24Z","title":"Text-Conditioned Background Generation for Editable Multi-Layer Documents","summary":"We present a framework for document-centric background generation with multi-page editing and thematic continuity. To ensure text regions remain readable, we employ a \\emph{latent masking} formulation that softly attenuates updates in the diffusion space, inspired by smooth barrier functions in physics and numerical optimization. In addition, we introduce \\emph{Automated Readability Optimization (ARO)}, which automatically places semi-transparent, rounded backing shapes behind text regions. ARO determines the minimal opacity needed to satisfy perceptual contrast standards (WCAG 2.2) relative to the underlying background, ensuring readability while maintaining aesthetic harmony without human intervention. Multi-page consistency is maintained through a summarization-and-instruction process, where each page is distilled into a compact representation that recursively guides subsequent generations. This design reflects how humans build continuity by retaining prior context, ensuring that visual motifs evolve coherently across an entire document. Our method further treats a document as a structured composition in which text, figures, and backgrounds are preserved or regenerated as separate layers, allowing targeted background editing without compromising readability. Finally, user-provided prompts allow stylistic adjustments in color and texture, balancing automated consistency with flexible customization. Our training-free framework produces visually coherent, text-preserving, and thematically aligned documents, bridging generative modeling with natural design workflows.","authors":["Taewon Kang","Joseph K J","Chris Tensmeyer","Jihyung Kil","Wanrong Zhu","Ming C. Lin","Vlad I. Morariu"],"pdf_url":"","comment":null}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2512.17795v1","updated":"2025-12-19T17:01:03Z","published":"2025-12-19T17:01:03Z","title":"Intelligent Knowledge Mining Framework: Bridging AI Analysis and Trustworthy Preservation","summary":"The unprecedented proliferation of digital data presents significant challenges in access, integration, and value creation across all data-intensive sectors. Valuable information is frequently encapsulated within disparate systems, unstructured documents, and heterogeneous formats, creating silos that impede efficient utilization and collaborative decision-making. This paper introduces the Intelligent Knowledge Mining Framework (IKMF), a comprehensive conceptual model designed to bridge the critical gap between dynamic AI-driven analysis and trustworthy long-term preservation. The framework proposes a dual-stream architecture: a horizontal Mining Process that systematically transforms raw data into semantically rich, machine-actionable knowledge, and a parallel Trustworthy Archiving Stream that ensures the integrity, provenance, and computational reproducibility of these assets. By defining a blueprint for this symbiotic relationship, the paper provides a foundational model for transforming static repositories into living ecosystems that facilitate the flow of actionable intelligence from producers to consumers. This paper outlines the motivation, problem statement, and key research questions guiding the research and development of the framework, presents the underlying scientific methodology, and details its conceptual design and modeling.","authors":["Binh Vu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2510.20150v4","updated":"2025-12-19T16:58:20Z","published":"2025-10-23T02:56:00Z","title":"Rank-GRPO: Training LLM-based Conversational Recommender Systems with Reinforcement Learning","summary":"Large language models (LLMs) are reshaping the recommender system paradigm by enabling users to express preferences and receive recommendations through conversations. Yet, aligning LLMs to the recommendation task remains challenging: pretrained LLMs often generate out-of-catalog items, violate required output formats, and their ranking quality degrades sharply toward the end of the generated list. To this end, we propose ConvRec-R1, a two-stage framework for end-to-end training of LLM-based conversational recommender systems. In Stage 1, we construct a behavioral-cloning dataset with a Remap-Reflect-Adjust pipeline, which produces high-quality, catalog-grounded demonstrations from powerful blackbox LLMs to warm-start the RL training. In Stage 2, we propose Rank-GRPO, a principled extension of group relative policy optimization (GRPO) tailored to tasks with rank-style outputs. Rank-GRPO treats each rank in the recommendation list as the unit instead of token (too fine-grained) or sequence (too coarse), redefining rewards to remove non-causal credit assignment and introducing a rank-level importance ratio based on the geometric mean of rank-wise token probabilities to stabilize policy updates. Experiments on the public Reddit-v2 dataset show that ConvRec-R1 converges faster and achieves higher Recall and NDCG than GRPO-style baselines. Code and datasets are released at https://github.com/yaochenzhu/Rank-GRPO.","authors":["Yaochen Zhu","Harald Steck","Dawen Liang","Yinhan He","Vito Ostuni","Jundong Li","Nathan Kallus"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17733v1","updated":"2025-12-19T16:09:29Z","published":"2025-12-19T16:09:29Z","title":"Diversity Recommendation via Causal Deconfounding of Co-purchase Relations and Counterfactual Exposure","summary":"Beyond user-item modeling, item-to-item relationships are increasingly used to enhance recommendation. However, common methods largely rely on co-occurrence, making them prone to item popularity bias and user attributes, which degrades embedding quality and performance. Meanwhile, although diversity is acknowledged as a key aspect of recommendation quality, existing research offers limited attention to it, with a notable lack of causal perspectives and theoretical grounding. To address these challenges, we propose Cadence: Diversity Recommendation via Causal Deconfounding of Co-purchase Relations and Counterfactual Exposure - a plug-and-play framework built upon LightGCN as the backbone, primarily designed to enhance recommendation diversity while preserving accuracy. First, we compute the Unbiased Asymmetric Co-purchase Relationship (UACR) between items - excluding item popularity and user attributes - to construct a deconfounded directed item graph, with an aggregation mechanism to refine embeddings. Second, we leverage UACR to identify diverse categories of items that exhibit strong causal relevance to a user's interacted items but have not yet been engaged with. We then simulate their behavior under high-exposure scenarios, thereby significantly enhancing recommendation diversity while preserving relevance. Extensive experiments on real-world datasets demonstrate that our method consistently outperforms state-of-the-art diversity models in both diversity and accuracy, and further validates its effectiveness, transferability, and efficiency over baselines.","authors":["Jingmao Zhang","Zhiting Zhao","Yunqi Lin","Jianghong Ma","Tianjun Wei","Haijun Zhang","Xiaofeng Zhang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2508.07980v2","updated":"2025-12-19T12:28:34Z","published":"2025-08-11T13:38:58Z","title":"Early Explorations of Recommender Systems for Physical Activity and Well-being","summary":"As recommender systems increasingly guide physical actions, often through wearables and coaching tools, new challenges arise around how users interpret, trust, and respond to this advice. This paper introduces a conceptual framework for tangible recommendations that influence users' bodies, routines, and well-being. We describe three design dimensions: trust and interpretation, intent alignment, and consequence awareness. These highlight key limitations in applying conventional recommender logic to embodied settings. Through examples and design reflections, we outline how future systems can support long-term well-being, behavioral alignment, and socially responsible personalization.","authors":["Alan Said"],"pdf_url":"","comment":"Second International Workshop on Recommender Systems for Sustainability and Social Good (RecSoGood) in conjunction with ACM RecSys 2025"},{"id":"http://arxiv.org/abs/2512.17462v1","updated":"2025-12-19T11:25:18Z","published":"2025-12-19T11:25:18Z","title":"Behavioural Effects of Agentic Messaging: A Case Study on a Financial Service Application","summary":"Marketing and product personalisation provide a prominent and visible use-case for the application of Information Retrieval methods across several business domains. Recently, agentic approaches to these problems have been gaining traction. This work evaluates the behavioural and retention effects of agentic personalisation on a financial service application's customer communication system during a 2025 national tax filing period. Through a two month-long randomised controlled trial, we compare an agentic messaging approach against a business-as-usual (BAU) rule-based campaign system, focusing on two primary outcomes: unsubscribe behaviour and conversion timing. Empirical results show that agent-led messaging reduced unsubscribe events by 21\\% ($\\pm 0.01$) relative to BAU and increased early filing behaviour in the weeks preceding the national deadline. These findings demonstrate how adaptive, user-level decision-making systems can modulate engagement intensity whilst improving long-term retention indicators.","authors":["Olivier Jeunen","Schaun Wheeler"],"pdf_url":"","comment":"To appear in the 48th European Conference on Information Retrieval (ECIR '26) Industry Track"},{"id":"http://arxiv.org/abs/2512.17442v1","updated":"2025-12-19T10:54:42Z","published":"2025-12-19T10:54:42Z","title":"A Systematic Reproducibility Study of BSARec for Sequential Recommendation","summary":"In sequential recommendation (SR), the self-attention mechanism of Transformer-based models acts as a low-pass filter, limiting their ability to capture high-frequency signals that reflect short-term user interests. To overcome this, BSARec augments the Transformer encoder with a frequency layer that rescales high-frequency components using the Fourier transform. However, the overall effectiveness of BSARec and the roles of its individual components have yet to be systematically validated. We reproduce BSARec and show that it outperforms other SR methods on some datasets. To empirically assess whether BSARec improves performance on high-frequency signals, we propose a metric to quantify user history frequency and evaluate SR methods across different user groups. We compare digital signal processing (DSP) techniques and find that the discrete wavelet transform (DWT) offer only slight improvements over Fourier transforms, and DSP methods provide no clear advantage over simple residual connections. Finally, we explore padding strategies and find that non-constant padding significantly improves recommendation performance, whereas constant padding hinders the frequency rescaler's ability to capture high-frequency signals.","authors":["Jan Hutter","Hua Chang Bakker","Stan Fris","Madelon Bernardy","Yuanna Liu"],"pdf_url":"","comment":"Jan Hutter, Hua Chang Bakker, Stan Fris, Madelon Bernardy contributed equally to this work"},{"id":"http://arxiv.org/abs/2512.17389v1","updated":"2025-12-19T09:44:19Z","published":"2025-12-19T09:44:19Z","title":"The Mental World of Large Language Models in Recommendation: A Benchmark on Association, Personalization, and Knowledgeability","summary":"Large language models (LLMs) have shown potential in recommendation systems (RecSys) by using them as either knowledge enhancer or zero-shot ranker. A key challenge lies in the large semantic gap between LLMs and RecSys where the former internalizes language world knowledge while the latter captures personalized world of behaviors. Unfortunately, the research community lacks a comprehensive benchmark that evaluates the LLMs over their limitations and boundaries in RecSys so that we can draw a confident conclusion. To investigate this, we propose a benchmark named LRWorld containing over 38K high-quality samples and 23M tokens carefully compiled and generated from widely used public recommendation datasets. LRWorld categorizes the mental world of LLMs in RecSys as three main scales (association, personalization, and knowledgeability) spanned by ten factors with 31 measures (tasks). Based on LRWorld, comprehensive experiments on dozens of LLMs show that they are still not well capturing the deep neural personalized embeddings but can achieve good results on shallow memory-based item-item similarity. They are also good at perceiving item entity relations, entity hierarchical taxonomies, and item-item association rules when inferring user interests. Furthermore, LLMs show a promising ability in multimodal knowledge reasoning (movie poster and product image) and robustness to noisy profiles. None of them show consistently good performance over the ten factors. Model sizes, position bias, and more are ablated.","authors":["Guangneng Hu"],"pdf_url":"","comment":"21 pages, 13 figures, 27 tables, submission to KDD 2025"},{"id":"http://arxiv.org/abs/2507.23358v2","updated":"2025-12-19T08:52:20Z","published":"2025-07-31T09:08:59Z","title":"Text-to-SQL Task-oriented Dialogue Ontology Construction","summary":"Large language models (LLMs) are widely used as general-purpose knowledge sources, but they rely on parametric knowledge, limiting explainability and trustworthiness. In task-oriented dialogue (TOD) systems, this separation is explicit, using an external database structured by an explicit ontology to ensure explainability and controllability. However, building such ontologies requires manual labels or supervised training. We introduce TeQoDO: a Text-to-SQL task-oriented Dialogue Ontology construction method. Here, an LLM autonomously builds a TOD ontology from scratch using only its inherent SQL programming capabilities combined with concepts from modular TOD systems provided in the prompt. We show that TeQoDO outperforms transfer learning approaches, and its constructed ontology is competitive on a downstream dialogue state tracking task. Ablation studies demonstrate the key role of modular TOD system concepts. TeQoDO also scales to allow construction of much larger ontologies, which we investigate on a Wikipedia and arXiv dataset. We view this as a step towards broader application of ontologies.","authors":["Renato Vukovic","Carel van Niekerk","Michael Heck","Benjamin Ruppik","Hsien-Chin Lin","Shutong Feng","Nurul Lubis","Milica Gasic"],"pdf_url":"","comment":"Accepted to Transactions of the Association for Computational Linguistics"},{"id":"http://arxiv.org/abs/2512.17277v1","updated":"2025-12-19T06:49:55Z","published":"2025-12-19T06:49:55Z","title":"Warmer for Less: A Cost-Efficient Strategy for Cold-Start Recommendations at Pinterest","summary":"Pinterest is a leading visual discovery platform where recommender systems (RecSys) are key to delivering relevant, engaging, and fresh content to our users. In this paper, we study the problem of improving RecSys model predictions for cold-start (CS) items, which appear infrequently in the training data. Although this problem is well-studied in academia, few studies have addressed its root causes effectively at the scale of a platform like Pinterest. By investigating live traffic data, we identified several challenges of the CS problem and developed a corresponding solution for each: First, industrial-scale RecSys models must operate under tight computational constraints. Since CS items are a minority, any related improvements must be highly cost-efficient. To address this, our solutions were designed to be lightweight, collectively increasing the total parameters by only 5%. Second, CS items are represented only by non-historical (e.g., content or attribute) features, which models often treat as less important. To elevate their significance, we introduce a residual connection for the non-historical features. Third, CS items tend to receive lower prediction scores compared to non-CS items, reducing their likelihood of being surfaced. We mitigate this by incorporating a score regularization term into the model. Fourth, the labels associated with CS items are sparse, making it difficult for the model to learn from them. We apply the manifold mixup technique to address this data sparsity. Implemented together, our methods increased fresh content engagement at Pinterest by 10% without negatively impacting overall engagement and cost, and have been deployed to serve over 570 million users on Pinterest.","authors":["Saeed Ebrahimi","Weijie Jiang","Jaewon Yang","Olafur Gudmundsson","Yucheng Tu","Huizhong Duan"],"pdf_url":"","comment":"Submitted to the WWW'26"},{"id":"http://arxiv.org/abs/2503.22182v2","updated":"2025-12-19T06:24:33Z","published":"2025-03-28T07:00:33Z","title":"Sell It Before You Make It: Revolutionizing E-Commerce with Personalized AI-Generated Items","summary":"E-commerce has revolutionized retail, yet its traditional workflows remain inefficient, with significant resource costs tied to product design and inventory. This paper introduces a novel system deployed at Alibaba that uses AI-generated items (AIGI) to address these challenges with personalized text-to-image generation for e-commerce product design. AIGI enables an innovative business mode called \"sell it before you make it\", where merchants can design fashion items and generate photorealistic images with digital models based on textual descriptions. Only when the items have received a certain number of orders, do the merchants start to produce them, which largely reduces reliance on physical prototypes and thus accelerates time to market. For such a promising application, we identify the underlying key scientific challenge, i.e., capturing users' group-level personalized preferences towards multiple generated images. To this end, we propose a Personalized Group-Level Preference Alignment Framework for Diffusion Models (PerFusion). We first design PerFusion Reward Model for user preference estimation with a feature-crossing-based personalized plug-in. Then we develop PerFusion with a personalized adaptive network to model diverse preferences across users, and meanwhile derive the group-level preference optimization objective to model comparative behaviors among multiple images. Both offline and online experiments demonstrate the effectiveness of our proposed algorithm. The AI-generated items achieve over 13% relative improvements for both click-through rate and conversion rate, as well as 7.9% decrease in return rate, compared to their human-designed counterparts, validating the transformative potential of AIGI for e-commerce platforms.","authors":["Jianghao Lin","Peng Du","Jiaqi Liu","Weite Li","Yong Yu","Weinan Zhang","Yang Cao"],"pdf_url":"","comment":"Accepted by KDD 2026 ADS Track"},{"id":"http://arxiv.org/abs/2512.13168v2","updated":"2025-12-19T03:59:15Z","published":"2025-12-15T10:28:45Z","title":"Finch: Benchmarking Finance & Accounting across Spreadsheet-Centric Enterprise Workflows","summary":"We introduce a finance & accounting benchmark (Finch) for evaluating AI agents on real-world, enterprise-grade professional workflows -- interleaving data entry, structuring, formatting, web search, cross-file retrieval, calculation, modeling, validation, translation, visualization, and reporting. Finch is sourced from authentic enterprise workspaces at Enron (15,000 spreadsheets and 500,000 emails from 150 employees) and other financial institutions, preserving in-the-wild messiness across multimodal artifacts (text, tables, formulas, charts, code, and images) and spanning diverse domains such as budgeting, trading, and asset management.\n  We propose a workflow construction process that combines LLM-assisted discovery with expert annotation: (1) LLM-assisted, expert-verified derivation of workflows from real-world email threads and version histories of spreadsheet files, and (2) meticulous expert annotation for workflows, requiring over 700 hours of domain-expert effort. This yields 172 composite workflows with 384 tasks, involving 1,710 spreadsheets with 27 million cells, along with PDFs and other artifacts, capturing the intrinsically messy, long-horizon, knowledge-intensive, and collaborative nature of real-world enterprise work.\n  We conduct both human and automated evaluations of frontier AI systems including GPT 5.1, Claude Sonnet 4.5, Gemini 3 Pro, Grok 4, and Qwen 3 Max, and GPT 5.1 Pro spends 48 hours in total yet passes only 38.4% of workflows, while Claude Sonnet 4.5 passes just 25.0%. Comprehensive case studies further surface the challenges that real-world enterprise workflows pose for AI agents.","authors":["Haoyu Dong","Pengkun Zhang","Yan Gao","Xuanyu Dong","Yilin Cheng","Mingzhe Lu","Adina Yakefu","Shuxin Zheng"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2507.17061v4","updated":"2025-12-19T03:33:45Z","published":"2025-07-22T22:42:51Z","title":"Parallelism Meets Adaptiveness: Scalable Documents Understanding in Multi-Agent LLM Systems","summary":"Large language model (LLM) agents have shown increasing promise for collaborative task completion. However, existing multi-agent frameworks often rely on static workflows, fixed roles, and limited inter-agent communication, reducing their effectiveness in open-ended, high-complexity domains. This paper proposes a coordination framework that enables adaptiveness through three core mechanisms: dynamic task routing, bidirectional feedback, and parallel agent evaluation. The framework allows agents to reallocate tasks based on confidence and workload, exchange structured critiques to iteratively improve outputs, and crucially compete on high-ambiguity subtasks with evaluator-driven selection of the most suitable result. We instantiate these principles in a modular architecture and demonstrate substantial improvements in factual coverage, coherence, and efficiency over static and partially adaptive baselines. Our findings highlight the benefits of incorporating both adaptiveness and structured competition in multi-agent LLM systems.","authors":["Chengxuan Xia","Qianye Wu","Sixuan Tian","Yilun Hao"],"pdf_url":"","comment":"Accepted at AAAI 2026 Workshop on WoMAPF, Camera ready version"},{"id":"http://arxiv.org/abs/2512.17178v1","updated":"2025-12-19T02:36:51Z","published":"2025-12-19T02:36:51Z","title":"ABE-CLIP: Training-Free Attribute Binding Enhancement for Compositional Image-Text Matching","summary":"Contrastive Language-Image Pretraining (CLIP) has achieved remarkable performance in various multimodal tasks. However, it still struggles with compositional image-text matching, particularly in accurately associating objects with their corresponding attributes, because its inherent global representation often overlooks fine-grained semantics for attribute binding. Existing methods often require additional training or extensive hard negative sampling, yet they frequently show limited generalization to novel compositional concepts and fail to fundamentally address the drawbacks of global representations. In this paper, we propose ABE-CLIP, a novel training-free Attribute Binding Enhancement method designed to strengthen attribute-object binding in CLIP-like models. Specifically, we employ a Semantic Refinement Mechanism to refine token embeddings for both object and attribute phrases in the text, thereby mitigating attribute confusion and improving semantic precision. We further introduce a Local Token-Patch Alignment strategy that computes similarity scores between refined textual tokens and their most relevant image patches. By aggregating localized similarity scores, ABE-CLIP computes the final image-text similarity. Experiments on multiple datasets demonstrate that ABE-CLIP significantly improves attribute-object binding performance, even surpassing methods that require extensive training.","authors":["Qi Zhang","Yuxu Chen","Lei Deng","Lili Shen"],"pdf_url":"","comment":"10 pages, 8 figures"},{"id":"http://arxiv.org/abs/2512.17164v1","updated":"2025-12-19T01:57:17Z","published":"2025-12-19T01:57:17Z","title":"TCDE: Topic-Centric Dual Expansion of Queries and Documents with Large Language Models for Information Retrieval","summary":"Query Expansion (QE) enriches queries and Document Expansion (DE) enriches documents, and these two techniques are often applied separately. However, such separate application may lead to semantic misalignment between the expanded queries (or documents) and their relevant documents (or queries). To address this serious issue, we propose TCDE, a dual expansion strategy that leverages large language models (LLMs) for topic-centric enrichment on both queries and documents. In TCDE, we design two distinct prompt templates for processing each query and document. On the query side, an LLM is guided to identify distinct sub-topics within each query and generate a focused pseudo-document for each sub-topic. On the document side, an LLM is guided to distill each document into a set of core topic sentences. The resulting outputs are used to expand the original query and document. This topic-centric dual expansion process establishes semantic bridges between queries and their relevant documents, enabling better alignment for downstream retrieval models. Experiments on two challenging benchmarks, TREC Deep Learning and BEIR, demonstrate that TCDE achieves substantial improvements over strong state-of-the-art expansion baselines. In particular, on dense retrieval tasks, it outperforms several state-of-the-art methods, with a relative improvement of 2.8\\% in NDCG@10 on the SciFact dataset. Experimental results validate the effectiveness of our topic-centric and dual expansion strategy.","authors":["Yu Yang","Feng Tian","Ping Chen"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2510.22732v2","updated":"2025-12-19T23:16:01Z","published":"2025-10-26T16:03:39Z","title":"WebATLAS: An LLM Agent with Experience-Driven Memory and Action Simulation","summary":"Large Language Model (LLM) web agents often struggle with long-horizon web navigation and web task completion in new websites, producing inefficient action sequences unless fine-tuned on environment-specific data. We show that experience-driven memory, combined with look-ahead action simulation, is sufficient for LLM agents to adapt to unseen web environments by remembering past failures and predicting the consequences of future actions. We introduce WebATLAS (Actor-Critic Task-completion with Look-ahead Action Simulation), a memory-augmented LLM web agent that learns a lightweight internal model of the environment from interaction experience and performs hypothetical action rollouts before acting in the real world. WebATLAS builds a persistent cognitive map via curiosity-driven exploration, stores interaction outcomes as experience-based memory, and evaluates candidate actions in cognitive space using a planner--simulator--critic loop. This enables the agent to reuse past experience, avoid previously unsuccessful behaviors, and generate more efficient plans. We evaluate WebATLAS on the WebArena-Lite benchmark for autonomous web navigation and demonstrate a success rate of 63%, outperforming the previous state-of-the-art at 53.9%. Unlike previous systems, our modular architecture requires no website-specific LLM fine-tuning. Ablation studies confirm that experience-driven memory, look-ahead action simulation, and hierarchical replanning play complementary roles in enabling robust, training-free web agents.","authors":["Jiali Cheng","Anjishnu Kumar","Roshan Lal","Rishi Rajasekaran","Hani Ramezani","Omar Zia Khan","Oleg Rokhlenko","Sunny Chiu-Webster","Gang Hua","Hadi Amiri"],"pdf_url":"","comment":"9 pages, NeurIPS 2025 Workshop on Language Agents and World Models"},{"id":"http://arxiv.org/abs/2512.18117v1","updated":"2025-12-19T22:50:49Z","published":"2025-12-19T22:50:49Z","title":"Factorized Transport Alignment for Multimodal and Multiview E-commerce Representation Learning","summary":"The rapid growth of e-commerce requires robust multimodal representations that capture diverse signals from user-generated listings. Existing vision-language models (VLMs) typically align titles with primary images, i.e., single-view, but overlook non-primary images and auxiliary textual views that provide critical semantics in open marketplaces such as Etsy or Poshmark. To this end, we propose a framework that unifies multimodal and multi-view learning through Factorized Transport, a lightweight approximation of optimal transport, designed for scalability and deployment efficiency. During training, the method emphasizes primary views while stochastically sampling auxiliary ones, reducing training cost from quadratic in the number of views to constant per item. At inference, all views are fused into a single cached embedding, preserving the efficiency of two-tower retrieval with no additional online overhead. On an industrial dataset of 1M product listings and 0.3M interactions, our approach delivers consistent improvements in cross-view and query-to-item retrieval, achieving up to +7.9% Recall@500 over strong multimodal baselines. Overall, our framework bridges scalability with optimal transport-based learning, making multi-view pretraining practical for large-scale e-commerce search.","authors":["Xiwen Chen","Yen-Chieh Lien","Susan Liu","María Castaños","Abolfazl Razi","Xiaoting Zhao","Congzhe Su"],"pdf_url":"","comment":"Accepted by WSDM'26"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2512.17908v1","updated":"2025-12-19T18:59:56Z","published":"2025-12-19T18:59:56Z","title":"Re-Depth Anything: Test-Time Depth Refinement via Self-Supervised Re-lighting","summary":"Monocular depth estimation remains challenging as recent foundation models, such as Depth Anything V2 (DA-V2), struggle with real-world images that are far from the training distribution. We introduce Re-Depth Anything, a test-time self-supervision framework that bridges this domain gap by fusing DA-V2 with the powerful priors of large-scale 2D diffusion models. Our method performs label-free refinement directly on the input image by re-lighting predicted depth maps and augmenting the input. This re-synthesis method replaces classical photometric reconstruction by leveraging shape from shading (SfS) cues in a new, generative context with Score Distillation Sampling (SDS). To prevent optimization collapse, our framework employs a targeted optimization strategy: rather than optimizing depth directly or fine-tuning the full model, we freeze the encoder and only update intermediate embeddings while also fine-tuning the decoder. Across diverse benchmarks, Re-Depth Anything yields substantial gains in depth accuracy and realism over the DA-V2, showcasing new avenues for self-supervision by augmenting geometric reasoning.","authors":["Ananta R. Bhattarai","Helge Rhodin"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17899v1","updated":"2025-12-19T18:58:11Z","published":"2025-12-19T18:58:11Z","title":"Distributionally Robust Imitation Learning: Layered Control Architecture for Certifiable Autonomy","summary":"Imitation learning (IL) enables autonomous behavior by learning from expert demonstrations. While more sample-efficient than comparative alternatives like reinforcement learning, IL is sensitive to compounding errors induced by distribution shifts. There are two significant sources of distribution shifts when using IL-based feedback laws on systems: distribution shifts caused by policy error and distribution shifts due to exogenous disturbances and endogenous model errors due to lack of learning. Our previously developed approaches, Taylor Series Imitation Learning (TaSIL) and $\\mathcal{L}_1$ -Distributionally Robust Adaptive Control (\\ellonedrac), address the challenge of distribution shifts in complementary ways. While TaSIL offers robustness against policy error-induced distribution shifts, \\ellonedrac offers robustness against distribution shifts due to aleatoric and epistemic uncertainties. To enable certifiable IL for learned and/or uncertain dynamical systems, we formulate \\textit{Distributionally Robust Imitation Policy (DRIP)} architecture, a Layered Control Architecture (LCA) that integrates TaSIL and~\\ellonedrac. By judiciously designing individual layer-centric input and output requirements, we show how we can guarantee certificates for the entire control pipeline. Our solution paves the path for designing fully certifiable autonomy pipelines, by integrating learning-based components, such as perception, with certifiable model-based decision-making through the proposed LCA approach.","authors":["Aditya Gahlawat","Ahmed Aboudonia","Sandeep Banik","Naira Hovakimyan","Nikolai Matni","Aaron D. Ames","Gioele Zardini","Alberto Speranzon"],"pdf_url":"","comment":"18 pages, 5 figures"},{"id":"http://arxiv.org/abs/2512.17897v1","updated":"2025-12-19T18:57:33Z","published":"2025-12-19T18:57:33Z","title":"RadarGen: Automotive Radar Point Cloud Generation from Cameras","summary":"We present RadarGen, a diffusion model for synthesizing realistic automotive radar point clouds from multi-view camera imagery. RadarGen adapts efficient image-latent diffusion to the radar domain by representing radar measurements in bird's-eye-view form that encodes spatial structure together with radar cross section (RCS) and Doppler attributes. A lightweight recovery step reconstructs point clouds from the generated maps. To better align generation with the visual scene, RadarGen incorporates BEV-aligned depth, semantic, and motion cues extracted from pretrained foundation models, which guide the stochastic generation process toward physically plausible radar patterns. Conditioning on images makes the approach broadly compatible, in principle, with existing visual datasets and simulation frameworks, offering a scalable direction for multimodal generative simulation. Evaluations on large-scale driving data show that RadarGen captures characteristic radar measurement distributions and reduces the gap to perception models trained on real data, marking a step toward unified generative simulation across sensing modalities.","authors":["Tomer Borreda","Fangqiang Ding","Sanja Fidler","Shengyu Huang","Or Litany"],"pdf_url":"","comment":"Project page: https://radargen.github.io/"},{"id":"http://arxiv.org/abs/2507.01939v4","updated":"2025-12-19T18:39:57Z","published":"2025-07-02T17:49:52Z","title":"SpecCLIP: Aligning and Translating Spectroscopic Measurements for Stars","summary":"In recent years, large language models (LLMs) have transformed natural language understanding through vast datasets and large-scale parameterization. Inspired by this success, we present SpecCLIP, a foundation model framework that extends LLM-inspired methodologies to stellar spectral analysis. Stellar spectra, akin to structured language, encode rich physical and chemical information about stars. By training foundation models on large-scale spectral datasets, our goal is to learn robust and informative embeddings that support diverse downstream applications. As a proof of concept, SpecCLIP involves pre-training on two spectral types--LAMOST low-resolution and Gaia XP--followed by contrastive alignment using the CLIP (Contrastive Language-Image Pre-training) framework, adapted to associate spectra from different instruments. This alignment is complemented by auxiliary decoders that preserve spectrum-specific information and enable translation (prediction) between spectral types, with the former achieved by maximizing mutual information between embeddings and input spectra. The result is a cross-spectrum framework enabling intrinsic calibration and flexible applications across instruments. We demonstrate that fine-tuning these models on moderate-sized labeled datasets improves adaptability to tasks such as stellar-parameter estimation and chemical-abundance determination. SpecCLIP also enhances the accuracy and precision of parameter estimates benchmarked against external survey data. Additionally, its similarity search and cross-spectrum prediction capabilities offer potential for anomaly detection. Our results suggest that contrastively trained foundation models enriched with spectrum-aware decoders can advance precision stellar spectroscopy. Our code SpecCLIP is publicly available at https://github.com/Xiaosheng-Zhao/SpecCLIP","authors":["Xiaosheng Zhao","Yang Huang","Guirong Xue","Xiao Kong","Jifeng Liu","Xiaoyu Tang","Timothy C. Beers","Yuan-Sen Ting","A-Li Luo"],"pdf_url":"","comment":"29 pages, 8 figures, 6 tables. Accepted for publication in ApJ. Comments welcome"},{"id":"http://arxiv.org/abs/2512.17884v1","updated":"2025-12-19T18:36:24Z","published":"2025-12-19T18:36:24Z","title":"Regularized Random Fourier Features and Finite Element Reconstruction for Operator Learning in Sobolev Space","summary":"Operator learning is a data-driven approximation of mappings between infinite-dimensional function spaces, such as the solution operators of partial differential equations. Kernel-based operator learning can offer accurate, theoretically justified approximations that require less training than standard methods. However, they can become computationally prohibitive for large training sets and can be sensitive to noise. We propose a regularized random Fourier feature (RRFF) approach, coupled with a finite element reconstruction map (RRFF-FEM), for learning operators from noisy data. The method uses random features drawn from multivariate Student's $t$ distributions, together with frequency-weighted Tikhonov regularization that suppresses high-frequency noise. We establish high-probability bounds on the extreme singular values of the associated random feature matrix and show that when the number of features $N$ scales like $m \\log m$ with the number of training samples $m$, the system is well-conditioned, which yields estimation and generalization guarantees. Detailed numerical experiments on benchmark PDE problems, including advection, Burgers', Darcy flow, Helmholtz, Navier-Stokes, and structural mechanics, demonstrate that RRFF and RRFF-FEM are robust to noise and achieve improved performance with reduced training time compared to the unregularized random feature model, while maintaining competitive accuracy relative to kernel and neural operator tests.","authors":["Xinyue Yu","Hayden Schaeffer"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17878v1","updated":"2025-12-19T18:31:27Z","published":"2025-12-19T18:31:27Z","title":"Weighted Stochastic Differential Equation to Implement Wasserstein-Fisher-Rao Gradient Flow","summary":"Score-based diffusion models currently constitute the state of the art in continuous generative modeling. These methods are typically formulated via overdamped or underdamped Ornstein--Uhlenbeck-type stochastic differential equations, in which sampling is driven by a combination of deterministic drift and Brownian diffusion, resulting in continuous particle trajectories in the ambient space. While such dynamics enjoy exponential convergence guarantees for strongly log-concave target distributions, it is well known that their mixing rates deteriorate exponentially in the presence of nonconvex or multimodal landscapes, such as double-well potentials. Since many practical generative modeling tasks involve highly non-log-concave target distributions, considerable recent effort has been devoted to developing sampling schemes that improve exploration beyond classical diffusion dynamics.\n  A promising line of work leverages tools from information geometry to augment diffusion-based samplers with controlled mass reweighting mechanisms. This perspective leads naturally to Wasserstein--Fisher--Rao (WFR) geometries, which couple transport in the sample space with vertical (reaction) dynamics on the space of probability measures. In this work, we formulate such reweighting mechanisms through the introduction of explicit correction terms and show how they can be implemented via weighted stochastic differential equations using the Feynman--Kac representation. Our study provides a preliminary but rigorous investigation of WFR-based sampling dynamics, and aims to clarify their geometric and operator-theoretic structure as a foundation for future theoretical and algorithmic developments.","authors":["Herlock Rahimi"],"pdf_url":"","comment":"26 pages, 1 figure"},{"id":"http://arxiv.org/abs/2512.17877v1","updated":"2025-12-19T18:31:07Z","published":"2025-12-19T18:31:07Z","title":"Learning vertical coordinates via automatic differentiation of a dynamical core","summary":"Terrain-following coordinates in atmospheric models often imprint their grid structure onto the solution, particularly over steep topography, where distorted coordinate layers can generate spurious horizontal and vertical motion. Standard formulations, such as hybrid or SLEVE coordinates, mitigate these errors by using analytic decay functions controlled by heuristic scale parameters that are typically tuned by hand and fixed a priori. In this work, we propose a framework to define a parametric vertical coordinate system as a learnable component within a differentiable dynamical core. We develop an end-to-end differentiable numerical solver for the two-dimensional non-hydrostatic Euler equations on an Arakawa C-grid, and introduce a NEUral Vertical Enhancement (NEUVE) terrain-following coordinate based on an integral transformed neural network that guarantees monotonicity. A key feature of our approach is the use of automatic differentiation to compute exact geometric metric terms, thereby eliminating truncation errors associated with finite-difference coordinate derivatives. By coupling simulation errors through the time integration to the parameterization, our formulation finds a grid structure optimized for both the underlying physics and numerics. Using several standard tests, we demonstrate that these learned coordinates reduce the mean squared error by a factor of 1.4 to 2 in non-linear statistical benchmarks, and eliminate spurious vertical velocity striations over steep topography.","authors":["Tim Whittaker","Seth Taylor","Elsa Cardoso-Bihlo","Alejandro Di Luca","Alex Bihlo"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.15692v2","updated":"2025-12-19T18:30:30Z","published":"2025-12-17T18:47:31Z","title":"mimic-video: Video-Action Models for Generalizable Robot Control Beyond VLAs","summary":"Prevailing Vision-Language-Action Models (VLAs) for robotic manipulation are built upon vision-language backbones pretrained on large-scale, but disconnected static web data. As a result, despite improved semantic generalization, the policy must implicitly infer complex physical dynamics and temporal dependencies solely from robot trajectories. This reliance creates an unsustainable data burden, necessitating continuous, large-scale expert data collection to compensate for the lack of innate physical understanding. We contend that while vision-language pretraining effectively captures semantic priors, it remains blind to physical causality. A more effective paradigm leverages video to jointly capture semantics and visual dynamics during pretraining, thereby isolating the remaining task of low-level control. To this end, we introduce mimic-video, a novel Video-Action Model (VAM) that pairs a pretrained Internet-scale video model with a flow matching-based action decoder conditioned on its latent representations. The decoder serves as an Inverse Dynamics Model (IDM), generating low-level robot actions from the latent representation of video-space action plans. Our extensive evaluation shows that our approach achieves state-of-the-art performance on simulated and real-world robotic manipulation tasks, improving sample efficiency by 10x and convergence speed by 2x compared to traditional VLA architectures.","authors":["Jonas Pai","Liam Achenbach","Victoriano Montesinos","Benedek Forrai","Oier Mees","Elvis Nava"],"pdf_url":"","comment":"Revised Introduction, Related Work, and Appendix. Additional minor notational and grammatical fixes"},{"id":"http://arxiv.org/abs/2512.17875v1","updated":"2025-12-19T18:26:58Z","published":"2025-12-19T18:26:58Z","title":"Visually Prompted Benchmarks Are Surprisingly Fragile","summary":"A key challenge in evaluating VLMs is testing models' ability to analyze visual content independently from their textual priors. Recent benchmarks such as BLINK probe visual perception through visual prompting, where questions about visual content are paired with coordinates to which the question refers, with the coordinates explicitly marked in the image itself. While these benchmarks are an important part of VLM evaluation, we find that existing models are surprisingly fragile to seemingly irrelevant details of visual prompting: simply changing a visual marker from red to blue can completely change rankings among models on a leaderboard. By evaluating nine commonly-used open- and closed-source VLMs on two visually prompted tasks, we demonstrate how details in benchmark setup, including visual marker design and dataset size, have a significant influence on model performance and leaderboard rankings. These effects can even be exploited to lift weaker models above stronger ones; for instance, slightly increasing the size of the visual marker results in open-source InternVL3-8B ranking alongside or better than much larger proprietary models like Gemini 2.5 Pro. We further show that low-level inference choices that are often ignored in benchmarking, such as JPEG compression levels in API calls, can also cause model lineup changes. These details have substantially larger impacts on visually prompted benchmarks than on conventional semantic VLM evaluations. To mitigate this instability, we curate existing datasets to create VPBench, a larger visually prompted benchmark with 16 visual marker variants. VPBench and additional analysis tools are released at https://lisadunlap.github.io/vpbench/.","authors":["Haiwen Feng","Long Lian","Lisa Dunlap","Jiahao Shu","XuDong Wang","Renhao Wang","Trevor Darrell","Alane Suhr","Angjoo Kanazawa"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2511.18214v2","updated":"2025-12-19T18:23:00Z","published":"2025-11-22T23:13:04Z","title":"Deep Gaussian Process Proximal Policy Optimization","summary":"Uncertainty estimation for Reinforcement Learning (RL) is a critical component in control tasks where agents must balance safe exploration and efficient learning. While deep neural networks have enabled breakthroughs in RL, they often lack calibrated uncertainty estimates. We introduce Deep Gaussian Process Proximal Policy Optimization (GPPO), a scalable, model-free actor-critic algorithm that leverages Deep Gaussian Processes (DGPs) to approximate both the policy and value function. GPPO maintains competitive performance with respect to Proximal Policy Optimization on standard high-dimensional continuous control benchmarks while providing well-calibrated uncertainty estimates that can inform safer and more effective exploration.","authors":["Matthijs van der Lende","Juan Cardenas-Cartagena"],"pdf_url":"","comment":"Withdrawn by the authors as the manuscript is not yet complete; no updated version is available at this time"},{"id":"http://arxiv.org/abs/2412.15184v2","updated":"2025-12-19T18:17:28Z","published":"2024-12-19T18:55:17Z","title":"Data for Mathematical Copilots: Better Ways of Presenting Proofs for Machine Learning","summary":"The datasets and benchmarks commonly used to train and evaluate the mathematical capabilities of AI-based mathematical copilots (primarily large language models) exhibit several shortcomings and misdirections. These range from a restricted scope of mathematical complexity to limited fidelity in capturing aspects beyond the final, written proof (e.g. motivating the proof, or representing the thought processes leading to a proof). These issues are compounded by a dynamic reminiscent of Goodhart's law: as benchmark performance becomes the primary target for model development, the benchmarks themselves become less reliable indicators of genuine mathematical capability. We systematically explore these limitations and contend that enhancing the capabilities of large language models, or any forthcoming advancements in AI-based mathematical assistants (copilots or ``thought partners''), necessitates a course correction both in the design of mathematical datasets and the evaluation criteria of the models' mathematical ability. In particular, it is necessary for benchmarks to move beyond the existing result-based datasets that map theorem statements directly to proofs, and instead focus on datasets that translate the richer facets of mathematical research practice into data that LLMs can learn from. This includes benchmarks that supervise the proving process and the proof discovery process itself, and we advocate for mathematical dataset developers to consider the concept of \"motivated proof\", introduced by G. Pólya in 1949, which can serve as a blueprint for datasets that offer a better proof learning signal, alleviating some of the mentioned limitations.","authors":["Simon Frieder","Jonas Bayer","Sam Looi","Jacob Loader","Julius Berner","Katherine M. Collins","András Juhász","Fabian Ruehle","Sean Welleck","Gabriel Poesia","Ryan-Rhys Griffiths","Adrian Weller","Anirudh Goyal","Cameron Freer","Thomas Lukasiewicz","Timothy Gowers"],"pdf_url":"","comment":"59 pages"},{"id":"http://arxiv.org/abs/2501.10321v3","updated":"2025-12-19T18:08:16Z","published":"2025-01-17T17:51:22Z","title":"Towards Human-Guided, Data-Centric LLM Co-Pilots","summary":"Machine learning (ML) has the potential to revolutionize various domains, but its adoption is often hindered by the disconnect between the needs of domain experts and translating these needs into robust and valid ML tools. Despite recent advances in LLM-based co-pilots to democratize ML for non-technical domain experts, these systems remain predominantly focused on model-centric aspects while overlooking critical data-centric challenges. This limitation is problematic in complex real-world settings where raw data often contains complex issues, such as missing values, label noise, and domain-specific nuances requiring tailored handling. To address this we introduce CliMB-DC, a human-guided, data-centric framework for LLM co-pilots that combines advanced data-centric tools with LLM-driven reasoning to enable robust, context-aware data processing. At its core, CliMB-DC introduces a novel, multi-agent reasoning system that combines a strategic coordinator for dynamic planning and adaptation with a specialized worker agent for precise execution. Domain expertise is then systematically incorporated to guide the reasoning process using a human-in-the-loop approach. To guide development, we formalize a taxonomy of key data-centric challenges that co-pilots must address. Thereafter, to address the dimensions of the taxonomy, we integrate state-of-the-art data-centric tools into an extensible, open-source architecture, facilitating the addition of new tools from the research community. Empirically, using real-world healthcare datasets we demonstrate CliMB-DC's ability to transform uncurated datasets into ML-ready formats, significantly outperforming existing co-pilot baselines for handling data-centric challenges. CliMB-DC promises to empower domain experts from diverse domains -- healthcare, finance, social sciences and more -- to actively participate in driving real-world impact using ML.","authors":["Evgeny Saveliev","Jiashuo Liu","Nabeel Seedat","Anders Boyd","Mihaela van der Schaar"],"pdf_url":"","comment":"Saveliev, Liu & Seedat contributed equally"},{"id":"http://arxiv.org/abs/2410.06800v2","updated":"2025-12-19T18:07:37Z","published":"2024-10-09T11:54:33Z","title":"Low-Rank Filtering and Smoothing for Sequential Deep Learning","summary":"Learning multiple tasks sequentially requires neural networks to balance retaining knowledge, yet being flexible enough to adapt to new tasks. Regularizing network parameters is a common approach, but it rarely incorporates prior knowledge about task relationships, and limits information flow to future tasks only. We propose a Bayesian framework that treats the network's parameters as the state space of a nonlinear Gaussian model, unlocking two key capabilities: (1) A principled way to encode domain knowledge about task relationships, allowing, e.g., control over which layers should adapt between tasks. (2) A novel application of Bayesian smoothing, allowing task-specific models to also incorporate knowledge from models learned later. This does not require direct access to their data, which is crucial, e.g., for privacy-critical applications. These capabilities rely on efficient filtering and smoothing operations, for which we propose diagonal plus low-rank approximations of the precision matrix in the Laplace approximation (LR-LGF). Empirical results demonstrate the efficiency of LR-LGF and the benefits of the unlocked capabilities.","authors":["Joanna Sliwa","Frank Schneider","Nathanael Bosch","Agustinus Kristiadi","Philipp Hennig"],"pdf_url":"","comment":"Revised version: improved presentation and added experiments"},{"id":"http://arxiv.org/abs/2411.19908v6","updated":"2025-12-19T17:48:29Z","published":"2024-11-29T18:12:50Z","title":"Another look at inference after prediction","summary":"From structural biology to epidemiology, predictions from machine learning (ML) models are increasingly used to complement costly gold-standard data to enable faster, more affordable, and scalable scientific inquiry. In response, prediction-based (PB) inference has emerged to accommodate statistical analysis using a large volume of predictions together with a small amount of gold-standard data. The goals of PB inference are two-fold: (i) to mitigate bias from errors in predictions and (ii) to improve efficiency relative to traditional inference using only the gold-standard data. While early PB inference methods focused on bias, their ability to enhance efficiency remains unclear. We revisit a popular PB inference method and show that a simple modification can be applied to guarantee improvements in efficiency beyond yielding valid inferences when the ML predictions are imperfect. The utility of this approach in leveraging prediction-based outcomes to enhance efficiency is demonstrated through extensive simulation studies and an application to the UK Biobank data. We further contextualize the problem of PB inference through historical literature from economics and statistics to highlight perspectives from classical methods in this contemporary problem.","authors":["Jessica Gronsbell","Jianhui Gao","Yaqi Shi","Zachary R. McCaw","David Cheng"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2509.11512v2","updated":"2025-12-19T17:47:28Z","published":"2025-09-15T01:53:30Z","title":"Machine Learning-Driven Predictive Resource Management in Complex Science Workflows","summary":"The collaborative efforts of large communities in science experiments, often comprising thousands of global members, reflect a monumental commitment to exploration and discovery. Recently, advanced and complex data processing has gained increasing importance in science experiments. Data processing workflows typically consist of multiple intricate steps, and the precise specification of resource requirements is crucial for each step to allocate optimal resources for effective processing. Estimating resource requirements in advance is challenging due to a wide range of analysis scenarios, varying skill levels among community members, and the continuously increasing spectrum of computing options. One practical approach to mitigate these challenges involves initially processing a subset of each step to measure precise resource utilization from actual processing profiles before completing the entire step. While this two-staged approach enables processing on optimal resources for most of the workflow, it has drawbacks such as initial inaccuracies leading to potential failures and suboptimal resource usage, along with overhead from waiting for initial processing completion, which is critical for fast-turnaround analyses. In this context, our study introduces a novel pipeline of machine learning models within a comprehensive workflow management system, the Production and Distributed Analysis (PanDA) system. These models employ advanced machine learning techniques to predict key resource requirements, overcoming challenges posed by limited upfront knowledge of characteristics at each step. Accurate forecasts of resource requirements enable informed and proactive decision-making in workflow management, enhancing the efficiency of handling diverse, complex workflows across heterogeneous resources.","authors":["Tasnuva Chowdhury","Tadashi Maeno","Fatih Furkan Akman","Joseph Boudreau","Sankha Dutta","Shengyu Feng","Adolfy Hoisie","Kuan-Chieh Hsu","Raees Khan","Jaehyung Kim","Ozgur O. Kilic","Scott Klasky","Alexei Klimentov","Tatiana Korchuganova","Verena Ingrid Martinez Outschoorn","Paul Nilsson","David K. Park","Norbert Podhorszki","Yihui Ren","John Rembrandt Steele","Frédéric Suter","Sairam Sri Vatsavai","Torre Wenaus","Wei Yang","Yiming Yang","Shinjae Yoo"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17820v1","updated":"2025-12-19T17:24:12Z","published":"2025-12-19T17:24:12Z","title":"Exploiting ID-Text Complementarity via Ensembling for Sequential Recommendation","summary":"Modern Sequential Recommendation (SR) models commonly utilize modality features to represent items, motivated in large part by recent advancements in language and vision modeling. To do so, several works completely replace ID embeddings with modality embeddings, claiming that modality embeddings render ID embeddings unnecessary because they can match or even exceed ID embedding performance. On the other hand, many works jointly utilize ID and modality features, but posit that complex fusion strategies, such as multi-stage training and/or intricate alignment architectures, are necessary for this joint utilization. However, underlying both these lines of work is a lack of understanding of the complementarity of ID and modality features. In this work, we address this gap by studying the complementarity of ID- and text-based SR models. We show that these models do learn complementary signals, meaning that either should provide performance gain when used properly alongside the other. Motivated by this, we propose a new SR method that preserves ID-text complementarity through independent model training, then harnesses it through a simple ensembling strategy. Despite this method's simplicity, we show it outperforms several competitive SR baselines, implying that both ID and text features are necessary to achieve state-of-the-art SR performance but complex fusion architectures are not.","authors":["Liam Collins","Bhuvesh Kumar","Clark Mingxuan Ju","Tong Zhao","Donald Loveland","Leonardo Neves","Neil Shah"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2506.10892v3","updated":"2025-12-19T17:14:07Z","published":"2025-06-12T16:55:35Z","title":"The Diffusion Duality","summary":"Uniform-state discrete diffusion models hold the promise of fast text generation due to their inherent ability to self-correct. However, they are typically outperformed by autoregressive models and masked diffusion models. In this work, we narrow this performance gap by leveraging a key insight: Uniform-state diffusion processes naturally emerge from an underlying Gaussian diffusion. Our method, Duo, transfers powerful techniques from Gaussian diffusion to improve both training and sampling. First, we introduce a curriculum learning strategy guided by the Gaussian process, doubling training speed by reducing variance. Models trained with curriculum learning surpass autoregressive models in zero-shot perplexity on 3 of 7 benchmarks. Second, we present Discrete Consistency Distillation, which adapts consistency distillation from the continuous to the discrete setting. This algorithm unlocks few-step generation in diffusion language models by accelerating sampling by two orders of magnitude. We provide the code, model checkpoints, and video tutorials on the project page: http://s-sahoo.github.io/duo","authors":["Subham Sekhar Sahoo","Justin Deschenaux","Aaron Gokaslan","Guanghan Wang","Justin Chiu","Volodymyr Kuleshov"],"pdf_url":"","comment":"ICML 2025. We provide the code at: https://github.com/s-sahoo/duo [v3] includes improved theory, clearer presentation, and a new future work section"},{"id":"http://arxiv.org/abs/2512.17800v1","updated":"2025-12-19T17:02:58Z","published":"2025-12-19T17:02:58Z","title":"Domain-Aware Quantum Circuit for QML","summary":"Designing parameterized quantum circuits (PQCs) that are expressive, trainable, and robust to hardware noise is a central challenge for quantum machine learning (QML) on noisy intermediate-scale quantum (NISQ) devices. We present a Domain-Aware Quantum Circuit (DAQC) that leverages image priors to guide locality-preserving encoding and entanglement via non-overlapping DCT-style zigzag windows. The design employs interleaved encode-entangle-train cycles, where entanglement is applied among qubits hosting neighboring pixels, aligned to device connectivity. This staged, locality-preserving information flow expands the effective receptive field without deep global mixing, enabling efficient use of limited depth and qubits. The design concentrates representational capacity on short-range correlations, reduces long-range two-qubit operations, and encourages stable optimization, thereby mitigating depth-induced and globally entangled barren-plateau effects. We evaluate DAQC on MNIST, FashionMNIST, and PneumoniaMNIST datasets. On quantum hardware, DAQC achieves performance competitive with strong classical baselines (e.g., ResNet-18/50, DenseNet-121, EfficientNet-B0) and substantially outperforming Quantum Circuit Search (QCS) baselines. To the best of our knowledge, DAQC, which uses a quantum feature extractor with only a linear classical readout (no deep classical backbone), currently achieves the best reported performance on real quantum hardware for QML-based image classification tasks. Code and pretrained models are available at: https://github.com/gurinder-hub/DAQC.","authors":["Gurinder Singh","Thaddeus Pellegrini","Kenneth M. Merz,"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2509.18057v6","updated":"2025-12-19T16:58:50Z","published":"2025-09-22T17:30:33Z","title":"Reinforced Generation of Combinatorial Structures: Hardness of Approximation","summary":"Can AI based methods help us make advances in complexity theory? We provide evidence towards answering this in the affirmative, using AlphaEvolve (an LLM code mutation agent) to obtain new results in three settings:\n  a) We improve a recent result of Kunisky and Yu to obtain near-optimal upper and (conditional) lower bounds on certification algorithms for MAX-CUT and MAX-Independent Set on random 3- and 4-regular graphs. Our improved lower bounds are obtained by constructing nearly extremal Ramanujan graphs on as many as $163$ vertices, and our upper bounds are obtained via analytical arguments.\n  b) We obtain new inapproximability results for MAX-4-CUT and MAX-3-CUT, proving that it is NP-hard to approximate them within factors of $0.987$ and $0.9649$ respectively, using AlphaEvolve to discover new gadget reductions. Our MAX-4-CUT result improves upon the SOTA of $0.9883$, and our MAX-3-CUT result improves on the current best gadget-based inapproximability result of $0.9853$, but falls short of the SOTA of $16/17$ that relies on a custom PCP (rather than a reduction from ``standard'' Håstad-style PCPs).\n  c) Inapproximability for the metric Traveling Salesman Problem (TSP): We show that it is NP-hard to approximate the minimum cost tour within a factor of $111/110$ using AlphaEvolve to discover a new gadget, thus improving the SOTA of $117/116$. Along the way, we provide new modular soundness and completeness arguments that can be of independent interest.\n  A key technical challenge we faced: verifying a candidate construction produced by AlphaEvolve is costly (sometimes requiring time exponential in the size of the construction). We used AlphaEvolve itself to evolve the verification procedure to be faster (sometimes by $10,000\\times$ for our gadgets). Our results suggest that gadget based proofs would benefit from a pass through AI-based tools to obtain stronger results.","authors":["Ansh Nagda","Prabhakar Raghavan","Abhradeep Thakurta"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17788v1","updated":"2025-12-19T16:58:31Z","published":"2025-12-19T16:58:31Z","title":"Calibratable Disambiguation Loss for Multi-Instance Partial-Label Learning","summary":"Multi-instance partial-label learning (MIPL) is a weakly supervised framework that extends the principles of multi-instance learning (MIL) and partial-label learning (PLL) to address the challenges of inexact supervision in both instance and label spaces. However, existing MIPL approaches often suffer from poor calibration, undermining classifier reliability. In this work, we propose a plug-and-play calibratable disambiguation loss (CDL) that simultaneously improves classification accuracy and calibration performance. The loss has two instantiations: the first one calibrates predictions based on probabilities from the candidate label set, while the second one integrates probabilities from both candidate and non-candidate label sets. The proposed CDL can be seamlessly incorporated into existing MIPL and PLL frameworks. We provide a theoretical analysis that establishes the lower bound and regularization properties of CDL, demonstrating its superiority over conventional disambiguation losses. Experimental results on benchmark and real-world datasets confirm that our CDL significantly enhances both classification and calibration performance.","authors":["Wei Tang","Yin-Fang Yang","Weijia Zhang","Min-Ling Zhang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2405.00645v3","updated":"2025-12-19T16:57:39Z","published":"2024-05-01T17:18:46Z","title":"HGQ: High Granularity Quantization for Real-time Neural Networks on FPGAs","summary":"Neural networks with sub-microsecond inference latency are required by many critical applications. Targeting such applications deployed on FPGAs, we present High Granularity Quantization (HGQ), a quantization-aware training framework that optimizes parameter bit-widths through gradient descent. Unlike conventional methods, HGQ determines the optimal bit-width for each parameter independently, making it suitable for hardware platforms supporting heterogeneous arbitrary precision arithmetic. In our experiments, HGQ shows superior performance compared to existing network compression methods, achieving orders of magnitude reduction in resource consumption and latency while maintaining the accuracy on several benchmark tasks. These improvements enable the deployment of complex models previously infeasible due to resource or latency constraints. HGQ is open-source and is used for developing next-generation trigger systems at the CERN ATLAS and CMS experiments for particle physics, enabling the use of advanced machine learning models for real-time data selection with sub-microsecond latency.","authors":["Chang Sun","Zhiqiang Que","Thea K. Årrestad","Vladimir Loncar","Jennifer Ngadiuba","Wayne Luk","Maria Spiropulu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17774v1","updated":"2025-12-19T16:45:23Z","published":"2025-12-19T16:45:23Z","title":"MedNeXt-v2: Scaling 3D ConvNeXts for Large-Scale Supervised Representation Learning in Medical Image Segmentation","summary":"Large-scale supervised pretraining is rapidly reshaping 3D medical image segmentation. However, existing efforts focus primarily on increasing dataset size and overlook the question of whether the backbone network is an effective representation learner at scale. In this work, we address this gap by revisiting ConvNeXt-based architectures for volumetric segmentation and introducing MedNeXt-v2, a compound-scaled 3D ConvNeXt that leverages improved micro-architecture and data scaling to deliver state-of-the-art performance. First, we show that routinely used backbones in large-scale pretraining pipelines are often suboptimal. Subsequently, we use comprehensive backbone benchmarking prior to scaling and demonstrate that stronger from scratch performance reliably predicts stronger downstream performance after pretraining. Guided by these findings, we incorporate a 3D Global Response Normalization module and use depth, width, and context scaling to improve our architecture for effective representation learning. We pretrain MedNeXt-v2 on 18k CT volumes and demonstrate state-of-the-art performance when fine-tuning across six challenging CT and MR benchmarks (144 structures), showing consistent gains over seven publicly released pretrained models. Beyond improvements, our benchmarking of these models also reveals that stronger backbones yield better results on similar data, representation scaling disproportionately benefits pathological segmentation, and that modality-specific pretraining offers negligible benefit once full finetuning is applied. In conclusion, our results establish MedNeXt-v2 as a strong backbone for large-scale supervised representation learning in 3D Medical Image Segmentation. Our code and pretrained models are made available with the official nnUNet repository at: https://www.github.com/MIC-DKFZ/nnUNet","authors":["Saikat Roy","Yannick Kirchhoff","Constantin Ulrich","Maximillian Rokuss","Tassilo Wald","Fabian Isensee","Klaus Maier-Hein"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17771v1","updated":"2025-12-19T16:43:07Z","published":"2025-12-19T16:43:07Z","title":"Easy Adaptation: An Efficient Task-Specific Knowledge Injection Method for Large Models in Resource-Constrained Environments","summary":"While the enormous parameter scale endows Large Models (LMs) with unparalleled performance, it also limits their adaptability across specific tasks. Parameter-Efficient Fine-Tuning (PEFT) has emerged as a critical approach for effectively adapting LMs to a diverse range of downstream tasks. However, existing PEFT methods face two primary challenges: (1) High resource cost. Although PEFT methods significantly reduce resource demands compared to full fine-tuning, it still requires substantial time and memory, making it impractical in resource-constrained environments. (2) Parameter dependency. PEFT methods heavily rely on updating a subset of parameters associated with LMs to incorporate task-specific knowledge. Yet, due to increasing competition in the LMs landscape, many companies have adopted closed-source policies for their leading models, offering access only via Application Programming Interface (APIs). Whereas, the expense is often cost-prohibitive and difficult to sustain, as the fine-tuning process of LMs is extremely slow. Even if small models perform far worse than LMs in general, they can achieve superior results on particular distributions while requiring only minimal resources. Motivated by this insight, we propose Easy Adaptation (EA), which designs Specific Small Models (SSMs) to complement the underfitted data distribution for LMs. Extensive experiments show that EA matches the performance of PEFT on diverse tasks without accessing LM parameters, and requires only minimal resources.","authors":["Dong Chen","Zhengqing Hu","Shixing Zhao","Yibo Guo"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17762v1","updated":"2025-12-19T16:34:27Z","published":"2025-12-19T16:34:27Z","title":"Can You Hear Me Now? A Benchmark for Long-Range Graph Propagation","summary":"Effectively capturing long-range interactions remains a fundamental yet unresolved challenge in graph neural network (GNN) research, critical for applications across diverse fields of science. To systematically address this, we introduce ECHO (Evaluating Communication over long HOps), a novel benchmark specifically designed to rigorously assess the capabilities of GNNs in handling very long-range graph propagation. ECHO includes three synthetic graph tasks, namely single-source shortest paths, node eccentricity, and graph diameter, each constructed over diverse and structurally challenging topologies intentionally designed to introduce significant information bottlenecks. ECHO also includes two real-world datasets, ECHO-Charge and ECHO-Energy, which define chemically grounded benchmarks for predicting atomic partial charges and molecular total energies, respectively, with reference computations obtained at the density functional theory (DFT) level. Both tasks inherently depend on capturing complex long-range molecular interactions. Our extensive benchmarking of popular GNN architectures reveals clear performance gaps, emphasizing the difficulty of true long-range propagation and highlighting design choices capable of overcoming inherent limitations. ECHO thereby sets a new standard for evaluating long-range information propagation, also providing a compelling example for its need in AI for science.","authors":["Luca Miglior","Matteo Tolloso","Alessio Gravina","Davide Bacciu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17759v1","updated":"2025-12-19T16:32:31Z","published":"2025-12-19T16:32:31Z","title":"Breast Cancer Neoadjuvant Chemotherapy Treatment Response Prediction Using Aligned Longitudinal MRI and Clinical Data","summary":"Aim: This study investigates treatment response prediction to neoadjuvant chemotherapy (NACT) in breast cancer patients, using longitudinal contrast-enhanced magnetic resonance images (CE-MRI) and clinical data. The goal is to develop machine learning (ML) models to predict pathologic complete response (PCR binary classification) and 5-year relapse-free survival status (RFS binary classification). Method: The proposed framework includes tumour segmentation, image registration, feature extraction, and predictive modelling. Using the image registration method, MRI image features can be extracted and compared from the original tumour site at different time points, therefore monitoring the intratumor changes during NACT process. Four feature extractors, including one radiomics and three deep learning-based (MedicalNet, Segformer3D, SAM-Med3D) were implemented and compared. In combination with three feature selection methods and four ML models, predictive models are built and compared. Results: The proposed image registration-based feature extraction consistently improves the predictive models. In the PCR and RFS classification tasks logistic regression model trained on radiomic features performed the best with an AUC of 0.88 and classification accuracy of 0.85 for PCR classification, and AUC of 0.78 and classification accuracy of 0.72 for RFS classification. Conclusions: It is evidenced that the image registration method has significantly improved performance in longitudinal feature learning in predicting PCR and RFS. The radiomics feature extractor is more effective than the pre-trained deep learning feature extractors, with higher performance and better interpretability.","authors":["Rahul Ravi","Ruizhe Li","Tarek Abdelfatah","Stephen Chan","Xin Chen"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2511.14745v2","updated":"2025-12-19T16:32:22Z","published":"2025-11-18T18:45:32Z","title":"Look-Ahead Reasoning on Learning Platforms","summary":"On many learning platforms, the optimization criteria guiding model training reflect the priorities of the designer rather than those of the individuals they affect. Consequently, users may act strategically to obtain more favorable outcomes. While past work has studied strategic user behavior on learning platforms, the focus has largely been on strategic responses to a deployed model, without considering the behavior of other users. In contrast, look-ahead reasoning takes into account that user actions are coupled, and -- at scale -- impact future predictions. Within this framework, we first formalize level-k thinking, a concept from behavioral economics, where users aim to outsmart their peers by looking one step ahead. We show that, while convergence to an equilibrium is accelerated, the equilibrium remains the same, providing no benefit of higher-level reasoning for individuals in the long run. Then, we focus on collective reasoning, where users take coordinated actions by optimizing through their joint impact on the model. By contrasting collective with selfish behavior, we characterize the benefits and limits of coordination; a new notion of alignment between the learner's and the users' utilities emerges as a key concept. Look-ahead reasoning can be seen as a generalization of algorithmic collective action; we thus offer the first results characterizing the utility trade-offs of coordination when contesting algorithmic systems.","authors":["Haiqing Zhu","Tijana Zrnic","Celestine Mendler-Dünner"],"pdf_url":"","comment":"published at NeurIPS 2025"},{"id":"http://arxiv.org/abs/2508.10501v4","updated":"2025-12-19T16:27:34Z","published":"2025-08-14T10:03:47Z","title":"PASS: Probabilistic Agentic Supernet Sampling for Interpretable and Adaptive Chest X-Ray Reasoning","summary":"Existing tool-augmented agentic systems are limited in the real world by (i) black-box reasoning steps that undermine trust of decision-making and pose safety risks, (ii) poor multimodal integration, which is inherently critical for healthcare tasks, and (iii) rigid and computationally inefficient agentic pipelines. We introduce PASS (Probabilistic Agentic Supernet Sampling), the first multimodal framework to address these challenges in the context of Chest X-Ray (CXR) reasoning. PASS adaptively samples agentic workflows over a multi-tool graph, yielding decision paths annotated with interpretable probabilities. Given the complex CXR reasoning task with multimodal medical data, PASS leverages its learned task-conditioned distribution over the agentic supernet. Thus, it adaptively selects the most suitable tool at each supernet layer, offering probability-annotated trajectories for post-hoc audits and directly enhancing medical AI safety. PASS also continuously compresses salient findings into an evolving personalized memory, while dynamically deciding whether to deepen its reasoning path or invoke an early exit for efficiency. To optimize a Pareto frontier balancing performance and cost, we design a novel three-stage training procedure, including expert knowledge warm-up, contrastive path-ranking, and cost-aware reinforcement learning. To facilitate rigorous evaluation, we introduce CAB-E, a comprehensive benchmark for multi-step, safety-critical, free-form CXR reasoning. Experiments across various benchmarks validate that PASS significantly outperforms strong baselines in multiple metrics (e.g., accuracy, LLM-Judge, semantic similarity, etc.) while balancing computational costs, pushing a new paradigm shift towards interpretable, adaptive, and multimodal medical agentic systems.","authors":["Yushi Feng","Junye Du","Yingying Hong","Qifan Wang","Lequan Yu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2510.24830v2","updated":"2025-12-19T16:21:05Z","published":"2025-10-28T16:42:53Z","title":"The Generation Phases of Flow Matching: a Denoising Perspective","summary":"Flow matching has achieved remarkable success, yet the factors influencing the quality of its generation process remain poorly understood. In this work, we adopt a denoising perspective and design a framework to empirically probe the generation process. Laying down the formal connections between flow matching models and denoisers, we provide a common ground to compare their performances on generation and denoising. This enables the design of principled and controlled perturbations to influence sample generation: noise and drift. This leads to new insights on the distinct dynamical phases of the generative process, enabling us to precisely characterize at which stage of the generative process denoisers succeed or fail and why this matters.","authors":["Anne Gagneux","Ségolène Martin","Rémi Gribonval","Mathurin Massias"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2408.07588v5","updated":"2025-12-19T16:19:39Z","published":"2024-08-14T14:40:00Z","title":"Adjusting Model Size in Continual Gaussian Processes: How Big is Big Enough?","summary":"Many machine learning models require setting a parameter that controls their size before training, e.g. number of neurons in DNNs, or inducing points in GPs. Increasing capacity typically improves performance until all the information from the dataset is captured. After this point, computational cost keeps increasing, without improved performance. This leads to the question \"How big is big enough?\" We investigate this problem for Gaussian processes (single-layer neural networks) in continual learning. Here, data becomes available incrementally, and the final dataset size will therefore not be known before training, preventing the use of heuristics for setting a fixed model size. We develop a method to automatically adjust model size while maintaining near-optimal performance. Our experimental procedure follows the constraint that any hyperparameters must be set without seeing dataset properties, and we show that our method performs well across diverse datasets without the need to adjust its hyperparameter, showing it requires less tuning than others.","authors":["Guiomar Pescador-Barrios","Sarah Filippi","Mark van der Wilk"],"pdf_url":"","comment":"9 pages main, 27 pages total, 13 figures, 9 tables, conference paper, minor correction"},{"id":"http://arxiv.org/abs/2512.17720v1","updated":"2025-12-19T15:54:36Z","published":"2025-12-19T15:54:36Z","title":"Mitigating Forgetting in Low Rank Adaptation","summary":"Parameter-efficient fine-tuning methods, such as Low-Rank Adaptation (LoRA), enable fast specialization of large pre-trained models to different downstream applications. However, this process often leads to catastrophic forgetting of the model's prior domain knowledge. We address this issue with LaLoRA, a weight-space regularization technique that applies a Laplace approximation to Low-Rank Adaptation. Our approach estimates the model's confidence in each parameter and constrains updates in high-curvature directions, preserving prior knowledge while enabling efficient target-domain learning. By applying the Laplace approximation only to the LoRA weights, the method remains lightweight. We evaluate LaLoRA by fine-tuning a Llama model for mathematical reasoning and demonstrate an improved learning-forgetting trade-off, which can be directly controlled via the method's regularization strength. We further explore different loss landscape curvature approximations for estimating parameter confidence, analyze the effect of the data used for the Laplace approximation, and study robustness across hyperparameters.","authors":["Joanna Sliwa","Frank Schneider","Philipp Hennig","Jose Miguel Hernandez-Lobato"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2501.10290v3","updated":"2025-12-19T15:43:55Z","published":"2025-01-17T16:34:45Z","title":"Pairwise Elimination with Instance-Dependent Guarantees for Bandits with Cost Subsidy","summary":"Multi-armed bandits (MAB) are commonly used in sequential online decision-making when the reward of each decision is an unknown random variable. In practice however, the typical goal of maximizing total reward may be less important than minimizing the total cost of the decisions taken, subject to a reward constraint. For example, we may seek to make decisions that have at least the reward of a reference ``default'' decision, with as low a cost as possible. This problem was recently introduced in the Multi-Armed Bandits with Cost Subsidy (MAB-CS) framework. MAB-CS is broadly applicable to problem domains where a primary metric (cost) is constrained by a secondary metric (reward), and the rewards are unknown. In our work, we address variants of MAB-CS including ones with reward constrained by the reward of a known reference arm or by the subsidized best reward. We introduce the Pairwise-Elimination (PE) algorithm for the known reference arm variant and generalize PE to PE-CS for the subsidized best reward variant. Our instance-dependent analysis of PE and PE-CS reveals that both algorithms have an order-wise logarithmic upper bound on Cost and Quality Regret, making our policies the first with such a guarantee. Moreover, by comparing our upper and lower bound results we establish that PE is order-optimal for all known reference arm problem instances. Finally, experiments are conducted using the MovieLens 25M and Goodreads datasets for both PE and PE-CS revealing the effectiveness of PE and the superior balance between performance and reliability offered by PE-CS compared to baselines from the literature.","authors":["Ishank Juneja","Carlee Joe-Wong","Osman Yağan"],"pdf_url":"","comment":"ICLR 2025 Conference Paper"},{"id":"http://arxiv.org/abs/2409.03735v3","updated":"2025-12-19T15:41:38Z","published":"2024-09-05T17:50:31Z","title":"Privacy Bias in Language Models: A Contextual Integrity-based Auditing Metric","summary":"As large language models (LLMs) are integrated into sociotechnical systems, it is crucial to examine the privacy biases they exhibit. We define privacy bias as the appropriateness value of information flows in responses from LLMs. A deviation between privacy biases and expected values, referred to as privacy bias delta, may indicate privacy violations. As an auditing metric, privacy bias can help (a) model trainers evaluate the ethical and societal impact of LLMs, (b) service providers select context-appropriate LLMs, and (c) policymakers assess the appropriateness of privacy biases in deployed LLMs. We formulate and answer a novel research question: how can we reliably examine privacy biases in LLMs and the factors that influence them? We present a novel approach for assessing privacy biases using a contextual integrity-based methodology to evaluate the responses from various LLMs. Our approach accounts for the sensitivity of responses across prompt variations, which hinders the evaluation of privacy biases. Finally, we investigate how privacy biases are affected by model capacities and optimizations.","authors":["Yan Shvartzshnaider","Vasisht Duddu"],"pdf_url":"","comment":"Privacy Enhancing Technologies Symposium (PETS), 2026"},{"id":"http://arxiv.org/abs/2512.17703v1","updated":"2025-12-19T15:36:27Z","published":"2025-12-19T15:36:27Z","title":"Revisiting the Broken Symmetry Phase of Solid Hydrogen: A Neural Network Variational Monte Carlo Study","summary":"The crystal structure of high-pressure solid hydrogen remains a fundamental open problem. Although the research frontier has mostly shifted toward ultra-high pressure phases above 400 GPa, we show that even the broken symmetry phase observed around 130~GPa requires revisiting due to its intricate coupling of electronic and nuclear degrees of freedom. Here, we develop a first principle quantum Monte Carlo framework based on a deep neural network wave function that treats both electrons and nuclei quantum mechanically within the constant pressure ensemble. Our calculations reveal an unreported ground-state structure candidate for the broken symmetry phase with $Cmcm$ space group symmetry, and we test its stability up to 96 atoms. The predicted structure quantitatively matches the experimental equation of state and X-ray diffraction patterns. Furthermore, our group-theoretical analysis shows that the $Cmcm$ structure is compatible with existing Raman and infrared spectroscopic data. Crucially, static density functional theory calculation reveals the $Cmcm$ structure as a dynamically unstable saddle point on the Born-Oppenheimer potential energy surface, demonstrating that a full quantum many-body treatment of the problem is necessary. These results shed new light on the phase diagram of high-pressure hydrogen and call for further experimental verifications.","authors":["Shengdu Chai","Chen Lin","Xinyang Dong","Yuqiang Li","Wanli Ouyang","Lei Wang","X. C. Xie"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2402.12118v3","updated":"2025-12-19T15:36:27Z","published":"2024-02-19T13:13:16Z","title":"Sparse, Efficient and Explainable Data Attribution with DualXDA","summary":"Data Attribution (DA) is an emerging approach in the field of eXplainable Artificial Intelligence (XAI), aiming to identify influential training datapoints which determine model outputs. It seeks to provide transparency about the model and individual predictions, e.g. for model debugging, identifying data-related causes of suboptimal performance. However, existing DA approaches suffer from prohibitively high computational costs and memory demands when applied to even medium-scale datasets and models, forcing practitioners to resort to approximations that may fail to capture the true inference process of the underlying model. Additionally, current attribution methods exhibit low sparsity, resulting in non-negligible attribution scores across a high number of training examples, hindering the discovery of decisive patterns in the data. In this work, we introduce DualXDA, a framework for sparse, efficient and explainable DA, comprised of two interlinked approaches, Dual Data Attribution (DualDA) and eXplainable Data Attribution (XDA): With DualDA, we propose a novel approach for efficient and effective DA, leveraging Support Vector Machine theory to provide fast and naturally sparse data attributions for AI predictions. In extensive quantitative analyses, we demonstrate that DualDA achieves high attribution quality, excels at solving a series of evaluated downstream tasks, while at the same time improving explanation time by a factor of up to 4,100,000x compared to the original Influence Functions method, and up to 11,000x compared to the method's most efficient approximation from literature to date. We further introduce XDA, a method for enhancing Data Attribution with capabilities from feature attribution methods to explain why training samples are relevant for the prediction of a test sample in terms of impactful features, which we showcase and verify qualitatively in detail.","authors":["Galip Ümit Yolcu","Moritz Weckbecker","Thomas Wiegand","Wojciech Samek","Sebastian Lapuschkin"],"pdf_url":"","comment":"Accepted to Transactions on Machine Learning Research (TMLR), 2025"},{"id":"http://arxiv.org/abs/2511.09801v2","updated":"2025-12-19T15:36:03Z","published":"2025-11-12T23:05:14Z","title":"Generalized infinite dimensional Alpha-Procrustes based geometries","summary":"This work extends the recently introduced Alpha-Procrustes family of Riemannian metrics for symmetric positive definite (SPD) matrices by incorporating generalized versions of the Bures-Wasserstein (GBW), Log-Euclidean, and Wasserstein distances. While the Alpha-Procrustes framework has unified many classical metrics in both finite- and infinite- dimensional settings, it previously lacked the structural components necessary to realize these generalized forms. We introduce a formalism based on unitized Hilbert-Schmidt operators and an extended Mahalanobis norm that allows the construction of robust, infinite-dimensional generalizations of GBW and Log-Hilbert-Schmidt distances. Our approach also incorporates a learnable regularization parameter that enhances geometric stability in high-dimensional comparisons. Preliminary experiments reproducing benchmarks from the literature demonstrate the improved performance of our generalized metrics, particularly in scenarios involving comparisons between datasets of varying dimension and scale. This work lays a theoretical and computational foundation for advancing robust geometric methods in machine learning, statistical inference, and functional data analysis.","authors":["Salvish Goomanee","Andi Han","Pratik Jawanpuria","Bamdev Mishra"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17696v1","updated":"2025-12-19T15:32:24Z","published":"2025-12-19T15:32:24Z","title":"Spatially-informed transformers: Injecting geostatistical covariance biases into self-attention for spatio-temporal forecasting","summary":"The modeling of high-dimensional spatio-temporal processes presents a fundamental dichotomy between the probabilistic rigor of classical geostatistics and the flexible, high-capacity representations of deep learning. While Gaussian processes offer theoretical consistency and exact uncertainty quantification, their prohibitive computational scaling renders them impractical for massive sensor networks. Conversely, modern transformer architectures excel at sequence modeling but inherently lack a geometric inductive bias, treating spatial sensors as permutation-invariant tokens without a native understanding of distance. In this work, we propose a spatially-informed transformer, a hybrid architecture that injects a geostatistical inductive bias directly into the self-attention mechanism via a learnable covariance kernel. By formally decomposing the attention structure into a stationary physical prior and a non-stationary data-driven residual, we impose a soft topological constraint that favors spatially proximal interactions while retaining the capacity to model complex dynamics. We demonstrate the phenomenon of ``Deep Variography'', where the network successfully recovers the true spatial decay parameters of the underlying process end-to-end via backpropagation. Extensive experiments on synthetic Gaussian random fields and real-world traffic benchmarks confirm that our method outperforms state-of-the-art graph neural networks. Furthermore, rigorous statistical validation confirms that the proposed method delivers not only superior predictive accuracy but also well-calibrated probabilistic forecasts, effectively bridging the gap between physics-aware modeling and data-driven learning.","authors":["Yuri Calleo"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17689v1","updated":"2025-12-19T15:24:49Z","published":"2025-12-19T15:24:49Z","title":"Imputation Uncertainty in Interpretable Machine Learning Methods","summary":"In real data, missing values occur frequently, which affects the interpretation with interpretable machine learning (IML) methods. Recent work considers bias and shows that model explanations may differ between imputation methods, while ignoring additional imputation uncertainty and its influence on variance and confidence intervals. We therefore compare the effects of different imputation methods on the confidence interval coverage probabilities of the IML methods permutation feature importance, partial dependence plots and Shapley values. We show that single imputation leads to underestimation of variance and that, in most cases, only multiple imputation is close to nominal coverage.","authors":["Pegah Golchian","Marvin N. Wright"],"pdf_url":"","comment":"19 pages, 15 Figures, accepted at conference: IJCAI 2025 Workshop on Explainable Artificial Intelligence (Montreal, Canada)"},{"id":"http://arxiv.org/abs/2512.17688v1","updated":"2025-12-19T15:23:44Z","published":"2025-12-19T15:23:44Z","title":"Convergence Guarantees for Federated SARSA with Local Training and Heterogeneous Agents","summary":"We present a novel theoretical analysis of Federated SARSA (FedSARSA) with linear function approximation and local training. We establish convergence guarantees for FedSARSA in the presence of heterogeneity, both in local transitions and rewards, providing the first sample and communication complexity bounds in this setting. At the core of our analysis is a new, exact multi-step error expansion for single-agent SARSA, which is of independent interest. Our analysis precisely quantifies the impact of heterogeneity, demonstrating the convergence of FedSARSA with multiple local updates. Crucially, we show that FedSARSA achieves linear speed-up with respect to the number of agents, up to higher-order terms due to Markovian sampling. Numerical experiments support our theoretical findings.","authors":["Paul Mangold","Eloïse Berthier","Eric Moulines"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2511.12817v2","updated":"2025-12-19T15:20:17Z","published":"2025-11-16T22:58:22Z","title":"Assessing Automated Fact-Checking for Medical LLM Responses with Knowledge Graphs","summary":"The recent proliferation of large language models (LLMs) holds the potential to revolutionize healthcare, with strong capabilities in diverse medical tasks. Yet, deploying LLMs in high-stakes healthcare settings requires rigorous verification and validation to understand any potential harm. This paper investigates the reliability and viability of using medical knowledge graphs (KGs) for the automated factuality evaluation of LLM-generated responses. To ground this investigation, we introduce FAITH, a framework designed to systematically probe the strengths and limitations of this KG-based approach. FAITH operates without reference answers by decomposing responses into atomic claims, linking them to a medical KG, and scoring them based on evidence paths. Experiments on diverse medical tasks with human subjective evaluations demonstrate that KG-grounded evaluation achieves considerably higher correlations with clinician judgments and can effectively distinguish LLMs with varying capabilities. It is also robust to textual variances. The inherent explainability of its scoring can further help users understand and mitigate the limitations of current LLMs. We conclude that while limitations exist, leveraging KGs is a prominent direction for automated factuality assessment in healthcare.","authors":["Shasha Zhou","Mingyu Huang","Jack Cole","Charles Britton","Ming Yin","Jan Wolber","Ke Li"],"pdf_url":"","comment":"Accepted as a conference paper at AAAI'26"},{"id":"http://arxiv.org/abs/2512.17678v1","updated":"2025-12-19T15:17:34Z","published":"2025-12-19T15:17:34Z","title":"You Only Train Once: Differentiable Subset Selection for Omics Data","summary":"Selecting compact and informative gene subsets from single-cell transcriptomic data is essential for biomarker discovery, improving interpretability, and cost-effective profiling. However, most existing feature selection approaches either operate as multi-stage pipelines or rely on post hoc feature attribution, making selection and prediction weakly coupled. In this work, we present YOTO (you only train once), an end-to-end framework that jointly identifies discrete gene subsets and performs prediction within a single differentiable architecture. In our model, the prediction task directly guides which genes are selected, while the learned subsets, in turn, shape the predictive representation. This closed feedback loop enables the model to iteratively refine both what it selects and how it predicts during training. Unlike existing approaches, YOTO enforces sparsity so that only the selected genes contribute to inference, eliminating the need to train additional downstream classifiers. Through a multi-task learning design, the model learns shared representations across related objectives, allowing partially labeled datasets to inform one another, and discovering gene subsets that generalize across tasks without additional training steps. We evaluate YOTO on two representative single-cell RNA-seq datasets, showing that it consistently outperforms state-of-the-art baselines. These results demonstrate that sparse, end-to-end, multi-task gene subset selection improves predictive performance and yields compact and meaningful gene subsets, advancing biomarker discovery and single-cell analysis.","authors":["Daphné Chopard","Jorge da Silva Gonçalves","Irene Cannistraci","Thomas M. Sutter","Julia E. Vogt"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2508.17702v2","updated":"2025-12-19T15:15:01Z","published":"2025-08-25T06:21:11Z","title":"MolMark: Safeguarding Molecular Structures through Learnable Atom-Level Watermarking","summary":"AI-driven molecular generation is reshaping drug discovery and materials design, yet the lack of protection mechanisms leaves AI-generated molecules vulnerable to unauthorized reuse and provenance ambiguity. Such limitation undermines both scientific reproducibility and intellectual property security. To address this challenge, we propose the first deep learning based watermarking framework for molecules (MolMark), which is exquisitely designed to embed high-fidelity digital signatures into molecules without compromising molecular functionalities. MolMark learns to modulate the chemically meaningful atom-level representations and enforce geometric robustness through SE(3)-invariant features, maintaining robustness under rotation, translation, and reflection. Additionally, MolMark integrates seamlessly with AI-based molecular generative models, enabling watermarking to be treated as a learned transformation with minimal interference to molecular structures. Experiments on benchmark datasets (QM9, GEOM-DRUG) and state-of-the-art molecular generative models (GeoBFN, GeoLDM) demonstrate that MolMark can embed 16-bit watermarks while retaining more than 90% of essential molecular properties, preserving downstream performance, and enabling >95% extraction accuracy under SE(3) transformations. MolMark establishes a principled pathway for unifying molecular generation with verifiable authorship, supporting trustworthy and accountable AI-driven molecular discovery.","authors":["Runwen Hu","Peilin Chen","Keyan Ding","Shiqi Wang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2502.10784v5","updated":"2025-12-19T15:14:56Z","published":"2025-02-15T12:28:51Z","title":"Preconditioned Inexact Stochastic ADMM for Deep Model","summary":"The recent advancement of foundation models (FMs) has brought about a paradigm shift, revolutionizing various sectors worldwide. The popular optimizers used to train these models are stochastic gradient descent-based algorithms, which face inherent limitations, such as slow convergence and stringent assumptions for convergence. In particular, data heterogeneity arising from distributed settings poses significant challenges to their theoretical and numerical performance. This paper develops an algorithm, PISA (Preconditioned Inexact Stochastic Alternating Direction Method of Multipliers). Grounded in rigorous theoretical guarantees, the algorithm converges under the sole assumption of Lipschitz continuity of the gradient on a bounded region, thereby removing the need for other conditions commonly imposed by stochastic methods. This capability enables the proposed algorithm to tackle the challenge of data heterogeneity effectively. Moreover, the algorithmic architecture enables scalable parallel computing and supports various preconditions, such as second-order information, second moment, and orthogonalized momentum by Newton-Schulz iterations. Incorporating the latter two preconditions in PISA yields two computationally efficient variants: SISA and NSISA. Comprehensive experimental evaluations for training or fine-tuning diverse deep models, including vision models, large language models, reinforcement learning models, generative adversarial networks, and recurrent neural networks, demonstrate superior numerical performance of SISA and NSISA compared to various state-of-the-art optimizers.","authors":["Shenglong Zhou","Ouya Wang","Ziyan Luo","Yongxu Zhu","Geoffrey Ye Li"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17671v1","updated":"2025-12-19T15:14:14Z","published":"2025-12-19T15:14:14Z","title":"Polyharmonic Cascade","summary":"This paper presents a deep machine learning architecture, the \"polyharmonic cascade\" -- a sequence of packages of polyharmonic splines, where each layer is rigorously derived from the theory of random functions and the principles of indifference. This makes it possible to approximate nonlinear functions of arbitrary complexity while preserving global smoothness and a probabilistic interpretation. For the polyharmonic cascade, a training method alternative to gradient descent is proposed: instead of directly optimizing the coefficients, one solves a single global linear system on each batch with respect to the function values at fixed \"constellations\" of nodes. This yields synchronized updates of all layers, preserves the probabilistic interpretation of individual layers and theoretical consistency with the original model, and scales well: all computations reduce to 2D matrix operations efficiently executed on a GPU. Fast learning without overfitting on MNIST is demonstrated.","authors":["Yuriy N. Bakhvalov"],"pdf_url":"","comment":"Part 3 of 4 in the \"Polyharmonic Cascade\" cycle. Proposes a non-SGD training method based on global linear solvers. Previous papers: arXiv:2512.12731, arXiv.2512.16718. Source code is available at: https://github.com/xolod7/polyharmonic-cascade"},{"id":"http://arxiv.org/abs/2402.04114v3","updated":"2025-12-19T15:11:58Z","published":"2024-02-06T16:06:59Z","title":"SCAFFLSA: Taming Heterogeneity in Federated Linear Stochastic Approximation and TD Learning","summary":"In this paper, we analyze the sample and communication complexity of the federated linear stochastic approximation (FedLSA) algorithm. We explicitly quantify the effects of local training with agent heterogeneity. We show that the communication complexity of FedLSA scales polynomially with the inverse of the desired accuracy $ε$. To overcome this, we propose SCAFFLSA a new variant of FedLSA that uses control variates to correct for client drift, and establish its sample and communication complexities. We show that for statistically heterogeneous agents, its communication complexity scales logarithmically with the desired accuracy, similar to Scaffnew. An important finding is that, compared to the existing results for Scaffnew, the sample complexity scales with the inverse of the number of agents, a property referred to as linear speed-up. Achieving this linear speed-up requires completely new theoretical arguments. We apply the proposed method to federated temporal difference learning with linear function approximation and analyze the corresponding complexity improvements.","authors":["Paul Mangold","Sergey Samsonov","Safwan Labbi","Ilya Levin","Reda Alami","Alexey Naumov","Eric Moulines"],"pdf_url":"","comment":"now with linear speed-up!"},{"id":"http://arxiv.org/abs/2512.17661v1","updated":"2025-12-19T15:04:24Z","published":"2025-12-19T15:04:24Z","title":"Vidarc: Embodied Video Diffusion Model for Closed-loop Control","summary":"Robotic arm manipulation in data-scarce settings is a highly challenging task due to the complex embodiment dynamics and diverse contexts. Recent video-based approaches have shown great promise in capturing and transferring the temporal and physical interactions by pre-training on Internet-scale video data. However, such methods are often not optimized for the embodiment-specific closed-loop control, typically suffering from high latency and insufficient grounding. In this paper, we present Vidarc (Video Diffusion for Action Reasoning and Closed-loop Control), a novel autoregressive embodied video diffusion approach augmented by a masked inverse dynamics model. By grounding video predictions with action-relevant masks and incorporating real-time feedback through cached autoregressive generation, Vidarc achieves fast, accurate closed-loop control. Pre-trained on one million cross-embodiment episodes, Vidarc surpasses state-of-the-art baselines, achieving at least a 15% higher success rate in real-world deployment and a 91% reduction in latency. We also highlight its robust generalization and error correction capabilities across previously unseen robotic platforms.","authors":["Yao Feng","Chendong Xiang","Xinyi Mao","Hengkai Tan","Zuyue Zhang","Shuhe Huang","Kaiwen Zheng","Haitian Liu","Hang Su","Jun Zhu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17660v1","updated":"2025-12-19T15:03:00Z","published":"2025-12-19T15:03:00Z","title":"Fraud detection in credit card transactions using Quantum-Assisted Restricted Boltzmann Machines","summary":"Use cases for emerging quantum computing platforms become economically relevant as the efficiency of processing and availability of quantum computers increase. We assess the performance of Restricted Boltzmann Machines (RBM) assisted by quantum computing, running on real quantum hardware and simulators, using a real dataset containing 145 million transactions provided by Stone, a leading Brazilian fintech, for credit card fraud detection. The results suggest that the quantum-assisted RBM method is able to achieve superior performance in most figures of merit in comparison to classical approaches, even using current noisy quantum annealers. Our study paves the way for implementing quantum-assisted RBMs for general fault detection in financial systems.","authors":["João Marcos Cavalcanti de Albuquerque Neto","Gustavo Castro do Amaral","Guilherme Penello Temporão"],"pdf_url":"","comment":"8 pages, 3 figures"},{"id":"http://arxiv.org/abs/2512.17659v1","updated":"2025-12-19T14:59:27Z","published":"2025-12-19T14:59:27Z","title":"Generative Multi-Objective Bayesian Optimization with Scalable Batch Evaluations for Sample-Efficient De Novo Molecular Design","summary":"Designing molecules that must satisfy multiple, often conflicting objectives is a central challenge in molecular discovery. The enormous size of chemical space and the cost of high-fidelity simulations have driven the development of machine learning-guided strategies for accelerating design with limited data. Among these, Bayesian optimization (BO) offers a principled framework for sample-efficient search, while generative models provide a mechanism to propose novel, diverse candidates beyond fixed libraries. However, existing methods that couple the two often rely on continuous latent spaces, which introduces both architectural entanglement and scalability challenges. This work introduces an alternative, modular \"generate-then-optimize\" framework for de novo multi-objective molecular design/discovery. At each iteration, a generative model is used to construct a large, diverse pool of candidate molecules, after which a novel acquisition function, qPMHI (multi-point Probability of Maximum Hypervolume Improvement), is used to optimally select a batch of candidates most likely to induce the largest Pareto front expansion. The key insight is that qPMHI decomposes additively, enabling exact, scalable batch selection via only simple ranking of probabilities that can be easily estimated with Monte Carlo sampling. We benchmark the framework against state-of-the-art latent-space and discrete molecular optimization methods, demonstrating significant improvements across synthetic benchmarks and application-driven tasks. Specifically, in a case study related to sustainable energy storage, we show that our approach quickly uncovers novel, diverse, and high-performing organic (quinone-based) cathode materials for aqueous redox flow battery applications.","authors":["Madhav R. Muthyala","Farshud Sorourifar","Tianhong Tan","You Peng","Joel A. Paulson"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17654v1","updated":"2025-12-19T14:52:04Z","published":"2025-12-19T14:52:04Z","title":"Estimating Spatially Resolved Radiation Fields Using Neural Networks","summary":"We present an in-depth analysis on how to build and train neural networks to estimate the spatial distribution of scattered radiation fields for radiation protection dosimetry in medical radiation fields, such as those found in Interventional Radiology and Cardiology. Therefore, we present three different synthetically generated datasets with increasing complexity for training, using a Monte-Carlo Simulation application based on Geant4. On those datasets, we evaluate convolutional and fully connected architectures of neural networks to demonstrate which design decisions work well for reconstructing the fluence and spectra distributions over the spatial domain of such radiation fields. All used datasets as well as our training pipeline are published as open source in separate repositories.","authors":["Felix Lehner","Pasquale Lombardo","Susana Castillo","Oliver Hupe","Marcus Magnor"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2412.01389v2","updated":"2025-12-19T14:49:41Z","published":"2024-12-02T11:22:19Z","title":"Refined Analysis of Federated Averaging and Federated Richardson-Romberg","summary":"In this paper, we present a novel analysis of \\FedAvg with constant step size, relying on the Markov property of the underlying process. We demonstrate that the global iterates of the algorithm converge to a stationary distribution and analyze its resulting bias and variance relative to the problem's solution. We provide a first-order bias expansion in both homogeneous and heterogeneous settings. Interestingly, this bias decomposes into two distinct components: one that depends solely on stochastic gradient noise and another on client heterogeneity. Finally, we introduce a new algorithm based on the Richardson-Romberg extrapolation technique to mitigate this bias.","authors":["Paul Mangold","Alain Durmus","Aymeric Dieuleveut","Sergey Samsonov","Eric Moulines"],"pdf_url":"","comment":"37 pages"},{"id":"http://arxiv.org/abs/2509.21647v2","updated":"2025-12-19T14:43:46Z","published":"2025-09-25T22:05:20Z","title":"Automated Machine Learning Pipeline: Large Language Models-Assisted Automated Dataset Generation for Training Machine-Learned Interatomic Potentials","summary":"Machine learning interatomic potentials (MLIPs) have become powerful tools to extend molecular simulations beyond the limits of quantum methods, offering near-quantum accuracy at much lower computational cost. Yet, developing reliable MLIPs remains difficult because it requires generating high-quality datasets, preprocessing atomic structures, and carefully training and validating models. In this work, we introduce an Automated Machine Learning Pipeline (AMLP) that unifies the entire workflow from dataset creation to model validation. AMLP employs large-language-model agents to assist with electronic-structure code selection, input preparation, and output conversion, while its analysis suite (AMLP-Analysis), based on ASE supports a range of molecular simulations. The pipeline is built on the MACE architecture and validated on acridine polymorphs, where, with a straightforward fine-tuning of a foundation model, mean absolute errors of ~1.7 meV/atom in energies and ~7.0 meV/Å in forces are achieved. The fitted MLIP reproduces DFT geometries with sub-Å accuracy and demonstrates stability during molecular dynamics simulations in the microcanonical and canonical ensembles.","authors":["Adam Lahouari","Jutta Rogal","Mark E. Tuckerman"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17636v1","updated":"2025-12-19T14:37:07Z","published":"2025-12-19T14:37:07Z","title":"Trust-Region Adaptive Policy Optimization","summary":"Post-training methods, especially Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL), play an important role in improving large language models' (LLMs) complex reasoning abilities. However, the dominant two-stage pipeline (SFT then RL) suffers from a key inconsistency: SFT enforces rigid imitation that suppresses exploration and induces forgetting, limiting RL's potential for improvements. We address this inefficiency with TRAPO (\\textbf{T}rust-\\textbf{R}egion \\textbf{A}daptive \\textbf{P}olicy \\textbf{O}ptimization), a hybrid framework that interleaves SFT and RL within each training instance by optimizing SFT loss on expert prefixes and RL loss on the model's own completions, unifying external supervision and self-exploration. To stabilize training, we introduce Trust-Region SFT (TrSFT), which minimizes forward KL divergence inside a trust region but attenuates optimization outside, effectively shifting toward reverse KL and yielding stable, mode-seeking updates favorable for RL. An adaptive prefix-selection mechanism further allocates expert guidance based on measured utility. Experiments on five mathematical reasoning benchmarks show that TRAPO consistently surpasses standard SFT, RL, and SFT-then-RL pipelines, as well as recent state-of-the-art approaches, establishing a strong new paradigm for reasoning-enhanced LLMs.","authors":["Mingyu Su","Jian Guan","Yuxian Gu","Minlie Huang","Hongning Wang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17630v1","updated":"2025-12-19T14:33:14Z","published":"2025-12-19T14:33:14Z","title":"Confidence-Credibility Aware Weighted Ensembles of Small LLMs Outperform Large LLMs in Emotion Detection","summary":"This paper introduces a confidence-weighted, credibility-aware ensemble framework for text-based emotion detection, inspired by Condorcet's Jury Theorem (CJT). Unlike conventional ensembles that often rely on homogeneous architectures, our approach combines architecturally diverse small transformer-based large language models (sLLMs) - BERT, RoBERTa, DistilBERT, DeBERTa, and ELECTRA, each fully fine-tuned for emotion classification. To preserve error diversity, we minimize parameter convergence while taking advantage of the unique biases of each model. A dual-weighted voting mechanism integrates both global credibility (validation F1 score) and local confidence (instance-level probability) to dynamically weight model contributions. Experiments on the DAIR-AI dataset demonstrate that our credibility-confidence ensemble achieves a macro F1 score of 93.5 percent, surpassing state-of-the-art benchmarks and significantly outperforming large-scale LLMs, including Falcon, Mistral, Qwen, and Phi, even after task-specific Low-Rank Adaptation (LoRA). With only 595M parameters in total, our small LLMs ensemble proves more parameter-efficient and robust than models up to 7B parameters, establishing that carefully designed ensembles of small, fine-tuned models can outperform much larger LLMs in specialized natural language processing (NLP) tasks such as emotion detection.","authors":["Menna Elgabry","Ali Hamdi"],"pdf_url":"","comment":"Accepted at IRICT 2025"},{"id":"http://arxiv.org/abs/2512.17629v1","updated":"2025-12-19T14:33:02Z","published":"2025-12-19T14:33:02Z","title":"SCOPE: Sequential Causal Optimization of Process Interventions","summary":"Prescriptive Process Monitoring (PresPM) recommends interventions during business processes to optimize key performance indicators (KPIs). In realistic settings, interventions are rarely isolated: organizations need to align sequences of interventions to jointly steer the outcome of a case. Existing PresPM approaches fall short in this respect. Many focus on a single intervention decision, while others treat multiple interventions independently, ignoring how they interact over time. Methods that do address these dependencies depend either on simulation or data augmentation to approximate the process to train a Reinforcement Learning (RL) agent, which can create a reality gap and introduce bias. We introduce SCOPE, a PresPM approach that learns aligned sequential intervention recommendations. SCOPE employs backward induction to estimate the effect of each candidate intervention action, propagating its impact from the final decision point back to the first. By leveraging causal learners, our method can utilize observational data directly, unlike methods that require constructing process approximations for reinforcement learning. Experiments on both an existing synthetic dataset and a new semi-synthetic dataset show that SCOPE consistently outperforms state-of-the-art PresPM techniques in optimizing the KPI. The novel semi-synthetic setup, based on a real-life event log, is provided as a reusable benchmark for future work on sequential PresPM.","authors":["Jakob De Moor","Hans Weytjens","Johannes De Smedt","Jochen De Weerdt"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2502.06658v3","updated":"2025-12-19T14:26:04Z","published":"2025-02-10T16:48:48Z","title":"Generating Samples to Probe Trained Models","summary":"There is a growing need for investigating how machine learning models operate. With this work, we aim to understand trained machine learning models by questioning their data preferences. We propose a mathematical framework that allows us to probe trained models and identify their preferred samples in various scenarios including prediction-risky, parameter-sensitive, or model-contrastive samples. To showcase our framework, we pose these queries to a range of models trained on a range of classification and regression tasks, and receive answers in the form of generated data.","authors":["Eren Mehmet Kıral","Nurşen Aydın","Ş. İlker Birbil"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17607v1","updated":"2025-12-19T14:12:17Z","published":"2025-12-19T14:12:17Z","title":"More Consistent Accuracy PINN via Alternating Easy-Hard Training","summary":"Physics-informed neural networks (PINNs) have recently emerged as a prominent paradigm for solving partial differential equations (PDEs), yet their training strategies remain underexplored. While hard prioritization methods inspired by finite element methods are widely adopted, recent research suggests that easy prioritization can also be effective. Nevertheless, we find that both approaches exhibit notable trade-offs and inconsistent performance across PDE types. To address this issue, we develop a hybrid strategy that combines the strengths of hard and easy prioritization through an alternating training algorithm. On PDEs with steep gradients, nonlinearity, and high dimensionality, the proposed method achieves consistently high accuracy, with relative L2 errors mostly in the range of O(10^-5) to O(10^-6), significantly surpassing baseline methods. Moreover, it offers greater reliability across diverse problems, whereas compared approaches often suffer from variable accuracy depending on the PDE. This work provides new insights into designing hybrid training strategies to enhance the performance and robustness of PINNs.","authors":["Zhaoqian Gao","Min Yanga"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17598v1","updated":"2025-12-19T14:05:20Z","published":"2025-12-19T14:05:20Z","title":"A Systems-Theoretic View on the Convergence of Algorithms under Disturbances","summary":"Algorithms increasingly operate within complex physical, social, and engineering systems where they are exposed to disturbances, noise, and interconnections with other dynamical systems. This article extends known convergence guarantees of an algorithm operating in isolation (i.e., without disturbances) and systematically derives stability bounds and convergence rates in the presence of such disturbances. By leveraging converse Lyapunov theorems, we derive key inequalities that quantify the impact of disturbances. We further demonstrate how our result can be utilized to assess the effects of disturbances on algorithmic performance in a wide variety of applications, including communication constraints in distributed learning, sensitivity in machine learning generalization, and intentional noise injection for privacy. This underpins the role of our result as a unifying tool for algorithm analysis in the presence of noise, disturbances, and interconnections with other dynamical systems.","authors":["Guner Dilsad Er","Sebastian Trimpe","Michael Muehlebach"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17594v1","updated":"2025-12-19T14:02:37Z","published":"2025-12-19T14:02:37Z","title":"MAD-OOD: A Deep Learning Cluster-Driven Framework for an Out-of-Distribution Malware Detection and Classification","summary":"Out of distribution (OOD) detection remains a critical challenge in malware classification due to the substantial intra family variability introduced by polymorphic and metamorphic malware variants. Most existing deep learning based malware detectors rely on closed world assumptions and fail to adequately model this intra class variation, resulting in degraded performance when confronted with previously unseen malware families. This paper presents MADOOD, a novel two stage, cluster driven deep learning framework for robust OOD malware detection and classification. In the first stage, malware family embeddings are modeled using class conditional spherical decision boundaries derived from Gaussian Discriminant Analysis (GDA), enabling statistically grounded separation of indistribution and OOD samples without requiring OOD data during training. Z score based distance analysis across multiple class centroids is employed to reliably identify anomalous samples in the latent space. In the second stage, a deep neural network integrates cluster based predictions, refined embeddings, and supervised classifier outputs to enhance final classification accuracy. Extensive evaluations on benchmark malware datasets comprising 25 known families and multiple novel OOD variants demonstrate that MADOOD significantly outperforms state of the art OOD detection methods, achieving an AUC of up to 0.911 on unseen malware families. The proposed framework provides a scalable, interpretable, and statistically principled solution for real world malware detection and anomaly identification in evolving cybersecurity environments.","authors":["Tosin Ige","Christopher Kiekintveld","Aritran Piplai","Asif Rahman","Olukunle Kolade","Sasidhar Kunapuli"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17593v1","updated":"2025-12-19T14:01:50Z","published":"2025-12-19T14:01:50Z","title":"A Unified Representation of Neural Networks Architectures","summary":"In this paper we consider the limiting case of neural networks (NNs) architectures when the number of neurons in each hidden layer and the number of hidden layers tend to infinity thus forming a continuum, and we derive approximation errors as a function of the number of neurons and/or hidden layers. Firstly, we consider the case of neural networks with a single hidden layer and we derive an integral infinite width neural representation that generalizes existing continuous neural networks (CNNs) representations. Then we extend this to deep residual CNNs that have a finite number of integral hidden layers and residual connections. Secondly, we revisit the relation between neural ODEs and deep residual NNs and we formalize approximation errors via discretization techniques. Then, we merge these two approaches into a unified homogeneous representation of NNs as a Distributed Parameter neural Network (DiPaNet) and we show that most of the existing finite and infinite-dimensional NNs architectures are related via homogeneization/discretization with the DiPaNet representation. Our approach is purely deterministic and applies to general, uniformly continuous matrix weight functions. Differences and similarities with neural fields are discussed along with further possible generalizations and applications of the DiPaNet framework.","authors":["Christophe Prieur","Mircea Lazar","Bogdan Robu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17592v1","updated":"2025-12-19T13:59:46Z","published":"2025-12-19T13:59:46Z","title":"Sharing Knowledge without Sharing Data: Stitches can improve ensembles of disjointly trained models","summary":"Deep learning has been shown to be very capable at performing many real-world tasks. However, this performance is often dependent on the presence of large and varied datasets. In some settings, like in the medical domain, data is often fragmented across parties, and cannot be readily shared. While federated learning addresses this situation, it is a solution that requires synchronicity of parties training a single model together, exchanging information about model weights. We investigate how asynchronous collaboration, where only already trained models are shared (e.g. as part of a publication), affects performance, and propose to use stitching as a method for combining models.\n  Through taking a multi-objective perspective, where performance on each parties' data is viewed independently, we find that training solely on a single parties' data results in similar performance when merging with another parties' data, when considering performance on that single parties' data, while performance on other parties' data is notably worse. Moreover, while an ensemble of such individually trained networks generalizes better, performance on each parties' own dataset suffers. We find that combining intermediate representations in individually trained models with a well placed pair of stitching layers allows this performance to recover to a competitive degree while maintaining improved generalization, showing that asynchronous collaboration can yield competitive results.","authors":["Arthur Guijt","Dirk Thierens","Ellen Kerkhof","Jan Wiersma","Tanja Alderliesten","Peter A. N. Bosman"],"pdf_url":"","comment":"35 pages, 11 figures"},{"id":"http://arxiv.org/abs/2412.12667v2","updated":"2025-12-19T13:54:51Z","published":"2024-12-17T08:36:47Z","title":"Embedding-Driven Data Distillation for 360-Degree IQA With Residual-Aware Refinement","summary":"This article identifies and addresses a fundamental bottleneck in data-driven 360-degree image quality assessment (IQA): the lack of intelligent, sample-level data selection. Hence, we propose a novel framework that introduces a critical refinement step between patches sampling and model training. The core of our contribution is an embedding similarity-based selection algorithm that distills an initial, potentially redundant set of patches into a compact, maximally informative subset. This is formulated as a regularized optimization problem that preserves intrinsic perceptual relationships in a low-dimensional space, using residual analysis to explicitly filter out irrelevant or redundant samples. Extensive experiments on three benchmark datasets (CVIQ, OIQA, MVAQD) demonstrate that our selection enables a baseline model to match or exceed the performance of using all sampled data while keeping only 40-50% of patches. Particularly, we demonstrate the universal applicability of our approach by integrating it with several state-of-the-art IQA models, incleasy to deploy. Most significantly, its value as a generic,uding CNN- and transformer-based architectures, consistently enabling them to maintain or improve performance with 20-40\\% reduced computational load. This work establishes that adaptive, post-sampling data refinement is a powerful and widely applicable strategy for achieving efficient and robust 360-degree IQA.","authors":["Abderrezzaq Sendjasni","Seif-Eddine Benkabou","Mohamed-Chaker Larabi"],"pdf_url":"","comment":"Submitted to IEEE Transactions on Image Processing"},{"id":"http://arxiv.org/abs/2512.17586v1","updated":"2025-12-19T13:52:19Z","published":"2025-12-19T13:52:19Z","title":"Learning Safe Autonomous Driving Policies Using Predictive Safety Representations","summary":"Safe reinforcement learning (SafeRL) is a prominent paradigm for autonomous driving, where agents are required to optimize performance under strict safety requirements. This dual objective creates a fundamental tension, as overly conservative policies limit driving efficiency while aggressive exploration risks safety violations. The Safety Representations for Safer Policy Learning (SRPL) framework addresses this challenge by equipping agents with a predictive model of future constraint violations and has shown promise in controlled environments. This paper investigates whether SRPL extends to real-world autonomous driving scenarios. Systematic experiments on the Waymo Open Motion Dataset (WOMD) and NuPlan demonstrate that SRPL can improve the reward-safety tradeoff, achieving statistically significant improvements in success rate (effect sizes r = 0.65-0.86) and cost reduction (effect sizes r = 0.70-0.83), with p < 0.05 for observed improvements. However, its effectiveness depends on the underlying policy optimizer and the dataset distribution. The results further show that predictive safety representations play a critical role in improving robustness to observation noise. Additionally, in zero-shot cross-dataset evaluation, SRPL-augmented agents demonstrate improved generalization compared to non-SRPL methods. These findings collectively demonstrate the potential of predictive safety representations to strengthen SafeRL for autonomous driving.","authors":["Mahesh Keswani","Raunak Bhattacharyya"],"pdf_url":"","comment":"8 pages, 4 figures. Submitted to ICRA 2026"},{"id":"http://arxiv.org/abs/2512.17585v1","updated":"2025-12-19T13:52:11Z","published":"2025-12-19T13:52:11Z","title":"SkinGenBench: Generative Model and Preprocessing Effects for Synthetic Dermoscopic Augmentation in Melanoma Diagnosis","summary":"This work introduces SkinGenBench, a systematic biomedical imaging benchmark that investigates how preprocessing complexity interacts with generative model choice for synthetic dermoscopic image augmentation and downstream melanoma diagnosis. Using a curated dataset of 14,116 dermoscopic images from HAM10000 and MILK10K across five lesion classes, we evaluate the two representative generative paradigms: StyleGAN2-ADA and Denoising Diffusion Probabilistic Models (DDPMs) under basic geometric augmentation and advanced artifact removal pipelines. Synthetic melanoma images are assessed using established perceptual and distributional metrics (FID, KID, IS), feature space analysis, and their impact on diagnostic performance across five downstream classifiers. Experimental results demonstrate that generative architecture choice has a stronger influence on both image fidelity and diagnostic utility than preprocessing complexity. StyleGAN2-ADA consistently produced synthetic images more closely aligned with real data distributions, achieving the lowest FID (~65.5) and KID (~0.05), while diffusion models generated higher variance samples at the cost of reduces perceptual fidelity and class anchoring. Advanced artifact removal yielded only marginal improvements in generative metrics and provided limited downstream diagnostic gains, suggesting possible suppression of clinically relevant texture cues. In contrast, synthetic data augmentation substantially improved melanoma detection with 8-15% absolute gains in melanoma F1-score, and ViT-B/16 achieving F1~0.88 and ROC-AUC~0.98, representing an improvement of approximately 14% over non-augmented baselines. Our code can be found at https://github.com/adarsh-crafts/SkinGenBench","authors":["N. A. Adarsh Pritam","Jeba Shiney O","Sanyam Jain"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2505.15215v2","updated":"2025-12-19T13:45:03Z","published":"2025-05-21T07:44:39Z","title":"Clustering and Pruning in Causal Data Fusion","summary":"Data fusion, the process of combining observational and experimental data, can enable the identification of causal effects that would otherwise remain non-identifiable. Although identification algorithms have been developed for specific scenarios, do-calculus remains the only general-purpose tool for causal data fusion, particularly when variables are present in some data sources but not others. However, approaches based on do-calculus may encounter computational challenges as the number of variables increases and the causal graph grows in complexity. Consequently, there exists a need to reduce the size of such models while preserving the essential features. For this purpose, we propose pruning (removing unnecessary variables) and clustering (combining variables) as preprocessing operations for causal data fusion. We generalize earlier results on a single data source and derive conditions for applying pruning and clustering in the case of multiple data sources. We give sufficient conditions for inferring the identifiability or non-identifiability of a causal effect in a larger graph based on a smaller graph and show how to obtain the corresponding identifying functional for identifiable causal effects. Examples from epidemiology and social science demonstrate the use of the results.","authors":["Otto Tabell","Santtu Tikka","Juha Karvanen"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17577v1","updated":"2025-12-19T13:44:23Z","published":"2025-12-19T13:44:23Z","title":"Machine Learning for Static and Single-Event Dynamic Complex Network Analysis","summary":"The primary objective of this thesis is to develop novel algorithmic approaches for Graph Representation Learning of static and single-event dynamic networks. In such a direction, we focus on the family of Latent Space Models, and more specifically on the Latent Distance Model which naturally conveys important network characteristics such as homophily, transitivity, and the balance theory. Furthermore, this thesis aims to create structural-aware network representations, which lead to hierarchical expressions of network structure, community characterization, the identification of extreme profiles in networks, and impact dynamics quantification in temporal networks. Crucially, the methods presented are designed to define unified learning processes, eliminating the need for heuristics and multi-stage processes like post-processing steps. Our aim is to delve into a journey towards unified network embeddings that are both comprehensive and powerful, capable of characterizing network structures and adeptly handling the diverse tasks that graph analysis offers.","authors":["Nikolaos Nakis"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17574v1","updated":"2025-12-19T13:40:13Z","published":"2025-12-19T13:40:13Z","title":"Enabling Disaggregated Multi-Stage MLLM Inference via GPU-Internal Scheduling and Resource Sharing","summary":"Multimodal large language models (MLLMs) extend LLMs with visual understanding through a three-stage pipeline: multimodal preprocessing, vision encoding, and LLM inference. While these stages enhance capability, they introduce significant system bottlenecks. First, multimodal preprocessing-especially video decoding-often dominates Time-to-First-Token (TTFT). Most systems rely on CPU-based decoding, which severely limits throughput, while existing GPU-based approaches prioritize throughput-oriented parallelism and fail to meet the latency-sensitive requirements of MLLM inference. Second, the vision encoder is a standalone, compute-intensive stage that produces visual embeddings and cannot be co-batched with LLM prefill or decoding. This heterogeneity forces inter-stage blocking and increases token-generation latency. Even when deployed on separate GPUs, these stages underutilize available compute and memory resources, reducing overall utilization and constraining system throughput.\n  To address these challenges, we present FlashCodec and UnifiedServe, two complementary designs that jointly optimize the end-to-end MLLM pipeline. FlashCodec accelerates the multimodal preprocessing stage through collaborative multi-GPU video decoding, reducing decoding latency while preserving high throughput. UnifiedServe optimizes the vision-to-text and inference stages using a logically decoupled their execution to eliminate inter-stage blocking, yet physically sharing GPU resources to maximize GPU system utilization. By carefully orchestrating execution across stages and minimizing interference, UnifiedServe Together, our proposed framework forms an end-to-end optimized stack that can serve up to 3.0$\\times$ more requests or enforce 1.5$\\times$ tighter SLOs, while achieving up to 4.4$\\times$ higher throughput compared to state-of-the-art systems.","authors":["Lingxiao Zhao","Haoran Zhou","Yuezhi Che","Dazhao Cheng"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17570v1","updated":"2025-12-19T13:36:31Z","published":"2025-12-19T13:36:31Z","title":"GreedySnake: Accelerating SSD-Offloaded LLM Training with Efficient Scheduling and Optimizer Step Overlapping","summary":"SSD-offloaded training offers a practical and promising approach to making LLM training cost-effective. Building on gradient accumulation with micro-batches, this paper introduces GreedySnake, a new SSD-offloaded training system that employs vertical scheduling, which executes all microbatches of a layer before proceeding to the next. Compared to existing systems that use horizontal scheduling (i.e., executing micro-batches sequentially), GreedySnake achieves higher training throughput with smaller batch sizes, bringing the system much closer to the ideal scenario predicted by the roofline model. To further mitigate the I/O bottleneck, GreedySnake overlaps part of the optimization step with the forward pass of the next iteration. Experimental results on A100 GPUs show that GreedySnake achieves saturated training throughput improvements over ZeRO-Infinity: 1.96x on 1 GPU and 1.93x on 4 GPUs for GPT-65B, and 2.53x on 1 GPU for GPT-175B. The code is open-sourced at https://github.com/npz7yyk/GreedySnake","authors":["Yikang Yue","Yishu Yin","Xuehai Qian"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17569v1","updated":"2025-12-19T13:35:32Z","published":"2025-12-19T13:35:32Z","title":"Bayesian Optimisation: Which Constraints Matter?","summary":"Bayesian optimisation has proven to be a powerful tool for expensive global black-box optimisation problems. In this paper, we propose new Bayesian optimisation variants of the popular Knowledge Gradient acquisition functions for problems with \\emph{decoupled} black-box constraints, in which subsets of the objective and constraint functions may be evaluated independently. In particular, our methods aim to take into account that often only a handful of the constraints may be binding at the optimum, and hence we should evaluate only relevant constraints when trying to optimise a function. We empirically benchmark these methods against existing methods and demonstrate their superiority over the state-of-the-art.","authors":["Xietao Wang Lin","Juan Ungredda","Max Butler","James Town","Alma Rahat","Hemant Singh","Juergen Branke"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17562v1","updated":"2025-12-19T13:32:19Z","published":"2025-12-19T13:32:19Z","title":"When De-noising Hurts: A Systematic Study of Speech Enhancement Effects on Modern Medical ASR Systems","summary":"Speech enhancement methods are commonly believed to improve the performance of automatic speech recognition (ASR) in noisy environments. However, the effectiveness of these techniques cannot be taken for granted in the case of modern large-scale ASR models trained on diverse, noisy data. We present a systematic evaluation of MetricGAN-plus-voicebank denoising on four state-of-the-art ASR systems: OpenAI Whisper, NVIDIA Parakeet, Google Gemini Flash 2.0, Parrotlet-a using 500 medical speech recordings under nine noise conditions. ASR performance is measured using semantic WER (semWER), a normalized word error rate (WER) metric accounting for domain-specific normalizations. Our results reveal a counterintuitive finding: speech enhancement preprocessing degrades ASR performance across all noise conditions and models. Original noisy audio achieves lower semWER than enhanced audio in all 40 tested configurations (4 models x 10 conditions), with degradations ranging from 1.1% to 46.6% absolute semWER increase. These findings suggest that modern ASR models possess sufficient internal noise robustness and that traditional speech enhancement may remove acoustic features critical for ASR. For practitioners deploying medical scribe systems in noisy clinical environments, our results indicate that preprocessing audio with noise reduction techniques might not just be computationally wasteful but also be potentially harmful to the transcription accuracy.","authors":["Sujal Chondhekar","Vasanth Murukuri","Rushabh Vasani","Sanika Goyal","Rajshree Badami","Anushree Rana","Sanjana SN","Karthik Pandia","Sulabh Katiyar","Neha Jagadeesh","Sankalp Gulati"],"pdf_url":"","comment":"Technical Report"},{"id":"http://arxiv.org/abs/2512.16715v2","updated":"2025-12-19T13:06:55Z","published":"2025-12-18T16:18:06Z","title":"Towards Reproducibility in Predictive Process Mining: SPICE -- A Deep Learning Library","summary":"In recent years, Predictive Process Mining (PPM) techniques based on artificial neural networks have evolved as a method for monitoring the future behavior of unfolding business processes and predicting Key Performance Indicators (KPIs). However, many PPM approaches often lack reproducibility, transparency in decision making, usability for incorporating novel datasets and benchmarking, making comparisons among different implementations very difficult. In this paper, we propose SPICE, a Python framework that reimplements three popular, existing baseline deep-learning-based methods for PPM in PyTorch, while designing a common base framework with rigorous configurability to enable reproducible and robust comparison of past and future modelling approaches. We compare SPICE to original reported metrics and with fair metrics on 11 datasets.","authors":["Oliver Stritzel","Nick Hühnerbein","Simon Rauch","Itzel Zarate","Lukas Fleischmann","Moike Buck","Attila Lischka","Christian Frey"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17534v1","updated":"2025-12-19T12:58:06Z","published":"2025-12-19T12:58:06Z","title":"HydroGym: A Reinforcement Learning Platform for Fluid Dynamics","summary":"Modeling and controlling fluid flows is critical for several fields of science and engineering, including transportation, energy, and medicine. Effective flow control can lead to, e.g., lift increase, drag reduction, mixing enhancement, and noise reduction. However, controlling a fluid faces several significant challenges, including high-dimensional, nonlinear, and multiscale interactions in space and time. Reinforcement learning (RL) has recently shown great success in complex domains, such as robotics and protein folding, but its application to flow control is hindered by a lack of standardized benchmark platforms and the computational demands of fluid simulations. To address these challenges, we introduce HydroGym, a solver-independent RL platform for flow control research. HydroGym integrates sophisticated flow control benchmarks, scalable runtime infrastructure, and state-of-the-art RL algorithms. Our platform includes 42 validated environments spanning from canonical laminar flows to complex three-dimensional turbulent scenarios, validated over a wide range of Reynolds numbers. We provide non-differentiable solvers for traditional RL and differentiable solvers that dramatically improve sample efficiency through gradient-enhanced optimization. Comprehensive evaluation reveals that RL agents consistently discover robust control principles across configurations, such as boundary layer manipulation, acoustic feedback disruption, and wake reorganization. Transfer learning studies demonstrate that controllers learned at one Reynolds number or geometry adapt efficiently to new conditions, requiring approximately 50% fewer training episodes. The HydroGym platform is highly extensible and scalable, providing a framework for researchers in fluid dynamics, machine learning, and control to add environments, surrogate models, and control algorithms to advance science and technology.","authors":["Christian Lagemann","Sajeda Mokbel","Miro Gondrum","Mario Rüttgers","Jared Callaham","Ludger Paehler","Samuel Ahnert","Nicholas Zolman","Kai Lagemann","Nikolaus Adams","Matthias Meinke","Wolfgang Schröder","Jean-Christophe Loiseau","Esther Lagemann","Steven L. Brunton"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2511.12346v2","updated":"2025-12-19T12:55:37Z","published":"2025-11-15T20:25:59Z","title":"CLAReSNet: When Convolution Meets Latent Attention for Hyperspectral Image Classification","summary":"Hyperspectral image (HSI) classification faces critical challenges, including high spectral dimensionality, complex spectral-spatial correlations, and limited training samples with severe class imbalance. While CNNs excel at local feature extraction and transformers capture long-range dependencies, their isolated application yields suboptimal results due to quadratic complexity and insufficient inductive biases. We propose CLAReSNet (Convolutional Latent Attention Residual Spectral Network), a hybrid architecture that integrates multi-scale convolutional extraction with transformer-style attention via an adaptive latent bottleneck. The model employs a multi-scale convolutional stem with deep residual blocks and an enhanced Convolutional Block Attention Module for hierarchical spatial features, followed by spectral encoder layers combining bidirectional RNNs (LSTM/GRU) with Multi-Scale Spectral Latent Attention (MSLA). MSLA reduces complexity from $\\mathcal{O}(T^2D)$ to $\\mathcal{O}(T\\log(T)D)$ by adaptive latent token allocation (8-64 tokens) that scales logarithmically with the sequence length. Hierarchical cross-attention fusion dynamically aggregates multi-level representations for robust classification. Experiments conducted on the Indian Pines and Salinas datasets show state-of-the-art performance, achieving overall accuracies of 99.71% and 99.96%, significantly surpassing HybridSN, SSRN, and SpectralFormer. The learned embeddings exhibit superior inter-class separability and compact intra-class clustering, validating CLAReSNet's effectiveness under severe class imbalance.","authors":["Asmit Bandyopadhyay","Anindita Das Bhattacharjee","Rakesh Das"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17531v1","updated":"2025-12-19T12:54:03Z","published":"2025-12-19T12:54:03Z","title":"NetworkFF: Unified Layer Optimization in Forward-Only Neural Networks","summary":"The Forward-Forward algorithm eliminates backpropagation's memory constraints and biological implausibility through dual forward passes with positive and negative data. However, conventional implementations suffer from critical inter-layer isolation, where layers optimize goodness functions independently without leveraging collective learning dynamics. This isolation constrains representational coordination and limits convergence efficiency in deeper architectures. This paper introduces Collaborative Forward-Forward (CFF) learning, extending the original algorithm through inter-layer cooperation mechanisms that preserve forward-only computation while enabling global context integration. Our framework implements two collaborative paradigms: Fixed CFF (F-CFF) with constant inter-layer coupling and Adaptive CFF (A-CFF) with learnable collaboration parameters that evolve during training. The collaborative goodness function incorporates weighted contributions from all layers, enabling coordinated feature learning while maintaining memory efficiency and biological plausibility. Comprehensive evaluation on MNIST and Fashion-MNIST demonstrates significant performance improvements over baseline Forward-Forward implementations. These findings establish inter-layer collaboration as a fundamental enhancement to Forward-Forward learning, with immediate applicability to neuromorphic computing architectures and energy-constrained AI systems.","authors":["Salar Beigzad"],"pdf_url":"","comment":"Conference paper, IEEE, 2025"},{"id":"http://arxiv.org/abs/2512.17527v1","updated":"2025-12-19T12:51:31Z","published":"2025-12-19T12:51:31Z","title":"SafeBench-Seq: A Homology-Clustered, CPU-Only Baseline for Protein Hazard Screening with Physicochemical/Composition Features and Cluster-Aware Confidence Intervals","summary":"Foundation models for protein design raise concrete biosecurity risks, yet the community lacks a simple, reproducible baseline for sequence-level hazard screening that is explicitly evaluated under homology control and runs on commodity CPUs. We introduce SafeBench-Seq, a metadata-only, reproducible benchmark and baseline classifier built entirely from public data (SafeProtein hazards and UniProt benigns) and interpretable features (global physicochemical descriptors and amino-acid composition). To approximate \"never-before-seen\" threats, we homology-cluster the combined dataset at <=40% identity and perform cluster-level holdouts (no cluster overlap between train/test). We report discrimination (AUROC/AUPRC) and screening-operating points (TPR@1% FPR; FPR@95% TPR) with 95% bootstrap confidence intervals (n=200), and we provide calibrated probabilities via CalibratedClassifierCV (isotonic for Logistic Regression / Random Forest; Platt sigmoid for Linear SVM). We quantify probability quality using Brier score, Expected Calibration Error (ECE; 15 bins), and reliability diagrams. Shortcut susceptibility is probed via composition-preserving residue shuffles and length-/composition-only ablations. Empirically, random splits substantially overestimate robustness relative to homology-clustered evaluation; calibrated linear models exhibit comparatively good calibration, while tree ensembles retain slightly higher Brier/ECE. SafeBench-Seq is CPU-only, reproducible, and releases metadata only (accessions, cluster IDs, split labels), enabling rigorous evaluation without distributing hazardous sequences.","authors":["Muhammad Haris Khan"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2410.13161v2","updated":"2025-12-19T12:40:33Z","published":"2024-10-17T02:30:44Z","title":"Non-Perturbative Trivializing Flows for Lattice Gauge Theories","summary":"Continuous normalizing flows are known to be highly expressive and flexible, which allows for easier incorporation of large symmetries and makes them a powerful computational tool for lattice field theories. Building on previous work, we present a general continuous normalizing flow architecture for matrix Lie groups that is equivariant under group transformations. We apply this to lattice gauge theories in two dimensions as a proof of principle and demonstrate competitive performance, showing its potential as a tool for future lattice computations.","authors":["Mathis Gerdes","Pim de Haan","Roberto Bondesan","Miranda C. N. Cheng"],"pdf_url":"","comment":"7+7 pages, 5 figures, 4 tables; expanded published version, added 2 appendices with computational cost analysis and numerical evaluations (added 1 table and 2 figures)"},{"id":"http://arxiv.org/abs/2512.17517v1","updated":"2025-12-19T12:35:57Z","published":"2025-12-19T12:35:57Z","title":"PathBench-MIL: A Comprehensive AutoML and Benchmarking Framework for Multiple Instance Learning in Histopathology","summary":"We introduce PathBench-MIL, an open-source AutoML and benchmarking framework for multiple instance learning (MIL) in histopathology. The system automates end-to-end MIL pipeline construction, including preprocessing, feature extraction, and MIL-aggregation, and provides reproducible benchmarking of dozens of MIL models and feature extractors. PathBench-MIL integrates visualization tooling, a unified configuration system, and modular extensibility, enabling rapid experimentation and standardization across datasets and tasks. PathBench-MIL is publicly available at https://github.com/Sbrussee/PathBench-MIL","authors":["Siemen Brussee","Pieter A. Valkema","Jurre A. J. Weijer","Thom Doeleman","Anne M. R. Schrader","Jesper Kers"],"pdf_url":"","comment":"14 Pages, 3 Figures, 2 Appendices"},{"id":"http://arxiv.org/abs/2512.17515v1","updated":"2025-12-19T12:32:57Z","published":"2025-12-19T12:32:57Z","title":"Resource-efficient medical image classification for edge devices","summary":"Medical image classification is a critical task in healthcare, enabling accurate and timely diagnosis. However, deploying deep learning models on resource-constrained edge devices presents significant challenges due to computational and memory limitations. This research investigates a resource-efficient approach to medical image classification by employing model quantization techniques. Quantization reduces the precision of model parameters and activations, significantly lowering computational overhead and memory requirements without sacrificing classification accuracy. The study focuses on the optimization of quantization-aware training (QAT) and post-training quantization (PTQ) methods tailored for edge devices, analyzing their impact on model performance across medical imaging datasets. Experimental results demonstrate that quantized models achieve substantial reductions in model size and inference latency, enabling real-time processing on edge hardware while maintaining clinically acceptable diagnostic accuracy. This work provides a practical pathway for deploying AI-driven medical diagnostics in remote and resource-limited settings, enhancing the accessibility and scalability of healthcare technologies.","authors":["Mahsa Lavaei","Zahra Abadi","Salar Beigzad","Alireza Maleki"],"pdf_url":"","comment":"Conference paper published in ICAMIDA 2025 (IEEE)"},{"id":"http://arxiv.org/abs/2504.12392v2","updated":"2025-12-19T12:20:38Z","published":"2025-04-16T18:01:05Z","title":"A Survey on Archetypal Analysis","summary":"Archetypal analysis (AA) was originally proposed in 1994 by Adele Cutler and Leo Breiman as a computational procedure for extracting distinct aspects, so-called archetypes, from observations, with each observational record approximated as a mixture (i.e., convex combination) of these archetypes. AA thereby provides straightforward, interpretable, and explainable representations for feature extraction and dimensionality reduction, facilitating the understanding of the structure of high-dimensional data and enabling wide applications across the sciences. However, AA also faces challenges, particularly as the associated optimization problem is non-convex. This is the first survey that provides researchers and data mining practitioners with an overview of the methodologies and opportunities that AA offers, surveying the many applications of AA across disparate fields of science, as well as best practices for modeling data with AA and its limitations. The survey concludes by explaining crucial future research directions concerning AA.","authors":["Aleix Alcacer","Irene Epifanio","Sebastian Mair","Morten Mørup"],"pdf_url":"","comment":"27 pages, 14 figures, under review"},{"id":"http://arxiv.org/abs/2410.18686v2","updated":"2025-12-19T12:12:39Z","published":"2024-10-24T12:32:19Z","title":"Hierarchical Multimodal LLMs with Semantic Space Alignment for Enhanced Time Series Classification","summary":"Time series classification plays a fundamental role in a wide range of real-world applications. Recently, large language models (LLMs) have demonstrated strong generalization and reasoning capacities, but directly applying them to time series classification remains non-trivial due to the representation gap between numerical sequences and linguistic semantics. In this paper, we propose HiTime, a hierarchical LLM-based framework for multimodal time series classification that bridges structured temporal representations with semantic reasoning in a generative paradigm. Specifically, we design a hierarchical sequence feature encoding module composed of a data-specific encoder and a task-specific encoder to extract complementary temporal features. To mitigate the embedding gap between time series representations and textual semantics, we further introduce a semantic space alignment module that jointly performs coarse-grained global modeling and fine-grained cross-modal correspondence. Building upon the above representations, we employ a parameter-efficient supervised fine-tuning strategy to activate the generative classification capability of the algined LLMs, thereby transforming conventional discriminative time series classification into a generative task. Extensive experiments on multiple benchmarks demonstrate that the proposed framework consistently outperforms state-of-the-art baselines. The code is publicly available at https://github.com/Xiaoyu-Tao/HiTime.","authors":["Xiaoyu Tao","Tingyue Pan","Mingyue Cheng","Yucong Luo","Qi Liu","Enhong Chen"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17488v1","updated":"2025-12-19T11:59:41Z","published":"2025-12-19T11:59:41Z","title":"TwinSegNet: A Digital Twin-Enabled Federated Learning Framework for Brain Tumor Analysis","summary":"Brain tumor segmentation is critical in diagnosis and treatment planning for the disease. Yet, current deep learning methods rely on centralized data collection, which raises privacy concerns and limits generalization across diverse institutions. In this paper, we propose TwinSegNet, which is a privacy-preserving federated learning framework that integrates a hybrid ViT-UNet model with personalized digital twins for accurate and real-time brain tumor segmentation. Our architecture combines convolutional encoders with Vision Transformer bottlenecks to capture local and global context. Each institution fine-tunes the global model of private data to form its digital twin. Evaluated on nine heterogeneous MRI datasets, including BraTS 2019-2021 and custom tumor collections, TwinSegNet achieves high Dice scores (up to 0.90%) and sensitivity/specificity exceeding 90%, demonstrating robustness across non-independent and identically distributed (IID) client distributions. Comparative results against centralized models such as TumorVisNet highlight TwinSegNet's effectiveness in preserving privacy without sacrificing performance. Our approach enables scalable, personalized segmentation for multi-institutional clinical settings while adhering to strict data confidentiality requirements.","authors":["Almustapha A. Wakili","Adamu Hussaini","Abubakar A. Musa","Woosub Jung","Wei Yu"],"pdf_url":"","comment":"IEEE Virtual Conference on Communications. 4-6 November 2025"},{"id":"http://arxiv.org/abs/2507.17860v3","updated":"2025-12-19T11:48:41Z","published":"2025-07-23T18:33:27Z","title":"Towards Facilitated Fairness Assessment of AI-based Skin Lesion Classifiers Through GenAI-based Image Synthesis","summary":"Recent advances in deep learning and on-device inference could transform routine screening for skin cancers. Along with the anticipated benefits of this technology, potential dangers arise from unforeseen and inherent biases. A significant obstacle is building evaluation datasets that accurately reflect key demographics, including sex, age, and race, as well as other underrepresented groups. To address this, we train a state-of-the-art generative model to generate synthetic data in a controllable manner to assess the fairness of publicly available skin cancer classifiers. To evaluate whether synthetic images can be used as a fairness testing dataset, we prepare a real-image dataset (MILK10K) as a benchmark and compare the True Positive Rate result of three models (DeepGuide, MelaNet, and SkinLesionDensnet). As a result, the classification tendencies observed in each model when tested on real and generated images showed similar patterns across different attribute data sets. We confirm that highly realistic synthetic images facilitate model fairness verification.","authors":["Ko Watanabe","Stanislav Frolov","Aya Hassan","David Dembinsky","Adriano Lucieri","Andreas Dengel"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17477v1","updated":"2025-12-19T11:44:12Z","published":"2025-12-19T11:44:12Z","title":"Deep Learning-Based Surrogate Creep Modelling in Inconel 625: A High-Temperature Alloy Study","summary":"Time-dependent deformation, particularly creep, in high-temperature alloys such as Inconel 625 is a key factor in the long-term reliability of components used in aerospace and energy systems. Although Inconel 625 shows excellent creep resistance, finite-element creep simulations in tools such as ANSYS remain computationally expensive, often requiring tens of minutes for a single 10,000-hour run. This work proposes deep learning based surrogate models to provide fast and accurate replacements for such simulations. Creep strain data was generated in ANSYS using the Norton law under uniaxial stresses of 50 to 150 MPa and temperatures of 700 to 1000 $^\\circ$C, and this temporal dataset was used to train two architectures: a BiLSTM Variational Autoencoder for uncertainty-aware and generative predictions, and a BiLSTM Transformer hybrid that employs self-attention to capture long-range temporal behavior. Both models act as surrogate predictors, with the BiLSTM-VAE offering probabilistic output and the BiLSTM-Transformer delivering high deterministic accuracy. Performance is evaluated using RMSE, MAE, and $R^2$. Results show that the BiLSTM-VAE provides stable and reliable creep strain forecasts, while the BiLSTM-Transformer achieves strong accuracy across the full time range. Latency tests indicate substantial speedup: while each ANSYS simulation requires 30 to 40 minutes for a given stress-temperature condition, the surrogate models produce predictions within seconds. The proposed framework enables rapid creep assessment for design optimization and structural health monitoring, and provides a scalable solution for high-temperature alloy applications.","authors":["Shubham Das","Kaushal Singhania","Amit Sadhu","Suprabhat Das","Arghya Nandi"],"pdf_url":"","comment":"Presented in 10th International Congress on Computational Mechanics and Simulation (ICCMS) 2025, IIT Bhubaneswar"},{"id":"http://arxiv.org/abs/2512.17473v1","updated":"2025-12-19T11:40:06Z","published":"2025-12-19T11:40:06Z","title":"Alternating Direction Method of Multipliers for Nonlinear Matrix Decompositions","summary":"We present an algorithm based on the alternating direction method of multipliers (ADMM) for solving nonlinear matrix decompositions (NMD). Given an input matrix $X \\in \\mathbb{R}^{m \\times n}$ and a factorization rank $r \\ll \\min(m, n)$, NMD seeks matrices $W \\in \\mathbb{R}^{m \\times r}$ and $H \\in \\mathbb{R}^{r \\times n}$ such that $X \\approx f(WH)$, where $f$ is an element-wise nonlinear function. We evaluate our method on several representative nonlinear models: the rectified linear unit activation $f(x) = \\max(0, x)$, suitable for nonnegative sparse data approximation, the component-wise square $f(x) = x^2$, applicable to probabilistic circuit representation, and the MinMax transform $f(x) = \\min(b, \\max(a, x))$, relevant for recommender systems. The proposed framework flexibly supports diverse loss functions, including least squares, $\\ell_1$ norm, and the Kullback-Leibler divergence, and can be readily extended to other nonlinearities and metrics. We illustrate the applicability, efficiency, and adaptability of the approach on real-world datasets, highlighting its potential for a broad range of applications.","authors":["Atharva Awari","Nicolas Gillis","Arnaud Vandaele"],"pdf_url":"","comment":"14 pages, 6 figures. Code available from https://gitlab.com/Atharva05/admm-for-nmd"},{"id":"http://arxiv.org/abs/2512.17470v1","updated":"2025-12-19T11:33:30Z","published":"2025-12-19T11:33:30Z","title":"Translating the Rashomon Effect to Sequential Decision-Making Tasks","summary":"The Rashomon effect describes the phenomenon where multiple models trained on the same data produce identical predictions while differing in which features they rely on internally. This effect has been studied extensively in classification tasks, but not in sequential decision-making, where an agent learns a policy to achieve an objective by taking actions in an environment. In this paper, we translate the Rashomon effect to sequential decision-making. We define it as multiple policies that exhibit identical behavior, visiting the same states and selecting the same actions, while differing in their internal structure, such as feature attributions. Verifying identical behavior in sequential decision-making differs from classification. In classification, predictions can be directly compared to ground-truth labels. In sequential decision-making with stochastic transitions, the same policy may succeed or fail on any single trajectory due to randomness. We address this using formal verification methods that construct and compare the complete probabilistic behavior of each policy in the environment. Our experiments demonstrate that the Rashomon effect exists in sequential decision-making. We further show that ensembles constructed from the Rashomon set exhibit greater robustness to distribution shifts than individual policies. Additionally, permissive policies derived from the Rashomon set reduce computational requirements for verification while maintaining optimal performance.","authors":["Dennis Gross","Jørn Eirik Betten","Helge Spieker"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17466v1","updated":"2025-12-19T11:29:56Z","published":"2025-12-19T11:29:56Z","title":"Linear Attention for Joint Power Optimization and User-Centric Clustering in Cell-Free Networks","summary":"Optimal AP clustering and power allocation are critical in user-centric cell-free massive MIMO systems. Existing deep learning models lack flexibility to handle dynamic network configurations. Furthermore, many approaches overlook pilot contamination and suffer from high computational complexity. In this paper, we propose a lightweight transformer model that overcomes these limitations by jointly predicting AP clusters and powers solely from spatial coordinates of user devices and AP. Our model is architecture-agnostic to users load, handles both clustering and power allocation without channel estimation overhead, and eliminates pilot contamination by assigning users to AP within a pilot reuse constraint. We also incorporate a customized linear attention mechanism to capture user-AP interactions efficiently and enable linear scalability with respect to the number of users. Numerical results confirm the model's effectiveness in maximizing the minimum spectral efficiency and providing near-optimal performance while ensuring adaptability and scalability in dynamic scenarios.","authors":["Irched Chafaa","Giacomo Bacci","Luca Sanguinetti"],"pdf_url":"","comment":"Submitted"},{"id":"http://arxiv.org/abs/2512.17462v1","updated":"2025-12-19T11:25:18Z","published":"2025-12-19T11:25:18Z","title":"Behavioural Effects of Agentic Messaging: A Case Study on a Financial Service Application","summary":"Marketing and product personalisation provide a prominent and visible use-case for the application of Information Retrieval methods across several business domains. Recently, agentic approaches to these problems have been gaining traction. This work evaluates the behavioural and retention effects of agentic personalisation on a financial service application's customer communication system during a 2025 national tax filing period. Through a two month-long randomised controlled trial, we compare an agentic messaging approach against a business-as-usual (BAU) rule-based campaign system, focusing on two primary outcomes: unsubscribe behaviour and conversion timing. Empirical results show that agent-led messaging reduced unsubscribe events by 21\\% ($\\pm 0.01$) relative to BAU and increased early filing behaviour in the weeks preceding the national deadline. These findings demonstrate how adaptive, user-level decision-making systems can modulate engagement intensity whilst improving long-term retention indicators.","authors":["Olivier Jeunen","Schaun Wheeler"],"pdf_url":"","comment":"To appear in the 48th European Conference on Information Retrieval (ECIR '26) Industry Track"},{"id":"http://arxiv.org/abs/2512.17460v1","updated":"2025-12-19T11:21:12Z","published":"2025-12-19T11:21:12Z","title":"When Data Quality Issues Collide: A Large-Scale Empirical Study of Co-Occurring Data Quality Issues in Software Defect Prediction","summary":"Software Defect Prediction (SDP) models are central to proactive software quality assurance, yet their effectiveness is often constrained by the quality of available datasets. Prior research has typically examined single issues such as class imbalance or feature irrelevance in isolation, overlooking that real-world data problems frequently co-occur and interact. This study presents, to our knowledge, the first large-scale empirical analysis in SDP that simultaneously examines five co-occurring data quality issues (class imbalance, class overlap, irrelevant features, attribute noise, and outliers) across 374 datasets and five classifiers. We employ Explainable Boosting Machines together with stratified interaction analysis to quantify both direct and conditional effects under default hyperparameter settings, reflecting practical baseline usage.\n  Our results show that co-occurrence is nearly universal: even the least frequent issue (attribute noise) appears alongside others in more than 93% of datasets. Irrelevant features and imbalance are nearly ubiquitous, while class overlap is the most consistently harmful issue. We identify stable tipping points around 0.20 for class overlap, 0.65-0.70 for imbalance, and 0.94 for irrelevance, beyond which most models begin to degrade. We also uncover counterintuitive patterns, such as outliers improving performance when irrelevant features are low, underscoring the importance of context-aware evaluation. Finally, we expose a performance-robustness trade-off: no single learner dominates under all conditions.\n  By jointly analyzing prevalence, co-occurrence, thresholds, and conditional effects, our study directly addresses a persistent gap in SDP research. Hence, moving beyond isolated analyses to provide a holistic, data-aware understanding of how quality issues shape model performance in real-world settings.","authors":["Emmanuel Charleson Dapaah","Jens Grabowski"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.11529v2","updated":"2025-12-19T11:20:16Z","published":"2025-12-12T12:59:38Z","title":"xGR: Efficient Generative Recommendation Serving at Scale","summary":"Recommendation system delivers substantial economic benefits by providing personalized predictions. Generative recommendation (GR) integrates LLMs to enhance the understanding of long user-item sequences. Despite employing attention-based architectures, GR's workload differs markedly from that of LLM serving. GR typically processes long prompt while producing short, fixed-length outputs, yet the computational cost of each decode phase is especially high due to the large beam width. In addition, since the beam search involves a vast item space, the sorting overhead becomes particularly time-consuming. We propose xGR, a GR-oriented serving system that meets strict low-latency requirements under highconcurrency scenarios. First, xGR unifies the processing of prefill and decode phases through staged computation and separated KV cache. Second, xGR enables early sorting termination and mask-based item filtering with data structure reuse. Third, xGR reconstructs the overall pipeline to exploit multilevel overlap and multi-stream parallelism. Our experiments with real-world recommendation service datasets demonstrate that xGR achieves at least 3.49x throughput compared to the state-of-the-art baseline under strict latency constraints.","authors":["Qingxiao Sun","Tongxuan Liu","Shen Zhang","Siyu Wu","Peijun Yang","Haotian Liang","Menxin Li","Xiaolong Ma","Zhiwei Liang","Ziyi Ren","Minchao Zhang","Xinyu Liu","Ke Zhang","Depei Qian","Hailong Yang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.02342v2","updated":"2025-12-19T11:15:38Z","published":"2025-12-02T02:24:32Z","title":"Safeguarded Stochastic Polyak Step Sizes for Non-smooth Optimization: Robust Performance Without Small (Sub)Gradients","summary":"The stochastic Polyak step size (SPS) has proven to be a promising choice for stochastic gradient descent (SGD), delivering competitive performance relative to state-of-the-art methods on smooth convex and non-convex optimization problems, including deep neural network training. However, extensions of this approach to non-smooth settings remain in their early stages, often relying on interpolation assumptions or requiring knowledge of the optimal solution. In this work, we propose a novel SPS variant, Safeguarded SPS (SPS$_{safe}$), for the stochastic subgradient method, and provide rigorous convergence guarantees for non-smooth convex optimization with no need for strong assumptions. We further incorporate momentum into the update rule, yielding equally tight theoretical results. On non-smooth convex benchmarks, our experiments are consistent with the theoretical predictions on how the safeguard affects the convergence neighborhood. On deep neural networks the proposed step size achieves competitive performance to existing adaptive baselines and exhibits stable behavior across a wide range of problem settings. Moreover, in these experiments, the gradient norms under our step size do not collapse to (near) zero, indicating robustness to vanishing gradients.","authors":["Dimitris Oikonomou","Nicolas Loizou"],"pdf_url":"","comment":"28 pages, 15 figures"},{"id":"http://arxiv.org/abs/2512.17453v1","updated":"2025-12-19T11:12:20Z","published":"2025-12-19T11:12:20Z","title":"A lightweight Spatial-Temporal Graph Neural Network for Long-term Time Series Forecasting","summary":"We propose Lite-STGNN, a lightweight spatial-temporal graph neural network for long-term multivariate forecasting that integrates decomposition-based temporal modeling with learnable sparse graph structure. The temporal module applies trend-seasonal decomposition, while the spatial module performs message passing with low-rank Top-$K$ adjacency learning and conservative horizon-wise gating, enabling spatial corrections that enhance a strong linear baseline. Lite-STGNN achieves state-of-the-art accuracy on four benchmark datasets for horizons up to 720 steps, while being parameter-efficient and substantially faster to train than transformer-based methods. Ablation studies show that the spatial module yields 4.6% improvement over the temporal baseline, Top-$K$ enhances locality by 3.3%, and learned adjacency matrices reveal domain-specific interaction dynamics. Lite-STGNN thus offers a compact, interpretable, and efficient framework for long-term multivariate time series forecasting.","authors":["Henok Tenaw Moges","Deshendran Moodley"],"pdf_url":"","comment":"9 pages, 5 figures, 2 tables. Accepted for presentation at the 18th International Conference on Agents and Artificial Intelligence (ICAART 2026), Marbella, Spain"},{"id":"http://arxiv.org/abs/2512.17452v1","updated":"2025-12-19T11:08:58Z","published":"2025-12-19T11:08:58Z","title":"Learning What to Write: Write-Gated KV for Efficient Long-Context Inference","summary":"Long-context LLM inference is bottlenecked by the quadratic attention complexity and linear KV cache growth. Prior approaches mitigate this via post-hoc selection or eviction but overlook the root inefficiency: indiscriminate writing to persistent memory. In this paper, we formalize KV cache management as a causal system of three primitives: KV Admission, Selection, and Eviction. We instantiate KV Admission via Write-Gated KV, a lightweight mechanism that learns to predict token utility before it enters the cache. By filtering out low-utility states early to maintain a compact global cache alongside a sliding local cache, Write-Gated KV reduces memory usage by 46-57% and delivers 3.03-3.45$\\times$ prefill and 1.89-2.56$\\times$ decode speedups on Llama model with negligible accuracy loss, all while remaining compatible with FlashAttention and paged-KV systems. These results demonstrate that learning what to write, is a principled and practical recipe for efficient long-context inference. Code is available at https://github.com/EMCLab-Sinica/WG-KV .","authors":["Yen-Chieh Huang","Rui Fang","Ming-Syan Chen","Pi-Cheng Hsiu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17450v1","updated":"2025-12-19T11:06:46Z","published":"2025-12-19T11:06:46Z","title":"MULTIAQUA: A multimodal maritime dataset and robust training strategies for multimodal semantic segmentation","summary":"Unmanned surface vehicles can encounter a number of varied visual circumstances during operation, some of which can be very difficult to interpret. While most cases can be solved only using color camera images, some weather and lighting conditions require additional information. To expand the available maritime data, we present a novel multimodal maritime dataset MULTIAQUA (Multimodal Aquatic Dataset). Our dataset contains synchronized, calibrated and annotated data captured by sensors of different modalities, such as RGB, thermal, IR, LIDAR, etc. The dataset is aimed at developing supervised methods that can extract useful information from these modalities in order to provide a high quality of scene interpretation regardless of potentially poor visibility conditions. To illustrate the benefits of the proposed dataset, we evaluate several multimodal methods on our difficult nighttime test set. We present training approaches that enable multimodal methods to be trained in a more robust way, thus enabling them to retain reliable performance even in near-complete darkness. Our approach allows for training a robust deep neural network only using daytime images, thus significantly simplifying data acquisition, annotation, and the training process.","authors":["Jon Muhovič","Janez Perš"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17444v1","updated":"2025-12-19T10:56:34Z","published":"2025-12-19T10:56:34Z","title":"Assessing Long-Term Electricity Market Design for Ambitious Decarbonization Targets using Multi-Agent Reinforcement Learning","summary":"Electricity systems are key to transforming today's society into a carbon-free economy. Long-term electricity market mechanisms, including auctions, support schemes, and other policy instruments, are critical in shaping the electricity generation mix. In light of the need for more advanced tools to support policymakers and other stakeholders in designing, testing, and evaluating long-term markets, this work presents a multi-agent reinforcement learning model capable of capturing the key features of decarbonizing energy systems. Profit-maximizing generation companies make investment decisions in the wholesale electricity market, responding to system needs, competitive dynamics, and policy signals. The model employs independent proximal policy optimization, which was selected for suitability to the decentralized and competitive environment. Nevertheless, given the inherent challenges of independent learning in multi-agent settings, an extensive hyperparameter search ensures that decentralized training yields market outcomes consistent with competitive behavior. The model is applied to a stylized version of the Italian electricity system and tested under varying levels of competition, market designs, and policy scenarios. Results highlight the critical role of market design for decarbonizing the electricity sector and avoiding price volatility. The proposed framework allows assessing long-term electricity markets in which multiple policy and market mechanisms interact simultaneously, with market participants responding and adapting to decarbonization pathways.","authors":["Javier Gonzalez-Ruiz","Carlos Rodriguez-Pardo","Iacopo Savelli","Alice Di Bella","Massimo Tavoni"],"pdf_url":"","comment":"Accepted to Energy and AI. Code available in https://github.com/jjgonzalez2491/MARLEY_V1"},{"id":"http://arxiv.org/abs/2512.15675v3","updated":"2025-12-19T10:55:54Z","published":"2025-12-17T18:28:04Z","title":"Stylized Synthetic Augmentation further improves Corruption Robustness","summary":"This paper proposes a training data augmentation pipeline that combines synthetic image data with neural style transfer in order to address the vulnerability of deep vision models to common corruptions. We show that although applying style transfer on synthetic images degrades their quality with respect to the common Frechet Inception Distance (FID) metric, these images are surprisingly beneficial for model training. We conduct a systematic empirical analysis of the effects of both augmentations and their key hyperparameters on the performance of image classifiers. Our results demonstrate that stylization and synthetic data complement each other well and can be combined with popular rule-based data augmentation techniques such as TrivialAugment, while not working with others. Our method achieves state-of-the-art corruption robustness on several small-scale image classification benchmarks, reaching 93.54%, 74.9% and 50.86% robust accuracy on CIFAR-10-C, CIFAR-100-C and TinyImageNet-C, respectively","authors":["Georg Siedel","Rojan Regmi","Abhirami Anand","Weijia Shao","Silvia Vock","Andrey Morozov"],"pdf_url":"","comment":"Accepted at VISAPP 2026 conference"},{"id":"http://arxiv.org/abs/2508.20717v2","updated":"2025-12-19T10:45:33Z","published":"2025-08-28T12:37:25Z","title":"Unified Acoustic Representations for Screening Neurological and Respiratory Pathologies from Voice","summary":"Voice-based health assessment offers unprecedented opportunities for scalable, non-invasive disease screening, yet existing approaches typically focus on single conditions and fail to leverage the rich, multi-faceted information embedded in speech. We present MARVEL (Multi-task Acoustic Representations for Voice-based Health Analysis), a privacy-conscious multitask learning framework that simultaneously detects nine distinct neurological, respiratory, and voice disorders using only derived acoustic features, eliminating the need for raw audio transmission. Our dual-branch architecture employs specialized encoders with task-specific heads sharing a common acoustic backbone, enabling effective cross-condition knowledge transfer. Evaluated on the large-scale Bridge2AI-Voice v2.0 dataset, MARVEL achieves an overall AUROC of 0.78, with exceptional performance on neurological disorders (AUROC = 0.89), particularly for Alzheimer's disease/mild cognitive impairment (AUROC = 0.97). Our framework consistently outperforms single-modal baselines by 5-19% and surpasses state-of-the-art self-supervised models on 7 of 9 tasks, while correlation analysis reveals that the learned representations exhibit meaningful similarities with established acoustic features, indicating that the model's internal representations are consistent with clinically recognized acoustic patterns. By demonstrating that a single unified model can effectively screen for diverse conditions, this work establishes a foundation for deployable voice-based diagnostics in resource-constrained and remote healthcare settings.","authors":["Ran Piao","Yuan Lu","Hareld Kemps","Tong Xia","Aaqib Saeed"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17426v1","updated":"2025-12-19T10:23:38Z","published":"2025-12-19T10:23:38Z","title":"Perfect reconstruction of sparse signals using nonconvexity control and one-step RSB message passing","summary":"We consider sparse signal reconstruction via minimization of the smoothly clipped absolute deviation (SCAD) penalty, and develop one-step replica-symmetry-breaking (1RSB) extensions of approximate message passing (AMP), termed 1RSB-AMP. Starting from the 1RSB formulation of belief propagation, we derive explicit update rules of 1RSB-AMP together with the corresponding state evolution (1RSB-SE) equations. A detailed comparison shows that 1RSB-AMP and 1RSB-SE agree remarkably well at the macroscopic level, even in parameter regions where replica-symmetric (RS) AMP, termed RS-AMP, diverges and where the 1RSB description itself is not expected to be thermodynamically exact. Fixed-point analysis of 1RSB-SE reveals a phase diagram consisting of success, failure, and diverging phases, as in the RS case. However, the diverging-region boundary now depends on the Parisi parameter due to the 1RSB ansatz, and we propose a new criterion -- minimizing the size of the diverging region -- rather than the conventional zero-complexity condition, to determine its value. Combining this criterion with the nonconvexity-control (NCC) protocol proposed in a previous RS study improves the algorithmic limit of perfect reconstruction compared with RS-AMP. Numerical solutions of 1RSB-SE and experiments with 1RSB-AMP confirm that this improved limit is achieved in practice, though the gain is modest and remains slightly inferior to the Bayes-optimal threshold. We also report the behavior of thermodynamic quantities -- overlaps, free entropy, complexity, and the non-self-averaging susceptibility -- that characterize the 1RSB phase in this problem.","authors":["Xiaosi Gu","Ayaka Sakata","Tomoyuki Obuchi"],"pdf_url":"","comment":"49 pages, 10 figures"},{"id":"http://arxiv.org/abs/2506.22376v4","updated":"2025-12-19T10:17:53Z","published":"2025-06-27T16:44:11Z","title":"OptScale: Probabilistic Optimality for Inference-time Scaling","summary":"Inference-time scaling has emerged as a powerful technique for enhancing the reasoning performance of Large Language Models (LLMs). However, existing approaches often rely on heuristic strategies for parallel sampling, lacking a principled foundation. To address this gap, we propose a probabilistic framework that formalizes the optimality of inference-time scaling under the assumption that parallel samples are independently and identically distributed (i.i.d.), and where the Best-of-$N$ selection strategy follows a probability distribution that can be estimated. Within this framework, we derive a theoretical lower bound on the required number of samples to achieve a target performance level, providing the first principled guidance for compute-efficient scaling. Leveraging this insight, we develop \\textsc{OptScale}, a practical algorithm that dynamically determines the optimal number of sampled responses. \\textsc{OptScale} employs a language model-based predictor to estimate probabilistic prior parameters, enabling the decision of the minimal number of samples needed that satisfy predefined performance thresholds and confidence levels. Extensive experiments on representative reasoning benchmarks (including MATH-500, GSM8K, AIME, and AMC) demonstrate that \\textsc{OptScale} significantly reduces sampling overhead while remaining better or on par with state-of-the-art reasoning performance. Our work offers both a theoretical foundation and a practical solution for principled inference-time scaling, addressing a critical gap in the efficient deployment of LLMs for complex reasoning.","authors":["Youkang Wang","Jian Wang","Rubing Chen","Xiao-Yong Wei"],"pdf_url":"","comment":"Accepted by AAAI-2026"},{"id":"http://arxiv.org/abs/2512.17419v1","updated":"2025-12-19T10:16:51Z","published":"2025-12-19T10:16:51Z","title":"SWE-Bench++: A Framework for the Scalable Generation of Software Engineering Benchmarks from Open-Source Repositories","summary":"Benchmarks like SWE-bench have standardized the evaluation of Large Language Models (LLMs) on repository-level software engineering tasks. However, these efforts remain limited by manual curation, static datasets, and a focus on Python-based bug fixes. We introduce SWE-Bench++, an automated framework that generates repository-level coding tasks from open-source GitHub projects. Unlike synthetic approaches, our pipeline harvests live pull requests to cover both bug fixes and feature requests across 11 languages. SWE-Bench++ turns GitHub pull requests (PRs) into reproducible, execution-based tasks via four stages: programmatic sourcing, environment synthesis, test oracle extraction, and quality assurance. A final hint-guided trajectory synthesis step converts instances that strong models fail on into training trajectories. Our initial benchmark consists of 11,133 instances from 3,971 repositories across 11 languages. On a subset of 1,782 instances of this benchmark, today's strongest models perform as follows: claude-sonnet-4.5 achieves 36.20% pass@10, gpt-5-2025-08-07 34.57%, gemini/gemini-2.5-pro 24.92%, and gpt-4o 16.89%. We further demonstrate the utility of our dataset by showing that fine-tuning on SWE-Bench++ instances yields measurable improvements on the SWE-bench Multilingual benchmark. SWE-Bench++ provides a scalable, multilingual benchmark for evaluating and improving repository-level code generation.","authors":["Lilin Wang","Lucas Ramalho","Alan Celestino","Phuc Anthony Pham","Yu Liu","Umang Kumar Sinha","Andres Portillo","Onassis Osunwa","Gabriel Maduekwe"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2511.04469v3","updated":"2025-12-19T10:05:59Z","published":"2025-11-06T15:44:07Z","title":"Towards Causal Market Simulators","summary":"Market generators using deep generative models have shown promise for synthetic financial data generation, but existing approaches lack causal reasoning capabilities essential for counterfactual analysis and risk assessment. We propose a Time-series Neural Causal Model VAE (TNCM-VAE) that combines variational autoencoders with structural causal models to generate counterfactual financial time series while preserving both temporal dependencies and causal relationships. Our approach enforces causal constraints through directed acyclic graphs in the decoder architecture and employs the causal Wasserstein distance for training. We validate our method on synthetic autoregressive models inspired by the Ornstein-Uhlenbeck process, demonstrating superior performance in counterfactual probability estimation with L1 distances as low as 0.03-0.10 compared to ground truth. The model enables financial stress testing, scenario analysis, and enhanced backtesting by generating plausible counterfactual market trajectories that respect underlying causal mechanisms.","authors":["Dennis Thumm","Luis Ontaneda Mijares"],"pdf_url":"","comment":"ICAIF 2025 Workshop on Rethinking Financial Time-Series"},{"id":"http://arxiv.org/abs/2512.17409v1","updated":"2025-12-19T10:01:52Z","published":"2025-12-19T10:01:52Z","title":"meval: A Statistical Toolbox for Fine-Grained Model Performance Analysis","summary":"Analyzing machine learning model performance stratified by patient and recording properties is becoming the accepted norm and often yields crucial insights about important model failure modes. Performing such analyses in a statistically rigorous manner is non-trivial, however. Appropriate performance metrics must be selected that allow for valid comparisons between groups of different sample sizes and base rates; metric uncertainty must be determined and multiple comparisons be corrected for, in order to assess whether any observed differences may be purely due to chance; and in the case of intersectional analyses, mechanisms must be implemented to find the most `interesting' subgroups within combinatorially many subgroup combinations. We here present a statistical toolbox that addresses these challenges and enables practitioners to easily yet rigorously assess their models for potential subgroup performance disparities. While broadly applicable, the toolbox is specifically designed for medical imaging applications. The analyses provided by the toolbox are illustrated in two case studies, one in skin lesion malignancy classification on the ISIC2020 dataset and one in chest X-ray-based disease classification on the MIMIC-CXR dataset.","authors":["Dishantkumar Sutariya","Eike Petersen"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17398v1","updated":"2025-12-19T09:50:23Z","published":"2025-12-19T09:50:23Z","title":"DeepShare: Sharing ReLU Across Channels and Layers for Efficient Private Inference","summary":"Private Inference (PI) uses cryptographic primitives to perform privacy preserving machine learning. In this setting, the owner of the network runs inference on the data of the client without learning anything about the data and without revealing any information about the model. It has been observed that a major computational bottleneck of PI is the calculation of the gate (i.e., ReLU), so a considerable amount of effort have been devoted to reducing the number of ReLUs in a given network.\n  We focus on the DReLU, which is the non-linear step function of the ReLU and show that one DReLU can serve many ReLU operations. We suggest a new activation module where the DReLU operation is only performed on a subset of the channels (Prototype channels), while the rest of the channels (replicate channels) replicates the DReLU of each of their neurons from the corresponding neurons in one of the prototype channels. We then extend this idea to work across different layers.\n  We show that this formulation can drastically reduce the number of DReLU operations in resnet type network. Furthermore, our theoretical analysis shows that this new formulation can solve an extended version of the XOR problem, using just one non-linearity and two neurons, something that traditional formulations and some PI specific methods cannot achieve. We achieve new SOTA results on several classification setups, and achieve SOTA results on image segmentation.","authors":["Yonathan Bornfeld","Shai Avidan"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2507.18242v3","updated":"2025-12-19T09:49:50Z","published":"2025-07-24T09:30:37Z","title":"Boosting Revisited: Benchmarking and Advancing LP-Based Ensemble Methods","summary":"Despite their theoretical appeal, totally corrective boosting methods based on linear programming have received limited empirical attention. In this paper, we conduct the first large-scale experimental study of six LP-based boosting formulations, including two novel methods, NM-Boost and QRLP-Boost, across 20 diverse datasets. We evaluate the use of both heuristic and optimal base learners within these formulations, and analyze not only accuracy, but also ensemble sparsity, margin distribution, anytime performance, and hyperparameter sensitivity. We show that totally corrective methods can outperform or match state-of-the-art heuristics like XGBoost and LightGBM when using shallow trees, while producing significantly sparser ensembles. We further show that these methods can thin pre-trained ensembles without sacrificing performance, and we highlight both the strengths and limitations of using optimal decision trees in this context.","authors":["Fabian Akkerman","Julien Ferry","Christian Artigues","Emmanuel Hebrard","Thibaut Vidal"],"pdf_url":"","comment":"Published in Transactions on Machine Learning Research (2025), see: https://openreview.net/forum?id=lscC4PZUE4"},{"id":"http://arxiv.org/abs/2505.17720v2","updated":"2025-12-19T09:48:05Z","published":"2025-05-23T10:37:12Z","title":"PEAR: Equal Area Weather Forecasting on the Sphere","summary":"Artificial intelligence is rapidly reshaping the natural sciences, with weather forecasting emerging as a flagship AI4Science application where machine learning models can now rival and even surpass traditional numerical simulations. Following the success of the landmark models Pangu Weather and Graphcast, outperforming traditional numerical methods for global medium-range forecasting, many novel data-driven methods have emerged. A common limitation shared by many of these models is their reliance on an equiangular discretization of the sphere which suffers from a much finer grid at the poles than around the equator. In contrast, in the Hierarchical Equal Area iso-Latitude Pixelization (HEALPix) of the sphere, each pixel covers the same surface area, removing unphysical biases. Motivated by a growing support for this grid in meteorology and climate sciences, we propose to perform weather forecasting with deep learning models which natively operate on the HEALPix grid. To this end, we introduce Pangu Equal ARea (PEAR), a transformer-based weather forecasting model which operates directly on HEALPix-features and outperforms the corresponding model on an equiangular grid without any computational overhead.","authors":["Hampus Linander","Christoffer Petersson","Daniel Persson","Jan E. Gerken"],"pdf_url":"","comment":"Published in the AI for Science workshop (NeurIPS 2025), 8 pages, 4 figures; 6 pages supplementary material"},{"id":"http://arxiv.org/abs/2510.23117v3","updated":"2025-12-19T09:46:41Z","published":"2025-10-27T08:38:17Z","title":"Seeing Structural Failure Before it Happens: An Image-Based Physics-Informed Neural Network (PINN) for Spaghetti Bridge Load Prediction","summary":"Physics Informed Neural Networks (PINNs) are gaining attention for their ability to embed physical laws into deep learning models, which is particularly useful in structural engineering tasks with limited data. This paper aims to explore the use of PINNs to predict the weight of small scale spaghetti bridges, a task relevant to understanding load limits and potential failure modes in simplified structural models. Our proposed framework incorporates physics-based constraints to the prediction model for improved performance. In addition to standard PINNs, we introduce a novel architecture named Physics Informed Kolmogorov Arnold Network (PIKAN), which blends universal function approximation theory with physical insights. The structural parameters provided as input to the model are collected either manually or through computer vision methods. Our dataset includes 15 real bridges, augmented to 100 samples, and our best model achieves an $R^2$ score of 0.9603 and a mean absolute error (MAE) of 10.50 units. From applied perspective, we also provide a web based interface for parameter entry and prediction. These results show that PINNs can offer reliable estimates of structural weight, even with limited data, and may help inform early stage failure analysis in lightweight bridge designs.\n  The complete data and code are available at https://github.com/OmerJauhar/PINNS-For-Spaghetti-Bridges.","authors":["Omer Jauhar Khan","Sudais Khan","Hafeez Anwar","Shahzeb Khan","Shams Ul Arifeen"],"pdf_url":"","comment":"14 pages, 21 figures. Preprint"},{"id":"http://arxiv.org/abs/2502.03480v2","updated":"2025-12-19T09:44:32Z","published":"2025-01-27T23:02:05Z","title":"Foundation for unbiased cross-validation of spatio-temporal models for species distribution modeling","summary":"Evaluating the predictive performance of species distribution models (SDMs) under realistic deployment scenarios requires careful handling of spatial and temporal dependencies in the data. Cross-validation (CV) is the standard approach for model evaluation, but its design strongly influences the validity of performance estimates. When SDMs are intended for spatial or temporal transfer, random CV can lead to overoptimistic results due to spatial autocorrelation (SAC) among neighboring observations.\n  We benchmark four machine learning algorithms (GBM, XGBoost, LightGBM, Random Forest) on two real-world presence-absence datasets, a temperate plant and an anadromous fish, using multiple CV designs: random, spatial, spatio-temporal, environmental, and forward-chaining. Two training data usage strategies (LAST FOLD and RETRAIN) are evaluated, with hyperparameter tuning performed within each CV scheme. Model performance is assessed on independent out-of-time test sets using AUC, MAE, and correlation metrics.\n  Random CV overestimates AUC by up to 0.16 and produces MAE values up to 80 percent higher than spatially blocked alternatives. Blocking at the empirical SAC range substantially reduces this bias. Training strategy affects evaluation outcomes: LAST FOLD yields smaller validation-test discrepancies under strong SAC, while RETRAIN achieves higher test AUC when SAC is weaker. Boosted ensemble models consistently perform best under spatially structured CV designs. We recommend a robust SDM workflow based on SAC-aware blocking, blocked hyperparameter tuning, and external temporal validation to improve reliability under spatial and temporal shifts.","authors":["Diana Koldasbayeva","Alexey Zaytsev"],"pdf_url":"","comment":"Accepted manuscript. Published in Ecological Informatics (2025)"},{"id":"http://arxiv.org/abs/2512.17381v1","updated":"2025-12-19T09:36:44Z","published":"2025-12-19T09:36:44Z","title":"Timely Information Updating for Mobile Devices Without and With ML Advice","summary":"This paper investigates an information update system in which a mobile device monitors a physical process and sends status updates to an access point (AP). A fundamental trade-off arises between the timeliness of the information maintained at the AP and the update cost incurred at the device. To address this trade-off, we propose an online algorithm that determines when to transmit updates using only available observations. The proposed algorithm asymptotically achieves the optimal competitive ratio against an adversary that can simultaneously manipulate multiple sources of uncertainty, including the operation duration, the information staleness, the update cost, and the availability of update opportunities. Furthermore, by incorporating machine learning (ML) advice of unknown reliability into the design, we develop an ML-augmented algorithm that asymptotically attains the optimal consistency-robustness trade-off, even when the adversary can additionally corrupt the ML advice. The optimal competitive ratio scales linearly with the range of update costs, but is unaffected by other uncertainties. Moreover, an optimal competitive online algorithm exhibits a threshold-like response to the ML advice: it either fully trusts or completely ignores the ML advice, as partially trusting the advice cannot improve the consistency without severely degrading the robustness. Extensive simulations in stochastic settings further validate the theoretical findings in the adversarial environment.","authors":["Yu-Pin Hsu","Yi-Hsuan Tseng"],"pdf_url":"","comment":"23 pages, journal version of arXiv:1901.03137, submitted for possible journal publication"},{"id":"http://arxiv.org/abs/2508.19011v3","updated":"2025-12-19T09:33:52Z","published":"2025-08-26T13:14:53Z","title":"STDiff: A State Transition Diffusion Framework for Time Series Imputation in Industrial Systems","summary":"Incomplete sensor data is a major obstacle in industrial time-series analytics. In wastewater treatment plants (WWTPs), key sensors show long, irregular gaps caused by fouling, maintenance, and outages. We introduce STDiff and STDiff-W, diffusion-based imputers that cast gap filling as state-space simulation under partial observability, where targets, controls, and exogenous signals may all be intermittently missing. STDiff learns a one-step transition model conditioned on observed values and masks, while STDiff-W extends this with a context encoder that jointly inpaints contiguous blocks, combining long-range consistency with short-term detail. On two WWTP datasets (one with synthetic block gaps from Agtrup and another with natural outages from Avedøre), STDiff-W achieves state-of-the-art accuracy compared with strong neural baselines such as SAITS, BRITS, and CSDI. Beyond point-error metrics, its reconstructions preserve realistic dynamics including oscillations, spikes, and regime shifts, and they achieve top or tied-top downstream one-step forecasting performance compared with strong neural baselines, indicating that preserving dynamics does not come at the expense of predictive utility. Ablation studies that drop, shuffle, or add noise to control or exogenous inputs consistently degrade NH4 and PO4 performance, with the largest deterioration observed when exogenous signals are removed, showing that the model captures meaningful dependencies. We conclude with practical guidance for deployment: evaluate performance beyond MAE using task-oriented and visual checks, include exogenous drivers, and balance computational cost against robustness to structured outages.","authors":["Gary Simethy","Daniel Ortiz-Arroyo","Petar Durdevic"],"pdf_url":"","comment":"Peer-reviewed and published in Expert Systems with Applications, Volume 302 (2026). This version reflects the published article"},{"id":"http://arxiv.org/abs/2410.22674v2","updated":"2025-12-19T09:23:30Z","published":"2024-10-30T03:52:21Z","title":"Dynamic PET Image Prediction Using a Network Combining Reversible and Irreversible Modules","summary":"Dynamic positron emission tomography (PET) images can reveal the distribution of tracers in the organism and the dynamic processes involved in biochemical reactions, and it is widely used in clinical practice. Despite the high effectiveness of dynamic PET imaging in studying the kinetics and metabolic processes of radiotracers. Pro-longed scan times can cause discomfort for both patients and medical personnel. This study proposes a dynamic frame prediction method for dynamic PET imaging, reduc-ing dynamic PET scanning time by applying a multi-module deep learning framework composed of reversible and irreversible modules. The network can predict kinetic parameter images based on the early frames of dynamic PET images, and then generate complete dynamic PET images. In validation experiments with simulated data, our network demonstrated good predictive performance for kinetic parameters and was able to reconstruct high-quality dynamic PET images. Additionally, in clinical data experiments, the network exhibited good generalization performance and attached that the proposed method has promising clinical application prospects.","authors":["Jie Sun","Junyan Zhang","Qian Xia","Chuanfu Sun","Yumei Chen","Yunjie Yang","Huafeng Liu","Wentao Zhu","Qiegen Liu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.12783v2","updated":"2025-12-19T09:22:54Z","published":"2025-12-14T17:48:13Z","title":"Credit Risk Estimation with Non-Financial Features: Evidence from a Synthetic Istanbul Dataset","summary":"Financial exclusion constrains entrepreneurship, increases income volatility, and widens wealth gaps. Underbanked consumers in Istanbul often have no bureau file because their earnings and payments flow through informal channels. To study how such borrowers can be evaluated we create a synthetic dataset of one hundred thousand Istanbul residents that reproduces first quarter 2025 TÜİK census marginals and telecom usage patterns. Retrieval augmented generation feeds these public statistics into the OpenAI o3 model, which synthesises realistic yet private records. Each profile contains seven socio demographic variables and nine alternative attributes that describe phone specifications, online shopping rhythm, subscription spend, car ownership, monthly rent, and a credit card flag. To test the impact of the alternative financial data CatBoost, LightGBM, and XGBoost are each trained in two versions. Demo models use only the socio demographic variables; Full models include both socio demographic and alternative attributes. Across five fold stratified validation the alternative block raises area under the curve by about one point three percentage and lifts balanced \\(F_{1}\\) from roughly 0.84 to 0.95, a fourteen percent gain. We contribute an open Istanbul 2025 Q1 synthetic dataset, a fully reproducible modeling pipeline, and empirical evidence that a concise set of behavioural attributes can approach bureau level discrimination power while serving borrowers who lack formal credit records. These findings give lenders and regulators a transparent blueprint for extending fair and safe credit access to the underbanked.","authors":["Atalay Denknalbant","Emre Sezdi","Zeki Furkan Kutlu","Polat Goktas"],"pdf_url":"","comment":"Substantial experimental errors were discovered that affect the validity of the results. Then, we want to withdraw the paper"},{"id":"http://arxiv.org/abs/2512.17375v1","updated":"2025-12-19T09:22:11Z","published":"2025-12-19T09:22:11Z","title":"AdvJudge-Zero: Binary Decision Flips in LLM-as-a-Judge via Adversarial Control Tokens","summary":"Reward models and LLM-as-a-Judge systems are central to modern post-training pipelines such as RLHF, DPO, and RLAIF, where they provide scalar feedback and binary decisions that guide model selection and RL-based fine-tuning. We show that these judge systems exhibit a recurring vulnerability: short sequences of low-perplexity control tokens can flip many binary evaluations from correct ``No'' judgments to incorrect ``Yes'' judgments by steering the last-layer logit gap. These control tokens are patterns that a policy model could plausibly generate during post-training, and thus represent realistic reward-hacking risks rather than worst-case adversarial strings. Our method, AdvJudge-Zero, uses the model's next-token distribution and beam-search exploration to discover diverse control-token sequences from scratch, and our analysis shows that the induced hidden-state perturbations concentrate in a low-rank ``soft mode'' that is anti-aligned with the judge's refusal direction. Empirically, these tokens cause very high false positive rates when large open-weight and specialized judge models score incorrect answers on math and reasoning benchmarks. Finally, we show that LoRA-based adversarial training on small sets of control-token-augmented examples can markedly reduce these false positives while preserving evaluation quality.","authors":["Tung-Ling Li","Yuhao Wu","Hongliang Liu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17367v1","updated":"2025-12-19T09:08:27Z","published":"2025-12-19T09:08:27Z","title":"Adversarially Robust Detection of Harmful Online Content: A Computational Design Science Approach","summary":"Social media platforms are plagued by harmful content such as hate speech, misinformation, and extremist rhetoric. Machine learning (ML) models are widely adopted to detect such content; however, they remain highly vulnerable to adversarial attacks, wherein malicious users subtly modify text to evade detection. Enhancing adversarial robustness is therefore essential, requiring detectors that can defend against diverse attacks (generalizability) while maintaining high overall accuracy. However, simultaneously achieving both optimal generalizability and accuracy is challenging. Following the computational design science paradigm, this study takes a sequential approach that first proposes a novel framework (Large Language Model-based Sample Generation and Aggregation, LLM-SGA) by identifying the key invariances of textual adversarial attacks and leveraging them to ensure that a detector instantiated within the framework has strong generalizability. Second, we instantiate our detector (Adversarially Robust Harmful Online Content Detector, ARHOCD) with three novel design components to improve detection accuracy: (1) an ensemble of multiple base detectors that exploits their complementary strengths; (2) a novel weight assignment method that dynamically adjusts weights based on each sample's predictability and each base detector's capability, with weights initialized using domain knowledge and updated via Bayesian inference; and (3) a novel adversarial training strategy that iteratively optimizes both the base detectors and the weight assignor. We addressed several limitations of existing adversarial robustness enhancement research and empirically evaluated ARHOCD across three datasets spanning hate speech, rumor, and extremist content. Results show that ARHOCD offers strong generalizability and improves detection accuracy under adversarial conditions.","authors":["Yidong Chai","Yi Liu","Mohammadreza Ebrahimi","Weifeng Li","Balaji Padmanabhan"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2510.14386v2","updated":"2025-12-19T09:00:50Z","published":"2025-10-16T07:37:59Z","title":"ASecond-Order SpikingSSM for Wearables","summary":"Spiking neural networks have garnered increasing attention due to their energy efficiency, multiplication-free computation, and sparse event-based processing. In parallel, state space models have emerged as scalable alternatives to transformers for long-range sequence modelling by avoiding quadratic dependence on sequence length. We propose SHaRe-SSM (Spiking Harmonic Resonate-and-Fire State Space Model), a second-order spiking SSM for classification and regression on ultra-long sequences. SHaRe-SSM outperforms transformers and first-order SSMs on average while eliminating matrix multiplications, making it highly suitable for resource-constrained applications. To ensure fast computation over tens of thousands of time steps, we leverage a parallel scan formulation of the underlying dynamical system. Furthermore, we introduce a kernel-based spiking regressor, which enables the accurate modelling of dependencies in sequences of up to 50k steps. Our results demonstrate that SHaRe-SSM achieves superior long-range modelling capability with energy efficiency (52.1x less than ANN-based second order SSM), positioning it as a strong candidate for resource-constrained devices such as wearables","authors":["Kartikay Agrawal","Abhijeet Vikram","Vedant Sharma","Vaishnavi Nagabhushana","Ayon Borthakur"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17352v1","updated":"2025-12-19T08:48:36Z","published":"2025-12-19T08:48:36Z","title":"Adaptive Graph Pruning with Sudden-Events Evaluation for Traffic Prediction using Online Semi-Decentralized ST-GNNs","summary":"Spatio-Temporal Graph Neural Networks (ST-GNNs) are well-suited for processing high-frequency data streams from geographically distributed sensors in smart mobility systems. However, their deployment at the edge across distributed compute nodes (cloudlets) createssubstantial communication overhead due to repeated transmission of overlapping node features between neighbouring cloudlets. To address this, we propose an adaptive pruning algorithm that dynamically filters redundant neighbour features while preserving the most informative spatial context for prediction. The algorithm adjusts pruning rates based on recent model performance, allowing each cloudlet to focus on regions experiencing traffic changes without compromising accuracy. Additionally, we introduce the Sudden Event Prediction Accuracy (SEPA), a novel event-focused metric designed to measure responsiveness to traffic slowdowns and recoveries, which are often missed by standard error metrics. We evaluate our approach in an online semi-decentralized setting with traditional FL, server-free FL, and Gossip Learning on two large-scale traffic datasets, PeMS-BAY and PeMSD7-M, across short-, mid-, and long-term prediction horizons. Experiments show that, in contrast to standard metrics, SEPA exposes the true value of spatial connectivity in predicting dynamic and irregular traffic. Our adaptive pruning algorithm maintains prediction accuracy while significantly lowering communication cost in all online semi-decentralized settings, demonstrating that communication can be reduced without compromising responsiveness to critical traffic events.","authors":["Ivan Kralj","Lodovico Giaretta","Gordan Ježić","Ivana Podnar Žarko","Šarūnas Girdzijauskas"],"pdf_url":"","comment":"19 pages, 6 figures, 5 tables, journal"},{"id":"http://arxiv.org/abs/2512.17341v1","updated":"2025-12-19T08:34:05Z","published":"2025-12-19T08:34:05Z","title":"Sharp Structure-Agnostic Lower Bounds for General Functional Estimation","summary":"The design of efficient nonparametric estimators has long been a central problem in statistics, machine learning, and decision making. Classical optimal procedures often rely on strong structural assumptions, which can be misspecified in practice and complicate deployment. This limitation has sparked growing interest in structure-agnostic approaches -- methods that debias black-box nuisance estimates without imposing structural priors. Understanding the fundamental limits of these methods is therefore crucial. This paper provides a systematic investigation of the optimal error rates achievable by structure-agnostic estimators. We first show that, for estimating the average treatment effect (ATE), a central parameter in causal inference, doubly robust learning attains optimal structure-agnostic error rates. We then extend our analysis to a general class of functionals that depend on unknown nuisance functions and establish the structure-agnostic optimality of debiased/double machine learning (DML). We distinguish two regimes -- one where double robustness is attainable and one where it is not -- leading to different optimal rates for first-order debiasing, and show that DML is optimal in both regimes. Finally, we instantiate our general lower bounds by deriving explicit optimal rates that recover existing results and extend to additional estimands of interest. Our results provide theoretical validation for widely used first-order debiasing methods and guidance for practitioners seeking optimal approaches in the absence of structural assumptions. This paper generalizes and subsumes the ATE lower bound established in \\citet{jin2024structure} by the same authors.","authors":["Jikai Jin","Vasilis Syrgkanis"],"pdf_url":"","comment":"95 pages; generalize and subsume partial results of arXiv:2402.14264 by the same authors"},{"id":"http://arxiv.org/abs/2512.17340v1","updated":"2025-12-19T08:33:48Z","published":"2025-12-19T08:33:48Z","title":"Penalized Fair Regression for Multiple Groups in Chronic Kidney Disease","summary":"Fair regression methods have the potential to mitigate societal bias concerns in health care, but there has been little work on penalized fair regression when multiple groups experience such bias. We propose a general regression framework that addresses this gap with unfairness penalties for multiple groups. Our approach is demonstrated for binary outcomes with true positive rate disparity penalties. It can be efficiently implemented through reduction to a cost-sensitive classification problem. We additionally introduce novel score functions for automatically selecting penalty weights. Our penalized fair regression methods are empirically studied in simulations, where they achieve a fairness-accuracy frontier beyond that of existing comparison methods. Finally, we apply these methods to a national multi-site primary care study of chronic kidney disease to develop a fair classifier for end-stage renal disease. There we find substantial improvements in fairness for multiple race and ethnicity groups who experience societal bias in the health care system without any appreciable loss in overall fit.","authors":["Carter H. Nakamoto","Lucia Lushi Chen","Agata Foryciarz","Sherri Rose"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.11575v2","updated":"2025-12-19T08:22:55Z","published":"2025-12-12T14:03:40Z","title":"In-Context Learning for Seismic Data Processing","summary":"Seismic processing transforms raw data into subsurface images essential for geophysical applications. Traditional methods face challenges, such as noisy data, and manual parameter tuning, among others. Recently deep learning approaches have proposed alternative solutions to some of these problems. However, important challenges of existing deep learning approaches are spatially inconsistent results across neighboring seismic gathers and lack of user-control. We address these limitations by introducing ContextSeisNet, an in-context learning model, to seismic demultiple processing. Our approach conditions predictions on a support set of spatially related example pairs: neighboring common-depth point gathers from the same seismic line and their corresponding labels. This allows the model to learn task-specific processing behavior at inference time by observing how similar gathers should be processed, without any retraining. This method provides both flexibility through user-defined examples and improved lateral consistency across seismic lines. On synthetic data, ContextSeisNet outperforms a U-Net baseline quantitatively and demonstrates enhanced spatial coherence between neighboring gathers. On field data, our model achieves superior lateral consistency compared to both traditional Radon demultiple and the U-Net baseline. Relative to the U-Net, ContextSeisNet also delivers improved near-offset performance and more complete multiple removal. Notably, ContextSeisNet achieves comparable field data performance despite being trained on 90% less data, demonstrating substantial data efficiency. These results establish ContextSeisNet as a practical approach for spatially consistent seismic demultiple with potential applicability to other seismic processing tasks.","authors":["Fabian Fuchs","Mario Ruben Fernandez","Norman Ettrich","Janis Keuper"],"pdf_url":"","comment":"Source code available under https://codeberg.org/fuchsfa/in-context-learning-seismic. In submission to Geophysics"},{"id":"http://arxiv.org/abs/2503.19041v4","updated":"2025-12-19T08:17:42Z","published":"2025-03-24T18:11:42Z","title":"LookAhead Tuning: Safer Language Models via Partial Answer Previews","summary":"Fine-tuning enables large language models (LLMs) to adapt to specific domains, but often compromises their previously established safety alignment. To mitigate the degradation of model safety during fine-tuning, we introduce LookAhead Tuning, a lightweight and effective data-driven approach that preserves safety during fine-tuning. The method introduces two simple strategies that modify training data by previewing partial answer prefixes, thereby minimizing perturbations to the model's initial token distributions and maintaining its built-in safety mechanisms. Comprehensive experiments demonstrate that LookAhead Tuning effectively maintains model safety without sacrificing robust performance on downstream tasks. Our findings position LookAhead Tuning as a reliable and efficient solution for the safe and effective adaptation of LLMs.","authors":["Kangwei Liu","Mengru Wang","Yujie Luo","Lin Yuan","Mengshu Sun","Lei Liang","Zhiqiang Zhang","Jun Zhou","Bryan Hooi","Shumin Deng"],"pdf_url":"","comment":"WSDM 2026 short"},{"id":"http://arxiv.org/abs/2512.12218v2","updated":"2025-12-19T08:16:24Z","published":"2025-12-13T07:04:42Z","title":"Journey Before Destination: On the importance of Visual Faithfulness in Slow Thinking","summary":"Reasoning-augmented vision language models (VLMs) generate explicit chains of thought that promise greater capability and transparency but also introduce new failure modes: models may reach correct answers via visually unfaithful intermediate steps, or reason faithfully yet fail on the final prediction. Standard evaluations that only measure final-answer accuracy cannot distinguish these behaviors. We introduce the visual faithfulness of reasoning chains as a distinct evaluation dimension, focusing on whether the perception steps of a reasoning chain are grounded in the image. We propose a training- and reference-free framework that decomposes chains into perception versus reasoning steps and uses off-the-shelf VLM judges for step-level faithfulness, additionally verifying this approach through a human meta-evaluation. Building on this metric, we present a lightweight self-reflection procedure that detects and locally regenerates unfaithful perception steps without any training. Across multiple reasoning-trained VLMs and perception-heavy benchmarks, our method reduces Unfaithful Perception Rate while preserving final-answer accuracy, improving the reliability of multimodal reasoning.","authors":["Rheeya Uppaal","Phu Mon Htut","Min Bai","Nikolaos Pappas","Zheng Qi","Sandesh Swamy"],"pdf_url":"","comment":"Preprint"},{"id":"http://arxiv.org/abs/2512.17325v1","updated":"2025-12-19T08:14:21Z","published":"2025-12-19T08:14:21Z","title":"Task Schema and Binding: A Double Dissociation Study of In-Context Learning","summary":"We provide causal mechanistic validation that in-context learning (ICL) decomposes into two separable mechanisms: Task Schema (abstract task type recognition) and Binding (specific input-output associations). Through activation patching experiments across 9 models from 7 Transformer families plus Mamba (370M-13B parameters), we establish three key findings:\n  1. Double dissociation: Task Schema transfers at 100% via late MLP patching; Binding transfers at 62% via residual stream patching -- proving separable mechanisms\n  2. Prior-Schema trade-off: Schema reliance inversely correlates with prior knowledge (Spearman rho = -0.596, p < 0.001, N=28 task-model pairs)\n  3. Architecture generality: The mechanism operates across all tested architectures including the non-Transformer Mamba\n  These findings offer a mechanistic account of the ICL puzzle that contrasts with prior views treating ICL as a monolithic mechanism (whether retrieval-based, gradient descent-like, or purely Bayesian). By establishing that Schema and Binding are neurally dissociable -- not merely behavioral modes -- we provide causal evidence for dual-process theories of ICL. Models rely on Task Schema when prior knowledge is absent, but prior knowledge interferes through attentional mis-routing (72.7% recency bias) rather than direct output competition (0%). This explains why arbitrary mappings succeed (zero prior leads to full Schema reliance) while factual overrides fail -- and reveals that the true bottleneck is attentional, not output-level. Practical implications: Understanding these dual mechanisms enables more efficient prompt engineering -- reliable schema transfer reduces required demonstrations for novel tasks, while prior-aware design can mitigate the 38% binding failure rate in high-prior scenarios, improving ICL system reliability in production deployments.","authors":["Chaeha Kim"],"pdf_url":"","comment":"20pages, 2figures"},{"id":"http://arxiv.org/abs/2512.17316v1","updated":"2025-12-19T07:59:36Z","published":"2025-12-19T07:59:36Z","title":"Explanation Beyond Intuition: A Testable Criterion for Inherent Explainability","summary":"Inherent explainability is the gold standard in Explainable Artificial Intelligence (XAI). However, there is not a consistent definition or test to demonstrate inherent explainability. Work to date either characterises explainability through metrics, or appeals to intuition - \"we know it when we see it\". We propose a globally applicable criterion for inherent explainability. The criterion uses graph theory for representing and decomposing models for structure-local explanation, and recomposing them into global explanations. We form the structure-local explanations as annotations, a verifiable hypothesis-evidence structure that allows for a range of explanatory methods to be used. This criterion matches existing intuitions on inherent explainability, and provides justifications why a large regression model may not be explainable but a sparse neural network could be. We differentiate explainable -- a model that allows for explanation -- and \\textit{explained} -- one that has a verified explanation. Finally, we provide a full explanation of PREDICT -- a Cox proportional hazards model of cardiovascular disease risk, which is in active clinical use in New Zealand. It follows that PREDICT is inherently explainable. This work provides structure to formalise other work on explainability, and allows regulators a flexible but rigorous test that can be used in compliance frameworks.","authors":["Michael Merry","Pat Riddle","Jim Warren"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17299v1","updated":"2025-12-19T07:27:30Z","published":"2025-12-19T07:27:30Z","title":"M2RU: Memristive Minion Recurrent Unit for Continual Learning at the Edge","summary":"Continual learning on edge platforms remains challenging because recurrent networks depend on energy-intensive training procedures and frequent data movement that are impractical for embedded deployments. This work introduces M2RU, a mixed-signal architecture that implements the minion recurrent unit for efficient temporal processing with on-chip continual learning. The architecture integrates weighted-bit streaming, which enables multi-bit digital inputs to be processed in crossbars without high-resolution conversion, and an experience replay mechanism that stabilizes learning under domain shifts. M2RU achieves 15 GOPS at 48.62 mW, corresponding to 312 GOPS per watt, and maintains accuracy within 5 percent of software baselines on sequential MNIST and CIFAR-10 tasks. Compared with a CMOS digital design, the accelerator provides 29X improvement in energy efficiency. Device-aware analysis shows an expected operational lifetime of 12.2 years under continual learning workloads. These results establish M2RU as a scalable and energy-efficient platform for real-time adaptation in edge-level temporal intelligence.","authors":["Abdullah M. Zyarah","Dhireesha Kudithipudi"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.13478v4","updated":"2025-12-19T07:21:39Z","published":"2025-12-15T16:14:32Z","title":"Non-Resolution Reasoning (NRR): A Computational Framework for Contextual Identity and Ambiguity Preservation","summary":"Current artificial intelligence systems, despite remarkable capabilities in text generation and pattern recognition, exhibit a fundamental architectural limitation: they resolve ambiguity prematurely. This premature semantic collapse -- the tendency to collapse multiple valid interpretations into a single output -- stems from classical identity assumptions embedded in standard neural architectures. We propose Non-Resolution Reasoning (NRR), a computational framework that treats ambiguity retention as a valid reasoning mode rather than a defect to be eliminated. NRR introduces three core principles: (1) Non-Identity ($A \\neq A$) -- the same symbol refers to different entities across contexts; (2) Approximate Identity ($A \\approx A$) -- entities share partial structural overlap without being identical; and (3) Non-Resolution -- conflicting interpretations can coexist without forced convergence. We formalize these principles through three architectural components: Multi-Vector Embeddings for context-dependent representation, Non-Collapsing Attention for parallel interpretation retention, and Contextual Identity Tracking (CIT) for maintaining $A \\neq A$ across inference. We demonstrate NRR's advantages through case studies in paradox handling, creative generation, and context-dependent reasoning. Crucially, we provide a minimal empirical validation on a synthetic context-shift task where an NRR-lite model achieves 90.9% out-of-distribution accuracy compared to 9.1% for standard architectures, demonstrating that ambiguity preservation enables structural generalization. NRR challenges the assumption that meaning must collapse to be useful, offering a foundation for AI systems capable of sophisticated ambiguity handling and creative reasoning. The question is not whether AI should resolve ambiguity, but when, how, and under whose control.","authors":["Kei Saito"],"pdf_url":"","comment":"16 pages, 1 figure. Updated version with corrected references and aligned acknowledgments"},{"id":"http://arxiv.org/abs/2510.16882v2","updated":"2025-12-19T07:13:05Z","published":"2025-10-19T15:32:01Z","title":"Utility-Diversity Aware Online Batch Selection for LLM Supervised Fine-tuning","summary":"Supervised fine-tuning (SFT) is a commonly used technique to adapt large language models (LLMs) to downstream tasks. In practice, SFT on a full dataset is computationally expensive and sometimes suffers from overfitting or bias amplification. This facilitates the rise of data curation in SFT, which prioritizes the most valuable data to optimze. This work studies the online batch selection family that dynamically scores and filters samples during the training process. However, existing popular methods often (i) rely merely on the utility of data to select a subset while neglecting other crucial factors like diversity, (ii) rely on external resources such as reference models or validation sets, and (iii) incur extra training time over full-dataset training. To address these limitations, this work develops \\textbf{UDS (Utility-Diversity Sampling)}, a framework for efficient online batch selection in SFT. UDS leverages the nuclear norm of the logits matrix to capture both data utility and intra-sample diversity, while estimating inter-sample diversity through efficient low-dimensional embedding comparisons with a lightweight memory buffer of historical samples. Such a design eliminates the need for external resources and unnecessary backpropagation, securing computational efficiency. Experiments on multiple benchmarks demonstrate that UDS consistently outperforms state-of-the-art online batch selection methods under varying data budgets, and significantly reduces training time compared to full-dataset fine-tuning. Code is available at https://github.com/gfyddha/UDS.","authors":["Heming Zou","Yixiu Mao","Yun Qu","Qi Wang","Xiangyang Ji"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17281v1","updated":"2025-12-19T06:56:24Z","published":"2025-12-19T06:56:24Z","title":"LibriVAD: A Scalable Open Dataset with Deep Learning Benchmarks for Voice Activity Detection","summary":"Robust Voice Activity Detection (VAD) remains a challenging task, especially under noisy, diverse, and unseen acoustic conditions. Beyond algorithmic development, a key limitation in advancing VAD research is the lack of large-scale, systematically controlled, and publicly available datasets. To address this, we introduce LibriVAD - a scalable open-source dataset derived from LibriSpeech and augmented with diverse real-world and synthetic noise sources. LibriVAD enables systematic control over speech-to-noise ratio, silence-to-speech ratio (SSR), and noise diversity, and is released in three sizes (15 GB, 150 GB, and 1.5 TB) with two variants (LibriVAD-NonConcat and LibriVAD-Concat) to support different experimental setups. We benchmark multiple feature-model combinations, including waveform, Mel-Frequency Cepstral Coefficients (MFCC), and Gammatone filter bank cepstral coefficients, and introduce the Vision Transformer (ViT) architecture for VAD. Our experiments show that ViT with MFCC features consistently outperforms established VAD models such as boosted deep neural network and convolutional long short-term memory deep neural network across seen, unseen, and out-of-distribution (OOD) conditions, including evaluation on the real-world VOiCES dataset. We further analyze the impact of dataset size and SSR on model generalization, experimentally showing that scaling up dataset size and balancing SSR noticeably and consistently enhance VAD performance under OOD conditions. All datasets, trained models, and code are publicly released to foster reproducibility and accelerate progress in VAD research.","authors":["Ioannis Stylianou","Achintya kr. Sarkar","Nauman Dawalatabad","James Glass","Zheng-Hua Tan"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2511.00040v2","updated":"2025-12-19T06:56:17Z","published":"2025-10-28T01:33:43Z","title":"Semi-Supervised Preference Optimization with Limited Feedback","summary":"The field of preference optimization has made outstanding contributions to the alignment of language models with human preferences. Despite these advancements, recent methods still rely heavily on substantial paired (labeled) feedback data, leading to substantial resource expenditures. To address these challenges, we study the problem of Semi-Supervised Preference Optimization (SSPO) in which the idea is to learn from both a small number of pairwise preference labels and a large pool of unpaired samples simultaneously. Our key theoretical contribution proves the existence of an optimal reward threshold capable of separating winning and losing responses with high probability, which enables a principled pseudo-labeling of unpaired data. By leveraging these pseudo-labels, SSPO effectively distills latent preferences from large-scale unpaired data, thus maintaining human alignment while drastically reducing acquisition costs. Extensive experiments across datasets validate this remarkable data efficiency; for instance, SSPO trained with Mistral-7B-Instruct on just 1% of UltraFeedback consistently surpasses strong baselines trained on 10% of UltraFeedback.","authors":["Seonggyun Lee","Sungjun Lim","Seojin Park","Soeun Cheon","Kyungwoo Song"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2405.15877v4","updated":"2025-12-19T06:49:59Z","published":"2024-05-24T18:40:20Z","title":"Basis Selection: Low-Rank Decomposition of Pretrained Large Language Models for Target Applications","summary":"Large language models (LLMs) significantly enhance the performance of various applications, but they are computationally intensive and energy-demanding. This makes it challenging to deploy them on devices with limited resources, such as personal computers and mobile/wearable devices, and results in substantial inference costs in resource-rich environments like cloud servers. To extend the use of LLMs, we introduce a low-rank decomposition approach to effectively compress these models, tailored to the requirements of specific applications. We observe that LLMs pretrained on general datasets contain many redundant components not needed for particular applications. Our method focuses on identifying and removing these redundant parts, retaining only the necessary elements for the target applications. Specifically, we represent the weight matrices of LLMs as a linear combination of base components. We then prune the irrelevant bases and enhance the model with new bases beneficial for specific applications. Deep compression results on the Llama 2-7b and -13B models, conducted on target applications including mathematical reasoning and code generation, show that our method significantly reduces model size while maintaining comparable accuracy to state-of-the-art low-rank compression techniques.","authors":["Yang Li","Daniel Agyei Asante","Changsheng Zhao","Ernie Chang","Yangyang Shi","Vikas Chandra"],"pdf_url":"","comment":"Transactions on Machine Learning Research (TMLR), 2025"},{"id":"http://arxiv.org/abs/2512.17277v1","updated":"2025-12-19T06:49:55Z","published":"2025-12-19T06:49:55Z","title":"Warmer for Less: A Cost-Efficient Strategy for Cold-Start Recommendations at Pinterest","summary":"Pinterest is a leading visual discovery platform where recommender systems (RecSys) are key to delivering relevant, engaging, and fresh content to our users. In this paper, we study the problem of improving RecSys model predictions for cold-start (CS) items, which appear infrequently in the training data. Although this problem is well-studied in academia, few studies have addressed its root causes effectively at the scale of a platform like Pinterest. By investigating live traffic data, we identified several challenges of the CS problem and developed a corresponding solution for each: First, industrial-scale RecSys models must operate under tight computational constraints. Since CS items are a minority, any related improvements must be highly cost-efficient. To address this, our solutions were designed to be lightweight, collectively increasing the total parameters by only 5%. Second, CS items are represented only by non-historical (e.g., content or attribute) features, which models often treat as less important. To elevate their significance, we introduce a residual connection for the non-historical features. Third, CS items tend to receive lower prediction scores compared to non-CS items, reducing their likelihood of being surfaced. We mitigate this by incorporating a score regularization term into the model. Fourth, the labels associated with CS items are sparse, making it difficult for the model to learn from them. We apply the manifold mixup technique to address this data sparsity. Implemented together, our methods increased fresh content engagement at Pinterest by 10% without negatively impacting overall engagement and cost, and have been deployed to serve over 570 million users on Pinterest.","authors":["Saeed Ebrahimi","Weijie Jiang","Jaewon Yang","Olafur Gudmundsson","Yucheng Tu","Huizhong Duan"],"pdf_url":"","comment":"Submitted to the WWW'26"},{"id":"http://arxiv.org/abs/2511.11571v2","updated":"2025-12-19T06:48:56Z","published":"2025-11-14T18:59:59Z","title":"Optimizing Mixture of Block Attention","summary":"Mixture of Block Attention (MoBA) (Lu et al., 2025) is a promising building block for efficiently processing long contexts in LLMs by enabling queries to sparsely attend to a small subset of key-value blocks, drastically reducing computational cost. However, the design principles governing MoBA's performance are poorly understood, and it lacks an efficient GPU implementation, hindering its practical adoption. In this paper, we first develop a statistical model to analyze MoBA's underlying mechanics. Our model reveals that performance critically depends on the router's ability to accurately distinguish relevant from irrelevant blocks based on query-key affinities. We derive a signal-to-noise ratio that formally connects architectural parameters to this retrieval accuracy. Guided by our analysis, we identify two key pathways for improvement: using smaller block sizes and applying a short convolution on keys to cluster relevant signals, which enhances routing accuracy. While theoretically better, small block sizes are inefficient on GPUs. To bridge this gap, we introduce FlashMoBA, a hardware-aware CUDA kernel that enables efficient MoBA execution even with the small block sizes our theory recommends. We validate our insights by training LLMs from scratch, showing that our improved MoBA models match the performance of dense attention baselines. FlashMoBA achieves up to 14.7x speedup over FlashAttention-2 for small blocks, making our theoretically-grounded improvements practical. Code is available at: https://github.com/mit-han-lab/flash-moba.","authors":["Guangxuan Xiao","Junxian Guo","Kasra Mazaheri","Song Han"],"pdf_url":"","comment":"The first two authors contributed equally to this work"},{"id":"http://arxiv.org/abs/2512.17276v1","updated":"2025-12-19T06:48:54Z","published":"2025-12-19T06:48:54Z","title":"Alzheimer's Disease Brain Network Mining","summary":"Machine learning approaches for Alzheimer's disease (AD) diagnosis face a fundamental challenges. Clinical assessments are expensive and invasive, leaving ground truth labels available for only a fraction of neuroimaging datasets. We introduce Multi view Adaptive Transport Clustering for Heterogeneous Alzheimer's Disease (MATCH-AD), a semi supervised framework that integrates deep representation learning, graph-based label propagation, and optimal transport theory to address this limitation. The framework leverages manifold structure in neuroimaging data to propagate diagnostic information from limited labeled samples to larger unlabeled populations, while using Wasserstein distances to quantify disease progression between cognitive states. Evaluated on nearly five thousand subjects from the National Alzheimer's Coordinating Center, encompassing structural MRI measurements from hundreds of brain regions, cerebrospinal fluid biomarkers, and clinical variables MATCHAD achieves near-perfect diagnostic accuracy despite ground truth labels for less than one-third of subjects. The framework substantially outperforms all baseline methods, achieving kappa indicating almost perfect agreement compared to weak agreement for the best baseline, a qualitative transformation in diagnostic reliability. Performance remains clinically useful even under severe label scarcity, and we provide theoretical convergence guarantees with proven bounds on label propagation error and transport stability. These results demonstrate that principled semi-supervised learning can unlock the diagnostic potential of the vast repositories of partially annotated neuroimaging data accumulating worldwide, substantially reducing annotation burden while maintaining accuracy suitable for clinical deployment.","authors":["Alireza Moayedikia","Sara Fin"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17273v1","updated":"2025-12-19T06:42:16Z","published":"2025-12-19T06:42:16Z","title":"MINPO: Memory-Informed Neural Pseudo-Operator to Resolve Nonlocal Spatiotemporal Dynamics","summary":"Many physical systems exhibit nonlocal spatiotemporal behaviors described by integro-differential equations (IDEs). Classical methods for solving IDEs require repeatedly evaluating convolution integrals, whose cost increases quickly with kernel complexity and dimensionality. Existing neural solvers can accelerate selected instances of these computations, yet they do not generalize across diverse nonlocal structures. In this work, we introduce the Memory-Informed Neural Pseudo-Operator (MINPO), a unified framework for modeling nonlocal dynamics arising from long-range spatial interactions and/or long-term temporal memory. MINPO, employing either Kolmogorov-Arnold Networks (KANs) or multilayer perceptron networks (MLPs) as encoders, learns the nonlocal operator and its inverse directly through neural representations, and then explicitly reconstruct the unknown solution fields. The learning is guarded by a lightweight nonlocal consistency loss term to enforce coherence between the learned operator and reconstructed solution. The MINPO formulation allows to naturally capture and efficiently resolve nonlocal spatiotemporal dependencies governed by a wide spectrum of IDEs and their subsets, including fractional PDEs. We evaluate the efficacy of MINPO in comparison with classical techniques and state-of-the-art neural-based strategies based on MLPs, such as A-PINN and fPINN, along with their newly-developed KAN variants, A-PIKAN and fPIKAN, designed to facilitate a fair comparison. Our study offers compelling evidence of the accuracy of MINPO and demonstrates its robustness in handling (i) diverse kernel types, (ii) different kernel dimensionalities, and (iii) the substantial computational demands arising from repeated evaluations of kernel integrals. MINPO, thus, generalizes beyond problem-specific formulations, providing a unified framework for systems governed by nonlocal operators.","authors":["Farinaz Mostajeran","Aruzhan Tleubek","Salah A Faroughi"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17270v1","updated":"2025-12-19T06:37:44Z","published":"2025-12-19T06:37:44Z","title":"Understanding Generalization in Role-Playing Models via Information Theory","summary":"Role-playing models (RPMs) are widely used in real-world applications but underperform when deployed in the wild. This degradation can be attributed to distribution shifts, including user, character, and dialogue compositional shifts. Existing methods like LLM-as-a-judge fall short in providing a fine-grained diagnosis of how these shifts affect RPM generalization, and thus there lack formal frameworks to characterize RPM generalization behaviors. To bridge these gaps, we introduce an information-theoretic metric, named reasoning-based effective mutual information difference (R-EMID), to measure RPM performance degradation in an interpretable way. We also derive an upper bound on R-EMID to predict the worst-case generalization performance of RPMs and theoretically reveal how various shifts contribute to the RPM performance degradation. Moreover, we propose a co-evolving reinforcement learning framework to adaptively model the connection among user, character, and dialogue context and thus enhance the estimation of dialogue response generation probability, which is critical for calculating R-EMID. Finally, we evaluate the generalization performance of various RPMs using R-EMID, finding that user shift poses the highest risk among all shifts and reinforcement learning is the most effective approach for enhancing RPM generalization.","authors":["Yongqi Li","Hao Lang","Fei Huang","Tieyun Qian","Yongbin Li"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17265v1","updated":"2025-12-19T06:29:51Z","published":"2025-12-19T06:29:51Z","title":"A Theoretical Analysis of State Similarity Between Markov Decision Processes","summary":"The bisimulation metric (BSM) is a powerful tool for analyzing state similarities within a Markov decision process (MDP), revealing that states closer in BSM have more similar optimal value functions. While BSM has been successfully utilized in reinforcement learning (RL) for tasks like state representation learning and policy exploration, its application to state similarity between multiple MDPs remains challenging. Prior work has attempted to extend BSM to pairs of MDPs, but a lack of well-established mathematical properties has limited further theoretical analysis between MDPs. In this work, we formally establish a generalized bisimulation metric (GBSM) for measuring state similarity between arbitrary pairs of MDPs, which is rigorously proven with three fundamental metric properties, i.e., GBSM symmetry, inter-MDP triangle inequality, and a distance bound on identical spaces. Leveraging these properties, we theoretically analyze policy transfer, state aggregation, and sampling-based estimation across MDPs, obtaining explicit bounds that are strictly tighter than existing ones derived from the standard BSM. Additionally, GBSM provides a closed-form sample complexity for estimation, improving upon existing asymptotic results based on BSM. Numerical results validate our theoretical findings and demonstrate the effectiveness of GBSM in multi-MDP scenarios.","authors":["Zhenyu Tao","Wei Xu","Xiaohu You"],"pdf_url":"","comment":"Submitted to an IEEE Transactions. arXiv admin note: substantial text overlap with arXiv:2509.18714"},{"id":"http://arxiv.org/abs/2512.17262v1","updated":"2025-12-19T06:25:05Z","published":"2025-12-19T06:25:05Z","title":"SHARP-QoS: Sparsely-gated Hierarchical Adaptive Routing for joint Prediction of QoS","summary":"Dependable service-oriented computing relies on multiple Quality of Service (QoS) parameters that are essential to assess service optimality. However, real-world QoS data are extremely sparse, noisy, and shaped by hierarchical dependencies arising from QoS interactions, and geographical and network-level factors, making accurate QoS prediction challenging. Existing methods often predict each QoS parameter separately, requiring multiple similar models, which increases computational cost and leads to poor generalization. Although recent joint QoS prediction studies have explored shared architectures, they suffer from negative transfer due to loss-scaling caused by inconsistent numerical ranges across QoS parameters and further struggle with inadequate representation learning, resulting in degraded accuracy. This paper presents an unified strategy for joint QoS prediction, called SHARP-QoS, that addresses these issues using three components. First, we introduce a dual mechanism to extract the hierarchical features from both QoS and contextual structures via hyperbolic convolution formulated in the Poincaré ball. Second, we propose an adaptive feature-sharing mechanism that allows feature exchange across informative QoS and contextual signals. A gated feature fusion module is employed to support dynamic feature selection among structural and shared representations. Third, we design an EMA-based loss balancing strategy that allows stable joint optimization, thereby mitigating the negative transfer. Evaluations on three datasets with two, three, and four QoS parameters demonstrate that SHARP-QoS outperforms both single- and multi-task baselines. Extensive study shows that our model effectively addresses major challenges, including sparsity, robustness to outliers, and cold-start, while maintaining moderate computational overhead, underscoring its capability for reliable joint QoS prediction.","authors":["Suraj Kumar","Arvind Kumar","Soumi Chattopadhyay"],"pdf_url":"","comment":"12 pages, 4 figures, 10 tables"},{"id":"http://arxiv.org/abs/2510.01384v3","updated":"2025-12-19T06:19:54Z","published":"2025-10-01T19:15:25Z","title":"Fine-Tuning Masked Diffusion for Provable Self-Correction","summary":"A natural desideratum for generative models is self-correction--detecting and revising low-quality tokens at inference. While Masked Diffusion Models (MDMs) have emerged as a promising approach for generative modeling in discrete spaces, their capacity for self-correction remains poorly understood. Prior attempts to incorporate self-correction into MDMs either require overhauling MDM architectures/training or rely on imprecise proxies for token quality, limiting their applicability. Motivated by this, we introduce PRISM--Plug-in Remasking for Inference-time Self-correction of Masked Diffusions--a lightweight, model-agnostic approach that applies to any pretrained MDM. Theoretically, PRISM defines a self-correction loss that provably learns per-token quality scores, without RL or a verifier. These quality scores are computed in the same forward pass with MDM and used to detect low-quality tokens. Empirically, PRISM advances MDM inference across domains and scales: Sudoku; unconditional text (170M); and code with LLaDA (8B).","authors":["Jaeyeon Kim","Seunggeun Kim","Taekyun Lee","David Z. Pan","Hyeji Kim","Sham Kakade","Sitan Chen"],"pdf_url":"","comment":"Authorship statement: Jaeyeon Kim and Seunggeun Kim contributed equally, and Taekyun Lee is also a co first author"},{"id":"http://arxiv.org/abs/2512.17259v1","updated":"2025-12-19T06:12:43Z","published":"2025-12-19T06:12:43Z","title":"Verifiability-First Agents: Provable Observability and Lightweight Audit Agents for Controlling Autonomous LLM Systems","summary":"As LLM-based agents grow more autonomous and multi-modal, ensuring they remain controllable, auditable, and faithful to deployer intent becomes critical. Prior benchmarks measured the propensity for misaligned behavior and showed that agent personalities and tool access significantly influence misalignment. Building on these insights, we propose a Verifiability-First architecture that (1) integrates run-time attestations of agent actions using cryptographic and symbolic methods, (2) embeds lightweight Audit Agents that continuously verify intent versus behavior using constrained reasoning, and (3) enforces challenge-response attestation protocols for high-risk operations. We introduce OPERA (Observability, Provable Execution, Red-team, Attestation), a benchmark suite and evaluation protocol designed to measure (i) detectability of misalignment, (ii) time to detection under stealthy strategies, and (iii) resilience of verifiability mechanisms to adversarial prompt and persona injection. Our approach shifts the evaluation focus from how likely misalignment is to how quickly and reliably misalignment can be detected and remediated.","authors":["Abhivansh Gupta"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.06699v2","updated":"2025-12-19T06:10:50Z","published":"2025-12-07T07:25:08Z","title":"Predictive Modeling of I/O Performance for Machine Learning Training Pipelines: A Data-Driven Approach to Storage Optimization","summary":"Modern machine learning training is increasingly bottlenecked by data I/O rather than compute. GPUs often sit idle at below 50% utilization waiting for data. This paper presents a machine learning approach to predict I/O performance and recommend optimal storage configurations for ML training pipelines. We collected 141 observations through systematic benchmarking across different storage backends (NVMe SSD, network-attached storage, in-memory filesystems), data formats, and access patterns, covering both low-level I/O operations and full training pipelines. After evaluating seven regression models and three classification approaches, XGBoost achieved the best performance with R-squared of 0.991, predicting I/O throughput within 11.8% error on average. Feature importance analysis revealed that throughput metrics and batch size are the primary performance drivers. This data-driven approach can reduce configuration time from days of trial-and-error to minutes of predictive recommendation. The methodology is reproducible and extensible to other resource management problems in ML systems. Code and data are available at https://github.com/knkarthik01/gpu_storage_ml_project","authors":["Karthik Prabhakar","Durgamadhab Mishra"],"pdf_url":"","comment":"20 pages, 10 figures"},{"id":"http://arxiv.org/abs/2509.25977v2","updated":"2025-12-19T06:08:01Z","published":"2025-09-30T09:09:33Z","title":"Data-Free Continual Learning of Server Models in Model-Heterogeneous Cloud-Device Collaboration","summary":"The rise of cloud-device collaborative computing has enabled intelligent services to be delivered across distributed edge devices while leveraging centralized cloud resources. In this paradigm, federated learning (FL) has become a key enabler for privacy-preserving model training without transferring raw data from edge devices to the cloud. However, with the continuous emergence of new data and increasing model diversity, traditional federated learning faces significant challenges, including inherent issues of data heterogeneity, model heterogeneity and catastrophic forgetting, along with new challenge of knowledge misalignment. In this study, we introduce FedDCL, a novel framework designed to enable data-free continual learning of the server model in a model-heterogeneous federated setting. We leverage pre-trained diffusion models to extract lightweight class-specific prototypes, which confer a threefold data-free advantage, enabling: (1) generation of synthetic data for the current task to augment training and counteract non-IID data distributions; (2) exemplar-free generative replay for retaining knowledge from previous tasks; and (3) data-free dynamic knowledge transfer from heterogeneous devices to the cloud server.Experimental results on various datasets demonstrate the effectiveness of FedDCL, showcasing its potential to enhance the generalizability and practical applicability of federated cloud-device collaboration in dynamic settings.","authors":["Xiao Zhang","Zengzhe Chen","Yuan Yuan","Yifei Zou","Fuzhen Zhuang","Wenyu Jiao","Yuke Wang","Dongxiao Yu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17257v1","updated":"2025-12-19T06:07:41Z","published":"2025-12-19T06:07:41Z","title":"Electric Vehicle Charging Load Forecasting: An Experimental Comparison of Machine Learning Methods","summary":"With the growing popularity of electric vehicles as a means of addressing climate change, concerns have emerged regarding their impact on electric grid management. As a result, predicting EV charging demand has become a timely and important research problem. While substantial research has addressed energy load forecasting in transportation, relatively few studies systematically compare multiple forecasting methods across different temporal horizons and spatial aggregation levels in diverse urban settings. This work investigates the effectiveness of five time series forecasting models, ranging from traditional statistical approaches to machine learning and deep learning methods. Forecasting performance is evaluated for short-, mid-, and long-term horizons (on the order of minutes, hours, and days, respectively), and across spatial scales ranging from individual charging stations to regional and city-level aggregations. The analysis is conducted on four publicly available real-world datasets, with results reported independently for each dataset. To the best of our knowledge, this is the first work to systematically evaluate EV charging demand forecasting across such a wide range of temporal horizons and spatial aggregation levels using multiple real-world datasets.","authors":["Iason Kyriakopoulos","Yannis Theodoridis"],"pdf_url":"","comment":"18 pages, 2 figures, 5 tables"},{"id":"http://arxiv.org/abs/2512.17254v1","updated":"2025-12-19T05:52:35Z","published":"2025-12-19T05:52:35Z","title":"Practical Framework for Privacy-Preserving and Byzantine-robust Federated Learning","summary":"Federated Learning (FL) allows multiple clients to collaboratively train a model without sharing their private data. However, FL is vulnerable to Byzantine attacks, where adversaries manipulate client models to compromise the federated model, and privacy inference attacks, where adversaries exploit client models to infer private data. Existing defenses against both backdoor and privacy inference attacks introduce significant computational and communication overhead, creating a gap between theory and practice. To address this, we propose ABBR, a practical framework for Byzantine-robust and privacy-preserving FL. We are the first to utilize dimensionality reduction to speed up the private computation of complex filtering rules in privacy-preserving FL. Additionally, we analyze the accuracy loss of vector-wise filtering in low-dimensional space and introduce an adaptive tuning strategy to minimize the impact of malicious models that bypass filtering on the global model. We implement ABBR with state-of-the-art Byzantine-robust aggregation rules and evaluate it on public datasets, showing that it runs significantly faster, has minimal communication overhead, and maintains nearly the same Byzantine-resilience as the baselines.","authors":["Baolei Zhang","Minghong Fang","Zhuqing Liu","Biao Yi","Peizhao Zhou","Yuan Wang","Tong Li","Zheli Liu"],"pdf_url":"","comment":"Accepted for publication in IEEE Transactions on Information Forensics and Security"},{"id":"http://arxiv.org/abs/2512.17251v1","updated":"2025-12-19T05:36:23Z","published":"2025-12-19T05:36:23Z","title":"AlignDP: Hybrid Differential Privacy with Rarity-Aware Protection for LLMs","summary":"Large language models are exposed to risks of extraction, distillation, and unauthorized fine-tuning. Existing defenses use watermarking or monitoring, but these act after leakage. We design AlignDP, a hybrid privacy lock that blocks knowledge transfer at the data interface. The key idea is to separate rare and non-rare fields. Rare fields are shielded by PAC indistinguishability, giving effective zero-epsilon local DP. Non-rare fields are privatized with RAPPOR, giving unbiased frequency estimates under local DP. A global aggregator enforces composition and budget. This two-tier design hides rare events and adds controlled noise to frequent events. We prove limits of PAC extension to global aggregation, give bounds for RAPPOR estimates, and analyze utility trade-off. A toy simulation confirms feasibility: rare categories remain hidden, frequent categories are recovered with small error.","authors":["Madhava Gaikwad"],"pdf_url":"","comment":"39th Conference on Neural Information Processing Systems (NeurIPS 2025) Workshop: LOCK-LLM Work-shop, NeurIPS 2025"},{"id":"http://arxiv.org/abs/2512.17245v1","updated":"2025-12-19T05:22:54Z","published":"2025-12-19T05:22:54Z","title":"Machine Learning Assisted Parameter Tuning on Wavelet Transform Amorphous Radial Distribution Function","summary":"Understanding atomic structures is crucial, yet amorphous materials remain challenging due to their irregular and non-periodic nature. The wavelet-transform radial distribution function (WT-RDF) offers a physics-based framework for analyzing amorphous structures, reliably predicting the first and second RDF peaks and overall curve trends in both binary Ge 0.25 Se 0.75 and ternary Ag x(Ge 0.25 Se 0.75)100-x (x=5,10,15,20,25) systems. Despite these strengths, WT-RDF shows limitations in amplitude accuracy, which affects quantitative analyses such as coordination numbers. This study addresses the issue by optimizing WT-RDF parameters using a machine learning approach, producing the enhanced WT-RDF+ framework. WT-RDF+ improves the precision of peak predictions and outperforms benchmark ML models, including RBF and LSTM, even when trained on only 25 percent of the binary dataset. These results demonstrate that WT-RDF+ is a robust and reliable model for structural characterization of amorphous materials, particularly Ge-Se systems, and support the efficient design and development of phase-change thin films for next-generation electronic devices and components.","authors":["Deriyan Senjaya","Stephen Ekaputra Limantoro"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2401.15502v3","updated":"2025-12-19T05:07:43Z","published":"2024-01-27T21:07:11Z","title":"Differentially private Bayesian tests","summary":"Differential privacy has emerged as an significant cornerstone in the realm of scientific hypothesis testing utilizing confidential data. In reporting scientific discoveries, Bayesian tests are widely adopted since they effectively circumnavigate the key criticisms of P-values, namely, lack of interpretability and inability to quantify evidence in support of the competing hypotheses. We present a novel differentially private Bayesian hypotheses testing framework that arise naturally under a principled data generative mechanism, inherently maintaining the interpretability of the resulting inferences. Furthermore, by focusing on differentially private Bayes factors based on widely used test statistics, we circumvent the need to model the complete data generative mechanism and ensure substantial computational benefits. We also provide a set of sufficient conditions to establish results on Bayes factor consistency under the proposed framework. The utility of the devised technology is showcased via several numerical experiments.","authors":["Abhisek Chakraborty","Saptati Datta"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.07540v2","updated":"2025-12-19T04:16:55Z","published":"2025-12-08T13:21:44Z","title":"Minimum Bayes Risk Decoding for Error Span Detection in Reference-Free Automatic Machine Translation Evaluation","summary":"Error Span Detection (ESD) extends automatic machine translation (MT) evaluation by localizing translation errors and labeling their severity. Current generative ESD methods typically use Maximum a Posteriori (MAP) decoding, assuming that the model-estimated probabilities are perfectly correlated with similarity to the human annotation, but we often observe higher likelihood assigned to an incorrect annotation than to the human one. We instead apply Minimum Bayes Risk (MBR) decoding to generative ESD. We use a sentence- or span-level similarity function for MBR decoding, which selects candidate hypotheses based on their approximate similarity to the human annotation. Experimental results on the WMT24 Metrics Shared Task show that MBR decoding significantly improves span-level performance and generally matches or outperforms MAP at the system and sentence levels. To reduce the computational cost of MBR decoding, we further distill its decisions into a model decoded via greedy search, removing the inference-time latency bottleneck.","authors":["Boxuan Lyu","Haiyue Song","Hidetaka Kamigaito","Chenchen Ding","Hideki Tanaka","Masao Utiyama","Kotaro Funakoshi","Manabu Okumura"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2502.00277v4","updated":"2025-12-19T04:07:05Z","published":"2025-02-01T02:24:31Z","title":"Regularized Langevin Dynamics for Combinatorial Optimization","summary":"This work proposes a simple yet effective sampling framework for combinatorial optimization (CO). Our method builds on discrete Langevin dynamics (LD), an efficient gradient-guided generative paradigm. However, we observe that directly applying LD often leads to limited exploration. To overcome this limitation, we propose the Regularized Langevin Dynamics (RLD), which enforces an expected distance between the sampled and current solutions, effectively avoiding local minima. We develop two CO solvers on top of RLD, one based on simulated annealing (SA), and the other one based on neural network (NN). Empirical results on three classic CO problems demonstrate that both of our methods can achieve comparable or better performance against the previous state-of-the-art (SOTA) SA- and NN-based solvers. In particular, our SA algorithm reduces the runtime of the previous SOTA SA method by up to 80\\%, while achieving equal or superior performance. In summary, RLD offers a promising framework for enhancing both traditional heuristics and NN models to solve CO problems. Our code is available at https://github.com/Shengyu-Feng/RLD4CO.","authors":["Shengyu Feng","Yiming Yang"],"pdf_url":"","comment":"ICML 2025"},{"id":"http://arxiv.org/abs/2511.23083v4","updated":"2025-12-19T03:50:56Z","published":"2025-11-28T11:14:15Z","title":"Spectral Concentration at the Edge of Stability: Information Geometry of Kernel Associative Memory","summary":"High-capacity kernel Hopfield networks exhibit a \\textit{Ridge of Optimization} characterized by extreme stability. While previously linked to \\textit{Spectral Concentration}, its origin remains elusive. Here, we analyze the network dynamics on a statistical manifold, revealing that the Ridge corresponds to the Edge of Stability, a critical boundary where the Fisher Information Matrix becomes singular. We demonstrate that the apparent Euclidean force antagonism is a manifestation of \\textit{Dual Equilibrium} in the Riemannian space. This unifies learning dynamics and capacity via the Minimum Description Length principle, offering a geometric theory of self-organized criticality.","authors":["Akira Tamamori"],"pdf_url":"","comment":"5 pages, 4 figures"},{"id":"http://arxiv.org/abs/2512.17213v1","updated":"2025-12-19T03:50:42Z","published":"2025-12-19T03:50:42Z","title":"CheXPO-v2: Preference Optimization for Chest X-ray VLMs with Knowledge Graph Consistency","summary":"Medical Vision-Language Models (VLMs) are prone to hallucinations, compromising clinical reliability. While reinforcement learning methods like Group Relative Policy Optimization (GRPO) offer a low-cost alignment solution, their reliance on sparse, outcome-based rewards inadvertently encourages models to \"overthink\" -- generating verbose, convoluted, and unverifiable Chain-of-Thought reasoning to justify answers. This focus on outcomes obscures factual errors and poses significant safety risks. To address this, we propose CheXPO-v2, a novel alignment framework that shifts from outcome to process supervision. Our core innovation is a Knowledge Graph Consistency Reward mechanism driven by Entity-Relation Matching. By explicitly parsing reasoning steps into structured \"Disease, Relation, Anatomy\" triplets, we provide fine-grained supervision that penalizes incoherent logic and hallucinations at the atomic level. Integrating this with a hard-example mining strategy, our approach significantly outperforms GRPO and state-of-the-art models on benchmarks like MIMIC-CXR-VQA. Crucially, CheXPO-v2 achieves new state-of-the-art accuracy using only 5k samples, demonstrating exceptional data efficiency while producing clinically sound and verifiable reasoning. The project source code is publicly available at: https://github.com/ecoxial2007/CheX-Phi4MM.","authors":["Xiao Liang","Yuxuan An","Di Wang","Jiawei Hu","Zhicheng Jiao","Bin Jing","Quan Wang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17209v1","updated":"2025-12-19T03:42:47Z","published":"2025-12-19T03:42:47Z","title":"Do Foundational Audio Encoders Understand Music Structure?","summary":"In music information retrieval (MIR) research, the use of pretrained foundational audio encoders (FAEs) has recently become a trend. FAEs pretrained on large amounts of music and audio data have been shown to improve performance on MIR tasks such as music tagging and automatic music transcription. However, their use for music structure analysis (MSA) remains underexplored. Although many open-source FAE models are available, only a small subset has been examined for MSA, and the impact of factors such as learning methods, training data, and model context length on MSA performance remains unclear. In this study, we conduct comprehensive experiments on 11 types of FAEs to investigate how these factors affect MSA performance. Our results demonstrate that FAEs using selfsupervised learning with masked language modeling on music data are particularly effective for MSA. These findings pave the way for future research in MSA.","authors":["Keisuke Toyama","Zhi Zhong","Akira Takahashi","Shusuke Takahashi","Yuki Mitsufuji"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2502.09496v2","updated":"2025-12-19T03:39:47Z","published":"2025-02-13T17:03:03Z","title":"On Agnostic PAC Learning in the Small Error Regime","summary":"Binary classification in the classic PAC model exhibits a curious phenomenon: Empirical Risk Minimization (ERM) learners are suboptimal in the realizable case yet optimal in the agnostic case. Roughly speaking, this owes itself to the fact that non-realizable distributions $\\mathcal{D}$ are simply more difficult to learn than realizable distributions -- even when one discounts a learner's error by $\\mathrm{err}(h^*_{\\mathcal{D}})$, the error of the best hypothesis in $\\mathcal{H}$ for $\\mathcal{D}$. Thus, optimal agnostic learners are permitted to incur excess error on (easier-to-learn) distributions $\\mathcal{D}$ for which $τ= \\mathrm{err}(h^*_{\\mathcal{D}})$ is small.\n  Recent work of Hanneke, Larsen, and Zhivotovskiy (FOCS `24) addresses this shortcoming by including $τ$ itself as a parameter in the agnostic error term. In this more fine-grained model, they demonstrate tightness of the error lower bound $τ+ Ω\\left(\\sqrt{\\frac{τ(d + \\log(1 / δ))}{m}} + \\frac{d + \\log(1 / δ)}{m} \\right)$ in a regime where $τ> d/m$, and leave open the question of whether there may be a higher lower bound when $τ\\approx d/m$, with $d$ denoting $\\mathrm{VC}(\\mathcal{H})$. In this work, we resolve this question by exhibiting a learner which achieves error $c \\cdot τ+ O \\left(\\sqrt{\\frac{τ(d + \\log(1 / δ))}{m}} + \\frac{d + \\log(1 / δ)}{m} \\right)$ for a constant $c \\leq 2.1$, thus matching the lower bound when $τ\\approx d/m$. Further, our learner is computationally efficient and is based upon careful aggregations of ERM classifiers, making progress on two other questions of Hanneke, Larsen, and Zhivotovskiy (FOCS `24). We leave open the interesting question of whether our approach can be refined to lower the constant from 2.1 to 1, which would completely settle the complexity of agnostic learning.","authors":["Julian Asilis","Mikael Møller Høgsgaard","Grigoris Velegkas"],"pdf_url":"","comment":"36 pages, NeurIPS 2025"},{"id":"http://arxiv.org/abs/2512.17203v1","updated":"2025-12-19T03:29:23Z","published":"2025-12-19T03:29:23Z","title":"Learning solution operator of dynamical systems with diffusion maps kernel ridge regression","summary":"Many scientific and engineering systems exhibit complex nonlinear dynamics that are difficult to predict accurately over long time horizons. Although data-driven models have shown promise, their performance often deteriorates when the geometric structures governing long-term behavior are unknown or poorly represented. We demonstrate that a simple kernel ridge regression (KRR) framework, when combined with a dynamics-aware validation strategy, provides a strong baseline for long-term prediction of complex dynamical systems. By employing a data-driven kernel derived from diffusion maps, the proposed Diffusion Maps Kernel Ridge Regression (DM-KRR) method implicitly adapts to the intrinsic geometry of the system's invariant set, without requiring explicit manifold reconstruction or attractor modeling, procedures that often limit predictive performance. Across a broad range of systems, including smooth manifolds, chaotic attractors, and high-dimensional spatiotemporal flows, DM-KRR consistently outperforms state-of-the-art random feature, neural-network and operator-learning methods in both accuracy and data efficiency. These findings underscore that long-term predictive skill depends not only on model expressiveness, but critically on respecting the geometric constraints encoded in the data through dynamically consistent model selection. Together, simplicity, geometry awareness, and strong empirical performance point to a promising path for reliable and efficient learning of complex dynamical systems.","authors":["Jiwoo Song","Daning Huang","John Harlim"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2508.19009v3","updated":"2025-12-19T03:29:08Z","published":"2025-08-26T13:14:29Z","title":"Dual-Distilled Heterogeneous Federated Learning with Adaptive Margins for Trainable Global Prototypes","summary":"Heterogeneous Federated Learning (HFL) has gained significant attention for its capacity to handle both model and data heterogeneity across clients. Prototype-based HFL methods emerge as a promising solution to address statistical and model heterogeneity as well as privacy challenges, paving the way for new advancements in HFL research. This method focuses on sharing class-representative prototypes among heterogeneous clients. However, aggregating these prototypes via standard weighted averaging often yields sub-optimal global knowledge. Specifically, the averaging approach induces a shrinking of the aggregated prototypes' decision margins, thereby degrading model performance in scenarios with model heterogeneity and non-IID data distributions. The propose FedProtoKD in a Heterogeneous Federated Learning setting, utilizing an enhanced dual-knowledge distillation mechanism to enhance system performance by leveraging clients' logits and prototype feature representations. The proposed framework aims to resolve the prototype margin-shrinking problem using a contrastive learning-based trainable server prototype by leveraging a class-wise adaptive prototype margin. Furthermore, the framework assess the importance of public samples using the closeness of the sample's prototype to its class representative prototypes, which enhances learning performance. FedProtoKD improved test accuracy by an average of 1.13% and up to 34.13% across various settings, significantly outperforming existing state-of-the-art HFL methods.","authors":["Fatema Siddika","Md Anwar Hossen","Wensheng Zhang","Anuj Sharma","Juan Pablo Muñoz","Ali Jannesari"],"pdf_url":"","comment":"11 pages, 8 figures"},{"id":"http://arxiv.org/abs/2512.17198v1","updated":"2025-12-19T03:25:05Z","published":"2025-12-19T03:25:05Z","title":"BumpNet: A Sparse Neural Network Framework for Learning PDE Solutions","summary":"We introduce BumpNet, a sparse neural network framework for PDE numerical solution and operator learning. BumpNet is based on meshless basis function expansion, in a similar fashion to radial-basis function (RBF) networks. Unlike RBF networks, the basis functions in BumpNet are constructed from ordinary sigmoid activation functions. This enables the efficient use of modern training techniques optimized for such networks. All parameters of the basis functions, including shape, location, and amplitude, are fully trainable. Model parsimony and h-adaptivity are effectively achieved through dynamically pruning basis functions during training. BumpNet is a general framework that can be combined with existing neural architectures for learning PDE solutions: here, we propose Bump-PINNs (BumpNet with physics-informed neural networks) for solving general PDEs; Bump-EDNN (BumpNet with evolutionary deep neural networks) to solve time-evolution PDEs; and Bump-DeepONet (BumpNet with deep operator networks) for PDE operator learning. Bump-PINNs are trained using the same collocation-based approach used by PINNs, Bump-EDNN uses a BumpNet only in the spatial domain and uses EDNNs to advance the solution in time, while Bump-DeepONets employ a BumpNet regression network as the trunk network of a DeepONet. Extensive numerical experiments demonstrate the efficiency and accuracy of the proposed architecture.","authors":["Shao-Ting Chiu","Ioannis G. Kevrekidis","Ulisses Braga-Neto"],"pdf_url":"","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2508.21052v2","updated":"2025-12-19T16:10:13Z","published":"2025-08-28T17:55:14Z","title":"FakeParts: a New Family of AI-Generated DeepFakes","summary":"We introduce FakeParts, a new class of deepfakes characterized by subtle, localized manipulations to specific spatial regions or temporal segments of otherwise authentic videos. Unlike fully synthetic content, these partial manipulations - ranging from altered facial expressions to object substitutions and background modifications - blend seamlessly with real elements, making them particularly deceptive and difficult to detect. To address the critical gap in detection, we present FakePartsBench, the first large-scale benchmark specifically designed to capture the full spectrum of partial deepfakes. Comprising over 81K (including 44K FakeParts) videos with pixel- and frame-level manipulation annotations, our dataset enables comprehensive evaluation of detection methods. Our user studies demonstrate that FakeParts reduces human detection accuracy by up to 26% compared to traditional deepfakes, with similar performance degradation observed in state-of-the-art detection models. This work identifies an urgent vulnerability in current detectors and provides the necessary resources to develop methods robust to partial manipulations.","authors":["Ziyi Liu","Firas Gabetni","Awais Hussain Sani","Xi Wang","Soobash Daiboo","Gaetan Brison","Gianni Franchi","Vicky Kalogeiton"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17650v1","updated":"2025-12-19T14:49:30Z","published":"2025-12-19T14:49:30Z","title":"Region-Constraint In-Context Generation for Instructional Video Editing","summary":"The In-context generation paradigm recently has demonstrated strong power in instructional image editing with both data efficiency and synthesis quality. Nevertheless, shaping such in-context learning for instruction-based video editing is not trivial. Without specifying editing regions, the results can suffer from the problem of inaccurate editing regions and the token interference between editing and non-editing areas during denoising. To address these, we present ReCo, a new instructional video editing paradigm that novelly delves into constraint modeling between editing and non-editing regions during in-context generation. Technically, ReCo width-wise concatenates source and target video for joint denoising. To calibrate video diffusion learning, ReCo capitalizes on two regularization terms, i.e., latent and attention regularization, conducting on one-step backward denoised latents and attention maps, respectively. The former increases the latent discrepancy of the editing region between source and target videos while reducing that of non-editing areas, emphasizing the modification on editing area and alleviating outside unexpected content generation. The latter suppresses the attention of tokens in the editing region to the tokens in counterpart of the source video, thereby mitigating their interference during novel object generation in target video. Furthermore, we propose a large-scale, high-quality video editing dataset, i.e., ReCo-Data, comprising 500K instruction-video pairs to benefit model training. Extensive experiments conducted on four major instruction-based video editing tasks demonstrate the superiority of our proposal.","authors":["Zhongwei Zhang","Fuchen Long","Wei Li","Zhaofan Qiu","Wu Liu","Ting Yao","Tao Mei"],"pdf_url":"","comment":"Project page: https://zhw-zhang.github.io/ReCo-page/"},{"id":"http://arxiv.org/abs/2512.17528v1","updated":"2025-12-19T12:51:40Z","published":"2025-12-19T12:51:40Z","title":"Voxel-GS: Quantized Scaffold Gaussian Splatting Compression with Run-Length Coding","summary":"Substantial Gaussian splatting format point clouds require effective compression. In this paper, we propose Voxel-GS, a simple yet highly effective framework that departs from the complex neural entropy models of prior work, instead achieving competitive performance using only a lightweight rate proxy and run-length coding. Specifically, we employ a differentiable quantization to discretize the Gaussian attributes of Scaffold-GS. Subsequently, a Laplacian-based rate proxy is devised to impose an entropy constraint, guiding the generation of high-fidelity and compact reconstructions. Finally, this integer-type Gaussian point cloud is compressed losslessly using Octree and run-length coding. Experiments validate that the proposed rate proxy accurately estimates the bitrate of run-length coding, enabling Voxel-GS to eliminate redundancy and optimize for a more compact representation. Consequently, our method achieves a remarkable compression ratio with significantly faster coding speeds than prior art. The code is available at https://github.com/zb12138/VoxelGS.","authors":["Chunyang Fu","Xiangrui Liu","Shiqi Wang","Zhu Li"],"pdf_url":"","comment":"Accepted by DCC 2026"},{"id":"http://arxiv.org/abs/2503.19041v4","updated":"2025-12-19T08:17:42Z","published":"2025-03-24T18:11:42Z","title":"LookAhead Tuning: Safer Language Models via Partial Answer Previews","summary":"Fine-tuning enables large language models (LLMs) to adapt to specific domains, but often compromises their previously established safety alignment. To mitigate the degradation of model safety during fine-tuning, we introduce LookAhead Tuning, a lightweight and effective data-driven approach that preserves safety during fine-tuning. The method introduces two simple strategies that modify training data by previewing partial answer prefixes, thereby minimizing perturbations to the model's initial token distributions and maintaining its built-in safety mechanisms. Comprehensive experiments demonstrate that LookAhead Tuning effectively maintains model safety without sacrificing robust performance on downstream tasks. Our findings position LookAhead Tuning as a reliable and efficient solution for the safe and effective adaptation of LLMs.","authors":["Kangwei Liu","Mengru Wang","Yujie Luo","Lin Yuan","Mengshu Sun","Lei Liang","Zhiqiang Zhang","Jun Zhou","Bryan Hooi","Shumin Deng"],"pdf_url":"","comment":"WSDM 2026 short"},{"id":"http://arxiv.org/abs/2512.17319v1","updated":"2025-12-19T08:07:51Z","published":"2025-12-19T08:07:51Z","title":"A Benchmark for Ultra-High-Resolution Remote Sensing MLLMs","summary":"Multimodal large language models (MLLMs) demonstrate strong perception and reasoning performance on existing remote sensing (RS) benchmarks. However, most prior benchmarks rely on low-resolution imagery, and some high-resolution benchmarks suffer from flawed reasoning-task designs. We show that text-only LLMs can perform competitively with multimodal vision-language models on RS reasoning tasks without access to images, revealing a critical mismatch between current benchmarks and the intended evaluation of visual understanding. To enable faithful assessment, we introduce RSHR-Bench, a super-high-resolution benchmark for RS visual understanding and reasoning. RSHR-Bench contains 5,329 full-scene images with a long side of at least 4,000 pixels, with up to about 3 x 10^8 pixels per image, sourced from widely used RS corpora and UAV collections. We design four task families: multiple-choice VQA, open-ended VQA, image captioning, and single-image evaluation. These tasks cover nine perception categories and four reasoning types, supporting multi-turn and multi-image dialog. To reduce reliance on language priors, we apply adversarial filtering with strong LLMs followed by rigorous human verification. Overall, we construct 3,864 VQA tasks, 3,913 image captioning tasks, and 500 fully human-written or verified single-image evaluation VQA pairs. Evaluations across open-source, closed-source, and RS-specific VLMs reveal persistent performance gaps in super-high-resolution scenarios. Code: https://github.com/Yunkaidang/RSHR","authors":["Yunkai Dang","Meiyi Zhu","Donghao Wang","Yizhuo Zhang","Jiacheng Yang","Qi Fan","Yuekun Yang","Wenbin Li","Feng Miao","Yang Gao"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.12284v2","updated":"2025-12-19T08:02:44Z","published":"2025-12-13T11:02:04Z","title":"V-Rex: Real-Time Streaming Video LLM Acceleration via Dynamic KV Cache Retrieval","summary":"Streaming video large language models (LLMs) are increasingly used for real-time multimodal tasks such as video captioning, question answering, conversational agents, and augmented reality. However, these models face fundamental memory and computational challenges because their key-value (KV) caches grow substantially with continuous streaming video input. This process requires an iterative prefill stage, which is a unique feature of streaming video LLMs. Due to its iterative prefill stage, it suffers from significant limitations, including extensive computation, substantial data transfer, and degradation in accuracy. Crucially, this issue is exacerbated for edge deployment, which is the primary target for these models.\n  In this work, we propose V-Rex, the first software-hardware co-designed accelerator that comprehensively addresses both algorithmic and hardware bottlenecks in streaming video LLM inference. At its core, V-Rex introduces ReSV, a training-free dynamic KV cache retrieval algorithm. ReSV exploits temporal and spatial similarity-based token clustering to reduce excessive KV cache memory across video frames. To fully realize these algorithmic benefits, V-Rex offers a compact, low-latency hardware accelerator with a dynamic KV cache retrieval engine (DRE), featuring bit-level and early-exit based computing units. V-Rex achieves unprecedented real-time of 3.9-8.3 FPS and energy-efficient streaming video LLM inference on edge deployment with negligible accuracy loss. While DRE only accounts for 2.2% power and 2.0% area, the system delivers 1.9-19.7x speedup and 3.1-18.5x energy efficiency improvements over AGX Orin GPU. This work is the first to comprehensively tackle KV cache retrieval across algorithms and hardware, enabling real-time streaming video LLM inference on resource-constrained edge devices.","authors":["Donghyuk Kim","Sejeong Yang","Wonjin Shin","Joo-Young Kim"],"pdf_url":"","comment":"14 pages, 20 figures, conference"},{"id":"http://arxiv.org/abs/2511.22046v2","updated":"2025-12-19T01:58:25Z","published":"2025-11-27T02:59:03Z","title":"AutoRec: Accelerating Loss Recovery for Live Streaming in a Multi-Supplier Market","summary":"Due to the limited permissions for upgrading dualside (i.e., server-side and client-side) loss tolerance schemes from the perspective of CDN vendors in a multi-supplier market, modern large-scale live streaming services are still using the automatic-repeat-request (ARQ) based paradigm for loss recovery, which only requires server-side modifications. In this paper, we first conduct a large-scale measurement study with up to 50 million live streams. We find that loss shows dynamics and live streaming contains frequent on-off mode switching in the wild. We further find that the recovery latency, enlarged by the ubiquitous retransmission loss, is a critical factor affecting live streaming's client-side QoE (e.g., video freezing). We then propose an enhanced recovery mechanism called AutoRec, which can transform the disadvantages of on-off mode switching into an advantage for reducing loss recovery latency without any modifications on the client side. AutoRec allows users to customize overhead tolerance and recovery latency tolerance and adaptively adjusts strategies as the network environment changes to ensure that recovery latency meets user demands whenever possible while keeping overhead under control. We implement AutoRec upon QUIC and evaluate it via testbed and real-world commercial services deployments. The experimental results demonstrate the practicability and profitability of AutoRec.","authors":["Tong Li","Xu Yan","Bo Wu","Cheng Luo","Fuyu Wang","Jiuxiang Zhu","Haoyi Fang","Xinle Du","Ke Xu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.18122v1","updated":"2025-12-19T23:02:39Z","published":"2025-12-19T23:02:39Z","title":"Accelerating End-to-End PDF to Markdown Conversion Through Assisted Generation","summary":"Converting data from machine-unreadable formats like PDFs into Markdown has the potential to enhance the accessibility of scientific research. Existing end-to-end decoder transformer models can transform screenshots of PDFs into Markdown, offering more flexibility than pipeline-based methods. Yet, decoding text token by token from scratch is inefficient, especially when dense text can be directly copied from the PDF. To address this challenge, this paper modifies Prompt Lookup Decoding (PLD) to extract candidate sequences directly from PDF files, leveraging the high n-gram overlap between PDFs and their Markdown equivalents. A new method, Copy Lookup Decoding (CLD), is introduced here to enhance PLD's candidate generation mechanism. Experiments demonstrate that CLD can accelerate the conversion process by up to 1.70$\\times$ at original quality. The codebase for this paper is open-source on GitHub (https://github.com/Fireblossom/CopyLookup).","authors":["Changxu Duan"],"pdf_url":"","comment":"Accepted NLDB 2025"},{"id":"http://arxiv.org/abs/2512.18115v1","updated":"2025-12-19T22:43:12Z","published":"2025-12-19T22:43:12Z","title":"Layout-Aware Text Editing for Efficient Transformation of Academic PDFs to Markdown","summary":"Academic documents stored in PDF format can be transformed into plain text structured markup languages to enhance accessibility and enable scalable digital library workflows. Markup languages allow for easier updates and customization, making academic content more adaptable and accessible to diverse usage, such as linguistic corpus compilation. Such documents, typically delivered in PDF format, contain complex elements including mathematical formulas, figures, headers, and tables, as well as densely layouted text. Existing end-to-end decoder transformer models can transform screenshots of documents into markup language. However, these models exhibit significant inefficiencies; their token-by-token decoding from scratch wastes a lot of inference steps in regenerating dense text that could be directly copied from PDF files. To solve this problem, we introduce EditTrans, a hybrid editing-generation model whose features allow identifying a queue of to-be-edited text from a PDF before starting to generate markup language. EditTrans contains a lightweight classifier fine-tuned from a Document Layout Analysis model on 162,127 pages of documents from arXiv. In our evaluations, EditTrans reduced the transformation latency up to 44.5% compared to end-to-end decoder transformer models, while maintaining transformation quality. Our code and reproducible dataset production scripts are open-sourced.","authors":["Changxu Duan"],"pdf_url":"","comment":"Accepted ICDAR 2025"}]},"2025-12-22T00:00:00Z":{"Artificial Intelligence":[{"id":"http://arxiv.org/abs/2512.19691v1","updated":"2025-12-22T18:59:34Z","published":"2025-12-22T18:59:34Z","title":"Scalably Enhancing the Clinical Validity of a Task Benchmark with Physician Oversight","summary":"Automating the calculation of clinical risk scores offers a significant opportunity to reduce physician administrative burden and enhance patient care. The current standard for evaluating this capability is MedCalc-Bench, a large-scale dataset constructed using LLM-based feature extraction and rule-based aggregation. However, treating such model-generated benchmarks as static oracles risks enshrining historical model errors as evaluation gold standards, a problem dangerously amplified when these datasets serve as reward signals for Reinforcement Learning (RL). In this work, we propose viewing benchmarks for complex tasks such as clinical score computation as ''in-progress living documents'' that should be periodically re-evaluated as the processes for creating them improve. We introduce a systematic, physician-in-the-loop pipeline that leverages advanced agentic verifiers to audit and relabel MedCalc-Bench, utilizing automated triage to reserve scarce clinician attention for the most contentious instances. Our audit reveals that a notable fraction of original labels diverge from medical ground truth due to extraction errors, calculator logic mismatches, and clinical ambiguity. To study whether this label noise meaningfully impacts downstream RL training, we fine-tune a Qwen3-8B model via Group Relative Policy Optimization (GRPO) and demonstrate that training on corrected labels yields an 8.7% absolute improvement in accuracy over the original baseline -- validating that label noise materially affects model evaluation. These findings underscore that in safety-critical domains, rigorous benchmark maintenance is a prerequisite for genuine model alignment.","authors":["Junze Ye","Daniel Tawfik","Alex J. Goodell","Nikhil V. Kotha","Mark K. Buyyounouski","Mohsen Bayati"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2510.09595v2","updated":"2025-12-22T18:56:01Z","published":"2025-10-10T17:54:24Z","title":"LiveOIBench: Can Large Language Models Outperform Human Contestants in Informatics Olympiads?","summary":"Competitive programming problems increasingly serve as valuable benchmarks to evaluate the coding capabilities of large language models (LLMs) due to their complexity and ease of verification. Yet, current coding benchmarks face limitations such as lack of exceptionally challenging problems, insufficient test case coverage, reliance on online platform APIs that limit accessibility. To address these issues, we introduce LiveOIBench, a comprehensive benchmark featuring 403 expert-curated Olympiad-level competitive programming problems, each with an average of 60 expert-designed test cases. The problems are sourced directly from 72 official contests of 14 Informatics Olympiads in different regions conducted between 2023 and 2025. LiveOIBench distinguishes itself through four key features: (1) meticulously curated high-quality tasks with detailed subtask rubrics and extensive private test cases; (2) direct integration of elite contestant performance data to enable informative comparison against top-performing humans; (3) planned continuous, contamination-free updates from newly released Olympiad problems; and (4) a self-contained evaluation system facilitating offline and easy-to-reproduce assessments. Benchmarking 34 popular general-purpose and reasoning LLMs, we find that GPT-5 achieves a notable 81.76th percentile, a strong result that nonetheless falls short of top human contestants, who usually place above 90th. In contrast, among open-weight reasoning models, GPT-OSS-120B achieves only a 60th percentile, underscoring significant capability disparities from frontier closed models. Detailed analyses indicate that robust reasoning models prioritize precise problem analysis over excessive exploration, suggesting future models should emphasize structured analysis and minimize unnecessary exploration. All data, code, and leaderboard results are publicly available on our website.","authors":["Kaijian Zou","Aaron Xiong","Yunxiang Zhang","Frederick Zhang","Yueqi Ren","Jirong Yang","Ayoung Lee","Shitanshu Bhushan","Lu Wang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19678v1","updated":"2025-12-22T18:53:50Z","published":"2025-12-22T18:53:50Z","title":"WorldWarp: Propagating 3D Geometry with Asynchronous Video Diffusion","summary":"Generating long-range, geometrically consistent video presents a fundamental dilemma: while consistency demands strict adherence to 3D geometry in pixel space, state-of-the-art generative models operate most effectively in a camera-conditioned latent space. This disconnect causes current methods to struggle with occluded areas and complex camera trajectories. To bridge this gap, we propose WorldWarp, a framework that couples a 3D structural anchor with a 2D generative refiner. To establish geometric grounding, WorldWarp maintains an online 3D geometric cache built via Gaussian Splatting (3DGS). By explicitly warping historical content into novel views, this cache acts as a structural scaffold, ensuring each new frame respects prior geometry. However, static warping inevitably leaves holes and artifacts due to occlusions. We address this using a Spatio-Temporal Diffusion (ST-Diff) model designed for a \"fill-and-revise\" objective. Our key innovation is a spatio-temporal varying noise schedule: blank regions receive full noise to trigger generation, while warped regions receive partial noise to enable refinement. By dynamically updating the 3D cache at every step, WorldWarp maintains consistency across video chunks. Consequently, it achieves state-of-the-art fidelity by ensuring that 3D logic guides structure while diffusion logic perfects texture. Project page: \\href{https://hyokong.github.io/worldwarp-page/}{https://hyokong.github.io/worldwarp-page/}.","authors":["Hanyang Kong","Xingyi Yang","Xiaoxu Zheng","Xinchao Wang"],"pdf_url":"","comment":"Project page: https://hyokong.github.io/worldwarp-page/"},{"id":"http://arxiv.org/abs/2512.19673v1","updated":"2025-12-22T18:51:48Z","published":"2025-12-22T18:51:48Z","title":"Bottom-up Policy Optimization: Your Language Model Policy Secretly Contains Internal Policies","summary":"Existing reinforcement learning (RL) approaches treat large language models (LLMs) as a single unified policy, overlooking their internal mechanisms. Understanding how policy evolves across layers and modules is therefore crucial for enabling more targeted optimization and raveling out complex reasoning mechanisms. In this paper, we decompose the language model policy by leveraging the intrinsic split of the Transformer residual stream and the equivalence between the composition of hidden states with the unembedding matrix and the resulting samplable policy. This decomposition reveals Internal Layer Policies, corresponding to contributions from individual layers, and Internal Modular Policies, which align with the self-attention and feed-forward network (FFN) components within each layer. By analyzing the entropy of internal policy, we find that: (a) Early layers keep high entropy for exploration, top layers converge to near-zero entropy for refinement, with convergence patterns varying across model series. (b) LLama's prediction space rapidly converges in the final layer, whereas Qwen-series models, especially Qwen3, exhibit a more human-like, progressively structured reasoning pattern. Motivated by these findings, we propose Bottom-up Policy Optimization (BuPO), a novel RL paradigm that directly optimizes the internal layer policy during early training. By aligning training objective at lower layer, BuPO reconstructs foundational reasoning capabilities and achieves superior performance. Extensive experiments on complex reasoning benchmarks demonstrates the effectiveness of our method. Our code is available at https://github.com/Trae1ounG/BuPO.","authors":["Yuqiao Tan","Minzheng Wang","Shizhu He","Huanxuan Liao","Chengfeng Zhao","Qiunan Lu","Tian Liang","Jun Zhao","Kang Liu"],"pdf_url":"","comment":"Preprint. Our code is available at https://github.com/Trae1ounG/BuPO"},{"id":"http://arxiv.org/abs/2509.23004v2","updated":"2025-12-22T18:45:53Z","published":"2025-09-26T23:50:25Z","title":"Bridging the Gap Between Scientific Laws Derived by AI Systems and Canonical Knowledge via Abductive Inference with AI-Noether","summary":"Advances in AI have shown great potential in contributing to the acceleration of scientific discovery. Symbolic regression can fit interpretable models to data, but these models are not necessarily derivable from established theory. Recent systems (e.g., AI-Descartes, AI-Hilbert) enforce derivability from prior knowledge. However, when existing theories are incomplete or incorrect, these machine-generated hypotheses may fall outside the theoretical scope. Automatically finding corrections to axiom systems to close this gap remains a central challenge in scientific discovery. We propose a solution: an open-source algebraic geometry-based system that, given an incomplete axiom system expressible as polynomials and a hypothesis that the axioms cannot derive, generates a minimal set of candidate axioms that, when added to the theory, provably derive the (possibly noisy) hypothesis. We illustrate the efficacy of our approach by showing that it can reconstruct key axioms required to derive the carrier-resolved photo-Hall effect, Einstein's relativistic laws, and several other laws.","authors":["Karan Srivastava","Sanjeeb Dash","Ryan Cory-Wright","Barry Trager","Cristina Cornelio","Lior Horesh"],"pdf_url":"","comment":"47 Pages (20+appendix), 14 Figures, Preprint: Updated for recent submission"},{"id":"http://arxiv.org/abs/2512.19663v1","updated":"2025-12-22T18:41:45Z","published":"2025-12-22T18:41:45Z","title":"Beyond CLIP: Knowledge-Enhanced Multimodal Transformers for Cross-Modal Alignment in Diabetic Retinopathy Diagnosis","summary":"Diabetic retinopathy (DR) is a leading cause of preventable blindness worldwide, demanding accurate automated diagnostic systems. While general-domain vision-language models like Contrastive Language-Image Pre-Training (CLIP) perform well on natural image tasks, they struggle in medical domain applications, particularly in cross-modal retrieval for ophthalmological images. We propose a novel knowledge-enhanced joint embedding framework that integrates retinal fundus images, clinical text, and structured patient data through a multimodal transformer architecture to address the critical gap in medical image-text alignment. Our approach employs separate encoders for each modality: a Vision Transformer (ViT-B/16) for retinal images, Bio-ClinicalBERT for clinical narratives, and a multilayer perceptron for structured demographic and clinical features. These modalities are fused through a joint transformer with modality-specific embeddings, trained using multiple objectives including contrastive losses between modality pairs, reconstruction losses for images and text, and classification losses for DR severity grading according to ICDR and SDRG schemes. Experimental results on the Brazilian Multilabel Ophthalmological Dataset (BRSET) demonstrate significant improvements over baseline models. Our framework achieves near-perfect text-to-image retrieval performance with Recall@1 of 99.94% compared to fine-tuned CLIP's 1.29%, while maintaining state-of-the-art classification accuracy of 97.05% for SDRG and 97.97% for ICDR. Furthermore, zero-shot evaluation on the unseen DeepEyeNet dataset validates strong generalizability with 93.95% Recall@1 versus 0.22% for fine-tuned CLIP. These results demonstrate that our multimodal training approach effectively captures cross-modal relationships in the medical domain, establishing both superior retrieval capabilities and robust diagnostic performance.","authors":["Argha Kamal Samanta","Harshika Goyal","Vasudha Joshi","Tushar Mungle","Pabitra Mitra"],"pdf_url":"","comment":"14 pages, 14 figures"},{"id":"http://arxiv.org/abs/2512.19654v1","updated":"2025-12-22T18:32:23Z","published":"2025-12-22T18:32:23Z","title":"Clustering with Label Consistency","summary":"Designing efficient, effective, and consistent metric clustering algorithms is a significant challenge attracting growing attention. Traditional approaches focus on the stability of cluster centers; unfortunately, this neglects the real-world need for stable point labels, i.e., stable assignments of points to named sets (clusters). In this paper, we address this gap by initiating the study of label-consistent metric clustering. We first introduce a new notion of consistency, measuring the label distance between two consecutive solutions. Then, armed with this new definition, we design new consistent approximation algorithms for the classical $k$-center and $k$-median problems.","authors":["Diptarka Chakraborty","Hendrik Fichtenberger","Bernhard Haeupler","Silvio Lattanzi","Ashkan Norouzi-Fard","Ola Svensson"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2306.00029v2","updated":"2025-12-22T18:29:22Z","published":"2023-05-31T05:24:48Z","title":"CodeTF: One-stop Transformer Library for State-of-the-art Code LLMs","summary":"Code intelligence plays a key role in transforming modern software engineering. Recently, deep learning-based models, especially Transformer-based large language models (LLMs), have demonstrated remarkable potential in tackling these tasks by leveraging massive open-source code data and programming language features. However, the development and deployment of such models often require expertise in both machine learning and software engineering, creating a barrier for the model adoption. In this paper, we present CodeTF, an open-source Transformer-based library for state-of-the-art Code LLMs and code intelligence. Following the principles of modular design and extensible framework, we design CodeTF with a unified interface to enable rapid access and development across different types of models, datasets and tasks. Our library supports a collection of pretrained Code LLM models and popular code benchmarks, including a standardized interface to train and serve code LLMs efficiently, and data features such as language-specific parsers and utility functions for extracting code attributes. In this paper, we describe the design principles, the architecture, key modules and components, and compare with other related library tools. Finally, we hope CodeTF is able to bridge the gap between machine learning/generative AI and software engineering, providing a comprehensive open-source solution for developers, researchers, and practitioners.","authors":["Nghi D. Q. Bui","Hung Le","Yue Wang","Junnan Li","Akhilesh Deepak Gotmare","Steven C. H. Hoi"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2505.01353v2","updated":"2025-12-22T18:27:42Z","published":"2025-05-02T15:43:37Z","title":"Differentiable Nonlinear Model Predictive Control","summary":"The efficient computation of parametric solution sensitivities is a key challenge in the integration of learning-enhanced methods with nonlinear model predictive control (MPC), as their availability is crucial for many learning algorithms. This paper discusses the computation of solution sensitivities of general nonlinear programs (NLPs) using the implicit function theorem (IFT) and smoothed optimality conditions treated in interior-point methods (IPM). We detail sensitivity computation within a sequential quadratic programming (SQP) method which employs an IPM for the quadratic subproblems. Previous works presented in the machine learning community are limited to convex or unconstrained formulations, or lack an implementation for efficient sensitivity evaluation. The publication is accompanied by an efficient open-source implementation within the acados framework, providing both forward and adjoint sensitivities for general optimal control problems, achieving speedups exceeding 3x over the state-of-the-art solvers mpc.pytorch and cvxpygen.","authors":["Jonathan Frey","Katrin Baumgärtner","Gianluca Frison","Dirk Reinhardt","Jasper Hoffmann","Leonard Fichtner","Sebastien Gros","Moritz Diehl"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2506.04049v2","updated":"2025-12-22T18:19:18Z","published":"2025-06-04T15:15:23Z","title":"WANDER: An Explainable Decision-Support Framework for HPC","summary":"High-performance computing (HPC) systems expose many interdependent configuration knobs that impact runtime, resource usage, power, and variability. Existing predictive tools model these outcomes, but do not support structured exploration, explanation, or guided reconfiguration. We present WANDER, a decision-support framework that synthesizes alternate configurations using counterfactual analysis aligned with user goals and constraints. We introduce a composite trade-off score that ranks suggestions based on prediction uncertainty, consistency between feature-target relationships using causal models, and similarity between feature distributions against historical data. To our knowledge, WANDER is the first such system to unify prediction, exploration, and explanation for HPC tuning under a common query interface. Across multiple datasets WANDER generates interpretable and trustworthy, human-readable alternatives that guide users to achieve their performance objectives.","authors":["Ankur Lahiry","Banooqa Banday","Tanzima Z. Islam"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2509.09312v3","updated":"2025-12-22T18:14:54Z","published":"2025-09-11T09:55:50Z","title":"Explaining Tournament Solutions with Minimal Supports","summary":"Tournaments are widely used models to represent pairwise dominance between candidates, alternatives, or teams. We study the problem of providing certified explanations for why a candidate appears among the winners under various tournament rules. To this end, we identify minimal supports, minimal sub-tournaments in which the candidate is guaranteed to win regardless of how the rest of the tournament is completed (that is, the candidate is a necessary winner of the sub-tournament). This notion corresponds to an abductive explanation for the question,\"Why does the winner win the tournament?\", a central concept in formal explainable AI. We focus on common tournament solutions: the top cycle, the uncovered set, the Copeland rule, the Borda rule, the maximin rule, and the weighted uncovered set. For each rule we determine the size of the smallest minimal supports, and we present polynomial-time algorithms to compute them for all solutions except for the weighted uncovered set, for which the problem is NP-complete. Finally, we show how minimal supports can serve to produce compact, certified, and intuitive explanations for tournament solutions.","authors":["Clément Contet","Umberto Grandi","Jérôme Mengin"],"pdf_url":"","comment":"This is the extended version of Contet, Grandi, and Mengin. 2026. Explaining Tournament Solutions with Minimal Supports. In Proceedings of the 40th AAAI Conference on Artificial Intelligence"},{"id":"http://arxiv.org/abs/2512.19620v1","updated":"2025-12-22T17:54:49Z","published":"2025-12-22T17:54:49Z","title":"Exploring the features used for summary evaluation by Human and GPT","summary":"Summary assessment involves evaluating how well a generated summary reflects the key ideas and meaning of the source text, requiring a deep understanding of the content. Large Language Models (LLMs) have been used to automate this process, acting as judges to evaluate summaries with respect to the original text. While previous research investigated the alignment between LLMs and Human responses, it is not yet well understood what properties or features are exploited by them when asked to evaluate based on a particular quality dimension, and there has not been much attention towards mapping between evaluation scores and metrics. In this paper, we address this issue and discover features aligned with Human and Generative Pre-trained Transformers (GPTs) responses by studying statistical and machine learning metrics. Furthermore, we show that instructing GPTs to employ metrics used by Human can improve their judgment and conforming them better with human responses.","authors":["Zahra Sadeghi","Evangelos Milios","Frank Rudzicz"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19609v1","updated":"2025-12-22T17:45:39Z","published":"2025-12-22T17:45:39Z","title":"MapTrace: Scalable Data Generation for Route Tracing on Maps","summary":"While Multimodal Large Language Models have achieved human-like performance on many visual and textual reasoning tasks, their proficiency in fine-grained spatial understanding, such as route tracing on maps remains limited. Unlike humans, who can quickly learn to parse and navigate maps, current models often fail to respect fundamental path constraints, in part due to the prohibitive cost and difficulty of collecting large-scale, pixel-accurate path annotations. To address this, we introduce a scalable synthetic data generation pipeline that leverages synthetic map images and pixel-level parsing to automatically produce precise annotations for this challenging task. Using this pipeline, we construct a fine-tuning dataset of 23k path samples across 4k maps, enabling models to acquire more human-like spatial capabilities. Using this dataset, we fine-tune both open-source and proprietary MLLMs. Results on MapBench show that finetuning substantially improves robustness, raising success rates by up to 6.4 points, while also reducing path-tracing error (NDTW). These gains highlight that fine-grained spatial reasoning, absent in pretrained models, can be explicitly taught with synthetic supervision.","authors":["Artemis Panagopoulou","Aveek Purohit","Achin Kulshrestha","Soroosh Yazdani","Mohit Goyal"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2505.23950v2","updated":"2025-12-22T17:36:54Z","published":"2025-05-29T19:00:42Z","title":"InterMT: Multi-Turn Interleaved Preference Alignment with Human Feedback","summary":"As multimodal large models (MLLMs) continue to advance across challenging tasks, a key question emerges: What essential capabilities are still missing? A critical aspect of human learning is continuous interaction with the environment -- not limited to language, but also involving multimodal understanding and generation. To move closer to human-level intelligence, models must similarly support multi-turn, multimodal interaction. In particular, they should comprehend interleaved multimodal contexts and respond coherently in ongoing exchanges. In this work, we present an initial exploration through the InterMT -- the first preference dataset for multi-turn multimodal interaction, grounded in real human feedback. In this exploration, we particularly emphasize the importance of human oversight, introducing expert annotations to guide the process, motivated by the fact that current MLLMs lack such complex interactive capabilities. InterMT captures human preferences at both global and local levels into nine sub-dimensions, consists of 15.6k prompts, 52.6k multi-turn dialogue instances, and 32.4k human-labeled preference pairs. To compensate for the lack of capability for multi-modal understanding and generation, we introduce an agentic workflow that leverages tool-augmented MLLMs to construct multi-turn QA instances. To further this goal, we introduce InterMT-Bench to assess the ability of MLLMs in assisting judges with multi-turn, multimodal tasks. We demonstrate the utility of \\InterMT through applications such as judge moderation and further reveal the multi-turn scaling law of judge model. We hope the open-source of our data can help facilitate further research on aligning current MLLMs to the next step. Our project website can be found at https://pku-intermt.github.io .","authors":["Boyuan Chen","Donghai Hong","Jiaming Ji","Jiacheng Zheng","Bowen Dong","Jiayi Zhou","Kaile Wang","Juntao Dai","Xuyao Wang","Wenqi Chen","Qirui Zheng","Wenxin Li","Sirui Han","Yike Guo","Yaodong Yang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2507.22782v3","updated":"2025-12-22T17:22:59Z","published":"2025-07-30T15:48:38Z","title":"Enhancing Multi-Agent Collaboration with Attention-Based Actor-Critic Policies","summary":"This paper introduces Team-Attention-Actor-Critic (TAAC), a reinforcement learning algorithm designed to enhance multi-agent collaboration in cooperative environments. TAAC employs a Centralized Training/Centralized Execution scheme incorporating multi-headed attention mechanisms in both the actor and critic. This design facilitates dynamic, inter-agent communication, allowing agents to explicitly query teammates, thereby efficiently managing the exponential growth of joint-action spaces while ensuring a high degree of collaboration. We further introduce a penalized loss function which promotes diverse yet complementary roles among agents. We evaluate TAAC in a simulated soccer environment against benchmark algorithms representing other multi-agent paradigms, including Proximal Policy Optimization and Multi-Agent Actor-Attention-Critic. We find that TAAC exhibits superior performance and enhanced collaborative behaviors across a variety of metrics (win rates, goal differentials, Elo ratings, inter-agent connectivity, balanced spatial distributions, and frequent tactical interactions such as ball possession swaps).","authors":["Hugo Garrido-Lestache Belinchon","Jeremy Kedziora"],"pdf_url":"","comment":"11 pages"},{"id":"http://arxiv.org/abs/2507.07935v6","updated":"2025-12-22T17:01:55Z","published":"2025-07-10T17:16:33Z","title":"Working with AI: Measuring the Applicability of Generative AI to Occupations","summary":"With generative AI emerging as a general-purpose technology, understanding its economic effects is among society's most pressing questions. Existing studies of AI impact have largely relied on predictions of AI capabilities or focused narrowly on individual firms. Drawing instead on real-world AI usage, we analyze a dataset of 200k anonymized conversations with Microsoft Bing Copilot to measure AI applicability to occupations. We use an LLM-based pipeline to classify the O*NET work activities assisted or performed by AI in each conversation. We find that the most common and successful AI-assisted work activities involve information work--the creation, processing, and communication of information. At the occupation level, we find widespread AI applicability cutting across sectors, as most occupations have information work components. Our methodology also allows us to predict which occupations are more likely to delegate tasks to AI and which are more likely to use AI to assist existing workflows.","authors":["Kiran Tomlinson","Sonia Jaffe","Will Wang","Scott Counts","Siddharth Suri"],"pdf_url":"","comment":"40 pages"},{"id":"http://arxiv.org/abs/2512.19576v1","updated":"2025-12-22T17:00:25Z","published":"2025-12-22T17:00:25Z","title":"LeLaR: The First In-Orbit Demonstration of an AI-Based Satellite Attitude Controller","summary":"Attitude control is essential for many satellite missions. Classical controllers, however, are time-consuming to design and sensitive to model uncertainties and variations in operational boundary conditions. Deep Reinforcement Learning (DRL) offers a promising alternative by learning adaptive control strategies through autonomous interaction with a simulation environment. Overcoming the Sim2Real gap, which involves deploying an agent trained in simulation onto the real physical satellite, remains a significant challenge. In this work, we present the first successful in-orbit demonstration of an AI-based attitude controller for inertial pointing maneuvers. The controller was trained entirely in simulation and deployed to the InnoCube 3U nanosatellite, which was developed by the Julius-Maximilians-Universität Würzburg in cooperation with the Technische Universität Berlin, and launched in January 2025. We present the AI agent design, the methodology of the training procedure, the discrepancies between the simulation and the observed behavior of the real satellite, and a comparison of the AI-based attitude controller with the classical PD controller of InnoCube. Steady-state metrics confirm the robust performance of the AI-based controller during repeated in-orbit maneuvers.","authors":["Kirill Djebko","Tom Baumann","Erik Dilger","Frank Puppe","Sergio Montenegro"],"pdf_url":"","comment":"55 pages, 27 figures, 29 tables. The maneuver telemetry datasets generated and analyzed during this work are available in the GitHub repository https://github.com/kdjebko/lelar-in-orbit-data"},{"id":"http://arxiv.org/abs/2512.19570v1","updated":"2025-12-22T16:52:37Z","published":"2025-12-22T16:52:37Z","title":"The Epistemological Consequences of Large Language Models: Rethinking collective intelligence and institutional knowledge","summary":"We examine epistemological threats posed by human and LLM interaction. We develop collective epistemology as a theory of epistemic warrant distributed across human collectives, using bounded rationality and dual process theory as background. We distinguish internalist justification, defined as reflective understanding of why a proposition is true, from externalist justification, defined as reliable transmission of truths. Both are necessary for collective rationality, but only internalist justification produces reflective knowledge. We specify reflective knowledge as follows: agents understand the evaluative basis of a claim, when that basis is unavailable agents consistently assess the reliability of truth sources, and agents have a duty to apply these standards within their domains of competence. We argue that LLMs approximate externalist reliabilism because they can reliably transmit information whose justificatory basis is established elsewhere, but they do not themselves possess reflective justification. Widespread outsourcing of reflective work to reliable LLM outputs can weaken reflective standards of justification, disincentivize comprehension, and reduce agents' capacity to meet professional and civic epistemic duties. To mitigate these risks, we propose a three tier norm program that includes an epistemic interaction model for individual use, institutional and organizational frameworks that seed and enforce norms for epistemically optimal outcomes, and deontic constraints at organizational and or legislative levels that instantiate discursive norms and curb epistemic vices.","authors":["Angjelin Hila"],"pdf_url":"","comment":"AI & Soc (2025)"},{"id":"http://arxiv.org/abs/2512.19569v1","updated":"2025-12-22T16:52:36Z","published":"2025-12-22T16:52:36Z","title":"Owning the Intelligence: Global AI Patents Landscape and Europe's Quest for Technological Sovereignty","summary":"Artificial intelligence has become a key arena of global technological competition and a central concern for Europe's quest for technological sovereignty. This paper analyzes global AI patenting from 2010 to 2023 to assess Europe's position in an increasingly bipolar innovation landscape dominated by the United States and China. Using linked patent, firm, ownership, and citation data, we examine the geography, specialization, and international diffusion of AI innovation. We find a highly concentrated patent landscape: China leads in patent volumes, while the United States dominates in citation impact and technological influence. Europe accounts for a limited share of AI patents but exhibits signals of relatively high patent quality. Technological proximity reveals global convergence toward U.S. innovation trajectories, with Europe remaining fragmented rather than forming an autonomous pole. Gravity-model estimates show that cross-border AI knowledge flows are driven primarily by technological capability and specialization, while geographic and institutional factors play a secondary role. EU membership does not significantly enhance intra-European knowledge diffusion, suggesting that technological capacity, rather than political integration, underpins participation in global AI innovation networks.","authors":["Lapo Santarlasci","Armando Rungi","Loredana Fattorini","Nestor Maslej"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19564v1","updated":"2025-12-22T16:46:40Z","published":"2025-12-22T16:46:40Z","title":"Results of the 2024 CommonRoad Motion Planning Competition for Autonomous Vehicles","summary":"Over the past decade, a wide range of motion planning approaches for autonomous vehicles has been developed to handle increasingly complex traffic scenarios. However, these approaches are rarely compared on standardized benchmarks, limiting the assessment of relative strengths and weaknesses. To address this gap, we present the setup and results of the 4th CommonRoad Motion Planning Competition held in 2024, conducted using the CommonRoad benchmark suite. This annual competition provides an open-source and reproducible framework for benchmarking motion planning algorithms. The benchmark scenarios span highway and urban environments with diverse traffic participants, including passenger cars, buses, and bicycles. Planner performance is evaluated along four dimensions: efficiency, safety, comfort, and compliance with selected traffic rules. This report introduces the competition format and provides a comparison of representative high-performing planners from the 2023 and 2024 editions.","authors":["Yanliang Huang","Xia Yan","Peiran Yin","Zhenduo Zhang","Zeyan Shao","Youran Wang","Haoliang Huang","Matthias Althoff"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19562v1","updated":"2025-12-22T16:44:23Z","published":"2025-12-22T16:44:23Z","title":"REALM: A Real-to-Sim Validated Benchmark for Generalization in Robotic Manipulation","summary":"Vision-Language-Action (VLA) models empower robots to understand and execute tasks described by natural language instructions. However, a key challenge lies in their ability to generalize beyond the specific environments and conditions they were trained on, which is presently difficult and expensive to evaluate in the real-world. To address this gap, we present REALM, a new simulation environment and benchmark designed to evaluate the generalization capabilities of VLA models, with a specific emphasis on establishing a strong correlation between simulated and real-world performance through high-fidelity visuals and aligned robot control. Our environment offers a suite of 15 perturbation factors, 7 manipulation skills, and more than 3,500 objects. Finally, we establish two task sets that form our benchmark and evaluate the π_{0}, π_{0}-FAST, and GR00T N1.5 VLA models, showing that generalization and robustness remain an open challenge. More broadly, we also show that simulation gives us a valuable proxy for the real-world and allows us to systematically probe for and quantify the weaknesses and failure modes of VLAs. Project page: https://martin-sedlacek.com/realm","authors":["Martin Sedlacek","Pavlo Yefanov","Georgy Ponimatkin","Jai Bardhan","Simon Pilc","Mederic Fourmy","Evangelos Kazakos","Cees G. M. Snoek","Josef Sivic","Vladimir Petrik"],"pdf_url":"","comment":"9 pages, 10 figures"},{"id":"http://arxiv.org/abs/2512.19560v1","updated":"2025-12-22T16:42:58Z","published":"2025-12-22T16:42:58Z","title":"BabyFlow: 3D modeling of realistic and expressive infant faces","summary":"Early detection of developmental disorders can be aided by analyzing infant craniofacial morphology, but modeling infant faces is challenging due to limited data and frequent spontaneous expressions. We introduce BabyFlow, a generative AI model that disentangles facial identity and expression, enabling independent control over both. Using normalizing flows, BabyFlow learns flexible, probabilistic representations that capture the complex, non-linear variability of expressive infant faces without restrictive linear assumptions. To address scarce and uncontrolled expressive data, we perform cross-age expression transfer, adapting expressions from adult 3D scans to enrich infant datasets with realistic and systematic expressive variants. As a result, BabyFlow improves 3D reconstruction accuracy, particularly in highly expressive regions such as the mouth, eyes, and nose, and supports synthesis and modification of infant expressions while preserving identity. Additionally, by integrating with diffusion models, BabyFlow generates high-fidelity 2D infant images with consistent 3D geometry, providing powerful tools for data augmentation and early facial analysis.","authors":["Antonia Alomar","Mireia Masias","Marius George Linguraru","Federico M. Sukno","Gemma Piella"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19557v1","updated":"2025-12-22T16:40:14Z","published":"2025-12-22T16:40:14Z","title":"Augmenting Intelligence: A Hybrid Framework for Scalable and Stable Explanations","summary":"Current approaches to Explainable AI (XAI) face a \"Scalability-Stability Dilemma.\" Post-hoc methods (e.g., LIME, SHAP) may scale easily but suffer from instability, while supervised explanation frameworks (e.g., TED) offer stability but require prohibitive human effort to label every training instance. This paper proposes a Hybrid LRR-TED framework that addresses this dilemma through a novel \"Asymmetry of Discovery.\" When applied to customer churn prediction, we demonstrate that automated rule learners (GLRM) excel at identifying broad \"Safety Nets\" (retention patterns) but struggle to capture specific \"Risk Traps\" (churn triggers)-a phenomenon we term the Anna Karenina Principle of Churn. By initialising the explanation matrix with automated safety rules and augmenting it with a Pareto-optimal set of just four human-defined risk rules, our approach achieves 94.00% predictive accuracy. This configuration outperforms the full 8-rule manual expert baseline while reducing human annotation effort by 50%, proposing a shift in the paradigm for Human-in-the-Loop AI: moving experts from the role of \"Rule Writers\" to \"Exception Handlers.\"","authors":["Lawrence Krukrubo","Julius Odede","Olawande Olusegun"],"pdf_url":"","comment":"5 pages, 2 figures, 2 tables. Code and experiments available at https://github.com/Lawrence-Krukrubo/IBM-Learn-XAI"},{"id":"http://arxiv.org/abs/2512.19554v1","updated":"2025-12-22T16:34:21Z","published":"2025-12-22T16:34:21Z","title":"CARE What Fails: Contrastive Anchored-REflection for Verifiable Multimodal","summary":"Group-relative reinforcement learning with verifiable rewards (RLVR) often wastes the most informative data it already has the failures. When all rollouts are wrong, gradients stall; when one happens to be correct, the update usually ignores why the others are close-but-wrong, and credit can be misassigned to spurious chains. We present CARE (Contrastive Anchored REflection), a failure-centric post-training framework for multimodal reasoning that turns errors into supervision. CARE combines: (i) an anchored-contrastive objective that forms a compact subgroup around the best rollout and a set of semantically proximate hard negatives, performs within-subgroup z-score normalization with negative-only scaling, and includes an all-negative rescue to prevent zero-signal batches; and (ii) Reflection-Guided Resampling (RGR), a one-shot structured self-repair that rewrites a representative failure and re-scores it with the same verifier, converting near-misses into usable positives without any test-time reflection. CARE improves accuracy and training smoothness while explicitly increasing the share of learning signal that comes from failures. On Qwen2.5-VL-7B, CARE lifts macro-averaged accuracy by 4.6 points over GRPO across six verifiable visual-reasoning benchmarks; with Qwen3-VL-8B it reaches competitive or state-of-the-art results on MathVista and MMMU-Pro under an identical evaluation protocol.","authors":["Yongxin Wang","Zhicheng Yang","Meng Cao","Mingfei Han","Haokun Lin","Yingying Zhu","Xiaojun Chang","Xiaodan Liang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19551v1","updated":"2025-12-22T16:31:30Z","published":"2025-12-22T16:31:30Z","title":"Towards Closed-Loop Embodied Empathy Evolution: Probing LLM-Centric Lifelong Empathic Motion Generation in Unseen Scenarios","summary":"In the literature, existing human-centric emotional motion generation methods primarily focus on boosting performance within a single scale-fixed dataset, largely neglecting the flexible and scale-increasing motion scenarios (e.g., sports, dance), whereas effectively learning these newly emerging scenarios can significantly enhance the model's real-world generalization ability. Inspired by this, this paper proposes a new LLM-Centric Lifelong Empathic Motion Generation (L^2-EMG) task, which aims to equip LLMs with the capability to continually acquire emotional motion generation knowledge across different unseen scenarios, potentially contributing to building a closed-loop and self-evolving embodied agent equipped with both empathy and intelligence. Further, this paper poses two key challenges in the L^2-EMG task, i.e., the emotion decoupling challenge and the scenario adapting challenge. To this end, this paper proposes an Emotion-Transferable and Scenario-Adapted Mixture of Experts (ES-MoE) approach which designs a causal-guided emotion decoupling block and a scenario-adapted expert constructing block to address the two challenges, respectively. Especially, this paper constructs multiple L^2-EMG datasets to validate the effectiveness of the ES-MoE approach. Extensive evaluations show that ES-MoE outperforms advanced baselines.","authors":["Jiawen Wang","Jingjing Wang Tianyang Chen","Min Zhang","Guodong Zhou"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2409.20302v6","updated":"2025-12-22T16:29:48Z","published":"2024-09-30T14:00:04Z","title":"OM4OV: Leveraging Ontology Matching for Ontology Versioning","summary":"Due to the dynamic nature of the Semantic Web, version control is necessary to capture time-varying information for widely used ontologies. Despite the long-standing recognition of ontology versioning (OV) as a crucial component of efficient ontology management, many approaches treat OV as similar to ontology matching (OM) and directly reuse OM systems for OV tasks. In this study, we systematically analyse the similarities and differences between OM and OV and formalise the OM4OV pipeline. The pipeline is implemented and evaluated in the state-of-the-art OM system Agent-OM. The experimental results indicate that OM systems can be reused for OV tasks, but without necessary extensions, the current OM4OV pipeline can produce skewed measurements, poor performance in detecting update entities, and limited explainability for false mappings. To tackle these issues, we propose an optimisation method called the cross-reference (CR) mechanism, building upon the existing alignments from OM to reduce the number of matching candidates and improve overall OV performance.","authors":["Zhangcheng Qiang","Kerry Taylor","Weiqing Wang"],"pdf_url":"","comment":"16 pages, 8 figures, 1 table"},{"id":"http://arxiv.org/abs/2410.24116v2","updated":"2025-12-22T16:25:04Z","published":"2024-10-31T16:46:23Z","title":"AIDOVECL: AI-generated Dataset of Outpainted Vehicles for Eye-level Classification and Localization","summary":"Image labeling is a critical bottleneck in the development of computer vision technologies, often constraining the potential of machine learning models due to the time-intensive nature of manual annotations. This work introduces a novel approach that leverages outpainting to mitigate the problem of annotated data scarcity by generating artificial contexts and annotations, significantly reducing manual labeling efforts. We apply this technique to a particularly acute challenge in autonomous driving, urban planning, and environmental monitoring: the lack of diverse, eye-level vehicle images in desired classes. Our dataset comprises AI-generated vehicle images obtained by detecting and cropping vehicles from manually selected seed images, which are then outpainted onto larger canvases to simulate varied real-world conditions. The outpainted images include detailed annotations, providing high-quality ground truth data. Advanced outpainting techniques and image quality assessments ensure visual fidelity and contextual relevance. Ablation results show that incorporating AIDOVECL improves overall detection performance by up to 10%, and delivers gains of up to 40% in settings with greater diversity of context, object scale, and placement, with underrepresented classes achieving up to 50% higher true positives. AIDOVECL enhances vehicle detection by augmenting real training data and supporting evaluation across diverse scenarios. By demonstrating outpainting as an automatic annotation paradigm, it offers a practical and versatile solution for building fine-grained datasets with reduced labeling effort across multiple machine learning domains. The code and links to datasets used in this study are available for further research and replication at https://github.com/amir-kazemi/aidovecl .","authors":["Amir Kazemi","Qurat ul ain Fatima","Volodymyr Kindratenko","Christopher Tessum"],"pdf_url":"","comment":"34 pages, 10 figures, 5 tables"},{"id":"http://arxiv.org/abs/2512.19535v1","updated":"2025-12-22T16:21:39Z","published":"2025-12-22T16:21:39Z","title":"CASA: Cross-Attention via Self-Attention for Efficient Vision-Language Fusion","summary":"Vision-language models (VLMs) are commonly trained by inserting image tokens from a pretrained vision encoder into the textual stream of a language model. This allows text and image information to fully attend to one another within the model, but becomes extremely costly for high-resolution images, long conversations, or streaming videos, both in memory and compute. VLMs leveraging cross-attention are an efficient alternative to token insertion but exhibit a clear performance gap, in particular on tasks involving fine-grained visual details. We find that a key to improving such models is to also enable local text-to-text interaction in the dedicated cross-attention layers. Building on this, we propose CASA, Cross-Attention via Self-Attention, a simple and efficient paradigm which substantially reduces the gap with full token insertion on common image understanding benchmarks, while enjoying the same scalability as cross-attention models when applied to long-context multimodal tasks such as streaming video captioning. For samples and code, please see our project page at https://kyutai.org/casa .","authors":["Moritz Böhle","Amélie Royer","Juliette Marrie","Edouard Grave","Patrick Pérez"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19530v1","updated":"2025-12-22T16:19:01Z","published":"2025-12-22T16:19:01Z","title":"Learning Continuous Solvent Effects from Transient Flow Data: A Graph Neural Network Benchmark on Catechol Rearrangement","summary":"Predicting reaction outcomes across continuous solvent composition ranges remains a critical challenge in organic synthesis and process chemistry. Traditional machine learning approaches often treat solvent identity as a discrete categorical variable, which prevents systematic interpolation and extrapolation across the solvent space. This work introduces the \\textbf{Catechol Benchmark}, a high-throughput transient flow chemistry dataset comprising 1,227 experimental yield measurements for the rearrangement of allyl-substituted catechol in 24 pure solvents and their binary mixtures, parameterized by continuous volume fractions ($\\% B$). We evaluate various architectures under rigorous leave-one-solvent-out and leave-one-mixture-out protocols to test generalization to unseen chemical environments.\n  Our results demonstrate that classical tabular methods (e.g., Gradient-Boosted Decision Trees) and large language model embeddings (e.g., Qwen-7B) struggle with quantitative precision, yielding Mean Squared Errors (MSE) of 0.099 and 0.129, respectively. In contrast, we propose a hybrid GNN-based architecture that integrates Graph Attention Networks (GATs) with Differential Reaction Fingerprints (DRFP) and learned mixture-aware solvent encodings. This approach achieves an \\textbf{MSE of 0.0039} ($\\pm$ 0.0003), representing a 60\\% error reduction over competitive baselines and a $>25\\times$ improvement over tabular ensembles. Ablation studies confirm that explicit molecular graph message-passing and continuous mixture encoding are essential for robust generalization. The complete dataset, evaluation protocols, and reference implementations are released to facilitate data-efficient reaction prediction and continuous solvent representation learning.","authors":["Hongsheng Xing","Qiuxin Si"],"pdf_url":"","comment":"13 pages, 6 figures"},{"id":"http://arxiv.org/abs/2512.19526v1","updated":"2025-12-22T16:18:00Z","published":"2025-12-22T16:18:00Z","title":"QuantiPhy: A Quantitative Benchmark Evaluating Physical Reasoning Abilities of Vision-Language Models","summary":"Understanding the physical world is essential for generalist AI agents. However, it remains unclear whether state-of-the-art vision perception models (e.g., large VLMs) can reason physical properties quantitatively. Existing evaluations are predominantly VQA-based and qualitative, offering limited insight into whether these models can infer the kinematic quantities of moving objects from video observations. To address this, we present QuantiPhy, the first benchmark designed to quantitatively measure a VLM's physical reasoning ability. Comprising more than 3.3K video-text instances with numerical ground truth, QuantiPhy evaluates a VLM's performance on estimating an object's size, velocity, and acceleration at a given timestamp, using one of these properties as an input prior. The benchmark standardizes prompts and scoring to assess numerical accuracy, enabling fair comparisons across models. Our experiments on state-of-the-art VLMs reveal a consistent gap between their qualitative plausibility and actual numerical correctness. We further provide an in-depth analysis of key factors like background noise, counterfactual priors, and strategic prompting and find that state-of-the-art VLMs lean heavily on pre-trained world knowledge rather than faithfully using the provided visual and textual inputs as references when reasoning kinematic properties quantitatively. QuantiPhy offers the first rigorous, scalable testbed to move VLMs beyond mere verbal plausibility toward a numerically grounded physical understanding.","authors":["Li Puyin","Tiange Xiang","Ella Mao","Shirley Wei","Xinye Chen","Adnan Masood","Li Fei-fei","Ehsan Adeli"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19516v1","updated":"2025-12-22T16:08:03Z","published":"2025-12-22T16:08:03Z","title":"LacaDM: A Latent Causal Diffusion Model for Multiobjective Reinforcement Learning","summary":"Multiobjective reinforcement learning (MORL) poses significant challenges due to the inherent conflicts between objectives and the difficulty of adapting to dynamic environments. Traditional methods often struggle to generalize effectively, particularly in large and complex state-action spaces. To address these limitations, we introduce the Latent Causal Diffusion Model (LacaDM), a novel approach designed to enhance the adaptability of MORL in discrete and continuous environments. Unlike existing methods that primarily address conflicts between objectives, LacaDM learns latent temporal causal relationships between environmental states and policies, enabling efficient knowledge transfer across diverse MORL scenarios. By embedding these causal structures within a diffusion model-based framework, LacaDM achieves a balance between conflicting objectives while maintaining strong generalization capabilities in previously unseen environments. Empirical evaluations on various tasks from the MOGymnasium framework demonstrate that LacaDM consistently outperforms the state-of-art baselines in terms of hypervolume, sparsity, and expected utility maximization, showcasing its effectiveness in complex multiobjective tasks.","authors":["Xueming Yan","Bo Yin","Yaochu Jin"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19512v1","updated":"2025-12-22T16:06:36Z","published":"2025-12-22T16:06:36Z","title":"Anatomy-R1: Enhancing Anatomy Reasoning in Multimodal Large Language Models via Anatomical Similarity Curriculum and Group Diversity Augmentation","summary":"Multimodal Large Language Models (MLLMs) have achieved impressive progress in natural image reasoning, yet their potential in medical imaging remains underexplored, especially in clinical anatomical surgical images. Anatomy understanding tasks demand precise understanding and clinically coherent answers, which are difficult to achieve due to the complexity of medical data and the scarcity of high-quality expert annotations. These challenges limit the effectiveness of conventional Supervised Fine-Tuning (SFT) strategies. While recent work has demonstrated that Group Relative Policy Optimization (GRPO) can enhance reasoning in MLLMs without relying on large amounts of data, we find two weaknesses that hinder GRPO's reasoning performance in anatomy recognition: 1) knowledge cannot be effectively shared between different anatomical structures, resulting in uneven information gain and preventing the model from converging, and 2) the model quickly converges to a single reasoning path, suppressing the exploration of diverse strategies. To overcome these challenges, we propose two novel methods. First, we implement a progressive learning strategy called Anatomical Similarity Curriculum Learning by controlling question difficulty via the similarity of answer choices, enabling the model to master complex problems incrementally. Second, we utilize question augmentation referred to as Group Diversity Question Augmentation to expand the model's search space for difficult queries, mitigating the tendency to produce uniform responses. Comprehensive experiments on the SGG-VQA and OmniMedVQA benchmarks show our method achieves a significant improvement across the two benchmarks, demonstrating its effectiveness in enhancing the medical reasoning capabilities of MLLMs. The code can be found in https://github.com/tomato996/Anatomy-R1","authors":["Ziyang Song","Zelin Zang","Zuyao Chen","Xusheng Liang","Dong Yi","Jinlin Wu","Hongbin Liu","Jiebo Luo"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19506v1","updated":"2025-12-22T16:00:55Z","published":"2025-12-22T16:00:55Z","title":"DK-STN: A Domain Knowledge Embedded Spatio-Temporal Network Model for MJO Forecast","summary":"Understanding and predicting the Madden-Julian Oscillation (MJO) is fundamental for precipitation forecasting and disaster prevention. To date, long-term and accurate MJO prediction has remained a challenge for researchers. Conventional MJO prediction methods using Numerical Weather Prediction (NWP) are resource-intensive, time-consuming, and highly unstable (most NWP methods are sensitive to seasons, with better MJO forecast results in winter). While existing Artificial Neural Network (ANN) methods save resources and speed forecasting, their accuracy never reaches the 28 days predicted by the state-of-the-art NWP method, i.e., the operational forecasts from ECMWF, since neural networks cannot handle climate data effectively. In this paper, we present a Domain Knowledge Embedded Spatio-Temporal Network (DK-STN), a stable neural network model for accurate and efficient MJO forecasting. It combines the benefits of NWP and ANN methods and successfully improves the forecast accuracy of ANN methods while maintaining a high level of efficiency and stability. We begin with a spatial-temporal network (STN) and embed domain knowledge in it using two key methods: (i) applying a domain knowledge enhancement method and (ii) integrating a domain knowledge processing method into network training. We evaluated DK-STN with the 5th generation of ECMWF reanalysis (ERA5) data and compared it with ECMWF. Given 7 days of climate data as input, DK-STN can generate reliable forecasts for the following 28 days in 1-2 seconds, with an error of only 2-3 days in different seasons. DK-STN significantly exceeds ECMWF in that its forecast accuracy is equivalent to ECMWF's, while its efficiency and stability are significantly superior.","authors":["Hongliang Li","Nong Zhang","Zhewen Xu","Xiang Li","Changzheng Liu","Chongbo Zhao","Jie Wu"],"pdf_url":"","comment":"18 pages, 10 figures"},{"id":"http://arxiv.org/abs/2505.20063v2","updated":"2025-12-22T15:49:59Z","published":"2025-05-26T14:47:59Z","title":"SAEs Are Good for Steering -- If You Select the Right Features","summary":"Sparse Autoencoders (SAEs) have been proposed as an unsupervised approach to learn a decomposition of a model's latent space. This enables useful applications such as steering - influencing the output of a model towards a desired concept - without requiring labeled data. Current methods identify SAE features to steer by analyzing the input tokens that activate them. However, recent work has highlighted that activations alone do not fully describe the effect of a feature on the model's output. In this work, we draw a distinction between two types of features: input features, which mainly capture patterns in the model's input, and output features, which have a human-understandable effect on the model's output. We propose input and output scores to characterize and locate these types of features, and show that high values for both scores rarely co-occur in the same features. These findings have practical implications: after filtering out features with low output scores, we obtain 2-3x improvements when steering with SAEs, making them competitive with supervised methods.","authors":["Dana Arad","Aaron Mueller","Yonatan Belinkov"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19494v1","updated":"2025-12-22T15:49:24Z","published":"2025-12-22T15:49:24Z","title":"Kolmogorov-Arnold Graph Neural Networks Applied to Inorganic Nanomaterials Dataset","summary":"The recent development of Kolmogorov-Arnold Networks (KANs) introduced new discoveries in the field of Graph Neural Networks (GNNs), expanding the existing set of models with KAN-based versions of GNNs, which often surpass the accuracy of MultiLayer Perceptron (MLP)-based GNNs. These models were widely tested on the graph datasets consisting of organic molecules; however, those studies disregarded the inorganic nanomaterials datasets. In this work, we close this gap by applying Kolmogorov-Arnold Graph Neural Networks (KAGNNs) to a recently published large inorganic nanomaterials dataset called CHILI. For this, we adapt and test KAGNNs appropriate for this dataset. Our experiments reveal that on the CHILI datasets, particularly on the CHILI-3K, KAGNNs substantially surpass conventional GNNs in classification, achieving state-of-the-art results.","authors":["Nikita Volzhin","Soowhan Yoon"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2505.15925v3","updated":"2025-12-22T15:37:49Z","published":"2025-05-21T18:24:36Z","title":"VERDI: VLM-Embedded Reasoning for Autonomous Driving","summary":"While autonomous driving (AD) stacks struggle with decision making under partial observability and real-world complexity, human drivers are capable of commonsense reasoning to make near-optimal decisions with limited information. Recent work has attempted to leverage finetuned Vision-Language Models (VLMs) for trajectory planning at inference time to emulate human behavior. Despite their success in benchmark evaluations, these methods are often impractical to deploy (a 70B parameter VLM inference at merely 8 tokens per second requires more than 160G of memory), and their monolithic network structure prohibits safety decomposition. To bridge this gap, we propose VLM-Embedded Reasoning for autonomous Driving (VERDI), a training-time framework that distills the reasoning process and commonsense knowledge of VLMs into the AD stack. VERDI augments modular differentiable end-to-end (e2e) AD models by aligning intermediate module outputs at the perception, prediction, and planning stages with text features explaining the driving reasoning process produced by VLMs. By encouraging alignment in latent space, VERDI enables the modular AD stack to internalize structured reasoning, without incurring the inference-time costs of large VLMs. We validate VERDI in both open-loop (NuScenes and Bench2Drive benchmarks) and closed-loop (HugSim Simulator) settings. We find that VERDI outperforms existing e2e methods that do not embed reasoning by up to 11% in $\\ell_{2}$ distance and 11% in driving performance, while maintaining real-time inference speed.","authors":["Bowen Feng","Zhiting Mei","Baiang Li","Julian Ost","Filippo Ghilotti","Roger Girgis","Anirudha Majumdar","Felix Heide"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19481v1","updated":"2025-12-22T15:32:45Z","published":"2025-12-22T15:32:45Z","title":"A Dataset and Preliminary Study of Using GPT-5 for Code-change Impact Analysis","summary":"Understanding source code changes and their impact on other code entities is a crucial skill in software development. However, the analysis of code changes and their impact is often performed manually and therefore is time-consuming. Recent advancements in AI, and in particular large language models (LLMs) show promises to help developers in various code analysis tasks. However, the extent to which this potential can be utilized for understanding code changes and their impact is underexplored. To address this gap, we study the capabilities of GPT-5 and GPT-5-mini to predict the code entities impacted by given source code changes. We construct a dataset containing information about seed-changes, change pairs, and change types for each commit. Existing datasets lack crucial information about seed changes and impacted code entities. Our experiments evaluate the LLMs in two configurations: (1) seed-change information and the parent commit tree and (2) seed-change information, the parent commit tree, and the diff hunk of each seed change. We found that both LLMs perform poorly in the two experiments, whereas GPT-5 outperforms GPT-5-mini. Furthermore, the provision of the diff hunks helps both models to slightly improve their performance.","authors":["Katharina Stengg","Christian Macho","Martin Pinzger"],"pdf_url":"","comment":"6 pages"},{"id":"http://arxiv.org/abs/2512.19472v1","updated":"2025-12-22T15:25:10Z","published":"2025-12-22T15:25:10Z","title":"Multi-Layer Confidence Scoring for Detection of Out-of-Distribution Samples, Adversarial Attacks, and In-Distribution Misclassifications","summary":"The recent explosive growth in Deep Neural Networks applications raises concerns about the black-box usage of such models, with limited trasparency and trustworthiness in high-stakes domains, which have been crystallized as regulatory requirements such as the European Union Artificial Intelligence Act. While models with embedded confidence metrics have been proposed, such approaches cannot be applied to already existing models without retraining, limiting their broad application. On the other hand, post-hoc methods, which evaluate pre-trained models, focus on solving problems related to improving the confidence in the model's predictions, and detecting Out-Of-Distribution or Adversarial Attacks samples as independent applications. To tackle the limited applicability of already existing methods, we introduce Multi-Layer Analysis for Confidence Scoring (MACS), a unified post-hoc framework that analyzes intermediate activations to produce classification-maps. From the classification-maps, we derive a score applicable for confidence estimation, detecting distributional shifts and adversarial attacks, unifying the three problems in a common framework, and achieving performances that surpass the state-of-the-art approaches in our experiments with the VGG16 and ViTb16 models with a fraction of their computational overhead.","authors":["Lorenzo Capelli","Leandro de Souza Rosa","Gianluca Setti","Mauro Mangia","Riccardo Rovatti"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2510.16416v2","updated":"2025-12-22T15:14:59Z","published":"2025-10-18T09:22:40Z","title":"SSL4RL: Revisiting Self-supervised Learning as Intrinsic Reward for Visual-Language Reasoning","summary":"Vision-language models (VLMs) have shown remarkable abilities by integrating large language models with visual inputs. However, they often fail to utilize visual evidence adequately, either depending on linguistic priors in vision-centric tasks or resorting to textual shortcuts during reasoning. Although reinforcement learning (RL) can align models with desired behaviors, its application to VLMs has been hindered by the lack of scalable and reliable reward mechanisms. To overcome this challenge, we propose SSL4RL, a novel framework that leverages self-supervised learning (SSL) tasks as a source of verifiable rewards for RL-based fine-tuning. Our approach reformulates SSL objectives-such as predicting image rotation or reconstructing masked patches-into dense, automatic reward signals, eliminating the need for human preference data or unreliable AI evaluators. Experiments show that SSL4RL substantially improves performance on both vision-centric and vision-language reasoning benchmarks. Furthermore, through systematic ablations, we identify key factors-such as task difficulty, model scale, and semantic alignment with the target domain-that influence the effectiveness of SSL4RL tasks, offering new design principles for future work. We also demonstrate the framework's generality by applying it to graph learning, where it yields significant gains. SSL4RL establishes a versatile and effective paradigm for aligning multimodal models using verifiable, self-supervised objectives.","authors":["Xiaojun Guo","Runyu Zhou","Yifei Wang","Qi Zhang","Chenheng Zhang","Stefanie Jegelka","Xiaohan Wang","Jiajun Chai","Guojun Yin","Wei Lin","Yisen Wang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19458v1","updated":"2025-12-22T15:03:57Z","published":"2025-12-22T15:03:57Z","title":"An Agentic Framework for Autonomous Materials Computation","summary":"Large Language Models (LLMs) have emerged as powerful tools for accelerating scientific discovery, yet their static knowledge and hallucination issues hinder autonomous research applications. Recent advances integrate LLMs into agentic frameworks, enabling retrieval, reasoning, and tool use for complex scientific workflows. Here, we present a domain-specialized agent designed for reliable automation of first-principles materials computations. By embedding domain expertise, the agent ensures physically coherent multi-step workflows and consistently selects convergent, well-posed parameters, thereby enabling reliable end-to-end computational execution. A new benchmark of diverse computational tasks demonstrates that our system significantly outperforms standalone LLMs in both accuracy and robustness. This work establishes a verifiable foundation for autonomous computational experimentation and represents a key step toward fully automated scientific discovery.","authors":["Zeyu Xia","Jinzhe Ma","Congjie Zheng","Shufei Zhang","Yuqiang Li","Hang Su","P. Hu","Changshui Zhang","Xingao Gong","Wanli Ouyang","Lei Bai","Dongzhan Zhou","Mao Su"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19456v1","updated":"2025-12-22T15:01:07Z","published":"2025-12-22T15:01:07Z","title":"Activations as Features: Probing LLMs for Generalizable Essay Scoring Representations","summary":"Automated essay scoring (AES) is a challenging task in cross-prompt settings due to the diversity of scoring criteria. While previous studies have focused on the output of large language models (LLMs) to improve scoring accuracy, we believe activations from intermediate layers may also provide valuable information. To explore this possibility, we evaluated the discriminative power of LLMs' activations in cross-prompt essay scoring task. Specifically, we used activations to fit probes and further analyzed the effects of different models and input content of LLMs on this discriminative power. By computing the directions of essays across various trait dimensions under different prompts, we analyzed the variation in evaluation perspectives of large language models concerning essay types and traits. Results show that the activations possess strong discriminative power in evaluating essay quality and that LLMs can adapt their evaluation perspectives to different traits and essay types, effectively handling the diversity of scoring criteria in cross-prompt settings.","authors":["Jinwei Chi","Ke Wang","Yu Chen","Xuanye Lin","Qiang Xu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2408.10566v5","updated":"2025-12-22T14:46:47Z","published":"2024-08-20T06:05:52Z","title":"Overcoming Growth-Induced Forgetting in Task-Agnostic Continual Learning","summary":"In continual learning (CL), model growth enhances adaptability to new data. However, when model growth is applied improperly, especially in task-agnostic CL, where the entire grown model is used for inference, it can lead to severe degradation of learned knowledge, a problem we term growth-induced forgetting. Most existing methods that adopt model growth to improve adaptability often overlook the forgetting issue, resulting in compromised knowledge retention, making them unsuitable for task-agnostic settings. To promote both adaptability and knowledge retention with model growth, we identify the key: gradient and parameter sparsity. Introducing SparseGrow, which increases gradient sparsity through layer expansion and gradient gating to enable focused updates on parameters while preserving critical parameters, thus inhibiting forgetting. Moreover, it promotes parameter sparsity with sparse initialization and training, aiming at better control of model plasticity, improving adaptability over new data. Extensive experiments across diverse datasets, task-agnostic settings, and a large number of tasks demonstrate the necessity of controlled layer expansion and validate the effectiveness of SparseGrow in achieving high adaptability while minimizing forgetting in continual learning. By enabling model growth with sparsified gradients and parameters, SparseGrow paves the way for building scalable lifelong learning systems capable of continual adaptation with better knowledge retention.","authors":["Yuqing Zhao","Jiannong Cao","Divya Saxena","Xiaoyun Liu","Changlin Song","Bo Yuan","Julie McCann"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2511.14396v4","updated":"2025-12-22T14:45:53Z","published":"2025-11-18T12:01:06Z","title":"Continuous Vision-Language-Action Co-Learning with Semantic-Physical Alignment for Behavioral Cloning","summary":"Language-conditioned manipulation facilitates human-robot interaction via behavioral cloning (BC), which learns control policies from human demonstrations and serves as a cornerstone of embodied AI. Overcoming compounding errors in sequential action decisions remains a central challenge to improving BC performance. Existing approaches mitigate compounding errors through data augmentation, expressive representation, or temporal abstraction. However, they suffer from physical discontinuities and semantic-physical misalignment, leading to inaccurate action cloning and intermittent execution. In this paper, we present Continuous vision-language-action Co-Learning with Semantic-Physical Alignment (CCoL), a novel BC framework that ensures temporally consistent execution and fine-grained semantic grounding. It generates robust and smooth action execution trajectories through continuous co-learning across vision, language, and proprioceptive inputs (e.g., robot internal states). Meanwhile, we anchor language semantics to visuomotor representations by a bidirectional cross-attention to learn contextual information for action generation, successfully overcoming the problem of semantic-physical misalignment. Extensive experiments show that CCoL achieves an average 8.0% relative improvement across three simulation suites, with up to 19.2% relative gain in human-demonstrated bimanual insertion tasks. Real-world tests on a 7-DoF robot further confirm CCoL's generalization under unseen and noisy object states.","authors":["Xiuxiu Qi","Yu Yang","Jiannong Cao","Luyao Bai","Chongshan Fan","Chengtai Cao","Hongpeng Wang"],"pdf_url":"","comment":"Accepted at AAAI 2026, the Project website is available at https://qhemu.github.io/CCoL/"},{"id":"http://arxiv.org/abs/2512.19438v1","updated":"2025-12-22T14:36:08Z","published":"2025-12-22T14:36:08Z","title":"MT-Mark: Rethinking Image Watermarking via Mutual-Teacher Collaboration with Adaptive Feature Modulation","summary":"Existing deep image watermarking methods follow a fixed embedding-distortion-extraction pipeline, where the embedder and extractor are weakly coupled through a final loss and optimized in isolation. This design lacks explicit collaboration, leaving no structured mechanism for the embedder to incorporate decoding-aware cues or for the extractor to guide embedding during training. To address this architectural limitation, we rethink deep image watermarking by reformulating embedding and extraction as explicitly collaborative components. To realize this reformulation, we introduce a Collaborative Interaction Mechanism (CIM) that establishes direct, bidirectional communication between the embedder and extractor, enabling a mutual-teacher training paradigm and coordinated optimization. Built upon this explicitly collaborative architecture, we further propose an Adaptive Feature Modulation Module (AFMM) to support effective interaction. AFMM enables content-aware feature regulation by decoupling modulation structure and strength, guiding watermark embedding toward stable image features while suppressing host interference during extraction. Under CIM, the AFMMs on both sides form a closed-loop collaboration that aligns embedding behavior with extraction objectives. This architecture-level redesign changes how robustness is learned in watermarking systems. Rather than relying on exhaustive distortion simulation, robustness emerges from coordinated representation learning between embedding and extraction. Experiments on real-world and AI-generated datasets demonstrate that the proposed method consistently outperforms state-of-the-art approaches in watermark extraction accuracy while maintaining high perceptual quality, showing strong robustness and generalization.","authors":["Fei Ge","Ying Huang","Jie Liu","Guixuan Zhang","Zhi Zeng","Shuwu Zhang","Hu Guan"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2505.18822v2","updated":"2025-12-22T14:30:53Z","published":"2025-05-24T18:46:50Z","title":"AdaCtrl: Towards Adaptive and Controllable Reasoning via Difficulty-Aware Budgeting","summary":"Modern large reasoning models demonstrate impressive problem-solving capabilities by employing sophisticated reasoning strategies. However, they often struggle to balance efficiency and effectiveness, frequently generating unnecessarily lengthy reasoning chains for simple problems. In this work, we propose AdaCtrl, a novel framework to support both difficulty-aware adaptive reasoning budget allocation and explicit user control over reasoning depth. AdaCtrl dynamically adjusts its reasoning length based on self-assessed problem difficulty, while also allowing users to manually control the budget to prioritize either efficiency or effectiveness. This is achieved through a two-stage training pipeline: an initial cold-start fine-tuning phase to instill the ability to self-aware difficulty and adjust reasoning budget, followed by a difficulty-aware reinforcement learning (RL) stage that refines the model's adaptive reasoning strategies and calibrates its difficulty assessments based on its evolving capabilities during online training. To enable intuitive user interaction, we design explicit length-triggered tags that function as a natural interface for budget control. Empirical results show that AdaCtrl adapts reasoning length based on estimated difficulty, compared to the standard training baseline that also incorporates fine-tuning and RL, it yields performance improvements and simultaneously reduces response length by 10.06% and 12.14% on the more challenging AIME2024 and AIME2025 datasets, which require elaborate reasoning, and by 62.05% and 91.04% on the MATH500 and GSM8K datasets, where more concise responses are sufficient. Furthermore, AdaCtrl enables precise user control over the reasoning budget, allowing for tailored responses to meet specific needs.","authors":["Shijue Huang","Hongru Wang","Wanjun Zhong","Zhaochen Su","Jiazhan Feng","Bowen Cao","Yi R. Fung"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19428v1","updated":"2025-12-22T14:29:18Z","published":"2025-12-22T14:29:18Z","title":"Attention Is Not What You Need","summary":"We revisit a basic question in sequence modeling: is explicit self-attention actually necessary for strong performance and reasoning? We argue that standard multi-head attention is best seen as a form of tensor lifting: hidden vectors are mapped into a high-dimensional space of pairwise interactions, and learning proceeds by constraining this lifted tensor through gradient descent. This mechanism is extremely expressive but mathematically opaque, because after many layers it becomes very hard to describe the model with a small family of explicit invariants.\n  To explore an alternative, we propose an attention-free architecture based on Grassmann flows. Instead of forming an L by L attention matrix, our Causal Grassmann layer (i) linearly reduces token states, (ii) encodes local token pairs as two-dimensional subspaces on a Grassmann manifold via Plucker coordinates, and (iii) fuses these geometric features back into the hidden states through gated mixing. Information therefore propagates by controlled deformations of low-rank subspaces over multi-scale local windows, so the core computation lives on a finite-dimensional manifold rather than in an unstructured tensor space.\n  On the Wikitext-2 language modeling benchmark, purely Grassmann-based models with 13 to 18 million parameters achieve validation perplexities within about 10 to 15 percent of size-matched Transformers. On the SNLI natural language inference task, a Grassmann-Plucker head on top of DistilBERT slightly outperforms a Transformer head, with best validation and test accuracies of 0.8550 and 0.8538 compared to 0.8545 and 0.8511. We analyze the complexity of Grassmann mixing, show linear scaling in sequence length for fixed rank, and argue that such manifold-based designs offer a more structured route toward geometric and invariant-based interpretations of neural reasoning.","authors":["Zhang Chong"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.15735v2","updated":"2025-12-22T14:25:02Z","published":"2025-12-05T22:52:22Z","title":"Deep Reinforcement Learning Optimization for Uncertain Nonlinear Systems via Event-Triggered Robust Adaptive Dynamic Programming","summary":"This work proposes a unified control architecture that couples a Reinforcement Learning (RL)-driven controller with a disturbance-rejection Extended State Observer (ESO), complemented by an Event-Triggered Mechanism (ETM) to limit unnecessary computations. The ESO is utilized to estimate the system states and the lumped disturbance in real time, forming the foundation for effective disturbance compensation. To obtain near-optimal behavior without an accurate system description, a value-iteration-based Adaptive Dynamic Programming (ADP) method is adopted for policy approximation. The inclusion of the ETM ensures that parameter updates of the learning module are executed only when the state deviation surpasses a predefined bound, thereby preventing excessive learning activity and substantially reducing computational load. A Lyapunov-oriented analysis is used to characterize the stability properties of the resulting closed-loop system. Numerical experiments further confirm that the developed approach maintains strong control performance and disturbance tolerance, while achieving a significant reduction in sampling and processing effort compared with standard time-triggered ADP schemes.","authors":["Ningwei Bai","Chi Pui Chan","Qichen Yin","Tengyang Gong","Yunda Yan","Zezhi Tang"],"pdf_url":"","comment":"we have identified some technical issues, including the mathematical derivation. After discussion, all authors have agreed that the analysis requires a thorough re-derivation to ensure correctness and rigor"},{"id":"http://arxiv.org/abs/2512.19410v1","updated":"2025-12-22T14:05:31Z","published":"2025-12-22T14:05:31Z","title":"Research Program: Theory of Learning in Dynamical Systems","summary":"Modern learning systems increasingly interact with data that evolve over time and depend on hidden internal state. We ask a basic question: when is such a dynamical system learnable from observations alone? This paper proposes a research program for understanding learnability in dynamical systems through the lens of next-token prediction. We argue that learnability in dynamical systems should be studied as a finite-sample question, and be based on the properties of the underlying dynamics rather than the statistical properties of the resulting sequence. To this end, we give a formulation of learnability for stochastic processes induced by dynamical systems, focusing on guarantees that hold uniformly at every time step after a finite burn-in period. This leads to a notion of dynamic learnability which captures how the structure of a system, such as stability, mixing, observability, and spectral properties, governs the number of observations required before reliable prediction becomes possible. We illustrate the framework in the case of linear dynamical systems, showing that accurate prediction can be achieved after finite observation without system identification, by leveraging improper methods based on spectral filtering. We survey the relationship between learning in dynamical systems and classical PAC, online, and universal prediction theories, and suggest directions for studying nonlinear and controlled systems.","authors":["Elad Hazan","Shai Shalev Shwartz","Nathan Srebro"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2412.14031v5","updated":"2025-12-22T13:49:48Z","published":"2024-12-18T16:51:47Z","title":"A Riemannian Optimization Perspective of the Gauss-Newton Method for Feedforward Neural Networks","summary":"In this work, we establish non-asymptotic convergence bounds for the Gauss-Newton method in training neural networks with smooth activations. In the underparameterized regime, the Gauss-Newton gradient flow in parameter space induces a Riemannian gradient flow on a low-dimensional embedded submanifold of the function space. Using tools from Riemannian optimization, we establish geodesic Polyak-Lojasiewicz and Lipschitz-smoothness conditions for the loss under appropriately chosen output scaling, yielding geometric convergence to the optimal in-class predictor at an explicit rate independent of the conditioning of the Gram matrix. In the overparameterized regime, we propose adaptive, curvature-aware regularization schedules that ensure fast geometric convergence to a global optimum at a rate independent of the minimum eigenvalue of the neural tangent kernel and, locally, of the modulus of strong convexity of the loss. These results demonstrate that Gauss-Newton achieves accelerated convergence rates in settings where first-order methods exhibit slow convergence due to ill-conditioned kernel matrices and loss landscapes.","authors":["Semih Cayci"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2501.10190v2","updated":"2025-12-22T13:46:47Z","published":"2025-01-17T13:37:58Z","title":"Temporal Causal Reasoning with (Non-Recursive) Structural Equation Models","summary":"Structural Equation Models (SEM) are the standard approach to representing causal dependencies between variables in causal models. In this paper we propose a new interpretation of SEMs when reasoning about Actual Causality, in which SEMs are viewed as mechanisms transforming the dynamics of exogenous variables into the dynamics of endogenous variables. This allows us to combine counterfactual causal reasoning with existing temporal logic formalisms, and to introduce a temporal logic, CPLTL, for causal reasoning about such structures. We show that the standard restriction to so-called \\textit{recursive} models (with no cycles in the dependency graph) is not necessary in our approach, allowing us to reason about mutually dependent processes and feedback loops. Finally, we introduce new notions of model equivalence for temporal causal models, and show that CPLTL has an efficient model-checking procedure.","authors":["Maksim Gladyshev","Natasha Alechina","Mehdi Dastani","Dragan Doder","Brian Logan"],"pdf_url":"","comment":"This is an extended version of the same title paper published in the proceeding of AAAI-25 conference (Gladyshev et al. 2025). This version contains an additional section, in which we introduce TSEMs with arbitrary long temporal delays between causal dependencies and prove that these models are equivalent to TSEMs with 1-step delays only introduced in the original version of the paper"},{"id":"http://arxiv.org/abs/2512.19396v1","updated":"2025-12-22T13:42:18Z","published":"2025-12-22T13:42:18Z","title":"EchoTrail-GUI: Building Actionable Memory for GUI Agents via Critic-Guided Self-Exploration","summary":"Contemporary GUI agents, while increasingly capable due to advances in Large Vision-Language Models (VLMs), often operate with a critical limitation: they treat each task in isolation, lacking a mechanism to systematically learn from past successes. This digital ''amnesia'' results in sub-optimal performance, repeated errors, and poor generalization to novel challenges. To bridge this gap, we introduce EchoTrail-GUI, a novel framework designed to mimic human-like experiential learning by equipping agents with a dynamic, accessible memory. Our framework operates in three distinct stages. First, during Experience Exploration, an agent autonomously interacts with GUI environments to build a curated database of successful task trajectories, validated by a reward model. Crucially, the entire knowledge base construction is thus fully automated, requiring no human supervision. Second, in the Memory Injection stage, upon receiving a new task, our system efficiently retrieves the most relevant past trajectories to serve as actionable ''memories''. Finally, during GUI Task Inference, these memories are injected as in-context guidance to inform the agent's reasoning and decision-making process. We demonstrate the efficacy of our approach on benchmarks including Android World and AndroidLab. The results show that EchoTrail-GUI significantly improves the task success rate and operational efficiency of baseline agents, validating the power of structured memory in creating more robust and intelligent GUI automation.","authors":["Runze Li","Yuwen Zhai","Bo Xu","LiWu Xu","Nian Shi","Wei Zhang","Ran Lin","Liang Wang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.13907v2","updated":"2025-12-22T13:38:32Z","published":"2025-12-15T21:24:29Z","title":"Assessing High-Risk AI Systems under the EU AI Act: From Legal Requirements to Technical Verification","summary":"The implementation of the AI Act requires practical mechanisms to verify compliance with legal obligations, yet concrete and operational mappings from high-level requirements to verifiable assessment activities remain limited, contributing to uneven readiness across Member States. This paper presents a structured mapping that translates high-level AI Act requirements into concrete, implementable verification activities applicable across the AI lifecycle. The mapping is derived through a systematic process in which legal requirements are decomposed into operational sub-requirements and grounded in authoritative standards and recognised practices. From this basis, verification activities are identified and characterised along two dimensions: the type of verification performed and the lifecycle target to which it applies. By making explicit the link between regulatory intent and technical and organisational assurance practices, the proposed mapping reduces interpretive uncertainty and provides a reusable reference for consistent, technology-agnostic compliance verification under the AI Act.","authors":["Alessio Buscemi","Tom Deckenbrunnen","Fahria Kabir","Kateryna Mishchenko","Nishat Mowla"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19387v1","updated":"2025-12-22T13:36:26Z","published":"2025-12-22T13:36:26Z","title":"DSTED: Decoupling Temporal Stabilization and Discriminative Enhancement for Surgical Workflow Recognition","summary":"Purpose: Surgical workflow recognition enables context-aware assistance and skill assessment in computer-assisted interventions. Despite recent advances, current methods suffer from two critical challenges: prediction jitter across consecutive frames and poor discrimination of ambiguous phases. This paper aims to develop a stable framework by selectively propagating reliable historical information and explicitly modeling uncertainty for hard sample enhancement.\n  Methods: We propose a dual-pathway framework DSTED with Reliable Memory Propagation (RMP) and Uncertainty-Aware Prototype Retrieval (UPR). RMP maintains temporal coherence by filtering and fusing high-confidence historical features through multi-criteria reliability assessment. UPR constructs learnable class-specific prototypes from high-uncertainty samples and performs adaptive prototype matching to refine ambiguous frame representations. Finally, a confidence-driven gate dynamically balances both pathways based on prediction certainty.\n  Results: Our method achieves state-of-the-art performance on AutoLaparo-hysterectomy with 84.36% accuracy and 65.51% F1-score, surpassing the second-best method by 3.51% and 4.88% respectively. Ablations reveal complementary gains from RMP (2.19%) and UPR (1.93%), with synergistic effects when combined. Extensive analysis confirms substantial reduction in temporal jitter and marked improvement on challenging phase transitions.\n  Conclusion: Our dual-pathway design introduces a novel paradigm for stable workflow recognition, demonstrating that decoupling the modeling of temporal consistency and phase ambiguity yields superior performance and clinical applicability.","authors":["Yueyao Chen","Kai-Ni Wang","Dario Tayupo","Arnaud Huaulm'e","Krystel Nyangoh Timoh","Pierre Jannin","Qi Dou"],"pdf_url":"","comment":"Early accepted to IPCAI 2026"},{"id":"http://arxiv.org/abs/2408.11607v3","updated":"2025-12-22T13:33:03Z","published":"2024-08-21T13:32:46Z","title":"Networked Communication for Mean-Field Games with Function Approximation and Empirical Mean-Field Estimation","summary":"Recent algorithms allow decentralised agents, possibly connected via a communication network, to learn equilibria in mean-field games from a non-episodic run of the empirical system. However, these algorithms are for tabular settings: this computationally limits the size of agents' observation space, meaning the algorithms cannot handle anything but small state spaces, nor generalise beyond policies depending only on the agent's local state to so-called 'population-dependent' policies. We address this limitation by introducing function approximation to the existing setting, drawing on the Munchausen Online Mirror Descent method that has previously been employed only in finite-horizon, episodic, centralised settings. While this permits us to include the mean field in the observation for players' policies, it is unrealistic to assume decentralised agents have access to this global information: we therefore also provide new algorithms allowing agents to locally estimate the global empirical distribution, and to improve this estimate via inter-agent communication. We prove theoretically that exchanging policy information helps networked agents outperform both independent and even centralised agents in function-approximation settings. Our experiments demonstrate this happening empirically, and show that the communication network allows decentralised agents to estimate the mean field for population-dependent policies.","authors":["Patrick Benjamin","Alessandro Abate"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2306.02766v6","updated":"2025-12-22T13:25:46Z","published":"2023-06-05T10:45:39Z","title":"Networked Communication for Decentralised Agents in Mean-Field Games","summary":"Methods like multi-agent reinforcement learning struggle to scale with growing population size. Mean-field games (MFGs) are a game-theoretic approach that can circumvent this by finding a solution for an abstract infinite population, which can then be used as an approximate solution for the $N$-agent problem. However, classical mean-field algorithms usually only work under restrictive conditions. We take steps to address this by introducing networked communication to MFGs, in particular to settings that use a single, non-episodic run of $N$ decentralised agents to simulate the infinite population, as is likely to be most reasonable in real-world deployments. We prove that our architecture's sample guarantees lie between those of earlier theoretical algorithms for the centralised- and independent-learning architectures, varying dependent on network structure and the number of communication rounds. However, the sample guarantees of the three theoretical algorithms do not actually result in practical convergence times. We thus contribute practical enhancements to all three algorithms allowing us to present their first empirical demonstrations. We then show that in practical settings where the theoretical hyperparameters are not observed, giving fewer loops but poorer estimation of the Q-function, our communication scheme still respects the earlier theoretical analysis: it considerably accelerates learning over the independent case, which hardly seems to learn at all, and often performs similarly to the centralised case, while removing the restrictive assumption of the latter. We provide ablations and additional studies showing that our networked approach also has advantages over both alternatives in terms of robustness to update failures and to changes in population size.","authors":["Patrick Benjamin","Alessandro Abate"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19379v1","updated":"2025-12-22T13:23:55Z","published":"2025-12-22T13:23:55Z","title":"OmniMER: Indonesian Multimodal Emotion Recognition via Auxiliary-Enhanced LLM Adaptation","summary":"Indonesian, spoken by over 200 million people, remains underserved in multimodal emotion recognition research despite its dominant presence on Southeast Asian social media platforms. We introduce IndoMER, the first multimodal emotion recognition benchmark for Indonesian, comprising 1,944 video segments from 203 speakers with temporally aligned text, audio, and visual annotations across seven emotion categories. The dataset exhibits realistic challenges including cross-modal inconsistency and long-tailed class distributions shaped by Indonesian cultural communication norms. To address these challenges, we propose OmniMER, a multimodal adaptation framework built upon Qwen2.5-Omni that enhances emotion recognition through three auxiliary modality-specific perception tasks: emotion keyword extraction for text, facial expression analysis for video, and prosody analysis for audio. These auxiliary tasks help the model identify emotion-relevant cues in each modality before fusion, reducing reliance on spurious correlations in low-resource settings. Experiments on IndoMER show that OmniMER achieves 0.582 Macro-F1 on sentiment classification and 0.454 on emotion recognition, outperforming the base model by 7.6 and 22.1 absolute points respectively. Cross-lingual evaluation on the Chinese CH-SIMS dataset further demonstrates the generalizability of the proposed framework. The dataset and code are publicly available. https://github.com/yanxm01/INDOMER","authors":["Xueming Yan","Boyan Xu","Yaochu Jin","Lixian Xiao","Wenlong Ye","Runyang Cai","Zeqi Zheng","Jingfa Liu","Aimin Yang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19367v1","updated":"2025-12-22T13:09:45Z","published":"2025-12-22T13:09:45Z","title":"Sprecher Networks: A Parameter-Efficient Kolmogorov-Arnold Architecture","summary":"We present Sprecher Networks (SNs), a family of trainable neural architectures inspired by the classical Kolmogorov-Arnold-Sprecher (KAS) construction for approximating multivariate continuous functions. Distinct from Multi-Layer Perceptrons (MLPs) with fixed node activations and Kolmogorov-Arnold Networks (KANs) featuring learnable edge activations, SNs utilize shared, learnable splines (monotonic and general) within structured blocks incorporating explicit shift parameters and mixing weights. Our approach directly realizes Sprecher's specific 1965 sum of shifted splines formula in its single-layer variant and extends it to deeper, multi-layer compositions. We further enhance the architecture with optional lateral mixing connections that enable intra-block communication between output dimensions, providing a parameter-efficient alternative to full attention mechanisms. Beyond parameter efficiency with $O(LN + LG)$ scaling (where $G$ is the knot count of the shared splines) versus MLPs' $O(LN^2)$, SNs admit a sequential evaluation strategy that reduces peak forward-intermediate memory from $O(N^2)$ to $O(N)$ (treating batch size as constant), making much wider architectures feasible under memory constraints. We demonstrate empirically that composing these blocks into deep networks leads to highly parameter and memory-efficient models, discuss theoretical motivations, and compare SNs with related architectures (MLPs, KANs, and networks with learnable node activations).","authors":["Christian Hägg","Kathlén Kohn","Giovanni Luca Marchetti","Boris Shapiro"],"pdf_url":"","comment":"37 pages"},{"id":"http://arxiv.org/abs/2512.19366v1","updated":"2025-12-22T13:08:58Z","published":"2025-12-22T13:08:58Z","title":"Learning General Policies with Policy Gradient Methods","summary":"While reinforcement learning methods have delivered remarkable results in a number of settings, generalization, i.e., the ability to produce policies that generalize in a reliable and systematic way, has remained a challenge. The problem of generalization has been addressed formally in classical planning where provable correct policies that generalize over all instances of a given domain have been learned using combinatorial methods. The aim of this work is to bring these two research threads together to illuminate the conditions under which (deep) reinforcement learning approaches, and in particular, policy optimization methods, can be used to learn policies that generalize like combinatorial methods do. We draw on lessons learned from previous combinatorial and deep learning approaches, and extend them in a convenient way. From the former, we model policies as state transition classifiers, as (ground) actions are not general and change from instance to instance. From the latter, we use graph neural networks (GNNs) adapted to deal with relational structures for representing value functions over planning states, and in our case, policies. With these ingredients in place, we find that actor-critic methods can be used to learn policies that generalize almost as well as those obtained using combinatorial approaches while avoiding the scalability bottleneck and the use of feature pools. Moreover, the limitations of the DRL methods on the benchmarks considered have little to do with deep learning or reinforcement learning algorithms, and result from the well-understood expressive limitations of GNNs, and the tradeoff between optimality and generalization (general policies cannot be optimal in some domains). Both of these limitations are addressed without changing the basic DRL methods by adding derived predicates and an alternative cost structure to optimize.","authors":["Simon Ståhlberg","Blai Bonet","Hector Geffner"],"pdf_url":"","comment":"In Proceedings of the 20th International Conference on Principles of Knowledge Representation and Reasoning (KR 2023)"},{"id":"http://arxiv.org/abs/2512.19355v1","updated":"2025-12-22T12:54:32Z","published":"2025-12-22T12:54:32Z","title":"First-Order Representation Languages for Goal-Conditioned RL","summary":"First-order relational languages have been used in MDP planning and reinforcement learning (RL) for two main purposes: specifying MDPs in compact form, and representing and learning policies that are general and not tied to specific instances or state spaces. In this work, we instead consider the use of first-order languages in goal-conditioned RL and generalized planning. The question is how to learn goal-conditioned and general policies when the training instances are large and the goal cannot be reached by random exploration alone. The technique of Hindsight Experience Replay (HER) provides an answer to this question: it relabels unsuccessful trajectories as successful ones by replacing the original goal with one that was actually achieved. If the target policy must generalize across states and goals, trajectories that do not reach the original goal states can enable more data- and time-efficient learning. In this work, we show that further performance gains can be achieved when states and goals are represented by sets of atoms. We consider three versions: goals as full states, goals as subsets of the original goals, and goals as lifted versions of these subgoals. The result is that the latter two successfully learn general policies on large planning instances with sparse rewards by automatically creating a curriculum of easier goals of increasing complexity. The experiments illustrate the computational gains of these versions, their limitations, and opportunities for addressing them.","authors":["Simon Ståhlberg","Hector Geffner"],"pdf_url":"","comment":"In Proceedings of the 40th AAAI Conference on Artificial Intelligence (AAAI 2026)"},{"id":"http://arxiv.org/abs/2512.19350v1","updated":"2025-12-22T12:49:12Z","published":"2025-12-22T12:49:12Z","title":"PENDULUM: A Benchmark for Assessing Sycophancy in Multimodal Large Language Models","summary":"Sycophancy, an excessive tendency of AI models to agree with user input at the expense of factual accuracy or in contradiction of visual evidence, poses a critical and underexplored challenge for multimodal large language models (MLLMs). While prior studies have examined this behavior in text-only settings of large language models, existing research on visual or multimodal counterparts remains limited in scope and depth of analysis. To address this gap, we introduce a comprehensive evaluation benchmark, \\textit{PENDULUM}, comprising approximately 2,000 human-curated Visual Question Answering pairs specifically designed to elicit sycophantic responses. The benchmark spans six distinct image domains of varying complexity, enabling a systematic investigation of how image type and inherent challenges influence sycophantic tendencies. Through extensive evaluation of state-of-the-art MLLMs. we observe substantial variability in model robustness and a pronounced susceptibility to sycophantic and hallucinatory behavior. Furthermore, we propose novel metrics to quantify sycophancy in visual reasoning, offering deeper insights into its manifestations across different multimodal contexts. Our findings highlight the urgent need for developing sycophancy-resilient architectures and training strategies to enhance factual consistency and reliability in future MLLMs. Our proposed dataset with MLLMs response are available at https://github.com/ashikiut/pendulum/.","authors":["A. B. M. Ashikur Rahman","Saeed Anwar","Muhammad Usman","Irfan Ahmad","Ajmal Mian"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2511.02864v3","updated":"2025-12-22T12:49:01Z","published":"2025-11-03T16:04:07Z","title":"Mathematical exploration and discovery at scale","summary":"AlphaEvolve (Novikov et al., 2025) is a generic evolutionary coding agent that combines the generative capabilities of LLMs with automated evaluation in an iterative evolutionary framework that proposes, tests, and refines algorithmic solutions to challenging scientific and practical problems. In this paper we showcase AlphaEvolve as a tool for autonomously discovering novel mathematical constructions and advancing our understanding of long-standing open problems.\n  To demonstrate its breadth, we considered a list of 67 problems spanning mathematical analysis, combinatorics, geometry, and number theory. The system rediscovered the best known solutions in most of the cases and discovered improved solutions in several. In some instances, AlphaEvolve is also able to generalize results for a finite number of input values into a formula valid for all input values. Furthermore, we are able to combine this methodology with Deep Think and AlphaProof in a broader framework where the additional proof-assistants and reasoning systems provide automated proof generation and further mathematical insights.\n  These results demonstrate that large language model-guided evolutionary search can autonomously discover mathematical constructions that complement human intuition, at times matching or even improving the best known results, highlighting the potential for significant new ways of interaction between mathematicians and AI systems. We present AlphaEvolve as a powerful new tool for mathematical discovery, capable of exploring vast search spaces to solve complex optimization problems at scale, often with significantly reduced requirements on preparation and computation time.","authors":["Bogdan Georgiev","Javier Gómez-Serrano","Terence Tao","Adam Zsolt Wagner"],"pdf_url":"","comment":"81 pages, 35 figures"},{"id":"http://arxiv.org/abs/2512.19349v1","updated":"2025-12-22T12:48:29Z","published":"2025-12-22T12:48:29Z","title":"VIGOR+: Iterative Confounder Generation and Validation via LLM-CEVAE Feedback Loop","summary":"Hidden confounding remains a fundamental challenge in causal inference from observational data. Recent advances leverage Large Language Models (LLMs) to generate plausible hidden confounders based on domain knowledge, yet a critical gap exists: LLM-generated confounders often exhibit semantic plausibility without statistical utility. We propose VIGOR+ (Variational Information Gain for iterative cOnfounder Refinement), a novel framework that closes the loop between LLM-based confounder generation and CEVAE-based statistical validation. Unlike prior approaches that treat generation and validation as separate stages, VIGOR+ establishes an iterative feedback mechanism: validation signals from CEVAE (including information gain, latent consistency metrics, and diagnostic messages) are transformed into natural language feedback that guides subsequent LLM generation rounds. This iterative refinement continues until convergence criteria are met. We formalize the feedback mechanism, prove convergence properties under mild assumptions, and provide a complete algorithmic framework.","authors":["JiaWei Zhu","ZiHeng Liu"],"pdf_url":"","comment":"7 pages,1 figure,4 tables"},{"id":"http://arxiv.org/abs/2412.14579v2","updated":"2025-12-22T12:47:43Z","published":"2024-12-19T06:57:37Z","title":"GSRender: Deduplicated Occupancy Prediction via Weakly Supervised 3D Gaussian Splatting","summary":"Weakly-supervised 3D occupancy perception is crucial for vision-based autonomous driving in outdoor environments. Previous methods based on NeRF often face a challenge in balancing the number of samples used. Too many samples can decrease efficiency, while too few can compromise accuracy, leading to variations in the mean Intersection over Union (mIoU) by 5-10 points. Furthermore, even with surrounding-view image inputs, only a single image is rendered from each viewpoint at any given moment. This limitation leads to duplicated predictions, which significantly impacts the practicality of the approach. However, this issue has largely been overlooked in existing research. To address this, we propose GSRender, which uses 3D Gaussian Splatting for weakly-supervised occupancy estimation, simplifying the sampling process. Additionally, we introduce the Ray Compensation module, which reduces duplicated predictions by compensating for features from adjacent frames. Finally, we redesign the dynamic loss to remove the influence of dynamic objects from adjacent frames. Extensive experiments show that our approach achieves SOTA results in RayIoU (+6.0), while also narrowing the gap with 3D- supervised methods. This work lays a solid foundation for weakly-supervised occupancy perception. The code is available at https://github.com/Jasper-sudo-Sun/GSRender.","authors":["Qianpu Sun","Changyong Shu","Sifan Zhou","Runxi Cheng","Yongxian Wei","Zichen Yu","Dawei Yang","Sirui Han","Yuan Chun"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.10713v2","updated":"2025-12-22T12:47:11Z","published":"2025-12-11T14:49:56Z","title":"PACIFIC: a framework for generating benchmarks to check Precise Automatically Checked Instruction Following In Code","summary":"Large Language Model (LLM)-based code assistants have emerged as a powerful application of generative AI, demonstrating impressive capabilities in code generation and comprehension. A key requirement for these systems is their ability to accurately follow user instructions. We present Precise Automatically Checked Instruction Following In Code (PACIFIC), a novel framework designed to automatically generate benchmarks that rigorously assess sequential instruction-following and code dry-running capabilities in LLMs, while allowing control over benchmark difficulty. PACIFIC produces benchmark variants with clearly defined expected outputs, enabling straightforward and reliable evaluation through simple output comparisons. In contrast to existing approaches that often rely on tool usage or agentic behavior, our work isolates and evaluates the LLM's intrinsic ability to reason through code behavior step-by-step without execution (dry running) and to follow instructions. Furthermore, our framework mitigates training data contamination by facilitating effortless generation of novel benchmark variations. We validate our framework by generating a suite of benchmarks spanning a range of difficulty levels and evaluating multiple state-of-the-art LLMs. Our results demonstrate that PACIFIC can produce increasingly challenging benchmarks that effectively differentiate instruction-following and dry running capabilities, even among advanced models. Overall, our framework offers a scalable, contamination-resilient methodology for assessing core competencies of LLMs in code-related tasks.","authors":["Itay Dreyfuss","Antonio Abu Nassar","Samuel Ackerman","Axel Ben David","Eitan Farchi","Rami Katan","Orna Raz","Marcel Zalmanovici"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2510.04978v4","updated":"2025-12-22T12:44:21Z","published":"2025-10-06T16:16:03Z","title":"Aligning Perception, Reasoning, Modeling and Interaction: A Survey on Physical AI","summary":"The rapid advancement of embodied intelligence and world models has intensified efforts to integrate physical laws into AI systems, yet physical perception and symbolic physics reasoning have developed along separate trajectories without a unified bridging framework. This work provides a comprehensive overview of physical AI, establishing clear distinctions between theoretical physics reasoning and applied physical understanding while systematically examining how physics-grounded methods enhance AI's real-world comprehension across structured symbolic reasoning, embodied systems, and generative models. Through rigorous analysis of recent advances, we advocate for intelligent systems that ground learning in both physical principles and embodied reasoning processes, transcending pattern recognition toward genuine understanding of physical laws. Our synthesis envisions next-generation world models capable of explaining physical phenomena and predicting future states, advancing safe, generalizable, and interpretable AI systems. We maintain a continuously updated resource at https://github.com/AI4Phys/Awesome-AI-for-Physics.","authors":["Kun Xiang","Terry Jingchen Zhang","Yinya Huang","Jixi He","Zirong Liu","Yueling Tang","Ruizhe Zhou","Lijing Luo","Youpeng Wen","Xiuwei Chen","Bingqian Lin","Jianhua Han","Hang Xu","Hanhui Li","Bin Dong","Xiaodan Liang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.07984v3","updated":"2025-12-22T12:27:35Z","published":"2025-12-08T19:15:08Z","title":"Restrictive Hierarchical Semantic Segmentation for Stratified Tooth Layer Detection","summary":"Accurate understanding of anatomical structures is essential for reliably staging certain dental diseases. A way of introducing this within semantic segmentation models is by utilising hierarchy-aware methodologies. However, existing hierarchy-aware segmentation methods largely encode anatomical structure through the loss functions, providing weak and indirect supervision. We introduce a general framework that embeds an explicit anatomical hierarchy into semantic segmentation by coupling a recurrent, level-wise prediction scheme with restrictive output heads and top-down feature conditioning. At each depth of the class tree, the backbone is re-run on the original image concatenated with logits from the previous level. Child class features are conditioned using Feature-wise Linear Modulation of their parent class probabilities, to modulate child feature spaces for fine grained detection. A probabilistic composition rule enforces consistency between parent and descendant classes. Hierarchical loss combines per-level class weighted Dice and cross entropy loss and a consistency term loss, ensuring parent predictions are the sum of their children. We validate our approach on our proposed dataset, TL-pano, containing 194 panoramic radiographs with dense instance and semantic segmentation annotations, of tooth layers and alveolar bone. Utilising UNet and HRNet as donor models across a 5-fold cross validation scheme, the hierarchical variants consistently increase IoU, Dice, and recall, particularly for fine-grained anatomies, and produce more anatomically coherent masks. However, hierarchical variants also demonstrated increased recall over precision, implying increased false positives. The results demonstrate that explicit hierarchical structuring improves both performance and clinical plausibility, especially in low data dental imaging regimes.","authors":["Ryan Banks","Camila Lindoni Azevedo","Hongying Tang","Yunpeng Li"],"pdf_url":"","comment":"Incorrect initial draft was submitted by mistake. Method, results and citations are incorrect"},{"id":"http://arxiv.org/abs/2512.19323v1","updated":"2025-12-22T12:17:47Z","published":"2025-12-22T12:17:47Z","title":"Alternative positional encoding functions for neural transformers","summary":"A key module in neural transformer-based deep architectures is positional encoding. This module enables a suitable way to encode positional information as input for transformer neural layers. This success has been rooted in the use of sinusoidal functions of various frequencies, in order to capture recurrent patterns of differing typical periods. In this work, an alternative set of periodic functions is proposed for positional encoding. These functions preserve some key properties of sinusoidal ones, while they depart from them in fundamental ways. Some tentative experiments are reported, where the original sinusoidal version is substantially outperformed. This strongly suggests that the alternative functions may have a wider use in other transformer architectures.","authors":["Ezequiel Lopez-Rubio","Macoris Decena-Gimenez","Rafael Marcos Luque-Baena"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19320v1","updated":"2025-12-22T12:13:17Z","published":"2025-12-22T12:13:17Z","title":"MAGIC: Achieving Superior Model Merging via Magnitude Calibration","summary":"The proliferation of pre-trained models has given rise to a wide array of specialised, fine-tuned models. Model merging aims to merge the distinct capabilities of these specialised models into a unified model, requiring minimal or even no additional training. A core objective of model merging is to ensure the merged model retains the behavioural characteristics of the specialised models, typically achieved through feature alignment. We identify that features consist of two critical components: direction and magnitude. Prior research has predominantly focused on directional alignment, while the influence of magnitude remains largely neglected, despite its pronounced vulnerability to perturbations introduced by common merging operations (e.g., parameter fusion and sparsification). Such perturbations to magnitude inevitably lead to feature deviations in the merged model from the specialised models, resulting in subsequent performance degradation. To address this, we propose MAGnItude Calibration (MAGIC), a plug-and-play framework that rectifies layer-wise magnitudes in feature and weight spaces, with three variants. Specifically, our Feature Space Calibration (FSC) realigns the merged model's features using a small set of unlabelled data, while Weight Space Calibration (WSC) extends this calibration to the weight space without requiring additional data. Combining these yields Dual Space Calibration (DSC). Comprehensive experiments demonstrate that MAGIC consistently boosts performance across diverse Computer Vision tasks (+4.3% on eight datasets) and NLP tasks (+8.0% on Llama) without additional training. Our code is available at: https://github.com/lyymuwu/MAGIC","authors":["Yayuan Li","Jian Zhang","Jintao Guo","Zihan Cheng","Lei Qi","Yinghuan Shi","Yang Gao"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19317v1","updated":"2025-12-22T12:07:33Z","published":"2025-12-22T12:07:33Z","title":"SafeMed-R1: Adversarial Reinforcement Learning for Generalizable and Robust Medical Reasoning in Vision-Language Models","summary":"Vision--Language Models (VLMs) show significant promise for Medical Visual Question Answering (VQA), yet their deployment in clinical settings is hindered by severe vulnerability to adversarial attacks. Standard adversarial training, while effective for simpler tasks, often degrades both generalization performance and the quality of generated clinical reasoning. We introduce SafeMed-R1, a hybrid defense framework that ensures robust performance while preserving high-quality, interpretable medical reasoning. SafeMed-R1 employs a two-stage approach: at training time, we integrate Adversarial Training with Group Relative Policy Optimization (AT-GRPO) to explicitly robustify the reasoning process against worst-case perturbations; at inference time, we augment the model with Randomized Smoothing to provide certified $L_2$-norm robustness guarantees. We evaluate SafeMed-R1 on the OmniMedVQA benchmark across eight medical imaging modalities comprising over 88,000 samples. Our experiments reveal that standard fine-tuned VLMs, despite achieving 95\\% accuracy on clean inputs, collapse to approximately 25\\% under PGD attacks. In contrast, SafeMed-R1 maintains 84.45\\% accuracy under the same adversarial conditions, representing a 59 percentage point improvement in robustness. Furthermore, we demonstrate that models trained with explicit chain-of-thought reasoning exhibit superior adversarial robustness compared to instruction-only variants, suggesting a synergy between interpretability and security in medical AI systems.","authors":["A. A. Gde Yogi Pramana","Jason Ray","Anthony Jaya","Michael Wijaya"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19311v1","updated":"2025-12-22T12:00:12Z","published":"2025-12-22T12:00:12Z","title":"MixFlow Training: Alleviating Exposure Bias with Slowed Interpolation Mixture","summary":"This paper studies the training-testing discrepancy (a.k.a. exposure bias) problem for improving the diffusion models. During training, the input of a prediction network at one training timestep is the corresponding ground-truth noisy data that is an interpolation of the noise and the data, and during testing, the input is the generated noisy data. We present a novel training approach, named MixFlow, for improving the performance. Our approach is motivated by the Slow Flow phenomenon: the ground-truth interpolation that is the nearest to the generated noisy data at a given sampling timestep is observed to correspond to a higher-noise timestep (termed slowed timestep), i.e., the corresponding ground-truth timestep is slower than the sampling timestep. MixFlow leverages the interpolations at the slowed timesteps, named slowed interpolation mixture, for post-training the prediction network for each training timestep. Experiments over class-conditional image generation (including SiT, REPA, and RAE) and text-to-image generation validate the effectiveness of our approach. Our approach MixFlow over the RAE models achieve strong generation results on ImageNet: 1.43 FID (without guidance) and 1.10 (with guidance) at 256 x 256, and 1.55 FID (without guidance) and 1.10 (with guidance) at 512 x 512.","authors":["Hui Li","Jiayue Lyu","Fu-Yun Wang","Kaihui Cheng","Siyu Zhu","Jingdong Wang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2503.04521v2","updated":"2025-12-22T11:48:52Z","published":"2025-03-06T15:08:31Z","title":"Dynamic Pricing for On-Demand DNN Inference in the Edge-AI Market","summary":"The convergence of edge computing and Artificial Intelligence (AI) gives rise to Edge-AI, which enables the deployment of real-time AI applications at the network edge. A key research challenge in Edge-AI is edge inference acceleration, which aims to realize low-latency high-accuracy Deep Neural Network (DNN) inference by offloading partitioned inference tasks from end devices to edge servers. However, existing research has yet to adopt a practical Edge-AI market perspective, which would explore the personalized inference needs of AI users (e.g., inference accuracy, latency, and task complexity), the revenue incentives for AI service providers that offer edge inference services, and multi-stakeholder governance within a market-oriented context. To bridge this gap, we propose an Auction-based Edge Inference Pricing Mechanism (AERIA) for revenue maximization to tackle the multi-dimensional optimization problem of DNN model partition, edge inference pricing, and resource allocation. We develop a multi-exit device-edge synergistic inference scheme for on-demand DNN inference acceleration, and theoretically analyze the auction dynamics amongst the AI service providers, AI users and edge infrastructure provider. Owing to the strategic mechanism design via randomized consensus estimate and cost sharing techniques, the Edge-AI market attains several desirable properties. These include competitiveness in revenue maximization, incentive compatibility, and envy-freeness, which are crucial to maintain the effectiveness, truthfulness, and fairness in auction outcomes. Extensive simulations based on four representative DNN inference workloads demonstrate that AERIA significantly outperforms several state-of-the-art approaches in revenue maximization. This validates the efficacy of AERIA for on-demand DNN inference in the Edge-AI market.","authors":["Songyuan Li","Jia Hu","Geyong Min","Haojun Huang","Jiwei Huang"],"pdf_url":"","comment":"Accepted for publication in IEEE Transactions on Mobile Computing. Index Terms: Edge-AI, DNN Inference Offloading, Resource Management, Dynamic Pricing, Auction Mechanism"},{"id":"http://arxiv.org/abs/2512.19299v1","updated":"2025-12-22T11:43:35Z","published":"2025-12-22T11:43:35Z","title":"Helios: A Foundational Language Model for Smart Energy Knowledge Reasoning and Application","summary":"In the global drive toward carbon neutrality, deeply coordinated smart energy systems underpin industrial transformation. However, the interdisciplinary, fragmented, and fast-evolving expertise in this domain prevents general-purpose LLMs, which lack domain knowledge and physical-constraint awareness, from delivering precise engineering-aligned inference and generation. To address these challenges, we introduce Helios, a large language model tailored to the smart energy domain, together with a comprehensive suite of resources to advance LLM research in this field. Specifically, we develop Enersys, a multi-agent collaborative framework for end-to-end dataset construction, through which we produce: (1) a smart energy knowledge base, EnerBase, to enrich the model's foundational expertise; (2) an instruction fine-tuning dataset, EnerInstruct, to strengthen performance on domain-specific downstream tasks; and (3) an RLHF dataset, EnerReinforce, to align the model with human preferences and industry standards. Leveraging these resources, Helios undergoes large-scale pretraining, SFT, and RLHF. We also release EnerBench, a benchmark for evaluating LLMs in smart energy scenarios, and demonstrate that our approach significantly enhances domain knowledge mastery, task execution accuracy, and alignment with human preferences.","authors":["Haoyu Jiang","Fanjie Zeng","Boan Qu","Xiaojie Lin","Wei Zhong"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19297v1","updated":"2025-12-22T11:40:47Z","published":"2025-12-22T11:40:47Z","title":"Causal-Guided Detoxify Backdoor Attack of Open-Weight LoRA Models","summary":"Low-Rank Adaptation (LoRA) has emerged as an efficient method for fine-tuning large language models (LLMs) and is widely adopted within the open-source community. However, the decentralized dissemination of LoRA adapters through platforms such as Hugging Face introduces novel security vulnerabilities: malicious adapters can be easily distributed and evade conventional oversight mechanisms. Despite these risks, backdoor attacks targeting LoRA-based fine-tuning remain relatively underexplored. Existing backdoor attack strategies are ill-suited to this setting, as they often rely on inaccessible training data, fail to account for the structural properties unique to LoRA, or suffer from high false trigger rates (FTR), thereby compromising their stealth. To address these challenges, we propose Causal-Guided Detoxify Backdoor Attack (CBA), a novel backdoor attack framework specifically designed for open-weight LoRA models. CBA operates without access to original training data and achieves high stealth through two key innovations: (1) a coverage-guided data generation pipeline that synthesizes task-aligned inputs via behavioral exploration, and (2) a causal-guided detoxification strategy that merges poisoned and clean adapters by preserving task-critical neurons. Unlike prior approaches, CBA enables post-training control over attack intensity through causal influence-based weight allocation, eliminating the need for repeated retraining. Evaluated across six LoRA models, CBA achieves high attack success rates while reducing FTR by 50-70\\% compared to baseline methods. Furthermore, it demonstrates enhanced resistance to state-of-the-art backdoor defenses, highlighting its stealth and robustness.","authors":["Linzhi Chen","Yang Sun","Hongru Wei","Yuqi Chen"],"pdf_url":"","comment":"NDSS 2026"},{"id":"http://arxiv.org/abs/2502.12489v2","updated":"2025-12-22T11:32:28Z","published":"2025-02-18T03:18:54Z","title":"A Comprehensive Survey on Generative AI for Video-to-Music Generation","summary":"The burgeoning growth of video-to-music generation can be attributed to the ascendancy of multimodal generative models. However, there is a lack of literature that comprehensively combs through the work in this field. To fill this gap, this paper presents a comprehensive review of video-to-music generation using deep generative AI techniques, focusing on three key components: conditioning input construction, conditioning mechanism, and music generation frameworks. We categorize existing approaches based on their designs for each component, clarifying the roles of different strategies. Preceding this, we provide a fine-grained categorization of video and music modalities, illustrating how different categories influence the design of components within the generation pipelines. Furthermore, we summarize available multimodal datasets and evaluation metrics while highlighting ongoing challenges in the field.","authors":["Shulei Ji","Songruoyao Wu","Zihao Wang","Shuyu Li","Kejun Zhang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19287v1","updated":"2025-12-22T11:30:19Z","published":"2025-12-22T11:30:19Z","title":"Vibe Reasoning: Eliciting Frontier AI Mathematical Capabilities -- A Case Study on IMO 2025 Problem 6","summary":"We introduce Vibe Reasoning, a human-AI collaborative paradigm for solving complex mathematical problems. Our key insight is that frontier AI models already possess the knowledge required to solve challenging problems -- they simply do not know how, what, or when to apply it. Vibe Reasoning transforms AI's latent potential into manifested capability through generic meta-prompts, agentic grounding, and model orchestration. We demonstrate this paradigm through IMO 2025 Problem 6, a combinatorial optimization problem where autonomous AI systems publicly reported failures. Our solution combined GPT-5's exploratory capabilities with Gemini 3 Pro's proof strengths, leveraging agentic workflows with Python code execution and file-based memory, to derive both the correct answer (2112) and a rigorous mathematical proof. Through iterative refinement across multiple attempts, we discovered the necessity of agentic grounding and model orchestration, while human prompts evolved from problem-specific hints to generic, transferable meta-prompts. We analyze why capable AI fails autonomously, how each component addresses specific failure modes, and extract principles for effective vibe reasoning. Our findings suggest that lightweight human guidance can unlock frontier models' mathematical reasoning potential. This is ongoing work; we are developing automated frameworks and conducting broader evaluations to further validate Vibe Reasoning's generality and effectiveness.","authors":["Jiaao Wu","Xian Zhang","Fan Yang","Yinpeng Dong"],"pdf_url":"","comment":"20 pages, 13 figures"},{"id":"http://arxiv.org/abs/2512.19280v1","updated":"2025-12-22T11:24:42Z","published":"2025-12-22T11:24:42Z","title":"Digital Twin-Driven Zero-Shot Fault Diagnosis of Axial Piston Pumps Using Fluid-Borne Noise Signals","summary":"Axial piston pumps are crucial components in fluid power systems, where reliable fault diagnosis is essential for ensuring operational safety and efficiency. Traditional data-driven methods require extensive labeled fault data, which is often impractical to obtain, while model-based approaches suffer from parameter uncertainties. This paper proposes a digital twin (DT)-driven zero-shot fault diagnosis framework utilizing fluid-borne noise (FBN) signals. The framework calibrates a high-fidelity DT model using only healthy-state data, generates synthetic fault signals for training deep learning classifiers, and employs a physics-informed neural network (PINN) as a virtual sensor for flow ripple estimation. Gradient-weighted class activation mapping (Grad-CAM) is integrated to visualize the decision-making process of neural networks, revealing that large kernels matching the subsequence length in time-domain inputs and small kernels in time-frequency domain inputs enable higher diagnostic accuracy by focusing on physically meaningful features. Experimental validations demonstrate that training on signals from the calibrated DT model yields diagnostic accuracies exceeding 95\\% on real-world benchmarks, while uncalibrated models result in significantly lower performance, highlighting the framework's effectiveness in data-scarce scenarios.","authors":["Chang Dong","Jianfeng Tao","Chengliang Liu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19275v1","updated":"2025-12-22T11:19:46Z","published":"2025-12-22T11:19:46Z","title":"Is Visual Realism Enough? Evaluating Gait Biometric Fidelity in Generative AI Human Animation","summary":"Generative AI (GenAI) models have revolutionized animation, enabling the synthesis of humans and motion patterns with remarkable visual fidelity. However, generating truly realistic human animation remains a formidable challenge, where even minor inconsistencies can make a subject appear unnatural. This limitation is particularly critical when AI-generated videos are evaluated for behavioral biometrics, where subtle motion cues that define identity are easily lost or distorted. The present study investigates whether state-of-the-art GenAI human animation models can preserve the subtle spatio-temporal details needed for person identification through gait biometrics. Specifically, we evaluate four different GenAI models across two primary evaluation tasks to assess their ability to i) restore gait patterns from reference videos under varying conditions of complexity, and ii) transfer these gait patterns to different visual identities. Our results show that while visual quality is mostly high, biometric fidelity remains low in tasks focusing on identification, suggesting that current GenAI models struggle to disentangle identity from motion. Furthermore, through an identity transfer task, we expose a fundamental flaw in appearance-based gait recognition: when texture is disentangled from motion, identification collapses, proving current GenAI models rely on visual attributes rather than temporal dynamics.","authors":["Ivan DeAndres-Tame","Chengwei Ye","Ruben Tolosana","Ruben Vera-Rodriguez","Shiqi Yu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.16301v2","updated":"2025-12-22T11:05:54Z","published":"2025-12-18T08:38:51Z","title":"Adaptation of Agentic AI","summary":"Cutting-edge agentic AI systems are built on foundation models that can be adapted to plan, reason, and interact with external tools to perform increasingly complex and specialized tasks. As these systems grow in capability and scope, adaptation becomes a central mechanism for improving performance, reliability, and generalization. In this paper, we unify the rapidly expanding research landscape into a systematic framework that spans both agent adaptations and tool adaptations. We further decompose these into tool-execution-signaled and agent-output-signaled forms of agent adaptation, as well as agent-agnostic and agent-supervised forms of tool adaptation. We demonstrate that this framework helps clarify the design space of adaptation strategies in agentic AI, makes their trade-offs explicit, and provides practical guidance for selecting or switching among strategies during system design. We then review the representative approaches in each category, analyze their strengths and limitations, and highlight key open challenges and future opportunities. Overall, this paper aims to offer a conceptual foundation and practical roadmap for researchers and practitioners seeking to build more capable, efficient, and reliable agentic AI systems.","authors":["Pengcheng Jiang","Jiacheng Lin","Zhiyi Shi","Zifeng Wang","Luxi He","Yichen Wu","Ming Zhong","Peiyang Song","Qizheng Zhang","Heng Wang","Xueqiang Xu","Hanwen Xu","Pengrui Han","Dylan Zhang","Jiashuo Sun","Chaoqi Yang","Kun Qian","Tian Wang","Changran Hu","Manling Li","Quanzheng Li","Hao Peng","Sheng Wang","Jingbo Shang","Chao Zhang","Jiaxuan You","Liyuan Liu","Pan Lu","Yu Zhang","Heng Ji","Yejin Choi","Dawn Song","Jimeng Sun","Jiawei Han"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2506.10035v2","updated":"2025-12-22T10:46:52Z","published":"2025-06-10T20:48:30Z","title":"FastFLUX: Pruning FLUX with Block-wise Replacement and Sandwich Training","summary":"Recent advancements in text-to-image (T2I) generation have led to the emergence of highly expressive models such as diffusion transformers (DiTs), exemplified by FLUX. However, their massive parameter sizes lead to slow inference, high memory usage, and poor deployability. Existing acceleration methods (e.g., single-step distillation and attention pruning) often suffer from significant performance degradation and incur substantial training costs. To address these limitations, we propose FastFLUX, an architecture-level pruning framework designed to enhance the inference efficiency of FLUX. At its core is the Block-wise Replacement with Linear Layers (BRLL) method, which replaces structurally complex residual branches in ResBlocks with lightweight linear layers while preserving the original shortcut connections for stability. Furthermore, we introduce Sandwich Training (ST), a localized fine-tuning strategy that leverages LoRA to supervise neighboring blocks, mitigating performance drops caused by structural replacement. Experiments show that our FastFLUX maintains high image quality under both qualitative and quantitative evaluations, while significantly improving inference speed, even with 20\\% of the hierarchy pruned. Our code will be available soon.","authors":["Fuhan Cai","Yong Guo","Jie Li","Wenbo Li","Jian Chen","Xiangzhong Fang"],"pdf_url":"","comment":"14 pages"},{"id":"http://arxiv.org/abs/2507.17860v4","updated":"2025-12-22T10:41:08Z","published":"2025-07-23T18:33:27Z","title":"Towards Facilitated Fairness Assessment of AI-based Skin Lesion Classifiers Through GenAI-based Image Synthesis","summary":"Recent advances in deep learning and on-device inference could transform routine screening for skin cancers. Along with the anticipated benefits of this technology, potential dangers arise from unforeseen and inherent biases. A significant obstacle is building evaluation datasets that accurately reflect key demographics, including sex, age, and race, as well as other underrepresented groups. To address this, we train a state-of-the-art generative model to generate synthetic data in a controllable manner to assess the fairness of publicly available skin cancer classifiers. To evaluate whether synthetic images can be used as a fairness testing dataset, we prepare a real-image dataset (MILK10K) as a benchmark and compare the True Positive Rate result of three models (DeepGuide, MelaNet, and SkinLesionDensnet). As a result, the classification tendencies observed in each model when tested on real and generated images showed similar patterns across different attribute data sets. We confirm that highly realistic synthetic images facilitate model fairness verification.","authors":["Ko Watanabe","Stanislav Frolov","Aya Hassan","David Dembinsky","Adriano Lucieri","Andreas Dengel"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19253v1","updated":"2025-12-22T10:40:03Z","published":"2025-12-22T10:40:03Z","title":"Machine Unlearning in the Era of Quantum Machine Learning: An Empirical Study","summary":"We present the first comprehensive empirical study of machine unlearning (MU) in hybrid quantum-classical neural networks. While MU has been extensively explored in classical deep learning, its behavior within variational quantum circuits (VQCs) and quantum-augmented architectures remains largely unexplored. First, we adapt a broad suite of unlearning methods to quantum settings, including gradient-based, distillation-based, regularization-based and certified techniques. Second, we introduce two new unlearning strategies tailored to hybrid models. Experiments across Iris, MNIST, and Fashion-MNIST, under both subset removal and full-class deletion, reveal that quantum models can support effective unlearning, but outcomes depend strongly on circuit depth, entanglement structure, and task complexity. Shallow VQCs display high intrinsic stability with minimal memorization, whereas deeper hybrid models exhibit stronger trade-offs between utility, forgetting strength, and alignment with retrain oracle. We find that certain methods, e.g. EU-k, LCA, and Certified Unlearning, consistently provide the best balance across metrics. These findings establish baseline empirical insights into quantum machine unlearning and highlight the need for quantum-aware algorithms and theoretical guarantees, as quantum machine learning systems continue to expand in scale and capability. We publicly release our code at: https://github.com/CrivoiCarla/HQML.","authors":["Carla Crivoi","Radu Tudor Ionescu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.16280v2","updated":"2025-12-22T10:37:08Z","published":"2025-12-18T07:59:15Z","title":"Love, Lies, and Language Models: Investigating AI's Role in Romance-Baiting Scams","summary":"Romance-baiting scams have become a major source of financial and emotional harm worldwide. These operations are run by organized crime syndicates that traffic thousands of people into forced labor, requiring them to build emotional intimacy with victims over weeks of text conversations before pressuring them into fraudulent cryptocurrency investments. Because the scams are inherently text-based, they raise urgent questions about the role of Large Language Models (LLMs) in both current and future automation.\n  We investigate this intersection by interviewing 145 insiders and 5 scam victims, performing a blinded long-term conversation study comparing LLM scam agents to human operators, and executing an evaluation of commercial safety filters. Our findings show that LLMs are already widely deployed within scam organizations, with 87% of scam labor consisting of systematized conversational tasks readily susceptible to automation. In a week-long study, an LLM agent not only elicited greater trust from study participants (p=0.007) but also achieved higher compliance with requests than human operators (46% vs. 18% for humans). Meanwhile, popular safety filters detected 0.0% of romance baiting dialogues. Together, these results suggest that romance-baiting scams may be amenable to full-scale LLM automation, while existing defenses remain inadequate to prevent their expansion.","authors":["Gilad Gressel","Rahul Pankajakshan","Shir Rozenfeld","Ling Li","Ivan Franceschini","Krishnashree Achuthan","Yisroel Mirsky"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19247v1","updated":"2025-12-22T10:29:51Z","published":"2025-12-22T10:29:51Z","title":"Auto-Prompting with Retrieval Guidance for Frame Detection in Logistics","summary":"Prompt engineering plays a critical role in adapting large language models (LLMs) to complex reasoning and labeling tasks without the need for extensive fine-tuning. In this paper, we propose a novel prompt optimization pipeline for frame detection in logistics texts, combining retrieval-augmented generation (RAG), few-shot prompting, chain-of-thought (CoT) reasoning, and automatic CoT synthesis (Auto-CoT) to generate highly effective task-specific prompts. Central to our approach is an LLM-based prompt optimizer agent that iteratively refines the prompts using retrieved examples, performance feedback, and internal self-evaluation. Our framework is evaluated on a real-world logistics text annotation task, where reasoning accuracy and labeling efficiency are critical. Experimental results show that the optimized prompts - particularly those enhanced via Auto-CoT and RAG - improve real-world inference accuracy by up to 15% compared to baseline zero-shot or static prompts. The system demonstrates consistent improvements across multiple LLMs, including GPT-4o, Qwen 2.5 (72B), and LLaMA 3.1 (70B), validating its generalizability and practical value. These findings suggest that structured prompt optimization is a viable alternative to full fine-tuning, offering scalable solutions for deploying LLMs in domain-specific NLP applications such as logistics.","authors":["Do Minh Duc","Quan Xuan Truong","Nguyen Tat Dat","Nguyen Van Vinh"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17452v2","updated":"2025-12-22T10:23:36Z","published":"2025-12-19T11:08:58Z","title":"Learning What to Write: Write-Gated KV for Efficient Long-Context Inference","summary":"Long-context LLM inference is bottlenecked by the quadratic attention complexity and linear KV cache growth. Prior approaches mitigate this via post-hoc selection or eviction but overlook the root inefficiency: indiscriminate writing to persistent memory. In this paper, we formalize KV cache management as a causal system of three primitives: KV Admission, Selection, and Eviction. We instantiate KV Admission via Write-Gated KV, a lightweight mechanism that learns to predict token utility before it enters the cache. By filtering out low-utility states early to maintain a compact global cache alongside a sliding local cache, Write-Gated KV reduces memory usage by 46-57% and delivers 3.03-3.45$\\times$ prefill and 1.89-2.56$\\times$ decode speedups on Llama model with negligible accuracy loss, all while remaining compatible with FlashAttention and paged-KV systems. These results demonstrate that learning what to write, is a principled and practical recipe for efficient long-context inference. Code is available at https://github.com/EMCLab-Sinica/WG-KV .","authors":["Yen-Chieh Huang","Pi-Cheng Hsiu","Rui Fang","Ming-Syan Chen"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2511.11646v2","updated":"2025-12-22T10:22:37Z","published":"2025-11-10T08:50:03Z","title":"What-If Decision Support for Product Line Extension Using Conditional Deep Generative Models","summary":"Product line extension is a strategically important managerial decision that requires anticipating how consumer segments and purchasing contexts may respond to hypothetical product designs that do not yet exist in the market. Such decisions are inherently uncertain because managers must infer future outcomes from historical purchase data without direct market observations. This study addresses this challenge by proposing a data-driven decision support framework that enables forward-looking what-if analysis based on historical transaction data. We introduce a Conditional Tabular Variational Autoencoder (CTVAE) that learns the conditional joint distribution of product attributes and consumer characteristics from large-scale tabular data. By conditioning the generative process on controllable design variables such as container type, volume, flavor, and calorie content, the proposed model generates synthetic consumer attribute distributions for hypothetical line-extended products. This enables systematic exploration of alternative design scenarios without costly market pretests. The framework is evaluated using home-scan panel data covering more than 20,000 consumers and 700 soft drink products. Empirical results show that the CTVAE outperforms existing tabular generative models in capturing conditional consumer attribute distributions. Simulation-based analyses further demonstrate that the generated synthetic data support knowledge-driven reasoning for assessing cannibalization risks and identifying potential target segments. These findings highlight the value of conditional deep generative models as core components of decision support systems for product line extension planning.","authors":["Yinxing Li","Tsukasa Ishigaki"],"pdf_url":"","comment":"25 pages"},{"id":"http://arxiv.org/abs/2512.19240v1","updated":"2025-12-22T10:21:40Z","published":"2025-12-22T10:21:40Z","title":"ChemATP: A Training-Free Chemical Reasoning Framework for Large Language Models","summary":"Large Language Models (LLMs) exhibit strong general reasoning but struggle in molecular science due to the lack of explicit chemical priors in standard string representations. Current solutions face a fundamental dilemma. Training-based methods inject priors into parameters, but this static coupling hinders rapid knowledge updates and often compromises the model's general reasoning capabilities. Conversely, existing training-free methods avoid these issues but rely on surface-level prompting, failing to provide the fine-grained atom-level priors essential for precise chemical reasoning. To address this issue, we introduce ChemATP, a framework that decouples chemical knowledge from the reasoning engine. By constructing the first atom-level textual knowledge base, ChemATP enables frozen LLMs to explicitly retrieve and reason over this information dynamically. This architecture ensures interpretability and adaptability while preserving the LLM's intrinsic general intelligence. Experiments show that ChemATP significantly outperforms training-free baselines and rivals state-of-the-art training-based models, demonstrating that explicit prior injection is a competitive alternative to implicit parameter updates.","authors":["Mingxu Zhang","Dazhong Shen","Qi Zhang","Ying Sun"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19238v1","updated":"2025-12-22T10:20:20Z","published":"2025-12-22T10:20:20Z","title":"Identifying Features Associated with Bias Against 93 Stigmatized Groups in Language Models and Guardrail Model Safety Mitigation","summary":"Large language models (LLMs) have been shown to exhibit social bias, however, bias towards non-protected stigmatized identities remain understudied. Furthermore, what social features of stigmas are associated with bias in LLM outputs is unknown. From psychology literature, it has been shown that stigmas contain six shared social features: aesthetics, concealability, course, disruptiveness, origin, and peril. In this study, we investigate if human and LLM ratings of the features of stigmas, along with prompt style and type of stigma, have effect on bias towards stigmatized groups in LLM outputs. We measure bias against 93 stigmatized groups across three widely used LLMs (Granite 3.0-8B, Llama-3.1-8B, Mistral-7B) using SocialStigmaQA, a benchmark that includes 37 social scenarios about stigmatized identities; for example deciding wether to recommend them for an internship. We find that stigmas rated by humans to be highly perilous (e.g., being a gang member or having HIV) have the most biased outputs from SocialStigmaQA prompts (60% of outputs from all models) while sociodemographic stigmas (e.g. Asian-American or old age) have the least amount of biased outputs (11%). We test if the amount of biased outputs could be decreased by using guardrail models, models meant to identify harmful input, using each LLM's respective guardrail model (Granite Guardian 3.0, Llama Guard 3.0, Mistral Moderation API). We find that bias decreases significantly by 10.4%, 1.4%, and 7.8%, respectively. However, we show that features with significant effect on bias remain unchanged post-mitigation and that guardrail models often fail to recognize the intent of bias in prompts. This work has implications for using LLMs in scenarios involving stigmatized groups and we suggest future work towards improving guardrail models for bias mitigation.","authors":["Anna-Maria Gueorguieva","Aylin Caliskan"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19234v1","updated":"2025-12-22T10:17:49Z","published":"2025-12-22T10:17:49Z","title":"DeliveryBench: Can Agents Earn Profit in Real World?","summary":"LLMs and VLMs are increasingly deployed as embodied agents, yet existing benchmarks largely revolve around simple short-term tasks and struggle to capture rich realistic constraints that shape real-world decision making. To close this gap, we propose DeliveryBench, a city-scale embodied benchmark grounded in the real-world profession of food delivery. Food couriers naturally operate under long-horizon objectives (maximizing net profit over hours) while managing diverse constraints, e.g., delivery deadline, transportation expense, vehicle battery, and necessary interactions with other couriers and customers. DeliveryBench instantiates this setting in procedurally generated 3D cities with diverse road networks, buildings, functional locations, transportation modes, and realistic resource dynamics, enabling systematic evaluation of constraint-aware, long-horizon planning. We benchmark a range of VLM-based agents across nine cities and compare them with human players. Our results reveal a substantial performance gap to humans, and find that these agents are short-sighted and frequently break basic commonsense constraints. Additionally, we observe distinct personalities across models (e.g., adventurous GPT-5 vs. conservative Claude), highlighting both the brittleness and the diversity of current VLM-based embodied agents in realistic, constraint-dense environments. Our code, data, and benchmark are available at https://deliverybench.github.io.","authors":["Lingjun Mao","Jiawei Ren","Kun Zhou","Jixuan Chen","Ziqiao Ma","Lianhui Qin"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19228v1","updated":"2025-12-22T10:08:25Z","published":"2025-12-22T10:08:25Z","title":"Generation of Programmatic Rules for Document Forgery Detection Using Large Language Models","summary":"Document forgery poses a growing threat to legal, economic, and governmental processes, requiring increasingly sophisticated verification mechanisms. One approach involves the use of plausibility checks, rule-based procedures that assess the correctness and internal consistency of data, to detect anomalies or signs of manipulation. Although these verification procedures are essential for ensuring data integrity, existing plausibility checks are manually implemented by software engineers, which is time-consuming. Recent advances in code generation with large language models (LLMs) offer new potential for automating and scaling the generation of these checks. However, adapting LLMs to the specific requirements of an unknown domain remains a significant challenge. This work investigates the extent to which LLMs, adapted on domain-specific code and data through different fine-tuning strategies, can generate rule-based plausibility checks for forgery detection on constrained hardware resources. We fine-tune open-source LLMs, Llama 3.1 8B and OpenCoder 8B, on structured datasets derived from real-world application scenarios and evaluate the generated plausibility checks on previously unseen forgery patterns. The results demonstrate that the models are capable of generating executable and effective verification procedures. This also highlights the potential of LLMs as scalable tools to support human decision-making in security-sensitive contexts where comprehensibility is required.","authors":["Valentin Schmidberger","Manuel Eberhardinger","Setareh Maghsudi","Johannes Maucher"],"pdf_url":"","comment":"Accepted at ICMLA 2025, the first two authors contributed equally"},{"id":"http://arxiv.org/abs/2512.19221v1","updated":"2025-12-22T10:02:53Z","published":"2025-12-22T10:02:53Z","title":"From Pixels to Predicates Structuring urban perception with scene graphs","summary":"Perception research is increasingly modelled using streetscapes, yet many approaches still rely on pixel features or object co-occurrence statistics, overlooking the explicit relations that shape human perception. This study proposes a three stage pipeline that transforms street view imagery (SVI) into structured representations for predicting six perceptual indicators. In the first stage, each image is parsed using an open-set Panoptic Scene Graph model (OpenPSG) to extract object predicate object triplets. In the second stage, compact scene-level embeddings are learned through a heterogeneous graph autoencoder (GraphMAE). In the third stage, a neural network predicts perception scores from these embeddings. We evaluate the proposed approach against image-only baselines in terms of accuracy, precision, and cross-city generalization. Results indicate that (i) our approach improves perception prediction accuracy by an average of 26% over baseline models, and (ii) maintains strong generalization performance in cross-city prediction tasks. Additionally, the structured representation clarifies which relational patterns contribute to lower perception scores in urban scenes, such as graffiti on wall and car parked on sidewalk. Overall, this study demonstrates that graph-based structure provides expressive, generalizable, and interpretable signals for modelling urban perception, advancing human-centric and context-aware urban analytics.","authors":["Yunlong Liu","Shuyang Li","Pengyuan Liu","Yu Zhang","Rudi Stouffs"],"pdf_url":"","comment":"10 pages, CAADRIA2026 presentation forthcoming"},{"id":"http://arxiv.org/abs/2512.19219v1","updated":"2025-12-22T10:02:10Z","published":"2025-12-22T10:02:10Z","title":"Towards Minimal Fine-Tuning of VLMs","summary":"We introduce Image-LoRA, a lightweight parameter efficient fine-tuning (PEFT) recipe for transformer-based vision-language models (VLMs). Image-LoRA applies low-rank adaptation only to the value path of attention layers within the visual-token span, reducing adapter-only training FLOPs roughly in proportion to the visual-token fraction. We further adapt only a subset of attention heads, selected using head influence scores estimated with a rank-1 Image-LoRA, and stabilize per-layer updates via selection-size normalization. Across screen-centric grounding and referring benchmarks spanning text-heavy to image-heavy regimes, Image-LoRA matches or closely approaches standard LoRA accuracy while using fewer trainable parameters and lower adapter-only training FLOPs. The method also preserves the pure-text reasoning performance of VLMs before and after fine-tuning, as further shown on GSM8K.","authors":["Tiange Luo","Lajanugen Logeswaran","Jaekyeom Kim","Justin Johnson","Honglak Lee"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.04618v2","updated":"2025-12-22T09:55:28Z","published":"2025-12-04T09:47:15Z","title":"Neural Decoding of Overt Speech from ECoG Using Vision Transformers and Contrastive Representation Learning","summary":"Speech Brain Computer Interfaces (BCIs) offer promising solutions to people with severe paralysis unable to communicate. A number of recent studies have demonstrated convincing reconstruction of intelligible speech from surface electrocorticographic (ECoG) or intracortical recordings by predicting a series of phonemes or words and using downstream language models to obtain meaningful sentences. A current challenge is to reconstruct speech in a streaming mode by directly regressing cortical signals into acoustic speech. While this has been achieved recently using intracortical data, further work is needed to obtain comparable results with surface ECoG recordings. In particular, optimizing neural decoders becomes critical in this case. Here we present an offline speech decoding pipeline based on an encoder-decoder deep neural architecture, integrating Vision Transformers and contrastive learning to enhance the direct regression of speech from ECoG signals. The approach is evaluated on two datasets, one obtained with clinical subdural electrodes in an epileptic patient, and another obtained with the fully implantable WIMAGINE epidural system in a participant of a motor BCI trial. To our knowledge this presents a first attempt to decode speech from a fully implantable and wireless epidural recording system offering perspectives for long-term use.","authors":["Mohamed Baha Ben Ticha","Xingchen Ran","Guillaume Saldanha","Gaël Le Godais","Philémon Roussel","Marc Aubert","Amina Fontanell","Thomas Costecalde","Lucas Struber","Serpil Karakas","Shaomin Zhang","Philippe Kahane","Guillaume Charvet","Stéphan Chabardès","Blaise Yvert"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2510.20647v2","updated":"2025-12-22T09:52:15Z","published":"2025-10-23T15:22:00Z","title":"The Reasoning Lingua Franca: A Double-Edged Sword for Multilingual AI","summary":"Large Reasoning Models (LRMs) achieve strong performance on mathematical, scientific, and other question-answering tasks, but their multilingual reasoning abilities remain underexplored. When presented with non-English questions, LRMs often default to reasoning in English, raising concerns about interpretability and the handling of linguistic and cultural nuances. We systematically compare an LRM's reasoning in English versus the language of the question. Our evaluation spans two tasks: MGSM and GPQA Diamond. Beyond measuring answer accuracy, we also analyze cognitive attributes in the reasoning traces. We find that English reasoning traces exhibit a substantially higher presence of these cognitive behaviors, and that reasoning in English generally yields higher final-answer accuracy, with the performance gap increasing as tasks become more complex. However, this English-centric strategy is susceptible to a key failure mode - getting \"Lost in Translation,\" where translation steps lead to errors that would have been avoided by question's language reasoning.","authors":["Alan Saji","Raj Dabre","Anoop Kunchukuttan","Ratish Puduppully"],"pdf_url":"","comment":"14 pages, 13 figures, 5 tables"},{"id":"http://arxiv.org/abs/2512.19210v1","updated":"2025-12-22T09:49:13Z","published":"2025-12-22T09:49:13Z","title":"Observer, Not Player: Simulating Theory of Mind in LLMs through Game Observation","summary":"We present an interactive framework for evaluating whether large language models (LLMs) exhibit genuine \"understanding\" in a simple yet strategic environment. As a running example, we focus on Rock-Paper-Scissors (RPS), which, despite its apparent simplicity, requires sequential reasoning, adaptation, and strategy recognition. Our system positions the LLM as an Observer whose task is to identify which strategies are being played and to articulate the reasoning behind this judgment. The purpose is not to test knowledge of Rock-Paper-Scissors itself, but to probe whether the model can exhibit mind-like reasoning about sequential behavior. To support systematic evaluation, we provide a benchmark consisting of both static strategies and lightweight dynamic strategies specified by well-prompted rules. We quantify alignment between the Observer's predictions and the ground-truth distributions induced by actual strategy pairs using three complementary signals: Cross-Entropy, Brier score, and Expected Value (EV) discrepancy. These metrics are further integrated into a unified score, the Union Loss, which balances calibration, sensitivity, and payoff alignment. Together with a Strategy Identification Rate (SIR) metric, our framework captures not only predictive accuracy but also whether the model can stably identify the latent strategies in play. The demo emphasizes interactivity, transparency, and reproducibility. Users can adjust LLM distributions in real time, visualize losses as they evolve, and directly inspect reasoning snippets to identify where and why failures occur. In doing so, our system provides a practical and interpretable proxy for mind-like inference in sequential games, offering insights into both the strengths and limitations of current LLM reasoning.","authors":["Jerry Wang","Ting Yiu Liu"],"pdf_url":"","comment":"Accepted at NeurIPS Workshop on Foundations of Reasoning in Language Models and Workshop on Bridging Language, Agent, and World Model"},{"id":"http://arxiv.org/abs/2512.19206v1","updated":"2025-12-22T09:44:26Z","published":"2025-12-22T09:44:26Z","title":"MixKVQ: Query-Aware Mixed-Precision KV Cache Quantization for Long-Context Reasoning","summary":"Long Chain-of-Thought (CoT) reasoning has significantly advanced the capabilities of Large Language Models (LLMs), but this progress is accompanied by substantial memory and latency overhead from the extensive Key-Value (KV) cache. Although KV cache quantization is a promising compression technique, existing low-bit quantization methods often exhibit severe performance degradation on complex reasoning tasks. Fixed-precision quantization struggles to handle outlier channels in the key cache, while current mixed-precision strategies fail to accurately identify components requiring high-precision representation. We find that an effective low-bit KV cache quantization strategy must consider two factors: a key channel's intrinsic quantization difficulty and its relevance to the query. Based on this insight, we propose MixKVQ, a novel plug-and-play method that introduces a lightweight, query-aware algorithm to identify and preserve critical key channels that need higher precision, while applying per-token quantization for value cache. Experiments on complex reasoning datasets demonstrate that our approach significantly outperforms existing low-bit methods, achieving performance comparable to a full-precision baseline at a substantially reduced memory footprint.","authors":["Tao Zhang","Ziqian Zeng","Hao Peng","Huiping Zhuang","Cen Chen"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2505.08438v3","updated":"2025-12-22T09:39:55Z","published":"2025-05-13T11:04:04Z","title":"A Survey of 3D Reconstruction with Event Cameras","summary":"Event cameras are rapidly emerging as powerful vision sensors for 3D reconstruction, uniquely capable of asynchronously capturing per-pixel brightness changes. Compared to traditional frame-based cameras, event cameras produce sparse yet temporally dense data streams, enabling robust and accurate 3D reconstruction even under challenging conditions such as high-speed motion, low illumination, and extreme dynamic range scenarios. These capabilities offer substantial promise for transformative applications across various fields, including autonomous driving, robotics, aerial navigation, and immersive virtual reality. In this survey, we present the first comprehensive review exclusively dedicated to event-based 3D reconstruction. Existing approaches are systematically categorised based on input modality into stereo, monocular, and multimodal systems, and further classified according to reconstruction methodologies, including geometry-based techniques, deep learning approaches, and neural rendering techniques such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS). Within each category, methods are chronologically organised to highlight the evolution of key concepts and advancements. Furthermore, we provide a detailed summary of publicly available datasets specifically suited to event-based reconstruction tasks. Finally, we discuss significant open challenges in dataset availability, standardised evaluation, effective representation, and dynamic scene reconstruction, outlining insightful directions for future research. This survey aims to serve as an essential reference and provides a clear and motivating roadmap toward advancing the state of the art in event-driven 3D reconstruction.","authors":["Chuanzhi Xu","Haoxian Zhou","Langyi Chen","Haodong Chen","Zeke Zexi Hu","Zhicheng Lu","Ying Zhou","Vera Chung","Qiang Qu","Weidong Cai"],"pdf_url":"","comment":"This survey has been accepted for publication in the Computational Visual Media Journal"},{"id":"http://arxiv.org/abs/2505.11792v3","updated":"2025-12-22T09:39:43Z","published":"2025-05-17T02:32:03Z","title":"Solver-Informed RL: Grounding Large Language Models for Authentic Optimization Modeling","summary":"Optimization modeling is fundamental to decision-making across diverse domains. Despite progress in automating optimization formulation from natural language descriptions, Large Language Models (LLMs) often struggle to generate formally correct and usable models against hallucinations, posing a challenge for reliable automation. Inspired by the success of Reinforcement Learning (RL) in enhancing Large Reasoning Models, we present Solver-Informed Reinforcement Learning (SIRL), a novel framework that significantly improves the authenticity of LLMs for optimization modeling using Reinforcement Learning with Verifiable Reward by leveraging external optimization solvers as verifiers. These verifiers automatically assess the executable code and the instance-level mathematical model represented by the associated LP file, yielding precise and comprehensive feedback signals -- including syntax, feasibility, and solution quality, serving as direct rewards for the RL process. This automated verification process, particularly from classic optimization solvers, also underpins our instance-enhanced self-consistency method to synthesize high-quality training data. Extensive experiments on diverse public benchmarks demonstrate that SIRL achieves state-of-the-art performance, substantially outperforming existing methods in generating accurate and executable optimization models. Our code is publicly available at https://github.com/Cardinal-Operations/SIRL.","authors":["Yitian Chen","Jingfan Xia","Siyu Shao","Dongdong Ge","Yinyu Ye"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19199v1","updated":"2025-12-22T09:36:24Z","published":"2025-12-22T09:36:24Z","title":"On the Koopman-Based Generalization Bounds for Multi-Task Deep Learning","summary":"The paper establishes generalization bounds for multitask deep neural networks using operator-theoretic techniques. The authors propose a tighter bound than those derived from conventional norm based methods by leveraging small condition numbers in the weight matrices and introducing a tailored Sobolev space as an expanded hypothesis space. This enhanced bound remains valid even in single output settings, outperforming existing Koopman based bounds. The resulting framework maintains key advantages such as flexibility and independence from network width, offering a more precise theoretical understanding of multitask deep learning in the context of kernel methods.","authors":["Mahdi Mohammadigohari","Giuseppe Di Fatta","Giuseppe Nicosia","Panos M. Pardalos"],"pdf_url":"","comment":"Accepted for publication in Lecture Notes in Computer Science (LNCS). Final version forthcoming"},{"id":"http://arxiv.org/abs/2512.19184v1","updated":"2025-12-22T09:18:30Z","published":"2025-12-22T09:18:30Z","title":"Operator-Based Generalization Bound for Deep Learning: Insights on Multi-Task Learning","summary":"This paper presents novel generalization bounds for vector-valued neural networks and deep kernel methods, focusing on multi-task learning through an operator-theoretic framework. Our key development lies in strategically combining a Koopman based approach with existing techniques, achieving tighter generalization guarantees compared to traditional norm-based bounds. To mitigate computational challenges associated with Koopman-based methods, we introduce sketching techniques applicable to vector valued neural networks. These techniques yield excess risk bounds under generic Lipschitz losses, providing performance guarantees for applications including robust and multiple quantile regression. Furthermore, we propose a novel deep learning framework, deep vector-valued reproducing kernel Hilbert spaces (vvRKHS), leveraging Perron Frobenius (PF) operators to enhance deep kernel methods. We derive a new Rademacher generalization bound for this framework, explicitly addressing underfitting and overfitting through kernel refinement strategies. This work offers novel insights into the generalization properties of multitask learning with deep learning architectures, an area that has been relatively unexplored until recent developments.","authors":["Mahdi Mohammadigohari","Giuseppe Di Fatta","Giuseppe Nicosia","Panos M. Pardalos"],"pdf_url":"","comment":"Accepted for publication in Lecture Notes in Computer Science (LNCS). Final version forthcoming"},{"id":"http://arxiv.org/abs/2512.17629v2","updated":"2025-12-22T09:18:11Z","published":"2025-12-19T14:33:02Z","title":"SCOPE: Sequential Causal Optimization of Process Interventions","summary":"Prescriptive Process Monitoring (PresPM) recommends interventions during business processes to optimize key performance indicators (KPIs). In realistic settings, interventions are rarely isolated: organizations need to align sequences of interventions to jointly steer the outcome of a case. Existing PresPM approaches fall short in this respect. Many focus on a single intervention decision, while others treat multiple interventions independently, ignoring how they interact over time. Methods that do address these dependencies depend either on simulation or data augmentation to approximate the process to train a Reinforcement Learning (RL) agent, which can create a reality gap and introduce bias. We introduce SCOPE, a PresPM approach that learns aligned sequential intervention recommendations. SCOPE employs backward induction to estimate the effect of each candidate intervention action, propagating its impact from the final decision point back to the first. By leveraging causal learners, our method can utilize observational data directly, unlike methods that require constructing process approximations for reinforcement learning. Experiments on both an existing synthetic dataset and a new semi-synthetic dataset show that SCOPE consistently outperforms state-of-the-art PresPM techniques in optimizing the KPI. The novel semi-synthetic setup, based on a real-life event log, is provided as a reusable benchmark for future work on sequential PresPM.","authors":["Jakob De Moor","Hans Weytjens","Johannes De Smedt","Jochen De Weerdt"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2505.09166v5","updated":"2025-12-22T09:17:43Z","published":"2025-05-14T05:59:23Z","title":"An Exploration of Default Images in Text-to-Image Generation","summary":"In the creative practice of text-to-image (TTI) generation, images are synthesized from textual prompts. By design, TTI models always yield an output, even if the prompt contains unknown terms. In this case, the model may generate default images: images that closely resemble each other across many unrelated prompts. Studying default images is valuable for designing better solutions for prompt engineering and TTI generation. We present the first investigation into default images on Midjourney. We describe an initial study in which we manually created input prompts triggering default images, and several ablation studies. Building on these, we conduct a computational analysis of over 750,000 images, revealing consistent default images across unrelated prompts. We also conduct an online user study investigating how default images may affect user satisfaction. Our work lays the foundation for understanding default images in TTI generation, highlighting their practical relevance as well as challenges and future research directions.","authors":["Hannu Simonen","Atte Kiviniemi","Hannah Johnston","Helena Barranha","Jonas Oppenlaender"],"pdf_url":"","comment":"31 pages, 10 figures"},{"id":"http://arxiv.org/abs/2512.19180v1","updated":"2025-12-22T09:16:08Z","published":"2025-12-22T09:16:08Z","title":"Practical Quantum-Classical Feature Fusion for complex data Classification","summary":"Hybrid quantum and classical learning aims to couple quantum feature maps with the robustness of classical neural networks, yet most architectures treat the quantum circuit as an isolated feature extractor and merge its measurements with classical representations by direct concatenation. This neglects that the quantum and classical branches constitute distinct computational modalities and limits reliable performance on complex, high dimensional tabular and semi structured data, including remote sensing, environmental monitoring, and medical diagnostics. We present a multimodal formulation of hybrid learning and propose a cross attention mid fusion architecture in which a classical representation queries quantum derived feature tokens through an attention block with residual connectivity. The quantum branch is kept within practical NISQ budgets and uses up to nine qubits. We evaluate on Wine, Breast Cancer, Forest CoverType, FashionMNIST, and SteelPlatesFaults, comparing a quantum only model, a classical baseline, residual hybrid models, and the proposed mid fusion model under a consistent protocol. Pure quantum and standard hybrid designs underperform due to measurement induced information loss, while cross attention mid fusion is consistently competitive and improves performance on the more complex datasets in most cases. These findings suggest that quantum derived information becomes most valuable when integrated through principled multimodal fusion rather than used in isolation or loosely appended to classical features.","authors":["Azadeh Alavi","Fatemeh Kouchmeshki","Abdolrahman Alavi"],"pdf_url":"","comment":"16 pages, 3 figues"},{"id":"http://arxiv.org/abs/2512.19178v1","updated":"2025-12-22T09:12:48Z","published":"2025-12-22T09:12:48Z","title":"Vision-Language-Policy Model for Dynamic Robot Task Planning","summary":"Bridging the gap between natural language commands and autonomous execution in unstructured environments remains an open challenge for robotics. This requires robots to perceive and reason over the current task scene through multiple modalities, and to plan their behaviors to achieve their intended goals. Traditional robotic task-planning approaches often struggle to bridge low-level execution with high-level task reasoning, and cannot dynamically update task strategies when instructions change during execution, which ultimately limits their versatility and adaptability to new tasks. In this work, we propose a novel language model-based framework for dynamic robot task planning. Our Vision-Language-Policy (VLP) model, based on a vision-language model fine-tuned on real-world data, can interpret semantic instructions and integrate reasoning over the current task scene to generate behavior policies that control the robot to accomplish the task. Moreover, it can dynamically adjust the task strategy in response to changes in the task, enabling flexible adaptation to evolving task requirements. Experiments conducted with different robots and a variety of real-world tasks show that the trained model can efficiently adapt to novel scenarios and dynamically update its policy, demonstrating strong planning autonomy and cross-embodiment generalization. Videos: https://robovlp.github.io/","authors":["Jin Wang","Kim Tien Ly","Jacques Cloete","Nikos Tsagarakis","Ioannis Havoutis"],"pdf_url":"","comment":"Manuscript under review"},{"id":"http://arxiv.org/abs/2512.19155v1","updated":"2025-12-22T08:52:07Z","published":"2025-12-22T08:52:07Z","title":"Can We Test Consciousness Theories on AI? Ablations, Markers, and Robustness","summary":"The search for reliable indicators of consciousness has fragmented into competing theoretical camps (Global Workspace Theory (GWT), Integrated Information Theory (IIT), and Higher-Order Theories (HOT)), each proposing distinct neural signatures. We adopt a synthetic neuro-phenomenology approach: constructing artificial agents that embody these mechanisms to test their functional consequences through precise architectural ablations impossible in biological systems. Across three experiments, we report dissociations suggesting these theories describe complementary functional layers rather than competing accounts. In Experiment 1, a no-rewire Self-Model lesion abolishes metacognitive calibration while preserving first-order task performance, yielding a synthetic blindsight analogue consistent with HOT predictions. In Experiment 2, workspace capacity proves causally necessary for information access: a complete workspace lesion produces qualitative collapse in access-related markers, while partial reductions show graded degradation, consistent with GWT's ignition framework. In Experiment 3, we uncover a broadcast-amplification effect: GWT-style broadcasting amplifies internal noise, creating extreme fragility. The B2 agent family is robust to the same latent perturbation; this robustness persists in a Self-Model-off / workspace-read control, cautioning against attributing the effect solely to $z_{\\text{self}}$ compression. We also report an explicit negative result: raw perturbational complexity (PCI-A) decreases under the workspace bottleneck, cautioning against naive transfer of IIT-adjacent proxies to engineered agents. These results suggest a hierarchical design principle: GWT provides broadcast capacity, while HOT provides quality control. We emphasize that our agents are not conscious; they are reference implementations for testing functional predictions of consciousness theories.","authors":["Yin Jun Phua"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19154v1","updated":"2025-12-22T08:50:30Z","published":"2025-12-22T08:50:30Z","title":"Beyond Sliding Windows: Learning to Manage Memory in Non-Markovian Environments","summary":"Recent success in developing increasingly general purpose agents based on sequence models has led to increased focus on the problem of deploying computationally limited agents within the vastly more complex real-world. A key challenge experienced in these more realistic domains is highly non-Markovian dependencies with respect to the agent's observations, which are less common in small controlled domains. The predominant approach for dealing with this in the literature is to stack together a window of the most recent observations (Frame Stacking), but this window size must grow with the degree of non-Markovian dependencies, which results in prohibitive computational and memory requirements for both action inference and learning. In this paper, we are motivated by the insight that in many environments that are highly non-Markovian with respect to time, the environment only causally depends on a relatively small number of observations over that time-scale. A natural direction would then be to consider meta-algorithms that maintain relatively small adaptive stacks of memories such that it is possible to express highly non-Markovian dependencies with respect to time while considering fewer observations at each step and thus experience substantial savings in both compute and memory requirements. Hence, we propose a meta-algorithm (Adaptive Stacking) for achieving exactly that with convergence guarantees and quantify the reduced computation and memory constraints for MLP, LSTM, and Transformer-based agents. Our experiments utilize popular memory tasks, which give us control over the degree of non-Markovian dependencies. This allows us to demonstrate that an appropriate meta-algorithm can learn the removal of memories not predictive of future rewards without excessive removal of important experiences. Code: https://github.com/geraudnt/adaptive-stacking","authors":["Geraud Nangue Tasse","Matthew Riemer","Benjamin Rosman","Tim Klinger"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2511.09392v4","updated":"2025-12-22T08:42:42Z","published":"2025-11-12T15:00:52Z","title":"Potent but Stealthy: Rethink Profile Pollution against Sequential Recommendation via Bi-level Constrained Reinforcement Paradigm","summary":"Sequential Recommenders, which exploit dynamic user intents through interaction sequences, is vulnerable to adversarial attacks. While existing attacks primarily rely on data poisoning, they require large-scale user access or fake profiles thus lacking practicality. In this paper, we focus on the Profile Pollution Attack that subtly contaminates partial user interactions to induce targeted mispredictions. Previous PPA methods suffer from two limitations, i.e., i) over-reliance on sequence horizon impact restricts fine-grained perturbations on item transitions, and ii) holistic modifications cause detectable distribution shifts. To address these challenges, we propose a constrained reinforcement driven attack CREAT that synergizes a bi-level optimization framework with multi-reward reinforcement learning to balance adversarial efficacy and stealthiness. We first develop a Pattern Balanced Rewarding Policy, which integrates pattern inversion rewards to invert critical patterns and distribution consistency rewards to minimize detectable shifts via unbalanced co-optimal transport. Then we employ a Constrained Group Relative Reinforcement Learning paradigm, enabling step-wise perturbations through dynamic barrier constraints and group-shared experience replay, achieving targeted pollution with minimal detectability. Extensive experiments demonstrate the effectiveness of CREAT.","authors":["Jiajie Su","Zihan Nan","Yunshan Ma","Xiaobo Xia","Xiaohua Feng","Weiming Liu","Xiang Chen","Xiaolin Zheng","Chaochao Chen"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19135v1","updated":"2025-12-22T08:28:08Z","published":"2025-12-22T08:28:08Z","title":"Understanding Chain-of-Thought in Large Language Models via Topological Data Analysis","summary":"With the development of large language models (LLMs), particularly with the introduction of the long reasoning chain technique, the reasoning ability of LLMs in complex problem-solving has been significantly enhanced. While acknowledging the power of long reasoning chains, we cannot help but wonder: Why do different reasoning chains perform differently in reasoning? What components of the reasoning chains play a key role? Existing studies mainly focus on evaluating reasoning chains from a functional perspective, with little attention paid to their structural mechanisms. To address this gap, this work is the first to analyze and evaluate the quality of the reasoning chain from a structural perspective. We apply persistent homology from Topological Data Analysis (TDA) to map reasoning steps into semantic space, extract topological features, and analyze structural changes. These changes reveal semantic coherence, logical redundancy, and identify logical breaks and gaps. By calculating homology groups, we assess connectivity and redundancy at various scales, using barcode and persistence diagrams to quantify stability and consistency. Our results show that the topological structural complexity of reasoning chains correlates positively with accuracy. More complex chains identify correct answers sooner, while successful reasoning exhibits simpler topologies, reducing redundancy and cycles, enhancing efficiency and interpretability. This work provides a new perspective on reasoning chain quality assessment and offers guidance for future optimization.","authors":["Chenghao Li","Chaoning Zhang","Yi Lu","Shuxu Chen","Xudong Wang","Jiaquan Zhang","Zhicheng Wang","Zhengxun Jin","Kuien Liu","Sung-Ho Bae","Guoqing Wang","Yang Yang","Hen Tao Shen"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.14806v4","updated":"2025-12-22T08:18:12Z","published":"2025-12-16T18:51:23Z","title":"Let the Barbarians In: How AI Can Accelerate Systems Performance Research","summary":"Artificial Intelligence (AI) is beginning to transform the research process by automating the discovery of new solutions. This shift depends on the availability of reliable verifiers, which AI-driven approaches require to validate candidate solutions. Research focused on improving systems performance is especially well-suited to this paradigm because system performance problems naturally admit such verifiers: candidates can be implemented in real systems or simulators and evaluated against predefined workloads. We term this iterative cycle of generation, evaluation, and refinement AI-Driven Research for Systems (ADRS). Using several open-source ADRS instances (i.e., OpenEvolve, GEPA, and ShinkaEvolve), we demonstrate across ten case studies (e.g., multi-region cloud scheduling, mixture-of-experts load balancing, LLM-based SQL, transaction scheduling) that ADRS-generated solutions can match or even outperform human state-of-the-art designs. Based on these findings, we outline best practices (e.g., level of prompt specification, amount of feedback, robust evaluation) for effectively using ADRS, and we discuss future research directions and their implications. Although we do not yet have a universal recipe for applying ADRS across all of systems research, we hope our preliminary findings, together with the challenges we identify, offer meaningful guidance for future work as researcher effort shifts increasingly toward problem formulation and strategic oversight.\n  Note: This paper is an extension of our prior work [14]. It adds extensive evaluation across multiple ADRS frameworks and provides deeper analysis and insights into best practices.","authors":["Audrey Cheng","Shu Liu","Melissa Pan","Zhifei Li","Shubham Agarwal","Mert Cemri","Bowen Wang","Alexander Krentsel","Tian Xia","Jongseok Park","Shuo Yang","Jeff Chen","Lakshya Agrawal","Ashwin Naren","Shulu Li","Ruiying Ma","Aditya Desai","Jiarong Xing","Koushik Sen","Matei Zaharia","Ion Stoica"],"pdf_url":"","comment":"arXiv admin note: substantial text overlap with arXiv:2510.06189"},{"id":"http://arxiv.org/abs/2512.13142v3","updated":"2025-12-22T07:53:31Z","published":"2025-12-15T09:50:00Z","title":"Can AI Understand What We Cannot Say? Measuring Multilevel Alignment Through Abortion Stigma Across Cognitive, Interpersonal, and Structural Levels","summary":"As large language models increasingly mediate stigmatized health decisions, their capacity to genuinely understand complex psychological and physiological phenomena remains poorly evaluated. Can AI understand what we cannot say? We investigate whether LLMs coherently represent abortion stigma across the cognitive, interpersonal, and structural levels where it operates. We systematically tested 627 demographically diverse personas across five leading LLMs using the validated Individual Level Abortion Stigma Scale (ILAS). Our multilevel analysis examined whether models coherently represent stigma at the cognitive level (self-judgment), interpersonal level (worries about judgment and isolation), and structural level (community condemnation and disclosure patterns), as well as overall stigma. Models fail tests of genuine understanding across all levels. They overestimate interpersonal stigma while underestimating cognitive stigma, assume uniform community condemnation, introduce demographic biases absent from human validation data, miss the empirically validated stigma-secrecy relationship, and contradict themselves within theoretical constructs. These patterns reveal that current alignment approaches ensure appropriate language but not coherent multilevel understanding. This work provides empirical evidence that current LLMs lack coherent multilevel understanding of psychological and physiological constructs. AI safety in high-stakes contexts demands new approaches to design (multilevel coherence), evaluation (continuous auditing), governance and regulation (mandatory audits, accountability, deployment restrictions), and AI literacy in domains where understanding what people cannot say determines whether support helps or harms.","authors":["Anika Sharma","Malavika Mampally","Chidaksh Ravuru","Kandyce Brennan","Neil Gaikwad"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2506.11712v3","updated":"2025-12-22T07:47:22Z","published":"2025-06-13T12:29:15Z","title":"Mitigating Hallucination Through Theory-Consistent Symmetric Multimodal Preference Optimization","summary":"Direct Preference Optimization (DPO) has emerged as an effective approach for mitigating hallucination in Multimodal Large Language Models (MLLMs). Although existing methods have achieved significant progress by utilizing vision-oriented contrastive objectives for enhancing MLLMs' attention to visual inputs and hence reducing hallucination, they suffer from non-rigorous optimization objective function and indirect preference supervision. To address these limitations, we propose a Symmetric Multimodal Preference Optimization (SymMPO), which conducts symmetric preference learning with direct preference supervision (i.e., response pairs) for visual understanding enhancement, while maintaining rigorous theoretical alignment with standard DPO. In addition to conventional ordinal preference learning, SymMPO introduces a preference margin consistency loss to quantitatively regulate the preference gap between symmetric preference pairs. Comprehensive evaluation across five benchmarks demonstrate SymMPO's superior performance, validating its effectiveness in hallucination mitigation of MLLMs.","authors":["Wenqi Liu","Xuemeng Song","Jiaxi Li","Yinwei Wei","Na Zheng","Jianhua Yin","Liqiang Nie"],"pdf_url":"","comment":"NeurIPS 2025"},{"id":"http://arxiv.org/abs/2512.19114v1","updated":"2025-12-22T07:35:16Z","published":"2025-12-22T07:35:16Z","title":"HyperLoad: A Cross-Modality Enhanced Large Language Model-Based Framework for Green Data Center Cooling Load Prediction","summary":"The rapid growth of artificial intelligence is exponentially escalating computational demand, inflating data center energy use and carbon emissions, and spurring rapid deployment of green data centers to relieve resource and environmental stress. Achieving sub-minute orchestration of renewables, storage, and loads, while minimizing PUE and lifecycle carbon intensity, hinges on accurate load forecasting. However, existing methods struggle to address small-sample scenarios caused by cold start, load distortion, multi-source data fragmentation, and distribution shifts in green data centers. We introduce HyperLoad, a cross-modality framework that exploits pre-trained large language models (LLMs) to overcome data scarcity. In the Cross-Modality Knowledge Alignment phase, textual priors and time-series data are mapped to a common latent space, maximizing the utility of prior knowledge. In the Multi-Scale Feature Modeling phase, domain-aligned priors are injected through adaptive prefix-tuning, enabling rapid scenario adaptation, while an Enhanced Global Interaction Attention mechanism captures cross-device temporal dependencies. The public DCData dataset is released for benchmarking. Under both data sufficient and data scarce settings, HyperLoad consistently surpasses state-of-the-art (SOTA) baselines, demonstrating its practicality for sustainable green data center management.","authors":["Haoyu Jiang","Boan Qu","Junjie Zhu","Fanjie Zeng","Xiaojie Lin","Wei Zhong"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19107v1","updated":"2025-12-22T07:21:07Z","published":"2025-12-22T07:21:07Z","title":"FC-MIR: A Mobile Screen Awareness Framework for Intent-Aware Recommendation based on Frame-Compressed Multimodal Trajectory Reasoning","summary":"Identifying user intent from mobile UI operation trajectories is critical for advancing UI understanding and enabling task automation agents. While Multimodal Large Language Models (MLLMs) excel at video understanding tasks, their real-time mobile deployment is constrained by heavy computational costs and inefficient redundant frame processing. To address these issues, we propose the FC-MIR framework: leveraging keyframe sampling and adaptive concatenation, it cuts visual redundancy to boost inference efficiency, while integrating state-of-the-art closed-source MLLMs or fine-tuned models (e.g., Qwen3-VL) for trajectory summarization and intent prediction. We further expand task scope to explore generating post-prediction operations and search suggestions, and introduce a fine-grained metric to evaluate the practical utility of summaries, predictions, and suggestions. For rigorous assessment, we construct a UI trajectory dataset covering scenarios from UI-Agents (Agent-I) and real user interactions (Person-I). Experimental results show our compression method retains performance at 50%-60% compression rates; both closed-source and fine-tuned MLLMs demonstrate strong intent summarization, supporting potential lightweight on-device deployment. However, MLLMs still struggle with useful and \"surprising\" suggestions, leaving room for improvement. Finally, we deploy the framework in a real-world setting, integrating UI perception and UI-Agent proxies to lay a foundation for future progress in this field.","authors":["Zhe Yang","Xiaoshuang Sheng","Zhengnan Zhang","Jidong Wu","Zexing Wang","Xin He","Shenghua Xu","Guanjing Xiong"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2509.25300v3","updated":"2025-12-22T07:20:59Z","published":"2025-09-29T17:10:35Z","title":"Scaling Behaviors of LLM Reinforcement Learning Post-Training: An Empirical Study in Mathematical Reasoning","summary":"While scaling laws for large language models (LLMs) during pre-training have been extensively studied, their behavior under reinforcement learning (RL) post-training remains largely unexplored. This paper presents a systematic empirical investigation of scaling behaviors in RL-based post-training, with a particular focus on mathematical reasoning. Based on a set of experiments across the full Qwen2.5 dense model series (0.5B to 72B), we characterize how model scale, data volume, and computational budget interact to shape performance. Our analysis leads to four key findings: 1. Larger models consistently exhibit superior learning efficiency on both compute and data metrics. 2. The relationship between test loss, compute, and data can be modeled by a predictive power-law which is robust across both base and instruction-tuned models. 3. Although larger models exhibit higher learning efficiency, the analytical learning efficiency term k(N) in the power-law reveals a latent saturation trend in learning efficiency as model size continues to increase. 4. In data-constrained regimes, repeated reuse of high-quality data proves highly effective, as final performance is primarily governed by the total number of optimization steps rather than the uniqueness of samples. Collectively, these results provide a principled foundation and practical guidelines for efficiently scaling the reasoning capabilities of LLMs through RL post-training.","authors":["Zelin Tan","Hejia Geng","Xiaohang Yu","Mulei Zhang","Guancheng Wan","Yifan Zhou","Qiang He","Xiangyuan Xue","Heng Zhou","Yutao Fan","Zhongzhi Li","Zaibin Zhang","Guibin Zhang","Chen Zhang","Zhenfei Yin","Philip Torr","Lei Bai"],"pdf_url":"","comment":"V3 version:27 pages, 14 figures, add code and dataset url"},{"id":"http://arxiv.org/abs/2507.02314v4","updated":"2025-12-22T07:17:17Z","published":"2025-07-03T04:54:37Z","title":"MAGIC: Few-Shot Mask-Guided Anomaly Inpainting with Prompt Perturbation, Spatially Adaptive Guidance, and Context Awareness","summary":"Few-shot anomaly generation is a key challenge in industrial quality control. Although diffusion models are promising, existing methods struggle: global prompt-guided approaches corrupt normal regions, and existing inpainting-based methods often lack the in-distribution diversity essential for robust downstream models. We propose MAGIC, a fine-tuned inpainting framework that generates high-fidelity anomalies that strictly adhere to the mask while maximizing this diversity. MAGIC introduces three complementary components: (i) Gaussian prompt perturbation, which prevents model overfitting in the few-shot setting by learning and sampling from a smooth manifold of realistic anomalies, (ii) spatially adaptive guidance that applies distinct guidance strengths to the anomaly and background regions, and (iii) context-aware mask alignment to relocate masks for plausible placement within the host object. Under consistent identical evaluation protocol, MAGIC outperforms state-of-the-art methods on diverse anomaly datasets in downstream tasks","authors":["JaeHyuck Choi","MinJun Kim","Je Hyeong Hong"],"pdf_url":"","comment":"46 pages, 47 figures. Code: https://github.com/SpatialAILab/MAGIC-Anomaly-generation"},{"id":"http://arxiv.org/abs/2512.19097v1","updated":"2025-12-22T07:07:43Z","published":"2025-12-22T07:07:43Z","title":"DIVER-1 : Deep Integration of Vast Electrophysiological Recordings at Scale","summary":"Electrophysiology signals such as EEG and iEEG are central to neuroscience, brain-computer interfaces, and clinical applications, yet existing foundation models remain limited in scale despite clear evidence that scaling improves performance. We introduce DIVER-1, a family of EEG and iEEG foundation models trained on the largest and most diverse corpus to date-5.3k hours of iEEG and 54k hours of EEG (1.6M channel-hours from over 17.7k subjects)-and scaled up to 1.82B parameters. We present the first systematic scaling law analysis for this domain, showing that they follow data-constrained scaling laws: for a given amount of data and compute, smaller models trained for extended epochs consistently outperform larger models trained briefly. This behavior contrasts with prior electrophysiology foundation models that emphasized model size over training duration. To achieve strong performance, we also design architectural innovations including any-variate attention, sliding temporal conditional positional encoding, and multi-domain reconstruction. DIVER-1 iEEG and EEG models each achieve state-of-the-art performance on their respective benchmarks, establishing a concrete guidelines for efficient scaling and resource allocation in electrophysiology foundation model development.","authors":["Danny Dongyeop Han","Yonghyeon Gwon","Ahhyun Lucy Lee","Taeyang Lee","Seong Jin Lee","Jubin Choi","Sebin Lee","Jihyun Bang","Seungju Lee","David Keetae Park","Shinjae Yoo","Chun Kee Chung","Jiook Cha"],"pdf_url":"","comment":"47 pages, 13 figures, 26 tables"},{"id":"http://arxiv.org/abs/2512.19096v1","updated":"2025-12-22T07:07:34Z","published":"2025-12-22T07:07:34Z","title":"Conditioning Accept-Desirability models in the context of AGM-like belief change","summary":"We discuss conditionalisation for Accept-Desirability models in an abstract decision-making framework, where uncertain rewards live in a general linear space, and events are special projection operators on that linear space. This abstract setting allows us to unify classical and quantum probabilities, and extend them to an imprecise probabilities context. We introduce a new conditioning rule for our Accept-Desirability models, based on the idea that observing an event introduces new indifferences between options. We associate a belief revision operator with our conditioning rule, and investigate which of the AGM axioms for belief revision still hold in our more general framework. We investigate two interesting special cases where all of these axioms are shown to still hold: classical propositional logic and full conditional probabilities.","authors":["Kathelijne Coussement","Gert de Cooman","Keano De Vos"],"pdf_url":"","comment":"46 pages, 1 table"},{"id":"http://arxiv.org/abs/2512.19093v1","updated":"2025-12-22T07:02:16Z","published":"2025-12-22T07:02:16Z","title":"Tool-Augmented Hybrid Ensemble Reasoning with Distillation for Bilingual Mathematical Problem Solving","summary":"Bilingual mathematical problem solving needs a clear link between language reasoning and symbolic calculation. Large language models often handle language well but are weak in accurate computation. This paper presents HERALD (Hybrid Ensemble Reasoning with Adaptive Learning and Distillation), a framework that joins reasoning and calculation using NuminaMath-7B-TIR, GPT-4o, and Mistral-7B. HERALD uses adaptive routing, tool-based reinforcement learning, and knowledge distillation to connect different reasoning paths. Confidence calibration keeps weighting stable, and dual-path checking keeps results correct. Reinforcement learning controls tool use to cut redundancy, and distillation lowers delay without hurting accuracy. The system shows that combining symbolic checking, adaptive ensembles, and bilingual fine-tuning helps achieve both fluent reasoning and precise calculation. HERALD offers a practical solution for multilingual mathematical reasoning with better accuracy, stability, and clarity.","authors":["Peiqing Lu","Yuan Zhang","Haoyun Zhang","Jiasen Zheng","Kejian Tong","Wenjun Wu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2511.17582v3","updated":"2025-12-22T06:51:54Z","published":"2025-11-15T17:55:47Z","title":"GateRA: Token-Aware Modulation for Parameter-Efficient Fine-Tuning","summary":"Parameter-efficient fine-tuning (PEFT) methods, such as LoRA, DoRA, and HiRA, enable lightweight adaptation of large pre-trained models via low-rank updates. However, existing PEFT approaches apply static, input-agnostic updates to all tokens, disregarding the varying importance and difficulty of different inputs. This uniform treatment can lead to overfitting on trivial content or under-adaptation on more informative regions, especially in autoregressive settings with distinct prefill and decoding dynamics. In this paper, we propose GateRA, a unified framework that introduces token-aware modulation to dynamically adjust the strength of PEFT updates. By incorporating adaptive gating into standard PEFT branches, GateRA enables selective, token-level adaptation, preserving pre-trained knowledge for well-modeled inputs while focusing capacity on challenging cases. Empirical visualizations reveal phase-sensitive behaviors, where GateRA automatically suppresses updates for redundant prefill tokens while emphasizing adaptation during decoding. To promote confident and efficient modulation, we further introduce an entropy-based regularization that encourages near-binary gating decisions. This regularization prevents diffuse update patterns and leads to interpretable, sparse adaptation without hard thresholding. Finally, we present a theoretical analysis showing that GateRA induces a soft gradient-masking effect over the PEFT path, enabling continuous and differentiable control over adaptation. Experiments on multiple commonsense reasoning benchmarks demonstrate that GateRA consistently outperforms or matches prior PEFT methods.","authors":["Jie Ou","Shuaihong Jiang","Yingjun Du","Cees G. M. Snoek"],"pdf_url":"","comment":"Accepted by AAAI 2026"},{"id":"http://arxiv.org/abs/2512.19084v1","updated":"2025-12-22T06:48:53Z","published":"2025-12-22T06:48:53Z","title":"$γ(3,4)$ `Attention' in Cognitive Agents: Ontology-Free Knowledge Representations With Promise Theoretic Semantics","summary":"The semantics and dynamics of `attention' are closely related to promise theoretic notions developed for autonomous agents and can thus easily be written down in promise framework. In this way one may establish a bridge between vectorized Machine Learning and Knowledge Graph representations without relying on language models implicitly. Our expectations for knowledge presume a degree of statistical stability, i.e. average invariance under repeated observation, or `trust' in the data. Both learning networks and knowledge graph representations can meaningfully coexist to preserve different aspects of data. While vectorized data are useful for probabilistic estimation, graphs preserve the intentionality of the source even under data fractionation. Using a Semantic Spacetime $γ(3,4)$ graph, one avoids complex ontologies in favour of classification of features by their roles in semantic processes. The latter favours an approach to reasoning under conditions of uncertainty. Appropriate attention to causal boundary conditions may lead to orders of magnitude compression of data required for such context determination, as required in the contexts of autonomous robotics, defence deployments, and ad hoc emergency services.","authors":["Mark Burgess"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19081v1","updated":"2025-12-22T06:42:46Z","published":"2025-12-22T06:42:46Z","title":"Population-Evolve: a Parallel Sampling and Evolutionary Method for LLM Math Reasoning","summary":"Test-time scaling has emerged as a promising direction for enhancing the reasoning capabilities of Large Language Models in last few years. In this work, we propose Population-Evolve, a training-free method inspired by Genetic Algorithms to optimize LLM reasoning. Our approach maintains a dynamic population of candidate solutions for each problem via parallel reasoning. By incorporating an evolve prompt, the LLM self-evolves its population in all iterations. Upon convergence, the final answer is derived via majority voting. Furthermore, we establish a unification framework that interprets existing test-time scaling strategies through the lens of genetic algorithms. Empirical results demonstrate that Population-Evolve achieves superior accuracy with low performance variance and computational efficiency. Our findings highlight the potential of evolutionary strategies to unlock the reasoning power of LLMs during inference.","authors":["Yanzhi Zhang","Yitong Duan","Zhaoxi Zhang","Jiyan He","Shuxin Zheng"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19069v1","updated":"2025-12-22T06:17:25Z","published":"2025-12-22T06:17:25Z","title":"Can abstract concepts from LLM improve SLM performance?","summary":"Large language models (LLMs) excel at diverse tasks, but their deployment on resource-constrained devices remains challenging. Existing methods like quantization, pruning, and distillation can reduce memory footprint but often demand extensive experimentation and careful infrastructure design. Leveraging existing techniques for extracting high-level concepts (represented as steering vectors) from larger models, we investigate their transferability to smaller language models (SLM) during inference. We demonstrate through extensive experimentation that these concepts can be effectively transferred to smaller models, irrespective of their family (e.g., Phi, Llama, Qwen), leading to performance improvements across a wide range of tasks. Furthermore, we introduce inference-time scaling to enhance performance by dynamically adjusting the steering intensity which has resulted in a 7-15\\% of accuracy improvement for Qwen3-0.6B.","authors":["Siddharth Tandon"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2410.15178v4","updated":"2025-12-22T06:16:10Z","published":"2024-10-19T18:46:17Z","title":"GUIDEd Agents: Enhancing Navigation Policies through Task-Specific Uncertainty Abstraction in Localization-Limited Environments","summary":"Autonomous vehicles performing navigation tasks in complex environments face significant challenges due to uncertainty in state estimation. In many scenarios, such as stealth operations or resource-constrained settings, accessing high-precision localization comes at a significant cost, forcing robots to rely primarily on less precise state estimates. Our key observation is that different tasks require varying levels of precision in different regions: a robot navigating a crowded space might need precise localization near obstacles but can operate effectively with less precision elsewhere. In this paper, we present a planning method for integrating task-specific uncertainty requirements directly into navigation policies. We introduce Task-Specific Uncertainty Maps (TSUMs), which abstract the acceptable levels of state estimation uncertainty across different regions. TSUMs align task requirements and environmental features using a shared representation space, generated via a domain-adapted encoder. Using TSUMs, we propose Generalized Uncertainty Integration for Decision-Making and Execution (GUIDE), a policy conditioning framework that incorporates these uncertainty requirements into robot decision-making. We find that TSUMs provide an effective way to abstract task-specific uncertainty requirements, and conditioning policies on TSUMs enables the robot to reason about the context-dependent value of certainty and adapt its behavior accordingly. We show how integrating GUIDE into reinforcement learning frameworks allows the agent to learn navigation policies that effectively balance task completion and uncertainty management without explicit reward engineering. We evaluate GUIDE on various real-world robotic navigation tasks and find that it demonstrates significant improvement in task completion rates compared to baseline methods that do not explicitly consider task-specific uncertainty.","authors":["Gokul Puthumanaillam","Paulo Padrao","Jose Fuentes","Leonardo Bobadilla","Melkior Ornik"],"pdf_url":"","comment":"Accepted for publication at RAL (Robotics and automation letters). Updated with the final version"},{"id":"http://arxiv.org/abs/2508.14075v2","updated":"2025-12-22T06:08:18Z","published":"2025-08-12T11:20:27Z","title":"Explainable Graph Spectral Clustering For GloVe-like Text Embeddings","summary":"In a previous paper, we proposed an introduction to the explainability of Graph Spectral Clustering results for textual documents, given that document similarity is computed as cosine similarity in term vector space.\n  In this paper, we generalize this idea by considering other embeddings of documents, in particular, based on the GloVe embedding idea.","authors":["Mieczysław A. Kłopotek","Sławomir T. Wierzchoń","Bartłomiej Starosta","Piotr Borkowski","Dariusz Czerski","Eryk Laskowski"],"pdf_url":"","comment":"47 pages, 19 tables, 11 figures"},{"id":"http://arxiv.org/abs/2512.19061v1","updated":"2025-12-22T05:59:13Z","published":"2025-12-22T05:59:13Z","title":"Fraud Detection Through Large-Scale Graph Clustering with Heterogeneous Link Transformation","summary":"Collaborative fraud, where multiple fraudulent accounts coordinate to exploit online payment systems, poses significant challenges due to the formation of complex network structures. Traditional detection methods that rely solely on high-confidence identity links suffer from limited coverage, while approaches using all available linkages often result in fragmented graphs with reduced clustering effectiveness. In this paper, we propose a novel graph-based fraud detection framework that addresses the challenge of large-scale heterogeneous graph clustering through a principled link transformation approach. Our method distinguishes between \\emph{hard links} (high-confidence identity relationships such as phone numbers, credit cards, and national IDs) and \\emph{soft links} (behavioral associations including device fingerprints, cookies, and IP addresses). We introduce a graph transformation technique that first identifies connected components via hard links, merges them into super-nodes, and then reconstructs a weighted soft-link graph amenable to efficient embedding and clustering. The transformed graph is processed using LINE (Large-scale Information Network Embedding) for representation learning, followed by HDBSCAN (Hierarchical Density-Based Spatial Clustering of Applications with Noise) for density-based cluster discovery. Experiments on a real-world payment platform dataset demonstrate that our approach achieves significant graph size reduction (from 25 million to 7.7 million nodes), doubles the detection coverage compared to hard-link-only baselines, and maintains high precision across identified fraud clusters. Our framework provides a scalable and practical solution for industrial-scale fraud detection systems.","authors":["Chi Liu"],"pdf_url":"","comment":"13 pages, 6 figures"},{"id":"http://arxiv.org/abs/2411.07019v7","updated":"2025-12-22T05:48:26Z","published":"2024-11-11T14:22:42Z","title":"UniHR: Hierarchical Representation Learning for Unified Knowledge Graph Link Prediction","summary":"Real-world knowledge graphs (KGs) contain not only standard triple-based facts, but also more complex, heterogeneous types of facts, such as hyper-relational facts with auxiliary key-value pairs, temporal facts with additional timestamps, and nested facts that imply relationships between facts. These richer forms of representation have attracted significant attention due to their enhanced expressiveness and capacity to model complex semantics in real-world scenarios. However, most existing studies suffer from two main limitations: (1) they typically focus on modeling only specific types of facts, thus making it difficult to generalize to real-world scenarios with multiple fact types; and (2) they struggle to achieve generalizable hierarchical (inter-fact and intra-fact) modeling due to the complexity of these representations. To overcome these limitations, we propose UniHR, a Unified Hierarchical Representation learning framework, which consists of a learning-optimized Hierarchical Data Representation (HiDR) module and a unified Hierarchical Structure Learning (HiSL) module. The HiDR module unifies hyper-relational KGs, temporal KGs, and nested factual KGs into triple-based representations. Then HiSL incorporates intra-fact and inter-fact message passing, focusing on enhancing both semantic information within individual facts and enriching the structural information between facts. To go beyond the unified method itself, we further explore the potential of unified representation in complex real-world scenarios. Extensive experiments on 9 datasets across 5 types of KGs demonstrate the effectiveness of UniHR and highlight the strong potential of unified representations. Code and data are available at https://github.com/zjukg/UniHR.","authors":["Zhiqiang Liu","Yin Hua","Mingyang Chen","Yichi Zhang","Zhuo Chen","Lei Liang","Wen Zhang"],"pdf_url":"","comment":"AAAI 2026 (oral)"},{"id":"http://arxiv.org/abs/2503.11185v2","updated":"2025-12-22T05:32:02Z","published":"2025-03-14T08:32:12Z","title":"Bleeding Pathways: Vanishing Discriminability in LLM Hidden States Fuels Jailbreak Attacks","summary":"LLMs remain vulnerable to jailbreak attacks that exploit adversarial prompts to circumvent safety measures. Current safety fine-tuning approaches face two critical limitations. First, they often fail to strike a balance between security and utility, where stronger safety measures tend to over-reject harmless user requests. Second, they frequently miss malicious intent concealed within seemingly benign tasks, leaving models exposed to exploitation. Our work identifies a fundamental cause of these issues: during response generation, an LLM's capacity to differentiate harmful from safe outputs deteriorates. Experimental evidence confirms this, revealing that the separability between hidden states for safe and harmful responses diminishes as generation progresses. This weakening discrimination forces models to make compliance judgments earlier in the generation process, restricting their ability to recognize developing harmful intent and contributing to both aforementioned failures. To mitigate this vulnerability, we introduce DEEPALIGN - an inherent defense framework that enhances the safety of LLMs. By applying contrastive hidden-state steering at the midpoint of response generation, DEEPALIGN amplifies the separation between harmful and benign hidden states, enabling continuous intrinsic toxicity detection and intervention throughout the generation process. Across diverse LLMs spanning varying architectures and scales, it reduced attack success rates of nine distinct jailbreak attacks to near-zero or minimal. Crucially, it preserved model capability while reducing over-refusal. Models equipped with DEEPALIGN exhibited up to 3.5% lower error rates in rejecting challenging benign queries and maintained standard task performance with less than 1% decline. This marks a substantial advance in the safety-utility Pareto frontier.","authors":["Yingjie Zhang","Tong Liu","Zhe Zhao","Guozhu Meng","Kai Chen"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19027v1","updated":"2025-12-22T04:53:40Z","published":"2025-12-22T04:53:40Z","title":"Recontextualization Mitigates Specification Gaming without Modifying the Specification","summary":"Developers often struggle to specify correct training labels and rewards. Perhaps they don't need to. We propose recontextualization, which reduces how often language models \"game\" training signals, performing misbehaviors those signals mistakenly reinforce. We show recontextualization prevents models from learning to 1) prioritize evaluation metrics over chat response quality; 2) special-case code to pass incorrect tests; 3) lie to users; and 4) become sycophantic. Our method works by generating completions from prompts discouraging misbehavior and then recontextualizing them as though they were in response to prompts permitting misbehavior. Recontextualization trains language models to resist misbehavior even when instructions permit it. This mitigates the reinforcement of misbehavior from misspecified training signals, reducing specification gaming without improving the supervision signal.","authors":["Ariana Azarbal","Victor Gillioz","Vladimir Ivanov","Bryce Woodworth","Jacob Drori","Nevan Wichers","Aram Ebtekar","Alex Cloud","Alexander Matt Turner"],"pdf_url":"","comment":"57 pages, 41 figures"},{"id":"http://arxiv.org/abs/2512.19026v1","updated":"2025-12-22T04:53:40Z","published":"2025-12-22T04:53:40Z","title":"Finer-Personalization Rank: Fine-Grained Retrieval Examines Identity Preservation for Personalized Generation","summary":"The rise of personalized generative models raises a central question: how should we evaluate identity preservation? Given a reference image (e.g., one's pet), we expect the generated image to retain precise details attached to the subject's identity. However, current generative evaluation metrics emphasize the overall semantic similarity between the reference and the output, and overlook these fine-grained discriminative details. We introduce Finer-Personalization Rank, an evaluation protocol tailored to identity preservation. Instead of pairwise similarity, Finer-Personalization Rank adopts a ranking view: it treats each generated image as a query against an identity-labeled gallery consisting of visually similar real images. Retrieval metrics (e.g., mean average precision) measure performance, where higher scores indicate that identity-specific details (e.g., a distinctive head spot) are preserved. We assess identity at multiple granularities -- from fine-grained categories (e.g., bird species, car models) to individual instances (e.g., re-identification). Across CUB, Stanford Cars, and animal Re-ID benchmarks, Finer-Personalization Rank more faithfully reflects identity retention than semantic-only metrics and reveals substantial identity drift in several popular personalization methods. These results position the gallery-based protocol as a principled and practical evaluation for personalized generation.","authors":["Connor Kilrain","David Carlyn","Julia Chae","Sara Beery","Wei-Lun Chao","Jianyang Gu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19025v1","updated":"2025-12-22T04:42:41Z","published":"2025-12-22T04:42:41Z","title":"The Erasure Illusion: Stress-Testing the Generalization of LLM Forgetting Evaluation","summary":"Machine unlearning aims to remove specific data influences from trained models, a capability essential for adhering to copyright laws and ensuring AI safety. Current unlearning metrics typically measure success by monitoring the model's performance degradation on the specific unlearning dataset ($D_u$). We argue that for Large Language Models (LLMs), this evaluation paradigm is insufficient and potentially misleading. Many real-world uses of unlearning--motivated by copyright or safety--implicitly target not only verbatim content in $D_u$, but also behaviors influenced by the broader generalizations the model derived from it. We demonstrate that LLMs can pass standard unlearning evaluation and appear to have ``forgotten'' the target knowledge, while simultaneously retaining strong capabilities on content that is semantically adjacent to $D_u$. This phenomenon indicates that erasing exact sentences does not necessarily equate to removing the underlying knowledge. To address this gap, we propose \\name, an automated stress-testing framework that generates a surrogate dataset, $\\tilde{D}_u$. This surrogate set is constructed to be semantically derived from $D_u$ yet sufficiently distinct in embedding space. By comparing unlearning metric scores between $D_u$ and $\\tilde{D}_u$, we can stress-test the reliability of the metric itself. Our extensive evaluation across three LLM families (Llama-3-8B, Qwen2.5-7B, and Zephyr-7B-$β$), three distinct datasets, and seven standard metrics reveals widespread inconsistencies. We find that current metrics frequently overestimate unlearning success, failing to detect retained knowledge exposed by our stress-test datasets.","authors":["Hengrui Jia","Taoran Li","Jonas Guan","Varun Chandrasekaran"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19024v1","updated":"2025-12-22T04:42:35Z","published":"2025-12-22T04:42:35Z","title":"IndoorUAV: Benchmarking Vision-Language UAV Navigation in Continuous Indoor Environments","summary":"Vision-Language Navigation (VLN) enables agents to navigate in complex environments by following natural language instructions grounded in visual observations. Although most existing work has focused on ground-based robots or outdoor Unmanned Aerial Vehicles (UAVs), indoor UAV-based VLN remains underexplored, despite its relevance to real-world applications such as inspection, delivery, and search-and-rescue in confined spaces. To bridge this gap, we introduce \\textbf{IndoorUAV}, a novel benchmark and method specifically tailored for VLN with indoor UAVs. We begin by curating over 1,000 diverse and structurally rich 3D indoor scenes from the Habitat simulator. Within these environments, we simulate realistic UAV flight dynamics to collect diverse 3D navigation trajectories manually, further enriched through data augmentation techniques. Furthermore, we design an automated annotation pipeline to generate natural language instructions of varying granularity for each trajectory. This process yields over 16,000 high-quality trajectories, comprising the \\textbf{IndoorUAV-VLN} subset, which focuses on long-horizon VLN. To support short-horizon planning, we segment long trajectories into sub-trajectories by selecting semantically salient keyframes and regenerating concise instructions, forming the \\textbf{IndoorUAV-VLA} subset. Finally, we introduce \\textbf{IndoorUAV-Agent}, a novel navigation model designed for our benchmark, leveraging task decomposition and multimodal reasoning. We hope IndoorUAV serves as a valuable resource to advance research on vision-language embodied AI in the indoor aerial navigation domain.","authors":["Xu Liu","Yu Liu","Hanshuo Qiu","Yang Qirong","Zhouhui Lian"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2510.11496v3","updated":"2025-12-22T04:40:04Z","published":"2025-10-13T15:04:38Z","title":"AndesVL Technical Report: An Efficient Mobile-side Multimodal Large Language Model","summary":"In recent years, while cloud-based MLLMs such as QwenVL, InternVL, GPT-4o, Gemini, and Claude Sonnet have demonstrated outstanding performance with enormous model sizes reaching hundreds of billions of parameters, they significantly surpass the limitations in memory, power consumption, and computing capacity of edge devices such as mobile phones. This paper introduces AndesVL, a suite of mobile-side MLLMs with 0.6B to 4B parameters based on Qwen3's LLM and various visual encoders. We comprehensively outline the model architectures, training pipeline, and training data of AndesVL, which achieves first-tier performance across a wide range of open-source benchmarks, including fields such as text-rich image understanding, reasoning and math, multi-image comprehension, general VQA, hallucination mitigation, multilingual understanding, and GUI-related tasks when compared with state-of-the-art models of a similar scale. Furthermore, we introduce a 1+N LoRA architecture alongside a Quantization-Aware LoRA Fine-Tuning (QALFT) framework to facilitate efficient task adaptation and model compression during mobile-side deployment of AndesVL. Moreover, utilizing our cache eviction algorithm -- OKV -- along with customized speculative decoding and compression strategies, we achieve a 6.7x peak decoding speedup ratio, up to 30.9% memory reduction, and 1.8 bits-per-weight when deploying AndesVL-4B on MediaTek Dimensity 9500 chips. We release all models on https://huggingface.co/OPPOer.","authors":["Zhiwei Jin","Xiaohui Song","Nan Wang","Yafei Liu","Chao Li","Xin Li","Ruichen Wang","Zhihao Li","Qi Qi","Long Cheng","Dongze Hao","Quanlong Zheng","Yanhao Zhang","Haobo Ji","Jian Ma","Zhitong Zheng","Zhenyi Lin","Haolin Deng","Xin Zou","Xiaojie Yin","Ruilin Wang","Liankai Cai","Haijing Liu","Yuqing Qiu","Ke Chen","Zixian Li","Chi Xie","Huafei Li","Chenxing Li","Chuangchuang Wang","Kai Tang","Zhiguang Zhu","Kai Tang","Wenmei Gao","Rui Wang","Jun Wu","Chao Liu","Qin Xie","Chen Chen","Haonan Lu"],"pdf_url":"","comment":"Tech report of OPPO AndesVL Team"},{"id":"http://arxiv.org/abs/2512.11469v2","updated":"2025-12-22T04:02:51Z","published":"2025-12-12T11:12:42Z","title":"Three methods, one problem: Classical and AI approaches to no-three-in-line","summary":"The No-Three-In-Line problem asks for the maximum number of points that can be placed on an n by n grid with no three collinear, representing a famous problem in combinatorial geometry. While classical methods like Integer Linear Programming (ILP) guarantee optimal solutions, they face exponential scaling with grid size, and recent advances in machine learning offer promising alternatives for pattern-based approximation. This paper presents the first systematic comparison of classical optimization and AI approaches to this problem, evaluating their performance against traditional algorithms. We apply PatternBoost transformer learning and reinforcement learning (PPO) to this problem for the first time, comparing them against ILP. ILP achieves provably optimal solutions up to 19 by 19 grids, while PatternBoost matches optimal performance up to 14 by 14 grids with 96% test loss reduction. PPO achieves perfect solutions on 10 by 10 grids but fails at 11 by 11 grids, where constraint violations prevent valid configurations. These results demonstrate that classical optimization remains essential for exact solutions while AI methods offer competitive performance on smaller instances, with hybrid approaches presenting the most promising direction for scaling to larger problem sizes.","authors":["Pranav Ramanathan","Thomas Prellberg","Matthew Lewis","Prathamesh Dinesh Joshi","Raj Abhijit Dandekar","Rajat Dandekar","Sreedath Panat"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19011v1","updated":"2025-12-22T04:00:35Z","published":"2025-12-22T04:00:35Z","title":"Efficient Jailbreak Mitigation Using Semantic Linear Classification in a Multi-Staged Pipeline","summary":"Prompt injection and jailbreaking attacks pose persistent security challenges to large language model (LLM)-based systems. We present an efficient and systematically evaluated defense architecture that mitigates these threats through a lightweight, multi-stage pipeline. Its core component is a semantic filter based on text normalization, TF-IDF representations, and a Linear SVM classifier. Despite its simplicity, this module achieves 93.4% accuracy and 96.5% specificity on held-out data, substantially reducing attack throughput while incurring negligible computational overhead.\n  Building on this efficient foundation, the full pipeline integrates complementary detection and mitigation mechanisms that operate at successive stages, providing strong robustness with minimal latency. In comparative experiments, our SVM-based configuration improves overall accuracy from 35.1% to 93.4% while reducing average time to completion from approximately 450s to 47s, yielding over 10 times lower latency than ShieldGemma. These results demonstrate that the proposed design simultaneously advances defensive precision and efficiency, addressing a core limitation of current model-based moderators.\n  Evaluation across a curated corpus of over 30,000 labeled prompts, including benign, jailbreak, and application-layer injections, confirms that staged, resource-efficient defenses can robustly secure modern LLM-driven applications.","authors":["Akshaj Prashanth Rao","Advait Singh","Saumya Kumaar Saksena","Dhruv Kumar"],"pdf_url":"","comment":"Under Review"},{"id":"http://arxiv.org/abs/2512.19007v1","updated":"2025-12-22T03:48:31Z","published":"2025-12-22T03:48:31Z","title":"The 6th International Verification of Neural Networks Competition (VNN-COMP 2025): Summary and Results","summary":"This report summarizes the 6th International Verification of Neural Networks Competition (VNN-COMP 2025), held as a part of the 8th International Symposium on AI Verification (SAIV), that was collocated with the 37th International Conference on Computer-Aided Verification (CAV). VNN-COMP is held annually to facilitate the fair and objective comparison of state-of-the-art neural network verification tools, encourage the standardization of tool interfaces, and bring together the neural network verification community. To this end, standardized formats for networks (ONNX) and specification (VNN-LIB) were defined, tools were evaluated on equal-cost hardware (using an automatic evaluation pipeline based on AWS instances), and tool parameters were chosen by the participants before the final test sets were made public. In the 2025 iteration, 8 teams participated on a diverse set of 16 regular and 9 extended benchmarks. This report summarizes the rules, benchmarks, participating tools, results, and lessons learned from this iteration of this competition.","authors":["Konstantin Kaulen","Tobias Ladner","Stanley Bak","Christopher Brix","Hai Duong","Thomas Flinkow","Taylor T. Johnson","Lukas Koller","Edoardo Manino","ThanhVu H Nguyen","Haoze Wu"],"pdf_url":"","comment":"Report on the results of VNN-COMP 2025. arXiv admin note: substantial text overlap with arXiv:2412.19985, arXiv:2312.16760, arXiv:2212.10376"},{"id":"http://arxiv.org/abs/2512.19004v1","updated":"2025-12-22T03:45:04Z","published":"2025-12-22T03:45:04Z","title":"Context-Aware Initialization for Reducing Generative Path Length in Diffusion Language Models","summary":"Diffusion Large Language Models (DLLMs) enable fully parallel token decoding but often remain impractical at inference time due to the many denoising iterations required to refine an information-free, fully masked initialization into coherent text. Most existing acceleration methods focus on traversing this generative trajectory more efficiently via improved solvers or sampling strategies. We advance a complementary perspective: shorten the trajectory itself by starting closer to the target distribution through context-aware initialization.\n  We propose a training-free interface that injects prompt-conditioned priors from a lightweight auxiliary model into the diffusion initialization, and instantiate it with two mechanisms: discrete token injection and representation-level embedding interpolation. Because injected priors can be imperfect and unmask-only decoding can over-commit early, we also introduce a simple confidence-based remasking mechanism as a form of prior skepticism. Preliminary evidence on GSM8K suggests that context-aware initialization can substantially reduce denoising iterations (about 35\\% fewer function evaluations in our setting), while also exposing a key open challenge: naive warm-starting can degrade final accuracy relative to strong diffusion baselines. We use these findings to motivate a research agenda around calibration, revision mechanisms, and representation alignment for reliable warm-started diffusion decoding.","authors":["Tongyuan Miao","Gary Huang","Kai Jun Han","Annie Jiang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.14554v3","updated":"2025-12-22T03:45:03Z","published":"2025-12-16T16:28:32Z","title":"VLegal-Bench: Cognitively Grounded Benchmark for Vietnamese Legal Reasoning of Large Language Models","summary":"The rapid advancement of large language models (LLMs) has enabled new possibilities for applying artificial intelligence within the legal domain. Nonetheless, the complexity, hierarchical organization, and frequent revisions of Vietnamese legislation pose considerable challenges for evaluating how well these models interpret and utilize legal knowledge. To address this gap, Vietnamese Legal Benchmark (VLegal-Bench) is introduced, the first comprehensive benchmark designed to systematically assess LLMs on Vietnamese legal tasks. Informed by Bloom's cognitive taxonomy, VLegal-Bench encompasses multiple levels of legal understanding through tasks designed to reflect practical usage scenarios. The benchmark comprises 10,450 samples generated through a rigorous annotation pipeline, where legal experts label and cross-validate each instance using our annotation system to ensure every sample is grounded in authoritative legal documents and mirrors real-world legal assistant workflows, including general legal questions and answers, retrieval-augmented generation, multi-step reasoning, and scenario-based problem solving tailored to Vietnamese law. By providing a standardized, transparent, and cognitively informed evaluation framework, VLegal-Bench establishes a solid foundation for assessing LLM performance in Vietnamese legal contexts and supports the development of more reliable, interpretable, and ethically aligned AI-assisted legal systems.","authors":["Nguyen Tien Dong","Minh-Anh Nguyen","Thanh Dat Hoang","Nguyen Tuan Ngoc","Dao Xuan Quang Minh","Phan Phi Hai","Nguyen Thi Ngoc Anh","Dang Van Tu","Binh Vu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19001v1","updated":"2025-12-22T03:39:43Z","published":"2025-12-22T03:39:43Z","title":"ORPR: An OR-Guided Pretrain-then-Reinforce Learning Model for Inventory Management","summary":"As the pursuit of synergy between Artificial Intelligence (AI) and Operations Research (OR) gains momentum in handling complex inventory systems, a critical challenge persists: how to effectively reconcile AI's adaptive perception with OR's structural rigor. To bridge this gap, we propose a novel OR-Guided \"Pretrain-then-Reinforce\" framework. To provide structured guidance, we propose a simulation-augmented OR model that generates high-quality reference decisions, implicitly capturing complex business constraints and managerial preferences. Leveraging these OR-derived decisions as foundational training labels, we design a domain-informed deep learning foundation model to establish foundational decision-making capabilities, followed by a reinforcement learning (RL) fine-tuning stage. Uniquely, we position RL as a deep alignment mechanism that enables the AI agent to internalize the optimality principles of OR, while simultaneously leveraging exploration for general policy refinement and allowing expert guidance for scenario-specific adaptation (e.g., promotional events). Validated through extensive numerical experiments and a field deployment at JD.com augmented by a Difference-in-Differences (DiD) analysis, our model significantly outperforms incumbent industrial practices, delivering real-world gains of a 5.27-day reduction in turnover and a 2.29% increase in in-stock rates, alongside a 29.95% decrease in holding costs. Contrary to the prevailing trend of brute-force model scaling, our study demonstrates that a lightweight, domain-informed model can deliver state-of-the-art performance and robust transferability when guided by structured OR logic. This approach offers a scalable and cost-effective paradigm for intelligent supply chain management, highlighting the value of deeply aligning AI with OR.","authors":["Lingjie Zhao","Xue Yu","Yongzhi Qi","Hao Hu","Jianshen Zhang","Yingzheng Ma","Shuyu Han","Wei Qi","Zuo-Jun Max Shen"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.18999v1","updated":"2025-12-22T03:33:43Z","published":"2025-12-22T03:33:43Z","title":"Evaluating the Challenges of LLMs in Real-world Medical Follow-up: A Comparative Study and An Optimized Framework","summary":"When applied directly in an end-to-end manner to medical follow-up tasks, Large Language Models (LLMs) often suffer from uncontrolled dialog flow and inaccurate information extraction due to the complexity of follow-up forms. To address this limitation, we designed and compared two follow-up chatbot systems: an end-to-end LLM-based system (control group) and a modular pipeline with structured process control (experimental group). Experimental results show that while the end-to-end approach frequently fails on lengthy and complex forms, our modular method-built on task decomposition, semantic clustering, and flow management-substantially improves dialog stability and extraction accuracy. Moreover, it reduces the number of dialogue turns by 46.73% and lowers token consumption by 80% to 87.5%. These findings highlight the necessity of integrating external control mechanisms when deploying LLMs in high-stakes medical follow-up scenarios.","authors":["Jinyan Liu","Zikang Chen","Qinchuan Wang","Tan Xie","Heming Zheng","Xudong Lv"],"pdf_url":"","comment":"10 pages,3 figures,conference ICCBB2025"},{"id":"http://arxiv.org/abs/2511.12808v3","updated":"2025-12-22T03:26:43Z","published":"2025-11-16T22:28:30Z","title":"Expressive Temporal Specifications for Reward Monitoring","summary":"Specifying informative and dense reward functions remains a pivotal challenge in Reinforcement Learning, as it directly affects the efficiency of agent training. In this work, we harness the expressive power of quantitative Linear Temporal Logic on finite traces (($\\text{LTL}_f[\\mathcal{F}]$)) to synthesize reward monitors that generate a dense stream of rewards for runtime-observable state trajectories. By providing nuanced feedback during training, these monitors guide agents toward optimal behaviour and help mitigate the well-known issue of sparse rewards under long-horizon decision making, which arises under the Boolean semantics dominating the current literature. Our framework is algorithm-agnostic and only relies on a state labelling function, and naturally accommodates specifying non-Markovian properties. Empirical results show that our quantitative monitors consistently subsume and, depending on the environment, outperform Boolean monitors in maximizing a quantitative measure of task completion and in reducing convergence time.","authors":["Omar Adalat","Francesco Belardinelli"],"pdf_url":"","comment":"Accepted at AAAI-26"},{"id":"http://arxiv.org/abs/2512.16214v2","updated":"2025-12-22T03:24:07Z","published":"2025-12-18T06:02:50Z","title":"PDE-Agent: A toolchain-augmented multi-agent framework for PDE solving","summary":"Solving Partial Differential Equations (PDEs) is a cornerstone of engineering and scientific research. Traditional methods for PDE solving are cumbersome, relying on manual setup and domain expertise. While Physics-Informed Neural Network (PINNs) introduced end-to-end neural network-based solutions, and frameworks like DeepXDE further enhanced automation, these approaches still depend on expert knowledge and lack full autonomy. In this work, we frame PDE solving as tool invocation via LLM-driven agents and introduce PDE-Agent, the first toolchain-augmented multi-agent collaboration framework, inheriting the reasoning capacity of LLMs and the controllability of external tools and enabling automated PDE solving from natural language descriptions. PDE-Agent leverages the strengths of multi-agent and multi-tool collaboration through two key innovations: (1) A Prog-Act framework with graph memory for multi-agent collaboration, which enables effective dynamic planning and error correction via dual-loop mechanisms (localized fixes and global revisions). (2) A Resource-Pool integrated with a tool-parameter separation mechanism for multi-tool collaboration. This centralizes the management of runtime artifacts and resolves inter-tool dependency gaps in existing frameworks. To validate and evaluate this new paradigm for PDE solving , we develop PDE-Bench, a multi-type PDE Benchmark for agent-based tool collaborative solving, and propose multi-level metrics for assessing tool coordination. Evaluations verify that PDE-Agent exhibits superior applicability and performance in complex multi-step, cross-step dependent tasks. This new paradigm of toolchain-augmented multi-agent PDE solving will further advance future developments in automated scientific computing. Our source code and dataset will be made publicly available.","authors":["Jianming Liu","Ren Zhu","Jian Xu","Kun Ding","Xu-Yao Zhang","Gaofeng Meng","Cheng-Lin Liu"],"pdf_url":"","comment":"Adding Affiliation Information on arXiv"},{"id":"http://arxiv.org/abs/2512.17370v2","updated":"2025-12-22T03:20:46Z","published":"2025-12-19T09:12:44Z","title":"TakeAD: Preference-based Post-optimization for End-to-end Autonomous Driving with Expert Takeover Data","summary":"Existing end-to-end autonomous driving methods typically rely on imitation learning (IL) but face a key challenge: the misalignment between open-loop training and closed-loop deployment. This misalignment often triggers driver-initiated takeovers and system disengagements during closed-loop execution. How to leverage those expert takeover data from disengagement scenarios and effectively expand the IL policy's capability presents a valuable yet unexplored challenge. In this paper, we propose TakeAD, a novel preference-based post-optimization framework that fine-tunes the pre-trained IL policy with this disengagement data to enhance the closed-loop driving performance. First, we design an efficient expert takeover data collection pipeline inspired by human takeover mechanisms in real-world autonomous driving systems. Then, this post optimization framework integrates iterative Dataset Aggregation (DAgger) for imitation learning with Direct Preference Optimization (DPO) for preference alignment. The DAgger stage equips the policy with fundamental capabilities to handle disengagement states through direct imitation of expert interventions. Subsequently, the DPO stage refines the policy's behavior to better align with expert preferences in disengagement scenarios. Through multiple iterations, the policy progressively learns recovery strategies for disengagement states, thereby mitigating the open-loop gap. Experiments on the closed-loop Bench2Drive benchmark demonstrate our method's effectiveness compared with pure IL methods, with comprehensive ablations confirming the contribution of each component.","authors":["Deqing Liu","Yinfeng Gao","Deheng Qian","Qichao Zhang","Xiaoqing Ye","Junyu Han","Yupeng Zheng","Xueyi Liu","Zhongpu Xia","Dawei Ding","Yifeng Pan","Dongbin Zhao"],"pdf_url":"","comment":"This work has been accepted by IEEE RA-L. Manuscript submitted: July, 8, 2025; Accepted: November, 24, 2025"},{"id":"http://arxiv.org/abs/2512.12608v2","updated":"2025-12-22T03:19:42Z","published":"2025-12-14T09:12:09Z","title":"Human-Inspired Learning for Large Language Models via Obvious Record and Maximum-Entropy Method Discovery","summary":"Large Language Models (LLMs) excel at extracting common patterns from large-scale corpora, yet they struggle with rare, low-resource, or previously unseen scenarios-such as niche hardware deployment issues or irregular IoT device behaviors-because such cases are sparsely represented in training data. Moreover, LLMs rely primarily on implicit parametric memory, which limits their ability to explicitly acquire, recall, and refine methods, causing them to behave predominantly as intuition-driven predictors rather than deliberate, method-oriented learners. Inspired by how humans learn from rare experiences, this paper proposes a human-inspired learning framework that integrates two complementary mechanisms. The first, Obvious Record, explicitly stores cause--result (or question--solution) relationships as symbolic memory, enabling persistent learning even from single or infrequent encounters. The second, Maximum-Entropy Method Discovery, prioritizes and preserves methods with high semantic dissimilarity, allowing the system to capture diverse and underrepresented strategies that are typically overlooked by next-token prediction. Verification on a benchmark of 60 semantically diverse question--solution pairs demonstrates that the proposed entropy-guided approach achieves stronger coverage of unseen questions and significantly greater internal diversity than a random baseline, confirming its effectiveness in discovering more generalizable and human-inspired methods.","authors":["Hong Su"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2402.06118v4","updated":"2025-12-22T03:14:10Z","published":"2024-02-09T01:00:14Z","title":"ViGoR: Improving Visual Grounding of Large Vision Language Models with Fine-Grained Reward Modeling","summary":"By combining natural language understanding, generation capabilities, and breadth of knowledge of large language models with image perception, recent large vision language models (LVLMs) have shown unprecedented visual reasoning capabilities. However, the generated text often suffers from inaccurate grounding in the visual input, resulting in errors such as hallucination of nonexistent scene elements, missing significant parts of the scene, and inferring incorrect attributes of and relationships between objects. To address these issues, we introduce a novel framework, ViGoR (Visual Grounding Through Fine-Grained Reward Modeling) that utilizes fine-grained reward modeling to significantly enhance the visual grounding of LVLMs over pre-trained baselines. This improvement is efficiently achieved using much cheaper human evaluations instead of full supervisions, as well as automated methods. We show the effectiveness of our approach through a variety of evaluation methods and benchmarks. Additionally, we released our human annotation (https://github.com/amazon-science/vigor) comprising 15,440 images and generated text pairs with fine-grained evaluations to contribute to related research in the community.","authors":["Siming Yan","Min Bai","Weifeng Chen","Xiong Zhou","Qixing Huang","Li Erran Li"],"pdf_url":"","comment":"Accepted by ECCV 2024"},{"id":"http://arxiv.org/abs/2512.18991v1","updated":"2025-12-22T03:13:08Z","published":"2025-12-22T03:13:08Z","title":"ICP-4D: Bridging Iterative Closest Point and LiDAR Panoptic Segmentation","summary":"Dominant paradigms for 4D LiDAR panoptic segmentation are usually required to train deep neural networks with large superimposed point clouds or design dedicated modules for instance association. However, these approaches perform redundant point processing and consequently become computationally expensive, yet still overlook the rich geometric priors inherently provided by raw point clouds. To this end, we introduce ICP-4D, a simple yet effective training-free framework that unifies spatial and temporal reasoning through geometric relations among instance-level point sets. Specifically, we apply the Iterative Closest Point (ICP) algorithm to directly associate temporally consistent instances by aligning the source and target point sets through the estimated transformation. To stabilize association under noisy instance predictions, we introduce a Sinkhorn-based soft matching. This exploits the underlying instance distribution to obtain accurate point-wise correspondences, resulting in robust geometric alignment. Furthermore, our carefully designed pipeline, which considers three instance types-static, dynamic, and missing-offers computational efficiency and occlusion-aware matching. Our extensive experiments across both SemanticKITTI and panoptic nuScenes demonstrate that our method consistently outperforms state-of-the-art approaches, even without additional training or extra point cloud inputs.","authors":["Gyeongrok Oh","Youngdong Jang","Jonghyun Choi","Suk-Ju Kang","Guang Lin","Sangpil Kim"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2503.04162v5","updated":"2025-12-22T03:07:50Z","published":"2025-03-06T07:25:19Z","title":"Semantic Retrieval Augmented Contrastive Learning for Sequential Recommendation","summary":"Contrastive learning has shown effectiveness in improving sequential recommendation models. However, existing methods still face challenges in generating high-quality contrastive pairs: they either rely on random perturbations that corrupt user preference patterns or depend on sparse collaborative data that generates unreliable contrastive pairs. Furthermore, existing approaches typically require predefined selection rules that impose strong assumptions, limiting the model's ability to autonomously learn optimal contrastive pairs. To address these limitations, we propose a novel approach named Semantic Retrieval Augmented Contrastive Learning (SRA-CL). SRA-CL leverages the semantic understanding and reasoning capabilities of LLMs to generate expressive embeddings that capture both user preferences and item characteristics. These semantic embeddings enable the construction of candidate pools for inter-user and intra-user contrastive learning through semantic-based retrieval. To further enhance the quality of the contrastive samples, we introduce a learnable sample synthesizer that optimizes the contrastive sample generation process during model training. SRA-CL adopts a plug-and-play design, enabling seamless integration with existing sequential recommendation architectures. Extensive experiments on four public datasets demonstrate the effectiveness and model-agnostic nature of our approach.","authors":["Ziqiang Cui","Yunpeng Weng","Xing Tang","Xiaokun Zhang","Shiwei Li","Peiyang Liu","Bowei He","Dugang Liu","Weihong Luo","Xiuqiang He","Chen Ma"],"pdf_url":"","comment":"Accepted by NeurIPS 2025. Code is available at: https://github.com/ziqiangcui/SRA-CL"},{"id":"http://arxiv.org/abs/2511.20277v2","updated":"2025-12-22T02:58:33Z","published":"2025-11-25T13:05:40Z","title":"HVAdam: A Full-Dimension Adaptive Optimizer","summary":"Adaptive optimizers such as Adam have achieved great success in training large-scale models like large language models and diffusion models. However, they often generalize worse than non-adaptive methods, such as SGD on classical architectures like CNNs. We identify a key cause of this performance gap: adaptivity in pre-conditioners, which limits the optimizer's ability to adapt to diverse optimization landscapes. To address this, we propose Anon (Adaptivity Non-restricted Optimizer with Novel convergence technique), a novel optimizer with continuously tunable adaptivity\n  , allowing it to interpolate between SGD-like and Adam-like behaviors and even extrapolate beyond both. To ensure convergence across the entire adaptivity spectrum, we introduce incremental delay update (IDU), a novel mechanism that is more flexible than AMSGrad's hard max-tracking strategy and enhances robustness to gradient noise. We theoretically establish convergence guarantees under both convex and non-convex settings. Empirically, Anon consistently outperforms state-of-the-art optimizers on representative image classification, diffusion, and language modeling tasks. These results demonstrate that adaptivity can serve as a valuable tunable design principle, and Anon provides the first unified and reliable framework capable of bridging the gap between classical and modern optimizers and surpassing their advantageous properties.","authors":["Yiheng Zhang","Shaowu Wu","Yuanzhuo Xu","Jiajun Wu","Shang Xu","Steve Drew","Xiaoguang Niu"],"pdf_url":"","comment":"Accepted at AAAI2025"},{"id":"http://arxiv.org/abs/2512.18986v1","updated":"2025-12-22T02:54:10Z","published":"2025-12-22T02:54:10Z","title":"R-GenIMA: Integrating Neuroimaging and Genetics with Interpretable Multimodal AI for Alzheimer's Disease Progression","summary":"Early detection of Alzheimer's disease (AD) requires models capable of integrating macro-scale neuroanatomical alterations with micro-scale genetic susceptibility, yet existing multimodal approaches struggle to align these heterogeneous signals. We introduce R-GenIMA, an interpretable multimodal large language model that couples a novel ROI-wise vision transformer with genetic prompting to jointly model structural MRI and single nucleotide polymorphisms (SNPs) variations. By representing each anatomically parcellated brain region as a visual token and encoding SNP profiles as structured text, the framework enables cross-modal attention that links regional atrophy patterns to underlying genetic factors. Applied to the ADNI cohort, R-GenIMA achieves state-of-the-art performance in four-way classification across normal cognition (NC), subjective memory concerns (SMC), mild cognitive impairment (MCI), and AD. Beyond predictive accuracy, the model yields biologically meaningful explanations by identifying stage-specific brain regions and gene signatures, as well as coherent ROI-Gene association patterns across the disease continuum. Attention-based attribution revealed genes consistently enriched for established GWAS-supported AD risk loci, including APOE, BIN1, CLU, and RBFOX1. Stage-resolved neuroanatomical signatures identified shared vulnerability hubs across disease stages alongside stage-specific patterns: striatal involvement in subjective decline, frontotemporal engagement during prodromal impairment, and consolidated multimodal network disruption in AD. These results demonstrate that interpretable multimodal AI can synthesize imaging and genetics to reveal mechanistic insights, providing a foundation for clinically deployable tools that enable earlier risk stratification and inform precision therapeutic strategies in Alzheimer's disease.","authors":["Kun Zhao","Siyuan Dai","Yingying Zhang","Guodong Liu","Pengfei Gu","Chenghua Lin","Paul M. Thompson","Alex Leow","Heng Huang","Lifang He","Liang Zhan","Haoteng Tang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.18969v1","updated":"2025-12-22T02:30:19Z","published":"2025-12-22T02:30:19Z","title":"Self-Attention with State-Object Weighted Combination for Compositional Zero Shot Learning","summary":"Object recognition has become prevalent across various industries. However, most existing applications are limited to identifying objects alone, without considering their associated states. The ability to recognize both the state and object simultaneously remains less common. One approach to address this is by treating state and object as a single category during training. However, this approach poses challenges in data collection and training since it requires comprehensive data for all possible combinations. Compositional Zero-shot Learning (CZSL) emerges as a viable solution by treating the state and object as distinct categories during training. CZSL facilitates the identification of novel compositions even in the absence of data for every conceivable combination. The current state-of-the-art method, KG-SP, addresses this issue by training distinct classifiers for states and objects, while leveraging a semantic model to evaluate the plausibility of composed compositions. However, KG-SP's accuracy in state and object recognition can be further improved, and it fails to consider the weighting of states and objects during composition. In this study, we propose SASOW, an enhancement of KG-SP that considers the weighting of states and objects while improving composition recognition accuracy. First, we introduce self-attention mechanisms into the classifiers for states and objects, leading to enhanced accuracy in recognizing both. Additionally, we incorporate the weighting of states and objects during composition to generate more reasonable and accurate compositions. Our validation process involves testing SASOW on three established benchmark datasets. Experimental outcomes affirm when compared against OW-CZSL approach, KG-SP, SASOW showcases improvements of 2.1%, 1.7%, and 0.4% in terms of accuracy for unseen compositions across the MIT-States, UT Zappos, and C-GQA datasets, respectively.","authors":["Cheng-Hong Chang","Pei-Hsuan Tsai"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.01457v3","updated":"2025-12-22T02:26:36Z","published":"2025-12-01T09:44:31Z","title":"Zero-Overhead Introspection for Adaptive Test-Time Compute","summary":"Large language models excel at reasoning but lack key aspects of introspection, including anticipating their own success and the computation required to achieve it. Humans use real-time introspection to decide how much effort to invest, when to make multiple attempts, when to stop, and when to signal success or failure. Without this, LLMs struggle to make intelligent meta-cognition decisions. Test-time scaling methods like Best-of-N drive up cost and latency by using a fixed budget of samples regardless of the marginal benefit of each one at any point in generation, and the absence of confidence signals can mislead people, prevent appropriate escalation to better tools, and undermine trustworthiness. Learned verifiers or reward models can provide confidence estimates, but do not enable adaptive inference and add substantial cost by requiring extra models or forward passes. We present ZIP-RC, an adaptive inference method that equips models with zero-overhead inference-time predictions of reward and cost. At every token, ZIP-RC reuses reserved or unused logits in the same forward pass as next-token prediction to output a joint distribution over final reward and remaining length -- no extra models, architecture change, or inference overhead. This full joint distribution is used to compute a sampling utility which is the linear combination of the expected maximum reward, total compute, and latency of set of samples if generated to completion. During inference, we maximize this utility with meta-actions that determine which prefix of tokens to continue or initiate sampling from. On mixed-difficulty mathematical benchmarks, ZIP-RC improves accuracy by up to 12% over majority voting at equal or lower average cost, and traces smooth Pareto frontiers between quality, compute, and latency. By providing real-time reward-cost introspection, ZIP-RC enables adaptive, efficient reasoning.","authors":["Rohin Manvi","Joey Hong","Tim Seyde","Maxime Labonne","Mathias Lechner","Sergey Levine"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17100v2","updated":"2025-12-22T02:23:31Z","published":"2025-12-18T21:56:08Z","title":"UniCoMTE: A Universal Counterfactual Framework for Explaining Time-Series Classifiers on ECG Data","summary":"Machine learning models, particularly deep neural networks, have demonstrated strong performance in classifying complex time series data. However, their black-box nature limits trust and adoption, especially in high-stakes domains such as healthcare. To address this challenge, we introduce UniCoMTE, a model-agnostic framework for generating counterfactual explanations for multivariate time series classifiers. The framework identifies temporal features that most heavily influence a model's prediction by modifying the input sample and assessing its impact on the model's prediction. UniCoMTE is compatible with a wide range of model architectures and operates directly on raw time series inputs. In this study, we evaluate UniCoMTE's explanations on a time series ECG classifier. We quantify explanation quality by comparing our explanations' comprehensibility to comprehensibility of established techniques (LIME and SHAP) and assessing their generalizability to similar samples. Furthermore, clinical utility is assessed through a questionnaire completed by medical experts who review counterfactual explanations presented alongside original ECG samples. Results show that our approach produces concise, stable, and human-aligned explanations that outperform existing methods in both clarity and applicability. By linking model predictions to meaningful signal patterns, the framework advances the interpretability of deep learning models for real-world time series applications.","authors":["Justin Li","Efe Sencan","Jasper Zheng Duan","Vitus J. Leung","Stephen Tsaur","Ayse K. Coskun"],"pdf_url":"","comment":"21 pages, 7 figures"},{"id":"http://arxiv.org/abs/2512.17145v2","updated":"2025-12-22T02:19:00Z","published":"2025-12-19T00:43:49Z","title":"Solomonoff-Inspired Hypothesis Ranking with LLMs for Prediction Under Uncertainty","summary":"Reasoning under uncertainty is a key challenge in AI, especially for real-world tasks, where problems with sparse data demands systematic generalisation. Existing approaches struggle to balance accuracy and simplicity when evaluating multiple candidate solutions. We propose a Solomonoff-inspired method that weights LLM-generated hypotheses by simplicity and predictive fit. Applied to benchmark (Mini-ARC) tasks, our method produces Solomonoff-weighted mixtures for per-cell predictions, yielding conservative, uncertainty-aware outputs even when hypotheses are noisy or partially incorrect. Compared to Bayesian Model Averaging (BMA), Solomonoff scoring spreads probability more evenly across competing hypotheses, while BMA concentrates weight on the most likely but potentially flawed candidates. Across tasks, this highlights the value of algorithmic information-theoretic priors for interpretable, reliable multi-hypothesis reasoning under uncertainty.","authors":["Josh Barber","Rourke Young","Cameron Coombe","Will Browne"],"pdf_url":"","comment":"10 pages, ACRA 2025, Submitted, Accepted and Presented"},{"id":"http://arxiv.org/abs/2408.15510v5","updated":"2025-12-22T02:12:55Z","published":"2024-08-28T03:45:49Z","title":"How Reliable are Causal Probing Interventions?","summary":"Causal probing aims to analyze foundation models by examining how intervening on their representation of various latent properties impacts their outputs. Recent works have cast doubt on the theoretical basis of several leading causal probing methods, but it has been unclear how to systematically evaluate the effectiveness of these methods in practice. To address this, we define two key causal probing desiderata: completeness (how thoroughly the representation of the target property has been transformed) and selectivity (how little non-targeted properties have been impacted). We find that there is an inherent tradeoff between the two, which we define as reliability, their harmonic mean. We introduce an empirical analysis framework to measure and evaluate these quantities, allowing us to make the first direct comparisons between different families of leading causal probing methods (e.g., linear vs. nonlinear, or concept removal vs. counterfactual interventions). We find that: (1) all methods show a clear tradeoff between completeness and selectivity; (2) more complete and reliable methods have a greater impact on LLM behavior; and (3) nonlinear interventions are almost always more reliable than linear interventions.\n  Our project webpage is available at: https://ahdavies6.github.io/causal_probing_reliability/","authors":["Marc Canby","Adam Davies","Chirag Rastogi","Julia Hockenmaier"],"pdf_url":"","comment":"In Proceedings of IJCNLP-AACL, 2025"},{"id":"http://arxiv.org/abs/2512.18956v1","updated":"2025-12-22T02:07:20Z","published":"2025-12-22T02:07:20Z","title":"Training Multimodal Large Reasoning Models Needs Better Thoughts: A Three-Stage Framework for Long Chain-of-Thought Synthesis and Selection","summary":"Large Reasoning Models (LRMs) have demonstrated remarkable performance on complex reasoning tasks through long Chain-of-Thought (CoT) reasoning. Extending these successes to multimodal reasoning remains challenging due to the increased complexity of integrating diverse input modalities and the scarcity of high-quality long CoT training data. Existing multimodal datasets and CoT synthesis methods still suffer from limited reasoning depth, modality conversion errors, and rigid generation pipelines, hindering model performance and stability. To this end, in this paper, we propose SynSelect, a novel three-stage Synthesis-Selection framework for generating high-quality long CoT data tailored to multimodal reasoning tasks. Specifically, SynSelect first leverages multiple heterogeneous multimodal LRMs to produce diverse candidate CoTs, and then applies both instance and batch level selection to filter high-quality CoTs that can effectively enhance the model's reasoning capabilities. Extensive experiments on multiple multimodal benchmarks demonstrate that models supervised fine-tuned on SynSelect-generated data significantly outperform baselines and achieve further improvements after reinforcement learning post-training. Our results validate SynSelect as an effective approach for advancing multimodal LRMs reasoning capabilities.","authors":["Yizhi Wang","Linan Yue","Min-Ling Zhang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.18950v1","updated":"2025-12-22T01:56:28Z","published":"2025-12-22T01:56:28Z","title":"Learning Hierarchical Procedural Memory for LLM Agents through Bayesian Selection and Contrastive Refinement","summary":"We present MACLA, a framework that decouples reasoning from learning by maintaining a frozen large language model while performing all adaptation in an external hierarchical procedural memory. MACLA extracts reusable procedures from trajectories, tracks reliability via Bayesian posteriors, selects actions through expected-utility scoring, and refines procedures by contrasting successes and failures. Across four benchmarks (ALFWorld, WebShop, TravelPlanner, InterCodeSQL), MACLA achieves 78.1 percent average performance, outperforming all baselines. On ALFWorld unseen tasks, MACLA reaches 90.3 percent with 3.1 percent positive generalization. The system constructs memory in 56 seconds, 2800 times faster than the state-of-the-art LLM parameter-training baseline, compressing 2851 trajectories into 187 procedures. Experimental results demonstrate that structured external memory with Bayesian selection and contrastive refinement enables sample-efficient, interpretable, and continually improving agents without LLM parameter updates.","authors":["Saman Forouzandeh","Wei Peng","Parham Moradi","Xinghuo Yu","Mahdi Jalili"],"pdf_url":"","comment":"Accepted at The 25th International Conference on Autonomous Agents and Multi-Agent Systems (AAMAS 2026). 21 pages including references, with 7 figures and 8 tables. Code is publicly available at the authors GitHub repository: https://github.com/S-Forouzandeh/MACLA-LLM-Agents-AAMAS-Conference"},{"id":"http://arxiv.org/abs/2509.06278v3","updated":"2025-12-22T01:55:37Z","published":"2025-09-08T02:00:31Z","title":"TableMind: An Autonomous Programmatic Agent for Tool-Augmented Table Reasoning","summary":"Table reasoning requires models to jointly perform comprehensive semantic understanding and precise numerical operations. Although recent large language model (LLM)-based methods have achieved promising results, most of them still rely on a single-turn reasoning paradigm that processes flattened tables in a single forward pass. This paradigm suffers from inherent limitations, including context overflow on large tables, weak sensitivity to continuous numerical values, and the absence of explicit tool-use and reflection. In this paper, we propose TableMind, a tuning-based autonomous programmatic table agent that simulates the human-like cognitive schema of the multi-turn interaction within a lightweight LLM. Instead of adopting a training-free workflow design, TableMind learns to internalize planning, action, and reflection through a principled two-stage training strategy. To bootstrap structured table reasoning capabilities, we construct and filter high-quality reasoning data for the supervised fine-tuning (SFT) stage. To enable precise code generation, we introduce a designed multi-perspective reward scheme and a novel optimization objective in the reinforcement learning (RL) stage. Extensive experiments on diverse benchmarks demonstrate that TableMind consistently outperforms previous baselines, validating the effectiveness of training autonomous agents to improve overall performance.","authors":["Chuang Jiang","Mingyue Cheng","Xiaoyu Tao","Qingyang Mao","Jie Ouyang","Qi Liu"],"pdf_url":"","comment":"Comments: 10 pages, 6 figures. Submitted to WSDM 2026"},{"id":"http://arxiv.org/abs/2512.18947v1","updated":"2025-12-22T01:51:26Z","published":"2025-12-22T01:51:26Z","title":"Clustering-based Transfer Learning for Dynamic Multimodal MultiObjective Evolutionary Algorithm","summary":"Dynamic multimodal multiobjective optimization presents the dual challenge of simultaneously tracking multiple equivalent pareto optimal sets and maintaining population diversity in time-varying environments. However, existing dynamic multiobjective evolutionary algorithms often neglect solution modality, whereas static multimodal multiobjective evolutionary algorithms lack adaptability to dynamic changes. To address above challenge, this paper makes two primary contributions. First, we introduce a new benchmark suite of dynamic multimodal multiobjective test functions constructed by fusing the properties of both dynamic and multimodal optimization to establish a rigorous evaluation platform. Second, we propose a novel algorithm centered on a Clustering-based Autoencoder prediction dynamic response mechanism, which utilizes an autoencoder model to process matched clusters to generate a highly diverse initial population. Furthermore, to balance the algorithm's convergence and diversity, we integrate an adaptive niching strategy into the static optimizer. Empirical analysis on 12 instances of dynamic multimodal multiobjective test functions reveals that, compared with several state-of-the-art dynamic multiobjective evolutionary algorithms and multimodal multiobjective evolutionary algorithms, our algorithm not only preserves population diversity more effectively in the decision space but also achieves superior convergence in the objective space.","authors":["Li Yan","Bolun Liu","Chao Li","Jing Liang","Kunjie Yu","Caitong Yue","Xuzhao Chai","Boyang Qu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.15109v2","updated":"2025-12-22T01:35:38Z","published":"2025-12-17T06:01:16Z","title":"Large Model Enabled Embodied Intelligence for 6G Integrated Perception, Communication, and Computation Network","summary":"The advent of sixth-generation (6G) places intelligence at the core of wireless architecture, fusing perception, communication, and computation into a single closed-loop. This paper argues that large artificial intelligence models (LAMs) can endow base stations with perception, reasoning, and acting capabilities, thus transforming them into intelligent base station agents (IBSAs). We first review the historical evolution of BSs from single-functional analog infrastructure to distributed, software-defined, and finally LAM-empowered IBSA, highlighting the accompanying changes in architecture, hardware platforms, and deployment. We then present an IBSA architecture that couples a perception-cognition-execution pipeline with cloud-edge-end collaboration and parameter-efficient adaptation. Subsequently,we study two representative scenarios: (i) cooperative vehicle-road perception for autonomous driving, and (ii) ubiquitous base station support for low-altitude uncrewed aerial vehicle safety monitoring and response against unauthorized drones. On this basis, we analyze key enabling technologies spanning LAM design and training, efficient edge-cloud inference, multi-modal perception and actuation, as well as trustworthy security and governance. We further propose a holistic evaluation framework and benchmark considerations that jointly cover communication performance, perception accuracy, decision-making reliability, safety, and energy efficiency. Finally, we distill open challenges on benchmarks, continual adaptation, trustworthy decision-making, and standardization. Together, this work positions LAM-enabled IBSAs as a practical path toward integrated perception, communication, and computation native, safety-critical 6G systems.","authors":["Zhuoran Li","Zhen Gao","Xinhua Liu","Zheng Wang","Xiaotian Zhou","Lei Liu","Yongpeng Wu","Wei Feng","Yongming Huang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2409.01382v2","updated":"2025-12-22T01:18:44Z","published":"2024-09-02T17:25:15Z","title":"Automatic Detection of LLM-Generated Code: A Comparative Case Study of Contemporary Models Across Function and Class Granularities","summary":"The adoption of Large Language Models (LLMs) for code generation risks incorporating vulnerable code into software systems. Existing detectors face two critical limitations: a lack of systematic cross-model validation and opaque \"black box\" operation. We address this through a comparative study of code generated by four distinct LLMs: GPT-3.5, Claude 3 Haiku, Claude Haiku 4.5, and GPT-OSS.\n  Analyzing 14,485 Python functions and 11,913 classes from the CodeSearchNet dataset, we generated corresponding code with all four LLMs. Using interpretable software metrics, we trained CatBoost classifiers for each configuration. Our analysis reveals that granularity effects dominate model differences by a factor of 8.6, with negligible feature overlap, indicating that function-level and class-level detection rely on fundamentally disjoint structural signatures.\n  We discover critical granularity-dependent inversions: while modern models (Claude, GPT-OSS) are more detectable at the class level, GPT-3.5 is an anomaly that uniquely excels at the function level. SHAP analysis identifies the Comment-to-Code Ratio as the sole universal discriminator. However, its predictive magnitude varies drastically across models, explaining why detectors trained on specific LLMs fail to generalize.\n  Our findings demonstrate that GPT-3.5's exceptional detectability (AUC-ROC 0.96) is unrepresentative of contemporary models (AUC-ROC approximately between 0.68 and 0.80). Robust detection requires moving beyond single-model studies to account for substantial diversity in structural fingerprints across architectures and granularities.","authors":["Musfiqur Rahman","SayedHassan Khatoonabadi","Ahmad Abdellatif","Emad Shihab"],"pdf_url":"","comment":"Submitted to a journal for potential publication"},{"id":"http://arxiv.org/abs/2512.18934v1","updated":"2025-12-22T00:51:39Z","published":"2025-12-22T00:51:39Z","title":"When Less is More: 8-bit Quantization Improves Continual Learning in Large Language Models","summary":"Catastrophic forgetting poses a fundamental challenge in continual learning, particularly when models are quantized for deployment efficiency. We systematically investigate the interplay between quantization precision (FP16, INT8, INT4) and replay buffer strategies in large language models, revealing unexpected dynamics. While FP16 achieves superior initial task performance (74.44% on NLU), we observe a striking inversion on subsequent tasks: quantized models outperform FP16 by 8-15% on final task forward accuracy, with INT4 achieving nearly double FP16's performance on Code generation (40% vs 20%). Critically, even minimal replay buffers (0.1%) dramatically improve retention - increasing NLU retention after Math training from 45% to 65% across all precision levels - with INT8 consistently achieving the optimal balance between learning plasticity and knowledge retention. We hypothesize that quantization-induced noise acts as implicit regularization, preventing the overfitting to new task gradients that plagues high-precision models. These findings challenge the conventional wisdom that higher precision is always preferable, suggesting instead that INT8 quantization offers both computational efficiency and superior continual learning dynamics. Our results provide practical guidelines for deploying compressed models in continual learning scenarios: small replay buffers (1-2%) suffice for NLU tasks, while Math and Code benefit from moderate buffers (5-10%), with quantized models requiring less replay than FP16 to achieve comparable retention. Code is available at https://github.com/Festyve/LessIsMore.","authors":["Michael S. Zhang","Rishi A. Ruia","Arnav Kewalram","Saathvik Dharmapuram","Utkarsh Sharma","Kevin Zhu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.18930v1","updated":"2025-12-22T00:36:22Z","published":"2025-12-22T00:36:22Z","title":"LouvreSAE: Sparse Autoencoders for Interpretable and Controllable Style Transfer","summary":"Artistic style transfer in generative models remains a significant challenge, as existing methods often introduce style only via model fine-tuning, additional adapters, or prompt engineering, all of which can be computationally expensive and may still entangle style with subject matter. In this paper, we introduce a training- and inference-light, interpretable method for representing and transferring artistic style. Our approach leverages an art-specific Sparse Autoencoder (SAE) on top of latent embeddings of generative image models. Trained on artistic data, our SAE learns an emergent, largely disentangled set of stylistic and compositional concepts, corresponding to style-related elements pertaining brushwork, texture, and color palette, as well as semantic and structural concepts. We call it LouvreSAE and use it to construct style profiles: compact, decomposable steering vectors that enable style transfer without any model updates or optimization. Unlike prior concept-based style transfer methods, our method requires no fine-tuning, no LoRA training, and no additional inference passes, enabling direct steering of artistic styles from only a few reference images. We validate our method on ArtBench10, achieving or surpassing existing methods on style evaluations (VGG Style Loss and CLIP Score Style) while being 1.7-20x faster and, critically, interpretable.","authors":["Raina Panda","Daniel Fein","Arpita Singhal","Mark Fiore","Maneesh Agrawala","Matyas Bohacek"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19935v1","updated":"2025-12-22T23:44:39Z","published":"2025-12-22T23:44:39Z","title":"Conditional Adversarial Fragility in Financial Machine Learning under Macroeconomic Stress","summary":"Machine learning models used in financial decision systems operate in nonstationary economic environments, yet adversarial robustness is typically evaluated under static assumptions. This work introduces Conditional Adversarial Fragility, a regime dependent phenomenon in which adversarial vulnerability is systematically amplified during periods of macroeconomic stress. We propose a regime aware evaluation framework for time indexed tabular financial classification tasks that conditions robustness assessment on external indicators of economic stress. Using volatility based regime segmentation as a proxy for macroeconomic conditions, we evaluate model behavior across calm and stress periods while holding model architecture, attack methodology, and evaluation protocols constant. Baseline predictive performance remains comparable across regimes, indicating that economic stress alone does not induce inherent performance degradation. Under adversarial perturbations, however, models operating during stress regimes exhibit substantially greater degradation across predictive accuracy, operational decision thresholds, and risk sensitive outcomes. We further demonstrate that this amplification propagates to increased false negative rates, elevating the risk of missed high risk cases during adverse conditions. To complement numerical robustness metrics, we introduce an interpretive governance layer based on semantic auditing of model explanations using large language models. Together, these results demonstrate that adversarial robustness in financial machine learning is a regime dependent property and motivate stress aware approaches to model risk assessment in high stakes financial deployments.","authors":["Samruddhi Baviskar"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19934v1","updated":"2025-12-22T23:42:45Z","published":"2025-12-22T23:42:45Z","title":"Vehicle-centric Perception via Multimodal Structured Pre-training","summary":"Vehicle-centric perception plays a crucial role in many intelligent systems, including large-scale surveillance systems, intelligent transportation, and autonomous driving. Existing approaches lack effective learning of vehicle-related knowledge during pre-training, resulting in poor capability for modeling general vehicle perception representations. To handle this problem, we propose VehicleMAE-V2, a novel vehicle-centric pre-trained large model. By exploring and exploiting vehicle-related multimodal structured priors to guide the masked token reconstruction process, our approach can significantly enhance the model's capability to learn generalizable representations for vehicle-centric perception. Specifically, we design the Symmetry-guided Mask Module (SMM), Contour-guided Representation Module (CRM) and Semantics-guided Representation Module (SRM) to incorporate three kinds of structured priors into token reconstruction including symmetry, contour and semantics of vehicles respectively. SMM utilizes the vehicle symmetry constraints to avoid retaining symmetric patches and can thus select high-quality masked image patches and reduce information redundancy. CRM minimizes the probability distribution divergence between contour features and reconstructed features and can thus preserve holistic vehicle structure information during pixel-level reconstruction. SRM aligns image-text features through contrastive learning and cross-modal distillation to address the feature confusion caused by insufficient semantic understanding during masked reconstruction. To support the pre-training of VehicleMAE-V2, we construct Autobot4M, a large-scale dataset comprising approximately 4 million vehicle images and 12,693 text descriptions. Extensive experiments on five downstream tasks demonstrate the superior performance of VehicleMAE-V2.","authors":["Wentao Wu","Xiao Wang","Chenglong Li","Jin Tang","Bin Luo"],"pdf_url":"","comment":"Journal extension of VehicleMAE (AAAI 2024)"},{"id":"http://arxiv.org/abs/2512.19928v1","updated":"2025-12-22T23:05:26Z","published":"2025-12-22T23:05:26Z","title":"Unified Brain Surface and Volume Registration","summary":"Accurate registration of brain MRI scans is fundamental for cross-subject analysis in neuroscientific studies. This involves aligning both the cortical surface of the brain and the interior volume. Traditional methods treat volumetric and surface-based registration separately, which often leads to inconsistencies that limit downstream analyses. We propose a deep learning framework, NeurAlign, that registers $3$D brain MRI images by jointly aligning both cortical and subcortical regions through a unified volume-and-surface-based representation. Our approach leverages an intermediate spherical coordinate space to bridge anatomical surface topology with volumetric anatomy, enabling consistent and anatomically accurate alignment. By integrating spherical registration into the learning, our method ensures geometric coherence between volume and surface domains. In a series of experiments on both in-domain and out-of-domain datasets, our method consistently outperforms both classical and machine learning-based registration methods -- improving the Dice score by up to 7 points while maintaining regular deformation fields. Additionally, it is orders of magnitude faster than the standard method for this task, and is simpler to use because it requires no additional inputs beyond an MRI scan. With its superior accuracy, fast inference, and ease of use, NeurAlign sets a new standard for joint cortical and subcortical registration.","authors":["S. Mazdak Abulnaga","Andrew Hoopes","Malte Hoffmann","Robin Magnet","Maks Ovsjanikov","Lilla Zöllei","John Guttag","Bruce Fischl","Adrian Dalca"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.15503v2","updated":"2025-12-22T23:04:16Z","published":"2025-12-17T14:45:33Z","title":"Attention in Motion: Secure Platooning via Transformer-based Misbehavior Detection","summary":"Vehicular platooning promises transformative improvements in transportation efficiency and safety through the coordination of multi-vehicle formations enabled by Vehicle-to-Everything (V2X) communication. However, the distributed nature of platoon coordination creates security vulnerabilities, allowing authenticated vehicles to inject falsified kinematic data, compromise operational stability, and pose a threat to passenger safety. Traditional misbehaviour detection approaches, which rely on plausibility checks and statistical methods, suffer from high False Positive (FP) rates and cannot capture the complex temporal dependencies inherent in multi-vehicle coordination dynamics. We present Attention In Motion (AIMformer), a transformer-based framework specifically tailored for real-time misbehaviour detection in vehicular platoons with edge deployment capabilities. AIMformer leverages multi-head self-attention mechanisms to simultaneously capture intra-vehicle temporal dynamics and inter-vehicle spatial correlations. It incorporates global positional encoding with vehicle-specific temporal offsets to handle join/exit maneuvers. We propose a Precision-Focused Binary Cross-Entropy (PFBCE) loss function that penalizes FPs to meet the requirements of safety-critical vehicular systems. Extensive evaluation across 4 platoon controllers, multiple attack vectors, and diverse mobility scenarios demonstrates superior performance ($\\geq$ 0.93) compared to state-of-the-art baseline architectures. A comprehensive deployment analysis utilizing TensorFlow Lite (TFLite), Open Neural Network Exchange (ONNX), and TensorRT achieves sub-millisecond inference latency, making it suitable for real-time operation on resource-constrained edge platforms. Hence, validating AIMformer is viable for both in-vehicle and roadside infrastructure deployment.","authors":["Konstantinos Kalogiannis","Ahmed Mohamed Hussain","Hexu Li","Panos Papadimitratos"],"pdf_url":"","comment":"16 pages and 10 figures"},{"id":"http://arxiv.org/abs/2512.19920v1","updated":"2025-12-22T22:51:48Z","published":"2025-12-22T22:51:48Z","title":"Mitigating LLM Hallucination via Behaviorally Calibrated Reinforcement Learning","summary":"LLM deployment in critical domains is currently impeded by persistent hallucinations--generating plausible but factually incorrect assertions. While scaling laws drove significant improvements in general capabilities, theoretical frameworks suggest hallucination is not merely stochastic error but a predictable statistical consequence of training objectives prioritizing mimicking data distribution over epistemic honesty. Standard RLVR paradigms, utilizing binary reward signals, inadvertently incentivize models as good test-takers rather than honest communicators, encouraging guessing whenever correctness probability exceeds zero. This paper presents an exhaustive investigation into behavioral calibration, which incentivizes models to stochastically admit uncertainty by abstaining when not confident, aligning model behavior with accuracy. Synthesizing recent advances, we propose and evaluate training interventions optimizing strictly proper scoring rules for models to output a calibrated probability of correctness. Our methods enable models to either abstain from producing a complete response or flag individual claims where uncertainty remains. Utilizing Qwen3-4B-Instruct, empirical analysis reveals behavior-calibrated reinforcement learning allows smaller models to surpass frontier models in uncertainty quantification--a transferable meta-skill decouplable from raw predictive accuracy. Trained on math reasoning tasks, our model's log-scale Accuracy-to-Hallucination Ratio gain (0.806) exceeds GPT-5's (0.207) in a challenging in-domain evaluation (BeyondAIME). Moreover, in cross-domain factual QA (SimpleQA), our 4B LLM achieves zero-shot calibration error on par with frontier models including Grok-4 and Gemini-2.5-Pro, even though its factual accuracy is much lower.","authors":["Jiayun Wu","Jiashuo Liu","Zhiyuan Zeng","Tianyang Zhan","Wenhao Huang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19914v1","updated":"2025-12-22T22:37:58Z","published":"2025-12-22T22:37:58Z","title":"A Time-efficient Prioritised Scheduling Algorithm to Optimise Initial Flock Formation of Drones","summary":"Drone applications continue to expand across various domains, with flocking offering enhanced cooperative capabilities but introducing significant challenges during initial formation. Existing flocking algorithms often struggle with efficiency and scalability, particularly when potential collisions force drones into suboptimal trajectories. This paper presents a time-efficient prioritised scheduling algorithm that improves the initial formation process of drone flocks. The method assigns each drone a priority based on its number of potential collisions and its likelihood of reaching its target position without permanently obstructing other drones. Using this hierarchy, each drone computes an appropriate delay to ensure a collision-free path. Simulation results show that the proposed algorithm successfully generates collision-free trajectories for flocks of up to 5000 drones and outperforms the coupling-degree-based heuristic prioritised planning method (CDH-PP) in both performance and computational efficiency.","authors":["Sujan Warnakulasooriya","Andreas Willig","Xiaobing Wu"],"pdf_url":"","comment":"35 pages"},{"id":"http://arxiv.org/abs/2512.19909v1","updated":"2025-12-22T22:29:57Z","published":"2025-12-22T22:29:57Z","title":"Modeling Non-Ergodic Path Effects Using Conditional Generative Model for Fourier Amplitude Spectra","summary":"Recent developments in non-ergodic ground-motion models (GMMs) explicitly model systematic spatial variations in source, site, and path effects, reducing standard deviation to 30-40% of ergodic models and enabling more accurate site-specific seismic hazard analysis. Current non-ergodic GMMs rely on Gaussian Process (GP) methods with prescribed correlation functions and thus have computational limitations for large-scale predictions. This study proposes a deep-learning approach called Conditional Generative Modeling for Fourier Amplitude Spectra (CGM-FAS) as an alternative to GP-based methods for modeling non-ergodic path effects in Fourier Amplitude Spectra (FAS). CGM-FAS uses a Conditional Variational Autoencoder architecture to learn spatial patterns and interfrequency correlation directly from data by using geographical coordinates of earthquakes and stations as conditional variables. Using San Francisco Bay Area earthquake data, we compare CGM-FAS against a recent GP-based GMM for the region and demonstrate consistent predictions of non-ergodic path effects. Additionally, CGM-FAS offers advantages compared to GP-based approaches in learning spatial patterns without prescribed correlation functions, capturing interfrequency correlations, and enabling rapid predictions, generating maps for 10,000 sites across 1,000 frequencies within 10 seconds using a few GB of memory. CGM-FAS hyperparameters can be tuned to ensure generated path effects exhibit variability consistent with the GP-based empirical GMM. This work demonstrates a promising direction for efficient non-ergodic ground-motion prediction across multiple frequencies and large spatial domains.","authors":["Maxime Lacour","Pu Ren","Rie Nakata","Nori Nakata","Michael Mahoney"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2407.04258v2","updated":"2025-12-22T22:17:14Z","published":"2024-07-05T05:08:06Z","title":"Reinforcement Learning for Unsupervised Video Summarization with Reward Generator Training","summary":"This paper presents a novel approach for unsupervised video summarization using reinforcement learning (RL), addressing limitations like unstable adversarial training and reliance on heuristic-based reward functions. The method operates on the principle that reconstruction fidelity serves as a proxy for informativeness, correlating summary quality with reconstruction ability. The summarizer model assigns importance scores to frames to generate the final summary. For training, RL is coupled with a unique reward generation pipeline that incentivizes improved reconstructions. This pipeline uses a generator model to reconstruct the full video from the selected summary frames; the similarity between the original and reconstructed video provides the reward signal. The generator itself is pre-trained self-supervisedly to reconstruct randomly masked frames. This two-stage training process enhances stability compared to adversarial architectures. Experimental results show strong alignment with human judgments and promising F-scores, validating the reconstruction objective.","authors":["Mehryar Abbasi","Hadi Hadizadeh","Parvaneh Saeedi"],"pdf_url":"","comment":"in IEEE Transactions on Circuits and Systems for Video Technology"},{"id":"http://arxiv.org/abs/2512.19905v1","updated":"2025-12-22T22:13:06Z","published":"2025-12-22T22:13:06Z","title":"Demystifying LLM-as-a-Judge: Analytically Tractable Model for Inference-Time Scaling","summary":"Recent developments in large language models have shown advantages in reallocating a notable share of computational resource from training time to inference time. However, the principles behind inference time scaling are not well understood. In this paper, we introduce an analytically tractable model of inference-time scaling: Bayesian linear regression with a reward-weighted sampler, where the reward is determined from a linear model, modeling LLM-as-a-judge scenario. We study this problem in the high-dimensional regime, where the deterministic equivalents dictate a closed-form expression for the posterior predictive mean and variance. We analyze the generalization error when training data are sampled from a teacher model. We draw $k$ inference-time samples and select via softmax at a temperature applied to a quadratic reward. When the reward is not too different from the teacher, the generalization error decreases monotonically with increasing inference time samples $k$. However, the specific reward that optimizes inference-time selection generally differs from the teacher. In contrast, substantial reward misspecification induces a finite optimal $k$ beyond which more sampling can increase the generalization error. For fixed $k$, there exists an optimal sampling temperature. We experimentally verify these facts in large language model inference with an additional large language model as a judge. In the \"best-of-$k$\" limit with the teacher as reward, we theoretically show that the generalization error decays as $Θ(1/k^2)$ and determine the leading coefficient via extreme value theory. These formulas delineate domains where scaling inference-time computation is provably preferable to collecting more data. Finally, we demonstrate that when task difficulty increases, the previously mentioned advantage of inference-time compute degrades.","authors":["Indranil Halder","Cengiz Pehlevan"],"pdf_url":"","comment":"27 pages"},{"id":"http://arxiv.org/abs/2512.19882v1","updated":"2025-12-22T21:16:52Z","published":"2025-12-22T21:16:52Z","title":"A Branch-and-Price Algorithm for Fast and Equitable Last-Mile Relief Aid Distribution","summary":"The distribution of relief supplies to shelters is a critical aspect of post-disaster humanitarian logistics. In major disasters, prepositioned supplies often fall short of meeting all demands. We address the problem of planning vehicle routes from a distribution center to shelters while allocating limited relief supplies. To balance efficiency and equity, we formulate a bi-objective problem: minimizing a Gini-index-based measure of inequity in unsatisfied demand for fair distribution and minimizing total travel time for timely delivery. We propose a Mixed Integer Programming (MIP) model and use the $ε$-constraint method to handle the bi-objective nature. By deriving mathematical properties of the optimal solution, we introduce valid inequalities and design an algorithm for optimal delivery allocations given feasible vehicle routes. A branch-and-price (B&P) algorithm is developed to solve the problem efficiently. Computational tests on realistic datasets from a past earthquake in Van, Turkey, and predicted data for Istanbul's Kartal region show that the B&P algorithm significantly outperforms commercial MIP solvers. Our bi-objective approach reduces aid distribution inequity by 34% without compromising efficiency. Results indicate that when time constraints are very loose or tight, lexicographic optimization prioritizing demand coverage over fairness is effective. For moderately restrictive time constraints, a balanced approach is essential to avoid inequitable outcomes.","authors":["Mahdi Mostajabdaveh","F. Sibel Salman","Walter J. Gutjahr"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.08093v2","updated":"2025-12-22T21:12:55Z","published":"2025-12-08T23:05:52Z","title":"Training LLMs for Honesty via Confessions","summary":"Large language models (LLMs) can be dishonest when reporting on their actions and beliefs -- for example, they may overstate their confidence in factual claims or cover up evidence of covert actions. Such dishonesty may arise due to the effects of reinforcement learning (RL), where challenges with reward shaping can result in a training process that inadvertently incentivizes the model to lie or misrepresent its actions.\n  In this work we propose a method for eliciting an honest expression of an LLM's shortcomings via a self-reported *confession*. A confession is an output, provided upon request after a model's original answer, that is meant to serve as a full account of the model's compliance with the letter and spirit of its policies and instructions. The reward assigned to a confession during training is solely based on its honesty, and does not impact positively or negatively the main answer's reward. As long as the \"path of least resistance\" for maximizing confession reward is to surface misbehavior rather than covering it up, this incentivizes models to be honest in their confessions. Our findings provide some justification this empirical assumption, especially in the case of egregious model misbehavior.\n  To demonstrate the viability of our approach, we train GPT-5-Thinking to produce confessions, and we evaluate its honesty in out-of-distribution scenarios measuring hallucination, instruction following, scheming, and reward hacking. We find that when the model lies or omits shortcomings in its \"main\" answer, it often confesses to these behaviors honestly, and this confession honesty modestly improves with training. Confessions can enable a number of inference-time interventions including monitoring, rejection sampling, and surfacing issues to the user.","authors":["Manas Joglekar","Jeremy Chen","Gabriel Wu","Jason Yosinski","Jasmine Wang","Boaz Barak","Amelia Glaese"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19879v1","updated":"2025-12-22T21:12:02Z","published":"2025-12-22T21:12:02Z","title":"Fine-Tuned In-Context Learners for Efficient Adaptation","summary":"When adapting large language models (LLMs) to a specific downstream task, two primary approaches are commonly employed: (1) prompt engineering, often with in-context few-shot learning, leveraging the model's inherent generalization abilities, and (2) fine-tuning on task-specific data, directly optimizing the model's parameters. While prompt-based methods excel in few-shot scenarios, their effectiveness often plateaus as more data becomes available. Conversely, fine-tuning scales well with data but may underperform when training examples are scarce. We investigate a unified approach that bridges these two paradigms by incorporating in-context learning directly into the fine-tuning process. Specifically, we fine-tune the model on task-specific data augmented with in-context examples, mimicking the structure of k-shot prompts. This approach, while requiring per-task fine-tuning, combines the sample efficiency of in-context learning with the performance gains of fine-tuning, leading to a method that consistently matches and often significantly exceeds both these baselines. To perform hyperparameter selection in the low-data regime, we propose to use prequential evaluation, which eliminates the need for expensive cross-validation and leverages all available data for training while simultaneously providing a robust validation signal. We conduct an extensive empirical study to determine which adaptation paradigm - fine-tuning, in-context learning, or our proposed unified approach offers the best predictive performance on a concrete data downstream-tasks.","authors":["Jorg Bornschein","Clare Lyle","Yazhe Li","Amal Rannen-Triki","Xu Owen He","Razvan Pascanu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2509.08151v2","updated":"2025-12-22T20:59:23Z","published":"2025-09-09T21:18:31Z","title":"Trust Semantics Distillation for Collaborator Selection via Memory-Augmented Agentic AI","summary":"Offloading computational tasks from resource-constrained devices to resource-abundant peers constitutes a critical paradigm for collaborative computing. Within this context, accurate trust evaluation of potential collaborating devices is essential for the effective execution of complex computing tasks. This trust evaluation process involves collecting diverse trust-related information from every potential collaborator and performing trust inference based on the collected data. However, when each resource-constrained device independently assesses all potential collaborators, frequent data exchange and complex reasoning can incur significant overhead and further degrade the timeliness of trust evaluation. To overcome these challenges, we propose a task-specific trust semantics distillation (TSD) model based on a large AI model (LAM)-enabled teacher-student agent architecture. Specifically, the teacher agent is deployed on a server with powerful computational capabilities and an augmented memory module to perform multidimensional trust-related data collection, task-specific trust semantics extraction, and task-collaborator matching analysis. Upon receiving task-specific evaluation requests from device-side student agents, the teacher agent transfers the trust semantics of potential collaborators to the student agents, enabling rapid and accurate collaborator selection. Experimental results demonstrate that the proposed TSD model can reduce collaborator evaluation time, decrease device resource consumption, and improve the accuracy of collaborator selection.","authors":["Botao Zhu","Jeslyn Wang","Dusit Niyato","Xianbin Wang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2511.17318v2","updated":"2025-12-22T20:46:19Z","published":"2025-11-21T15:36:41Z","title":"FORWARD: Dataset of a forwarder operating in rough terrain","summary":"We present FORWARD, a high-resolution multimodal dataset of a cut-to-length forwarder operating in rough terrain on two harvest sites in the middle part of Sweden. The forwarder is a large Komatsu model equipped with vehicle telematics sensors, including global positioning via satellite navigation, movement sensors, accelerometers, and engine sensors. The vehicle was additionally equipped with cameras, operator vibration sensors, and multiple IMUs. The data includes event time logs recorded at 5 Hz of driving speed, fuel consumption, vehicle position with centimeter accuracy, and crane use while the vehicle operates in forest areas, aerially laser-scanned with a resolution of around 1500 points per square meter. Production log files (StanForD standard) with time-stamped machine events, extensive video material, and terrain data in various formats are included as well. About 18 hours of regular wood extraction work during three days is annotated from 360-video material into individual work elements and included in the dataset. We also include scenario specifications of conducted experiments on forest roads and in terrain. Scenarios include repeatedly driving the same routes with and without steel tracks, different load weights, and different target driving speeds. The dataset is intended for developing models and algorithms for trafficability, perception, and autonomous control of forest machines using artificial intelligence, simulation, and experiments on physical testbeds. In part, we focus on forwarders traversing terrain, avoiding or handling obstacles, and loading or unloading logs, with consideration for efficiency, fuel consumption, safety, and environmental impact. Other benefits of the open dataset include the ability to explore auto-generation and calibration of forestry machine simulators and automation scenario descriptions using the data recorded in the field.","authors":["Mikael Lundbäck","Erik Wallin","Carola Häggström","Mattias Nyström","Andreas Grönlund","Mats Richardson","Petrus Jönsson","William Arnvik","Lucas Hedström","Arvid Fälldin","Martin Servin"],"pdf_url":"","comment":"28 pages, 22 figures"},{"id":"http://arxiv.org/abs/2512.19864v1","updated":"2025-12-22T20:38:30Z","published":"2025-12-22T20:38:30Z","title":"HARMON-E: Hierarchical Agentic Reasoning for Multimodal Oncology Notes to Extract Structured Data","summary":"Unstructured notes within the electronic health record (EHR) contain rich clinical information vital for cancer treatment decision making and research, yet reliably extracting structured oncology data remains challenging due to extensive variability, specialized terminology, and inconsistent document formats. Manual abstraction, although accurate, is prohibitively costly and unscalable. Existing automated approaches typically address narrow scenarios - either using synthetic datasets, restricting focus to document-level extraction, or isolating specific clinical variables (e.g., staging, biomarkers, histology) - and do not adequately handle patient-level synthesis across the large number of clinical documents containing contradictory information. In this study, we propose an agentic framework that systematically decomposes complex oncology data extraction into modular, adaptive tasks. Specifically, we use large language models (LLMs) as reasoning agents, equipped with context-sensitive retrieval and iterative synthesis capabilities, to exhaustively and comprehensively extract structured clinical variables from real-world oncology notes. Evaluated on a large-scale dataset of over 400,000 unstructured clinical notes and scanned PDF reports spanning 2,250 cancer patients, our method achieves an average F1-score of 0.93, with 100 out of 103 oncology-specific clinical variables exceeding 0.85, and critical variables (e.g., biomarkers and medications) surpassing 0.95. Moreover, integration of the agentic system into a data curation workflow resulted in 0.94 direct manual approval rate, significantly reducing annotation costs. To our knowledge, this constitutes the first exhaustive, end-to-end application of LLM-based agents for structured oncology data extraction at scale","authors":["Shashi Kant Gupta","Arijeet Pramanik","Jerrin John Thomas","Regina Schwind","Lauren Wiener","Avi Raju","Jeremy Kornbluth","Yanshan Wang","Zhaohui Su","Hrituraj Singh"],"pdf_url":"","comment":"39 Pages, Supplementary Included"},{"id":"http://arxiv.org/abs/2503.12511v3","updated":"2025-12-22T20:28:48Z","published":"2025-03-16T14:05:26Z","title":"SACTOR: LLM-Driven Correct and Idiomatic C to Rust Translation with Static Analysis and FFI-Based Verification","summary":"Translating software written in C to Rust has significant benefits in improving memory safety. However, manual translation is cumbersome, error-prone, and often produces unidiomatic code. Large language models (LLMs) have demonstrated promise in producing idiomatic translations, but offer no correctness guarantees. We propose SACTOR, an LLM-driven C-to-Rust translation tool that employs a two-step process: an initial \"unidiomatic\" translation to preserve semantics, followed by an \"idiomatic\" refinement to align with Rust standards. To validate correctness of our function-wise incremental translation that mixes C and Rust, we use end-to-end testing via the foreign function interface. We evaluate SACTOR on 200 programs from two public datasets and on two more real-world scenarios (a 50-sample subset of CRust-Bench and the libogg library), comparing multiple LLMs. Across datasets, SACTOR delivers high end-to-end correctness and produces safe, idiomatic Rust with up to 7 times fewer Clippy warnings; On CRust-Bench, SACTOR achieves an average (across samples) of 85% unidiomatic and 52% idiomatic success, and on libogg it attains full unidiomatic and up to 78% idiomatic coverage on GPT-5.","authors":["Tianyang Zhou","Ziyi Zhang","Haowen Lin","Somesh Jha","Mihai Christodorescu","Kirill Levchenko","Varun Chandrasekaran"],"pdf_url":"","comment":"35 pages, 15 figures Previously named as \"LLM-Driven Multi-step Translation from C to Rust using Static Analysis\""},{"id":"http://arxiv.org/abs/2509.10534v2","updated":"2025-12-22T20:13:10Z","published":"2025-09-05T14:22:27Z","title":"Decoupling the \"What\" and \"Where\" With Polar Coordinate Positional Embeddings","summary":"The attention mechanism in a Transformer architecture matches key to query based on both content -- the what -- and position in a sequence -- the where. We present an analysis indicating that what and where are entangled in the popular RoPE rotary position embedding. This entanglement can impair performance particularly when decisions require independent matches on these two factors. We propose an improvement to RoPE, which we call Polar Coordinate Position Embeddings or PoPE, that eliminates the what-where confound. PoPE is far superior on a diagnostic task requiring indexing solely by position or by content. On autoregressive sequence modeling in music, genomic, and natural language domains, Transformers using PoPE as the positional encoding scheme outperform baselines using RoPE with respect to evaluation loss (perplexity) and downstream task performance. On language modeling, these gains persist across model scale, from 124M to 774M parameters. Crucially, PoPE shows strong zero-shot length extrapolation capabilities compared not only to RoPE but even a method designed for extrapolation, YaRN, which requires additional fine tuning and frequency interpolation.","authors":["Anand Gopalakrishnan","Robert Csordás","Jürgen Schmidhuber","Michael C. Mozer"],"pdf_url":"","comment":"Comparison to YaRN added + additional bias visualization + model ablation"},{"id":"http://arxiv.org/abs/2411.18714v2","updated":"2025-12-22T20:12:21Z","published":"2024-11-27T19:38:43Z","title":"Explainable deep learning improves human mental models of self-driving cars","summary":"Self-driving cars increasingly rely on deep neural networks to achieve human-like driving. The opacity of such black-box planners makes it challenging for the human behind the wheel to accurately anticipate when they will fail, with potentially catastrophic consequences. While research into interpreting these systems has surged, most of it is confined to simulations or toy setups due to the difficulty of real-world deployment, leaving the practical utility of such techniques unknown. Here, we introduce the Concept-Wrapper Network (CW-Net), a method for explaining the behavior of machine-learning-based planners by grounding their reasoning in human-interpretable concepts. We deploy CW-Net on a real self-driving car and show that the resulting explanations improve the human driver's mental model of the car, allowing them to better predict its behavior. To our knowledge, this is the first demonstration that explainable deep learning integrated into self-driving cars can be both understandable and useful in a realistic deployment setting. CW-Net accomplishes this level of intelligibility while providing explanations which are causally faithful and do not sacrifice driving performance. Overall, our study establishes a general pathway to interpretability for autonomous agents by way of concept-based explanations, which could help make them more transparent and safe.","authors":["Eoin M. Kenny","Akshay Dharmavaram","Sang Uk Lee","Tung Phan-Minh","Shreyas Rajesh","Yunqing Hu","Laura Major","Momchil S. Tomov","Julie A. Shah"],"pdf_url":"","comment":"MST & JAS contributed equally to this work"},{"id":"http://arxiv.org/abs/2512.19849v1","updated":"2025-12-22T20:05:09Z","published":"2025-12-22T20:05:09Z","title":"UCCL-EP: Portable Expert-Parallel Communication","summary":"Mixture-of-Experts (MoE) workloads rely on expert parallelism (EP) to achieve high GPU efficiency. State-of-the-art EP communication systems such as DeepEP demonstrate strong performance but exhibit poor portability across heterogeneous GPU and NIC platforms. The poor portability is rooted in architecture: GPU-initiated token-level RDMA communication requires tight vertical integration between GPUs and NICs, e.g., GPU writes to NIC driver/MMIO interfaces.\n  We present UCCL-EP, a portable EP communication system that delivers DeepEP-level performance across heterogeneous GPU and NIC hardware. UCCL-EP replaces GPU-initiated RDMA with a high-throughput GPU-CPU control channel: compact token-routing commands are transferred to multithreaded CPU proxies, which then issue GPUDirect RDMA operations on behalf of GPUs. UCCL-EP further emulates various ordering semantics required by specialized EP communication modes using RDMA immediate data, enabling correctness on NICs that lack such ordering, e.g., AWS EFA. We implement UCCL-EP on NVIDIA and AMD GPUs with EFA and Broadcom NICs. On EFA, it outperforms the best existing EP solution by up to $2.1\\times$ for dispatch and combine throughput. On NVIDIA-only platform, UCCL-EP achieves comparable performance to the original DeepEP. UCCL-EP also improves token throughput on SGLang by up to 40% on the NVIDIA+EFA platform, and improves DeepSeek-V3 training throughput over the AMD Primus/Megatron-LM framework by up to 45% on a 16-node AMD+Broadcom platform.","authors":["Ziming Mao","Yihan Zhang","Chihan Cui","Kaichao You","Zhongjie Chen","Zhiying Xu","Scott Shenker","Costin Raiciu","Yang Zhou","Ion Stoica"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2506.00920v2","updated":"2025-12-22T19:46:45Z","published":"2025-06-01T09:20:44Z","title":"Position as Probability: Self-Supervised Transformers that Think Past Their Training for Length Extrapolation","summary":"Deep sequence models typically degrade in accuracy when test sequences significantly exceed their training lengths, yet many critical tasks--such as algorithmic reasoning, multi-step arithmetic, and compositional generalization--require robust length extrapolation. We introduce PRISM, a Probabilistic Relative-position Implicit Superposition Model, a novel positional encoding mechanism that enables Transformers to extrapolate accurately up to 10x beyond their training length. PRISM learns continuous relative positions through a differentiable histogram-filter update, preserving position uncertainty via a probabilistic superposition rather than conventional deterministic embeddings. Empirically, PRISM achieves state-of-the-art length extrapolation, successfully generalizing to previously intractable sequence lengths across algorithmic benchmarks--including arithmetic (addition, multiplication), SCAN compositionality tasks, and complex copy variants derived from DeepMind's recent datasets. Our analysis demonstrates that PRISM's stochastic positional encoding maintains sharp and interpretable internal states, providing a theoretical basis for reliable length generalization. These results advance the goal of neural sequence models that remain algorithmically robust at lengths far exceeding their training horizon.","authors":["Philip Heejun Lee"],"pdf_url":"","comment":"Note: v2: working paper; code, additional baselines, ablations, will follow in v3"},{"id":"http://arxiv.org/abs/2412.16631v2","updated":"2025-12-22T19:13:55Z","published":"2024-12-21T13:53:15Z","title":"Deep Learning for Spatio-Temporal Fusion in Land Surface Temperature Estimation: A Comprehensive Survey, Experimental Analysis, and Future Trends","summary":"Land Surface Temperature (LST) plays a key role in climate monitoring, urban heat assessment, and land-atmosphere interactions. However, current thermal infrared satellite sensors cannot simultaneously achieve high spatial and temporal resolution. Spatio-temporal fusion (STF) techniques address this limitation by combining complementary satellite data, one with high spatial but low temporal resolution, and another with high temporal but low spatial resolution. Existing STF techniques, from classical models to modern deep learning (DL) architectures, were primarily developed for surface reflectance (SR). Their application to thermal data remains limited and often overlooks LST-specific spatial and temporal variability. This study provides a focused review of DL-based STF methods for LST. We present a formal mathematical definition of the thermal fusion task, propose a refined taxonomy of relevant DL methods, and analyze the modifications required when adapting SR-oriented models to LST. To support reproducibility and benchmarking, we introduce a new dataset comprising 51 Terra MODIS-Landsat LST pairs from 2013 to 2024, and evaluate representative models to explore their behavior on thermal data. The analysis highlights performance gaps, architecture sensitivities, and open research challenges. The dataset and accompanying resources are publicly available at https://github.com/Sofianebouaziz1/STF-LST.","authors":["Sofiane Bouaziz","Adel Hafiane","Raphael Canals","Rachid Nedjai"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2509.23410v3","updated":"2025-12-22T19:09:57Z","published":"2025-09-27T16:57:28Z","title":"PATCH: Learnable Tile-level Hybrid Sparsity for LLMs","summary":"Large language models (LLMs) deliver impressive performance but incur prohibitive memory and compute costs at deployment. Model pruning is an effective way to reduce these overheads, yet existing approaches face challenges: unstructured sparsity, where nonzeros can appear anywhere, preserves accuracy but yields irregular access patterns that prevent GPU acceleration, while semi-structured 2:4 sparsity is hardware-friendly but enforces a rigid 50% pattern that degrades model quality. To bridge this gap, we introduce PATCH, a hybrid sparsity framework that enables a continuous sparsity ratio between 0% and 50%. PATCH partitions weight matrices into tiles, assigning each tile to be either dense or 2:4 sparse via a learnable mask selection mechanism. This design provides fine-grained control over accuracy-acceleration tradeoffs and supports non-uniform sparsity across layers, leading to superior overall quality. Across models from 0.5B to 8B parameters, PATCH consistently narrows the gap to dense accuracy while delivering practical speedups. For instance, on LLaMA-2 7B with an A6000 GPU, PATCH achieves 1.18x-1.38x end-to-end speedup over dense baselines while improving accuracy by 0.37%-2.96% compared to the state-of-the-art 2:4 pruning method, MaskLLM.","authors":["Younes Hourri","Mohammad Mozaffari","Maryam Mehri Dehnavi"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2507.11181v2","updated":"2025-12-22T19:02:17Z","published":"2025-07-15T10:36:43Z","title":"Mixture of Experts in Large Language Models","summary":"This paper presents a comprehensive review of the Mixture-of-Experts (MoE) architecture in large language models, highlighting its ability to significantly enhance model performance while maintaining minimal computational overhead. Through a systematic analysis spanning theoretical foundations, core architectural designs, and large language model (LLM) applications, we examine expert gating and routing mechanisms, hierarchical and sparse MoE configurations, meta-learning approaches, multimodal and multitask learning scenarios, real-world deployment cases, and recent advances and challenges in deep learning. Our analysis identifies key advantages of MoE, including superior model capacity compared to equivalent Bayesian approaches, improved task-specific performance, and the ability to scale model capacity efficiently. We also underscore the importance of ensuring expert diversity, accurate calibration, and reliable inference aggregation, as these are essential for maximizing the effectiveness of MoE architectures. Finally, this review outlines current research limitations, open challenges, and promising future directions, providing a foundation for continued innovation in MoE architecture and its applications.","authors":["Danyang Zhang","Junhao Song","Ziqian Bi","Xinyuan Song","Yingfang Yuan","Tianyang Wang","Joe Yeong","Junfeng Hao"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19799v1","updated":"2025-12-22T19:00:15Z","published":"2025-12-22T19:00:15Z","title":"PhysMaster: Building an Autonomous AI Physicist for Theoretical and Computational Physics Research","summary":"Advances in LLMs have produced agents with knowledge and operational capabilities comparable to human scientists, suggesting potential to assist, accelerate, and automate research. However, existing studies mainly evaluate such systems on well-defined benchmarks or general tasks like literature retrieval, limiting their end-to-end problem-solving ability in open scientific scenarios. This is particularly true in physics, which is abstract, mathematically intensive, and requires integrating analytical reasoning with code-based computation. To address this, we propose PhysMaster, an LLM-based agent functioning as an autonomous theoretical and computational physicist. PhysMaster couples absract reasoning with numerical computation and leverages LANDAU, the Layered Academic Data Universe, which preserves retrieved literature, curated prior knowledge, and validated methodological traces, enhancing decision reliability and stability. It also employs an adaptive exploration strategy balancing efficiency and open-ended exploration, enabling robust performance in ultra-long-horizon tasks. We evaluate PhysMaster on problems from high-energy theory, condensed matter theory to astrophysics, including: (i) acceleration, compressing labor-intensive research from months to hours; (ii) automation, autonomously executing hypothesis-driven loops ; and (iii) autonomous discovery, independently exploring open problems.","authors":["Tingjia Miao","Jiawen Dai","Jingkun Liu","Jinxin Tan","Muhua Zhang","Wenkai Jin","Yuwen Du","Tian Jin","Xianghe Pang","Zexi Liu","Tu Guo","Zhengliang Zhang","Yunjie Huang","Shuo Chen","Rui Ye","Yuzhi Zhang","Linfeng Zhang","Kun Chen","Wei Wang","Weinan E","Siheng Chen"],"pdf_url":"","comment":"32 pages, 10 figures"}],"Computation and Language":[{"id":"http://arxiv.org/abs/2512.19682v1","updated":"2025-12-22T18:57:13Z","published":"2025-12-22T18:57:13Z","title":"GenEnv: Difficulty-Aligned Co-Evolution Between LLM Agents and Environment Simulators","summary":"Training capable Large Language Model (LLM) agents is critically bottlenecked by the high cost and static nature of real-world interaction data. We address this by introducing GenEnv, a framework that establishes a difficulty-aligned co-evolutionary game between an agent and a scalable, generative environment simulator. Unlike traditional methods that evolve models on static datasets, GenEnv instantiates a dataevolving: the simulator acts as a dynamic curriculum policy, continuously generating tasks specifically tailored to the agent's ``zone of proximal development''. This process is guided by a simple but effective $α$-Curriculum Reward, which aligns task difficulty with the agent's current capabilities. We evaluate GenEnv on five benchmarks, including API-Bank, ALFWorld, BFCL, Bamboogle, and TravelPlanner. Across these tasks, GenEnv improves agent performance by up to \\textbf{+40.3\\%} over 7B baselines and matches or exceeds the average performance of larger models. Compared to Gemini 2.5 Pro-based offline data augmentation, GenEnv achieves better performance while using 3.3$\\times$ less data. By shifting from static supervision to adaptive simulation, GenEnv provides a data-efficient pathway for scaling agent capabilities.","authors":["Jiacheng Guo","Ling Yang","Peter Chen","Qixin Xiao","Yinjie Wang","Xinzhe Juan","Jiahao Qiu","Ke Shen","Mengdi Wang"],"pdf_url":"","comment":"Our codes are available at https://github.com/Gen-Verse/GenEnv"},{"id":"http://arxiv.org/abs/2510.09595v2","updated":"2025-12-22T18:56:01Z","published":"2025-10-10T17:54:24Z","title":"LiveOIBench: Can Large Language Models Outperform Human Contestants in Informatics Olympiads?","summary":"Competitive programming problems increasingly serve as valuable benchmarks to evaluate the coding capabilities of large language models (LLMs) due to their complexity and ease of verification. Yet, current coding benchmarks face limitations such as lack of exceptionally challenging problems, insufficient test case coverage, reliance on online platform APIs that limit accessibility. To address these issues, we introduce LiveOIBench, a comprehensive benchmark featuring 403 expert-curated Olympiad-level competitive programming problems, each with an average of 60 expert-designed test cases. The problems are sourced directly from 72 official contests of 14 Informatics Olympiads in different regions conducted between 2023 and 2025. LiveOIBench distinguishes itself through four key features: (1) meticulously curated high-quality tasks with detailed subtask rubrics and extensive private test cases; (2) direct integration of elite contestant performance data to enable informative comparison against top-performing humans; (3) planned continuous, contamination-free updates from newly released Olympiad problems; and (4) a self-contained evaluation system facilitating offline and easy-to-reproduce assessments. Benchmarking 34 popular general-purpose and reasoning LLMs, we find that GPT-5 achieves a notable 81.76th percentile, a strong result that nonetheless falls short of top human contestants, who usually place above 90th. In contrast, among open-weight reasoning models, GPT-OSS-120B achieves only a 60th percentile, underscoring significant capability disparities from frontier closed models. Detailed analyses indicate that robust reasoning models prioritize precise problem analysis over excessive exploration, suggesting future models should emphasize structured analysis and minimize unnecessary exploration. All data, code, and leaderboard results are publicly available on our website.","authors":["Kaijian Zou","Aaron Xiong","Yunxiang Zhang","Frederick Zhang","Yueqi Ren","Jirong Yang","Ayoung Lee","Shitanshu Bhushan","Lu Wang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19673v1","updated":"2025-12-22T18:51:48Z","published":"2025-12-22T18:51:48Z","title":"Bottom-up Policy Optimization: Your Language Model Policy Secretly Contains Internal Policies","summary":"Existing reinforcement learning (RL) approaches treat large language models (LLMs) as a single unified policy, overlooking their internal mechanisms. Understanding how policy evolves across layers and modules is therefore crucial for enabling more targeted optimization and raveling out complex reasoning mechanisms. In this paper, we decompose the language model policy by leveraging the intrinsic split of the Transformer residual stream and the equivalence between the composition of hidden states with the unembedding matrix and the resulting samplable policy. This decomposition reveals Internal Layer Policies, corresponding to contributions from individual layers, and Internal Modular Policies, which align with the self-attention and feed-forward network (FFN) components within each layer. By analyzing the entropy of internal policy, we find that: (a) Early layers keep high entropy for exploration, top layers converge to near-zero entropy for refinement, with convergence patterns varying across model series. (b) LLama's prediction space rapidly converges in the final layer, whereas Qwen-series models, especially Qwen3, exhibit a more human-like, progressively structured reasoning pattern. Motivated by these findings, we propose Bottom-up Policy Optimization (BuPO), a novel RL paradigm that directly optimizes the internal layer policy during early training. By aligning training objective at lower layer, BuPO reconstructs foundational reasoning capabilities and achieves superior performance. Extensive experiments on complex reasoning benchmarks demonstrates the effectiveness of our method. Our code is available at https://github.com/Trae1ounG/BuPO.","authors":["Yuqiao Tan","Minzheng Wang","Shizhu He","Huanxuan Liao","Chengfeng Zhao","Qiunan Lu","Tian Liang","Jun Zhao","Kang Liu"],"pdf_url":"","comment":"Preprint. Our code is available at https://github.com/Trae1ounG/BuPO"},{"id":"http://arxiv.org/abs/2512.19651v1","updated":"2025-12-22T18:23:37Z","published":"2025-12-22T18:23:37Z","title":"Exploring Zero-Shot ACSA with Unified Meaning Representation in Chain-of-Thought Prompting","summary":"Aspect-Category Sentiment Analysis (ACSA) provides granular insights by identifying specific themes within reviews and their associated sentiment. While supervised learning approaches dominate this field, the scarcity and high cost of annotated data for new domains present significant barriers. We argue that leveraging large language models (LLMs) in a zero-shot setting is a practical alternative where resources for data annotation are limited. In this work, we propose a novel Chain-of-Thought (CoT) prompting technique that utilises an intermediate Unified Meaning Representation (UMR) to structure the reasoning process for the ACSA task. We evaluate this UMR-based approach against a standard CoT baseline across three models (Qwen3-4B, Qwen3-8B, and Gemini-2.5-Pro) and four diverse datasets. Our findings suggest that UMR effectiveness may be model-dependent. Whilst preliminary results indicate comparable performance for mid-sized models such as Qwen3-8B, these observations warrant further investigation, particularly regarding the potential applicability to smaller model architectures. Further research is required to establish the generalisability of these findings across different model scales.","authors":["Filippos Ventirozos","Peter Appleby","Matthew Shardlow"],"pdf_url":"","comment":"9 pages, 3 figures, 3 tables"},{"id":"http://arxiv.org/abs/2512.19630v1","updated":"2025-12-22T18:04:24Z","published":"2025-12-22T18:04:24Z","title":"Diacritic Restoration for Low-Resource Indigenous Languages: Case Study with Bribri and Cook Islands Māori","summary":"We present experiments on diacritic restoration, a form of text normalization essential for natural language processing (NLP) tasks. Our study focuses on two extremely under-resourced languages: Bribri, a Chibchan language spoken in Costa Rica, and Cook Islands Māori, a Polynesian language spoken in the Cook Islands. Specifically, this paper: (i) compares algorithms for diacritics restoration in under-resourced languages, including tonal diacritics, (ii) examines the amount of data required to achieve target performance levels, (iii) contrasts results across varying resource conditions, and (iv) explores the related task of diacritic correction. We find that fine-tuned, character-level LLMs perform best, likely due to their ability to decompose complex characters into their UTF-8 byte representations. In contrast, massively multilingual models perform less effectively given our data constraints. Across all models, reliable performance begins to emerge with data budgets of around 10,000 words. Zero-shot approaches perform poorly in all cases. This study responds both to requests from the language communities and to broader NLP research questions concerning model performance and generalization in under-resourced contexts.","authors":["Rolando Coto-Solano","Daisy Li","Manoela Teleginski Ferraz","Olivia Sasse","Cha Krupka","Sharid Loáiciga","Sally Akevai Tenamu Nicholas"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19620v1","updated":"2025-12-22T17:54:49Z","published":"2025-12-22T17:54:49Z","title":"Exploring the features used for summary evaluation by Human and GPT","summary":"Summary assessment involves evaluating how well a generated summary reflects the key ideas and meaning of the source text, requiring a deep understanding of the content. Large Language Models (LLMs) have been used to automate this process, acting as judges to evaluate summaries with respect to the original text. While previous research investigated the alignment between LLMs and Human responses, it is not yet well understood what properties or features are exploited by them when asked to evaluate based on a particular quality dimension, and there has not been much attention towards mapping between evaluation scores and metrics. In this paper, we address this issue and discover features aligned with Human and Generative Pre-trained Transformers (GPTs) responses by studying statistical and machine learning metrics. Furthermore, we show that instructing GPTs to employ metrics used by Human can improve their judgment and conforming them better with human responses.","authors":["Zahra Sadeghi","Evangelos Milios","Frank Rudzicz"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19612v1","updated":"2025-12-22T17:47:49Z","published":"2025-12-22T17:47:49Z","title":"MauBERT: Universal Phonetic Inductive Biases for Few-Shot Acoustic Units Discovery","summary":"This paper introduces MauBERT, a multilingual extension of HuBERT that leverages articulatory features for robust cross-lingual phonetic representation learning. We continue HuBERT pre-training with supervision based on a phonetic-to-articulatory feature mapping in 55 languages. Our models learn from multilingual data to predict articulatory features or phones, resulting in language-independent representations that capture multilingual phonetic properties. Through comprehensive ABX discriminability testing, we show MauBERT models produce more context-invariant representations than state-of-the-art multilingual self-supervised learning models. Additionally, the models effectively adapt to unseen languages and casual speech with minimal self-supervised fine-tuning (10 hours of speech). This establishes an effective approach for instilling linguistic inductive biases in self-supervised speech models.","authors":["Angelo Ortiz Tandazo","Manel Khentout","Youssef Benchekroun","Thomas Hueber","Emmanuel Dupoux"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2506.05594v2","updated":"2025-12-22T17:37:53Z","published":"2025-06-05T21:12:51Z","title":"SoK: Are Watermarks in LLMs Ready for Deployment?","summary":"Large Language Models (LLMs) have transformed natural language processing, demonstrating impressive capabilities across diverse tasks. However, deploying these models introduces critical risks related to intellectual property violations and potential misuse, particularly as adversaries can imitate these models to steal services or generate misleading outputs. We specifically focus on model stealing attacks, as they are highly relevant to proprietary LLMs and pose a serious threat to their security, revenue, and ethical deployment. While various watermarking techniques have emerged to mitigate these risks, it remains unclear how far the community and industry have progressed in developing and deploying watermarks in LLMs.\n  To bridge this gap, we aim to develop a comprehensive systematization for watermarks in LLMs by 1) presenting a detailed taxonomy for watermarks in LLMs, 2) proposing a novel intellectual property classifier to explore the effectiveness and impacts of watermarks on LLMs under both attack and attack-free environments, 3) analyzing the limitations of existing watermarks in LLMs, and 4) discussing practical challenges and potential future directions for watermarks in LLMs. Through extensive experiments, we show that despite promising research outcomes and significant attention from leading companies and community to deploy watermarks, these techniques have yet to reach their full potential in real-world applications due to their unfavorable impacts on model utility of LLMs and downstream tasks. Our findings provide an insightful understanding of watermarks in LLMs, highlighting the need for practical watermarks solutions tailored to LLM deployment.","authors":["Kieu Dang","Phung Lai","NhatHai Phan","Yelong Shen","Ruoming Jin","Abdallah Khreishah","My T. Thai"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19585v1","updated":"2025-12-22T17:12:04Z","published":"2025-12-22T17:12:04Z","title":"Increasing the Thinking Budget is Not All You Need","summary":"Recently, a new wave of thinking-capable Large Language Models has emerged, demonstrating exceptional capabilities across a wide range of reasoning benchmarks. Early studies have begun to explore how the amount of compute in terms of the length of the reasoning process, the so-called thinking budget, impacts model performance. In this work, we propose a systematic investigation of the thinking budget as a key parameter, examining its interaction with various configurations such as self-consistency, reflection, and others. Our goal is to provide an informative, balanced comparison framework that considers both performance outcomes and computational cost. Among our findings, we discovered that simply increasing the thinking budget is not the most effective use of compute. More accurate responses can instead be achieved through alternative configurations, such as self-consistency and self-reflection.","authors":["Ignacio Iacobacci","Zhaozhi Qian","Faroq AL-Tam","Muhammad AL-Qurishi","Riad Souissi"],"pdf_url":"","comment":"4 pages, 4 figures, 3 tables"},{"id":"http://arxiv.org/abs/2509.20490v3","updated":"2025-12-22T17:02:44Z","published":"2025-09-24T19:08:01Z","title":"RadAgents: Multimodal Agentic Reasoning for Chest X-ray Interpretation with Radiologist-like Workflows","summary":"Agentic systems offer a potential path to solve complex clinical tasks through collaboration among specialized agents, augmented by tool use and external knowledge bases. Nevertheless, for chest X-ray (CXR) interpretation, prevailing methods remain limited: (i) reasoning is frequently neither clinically interpretable nor aligned with guidelines, reflecting mere aggregation of tool outputs; (ii) multimodal evidence is insufficiently fused, yielding text-only rationales that are not visually grounded; and (iii) systems rarely detect or resolve cross-tool inconsistencies and provide no principled verification mechanisms. To bridge the above gaps, we present RadAgents, a multi-agent framework that couples clinical priors with task-aware multimodal reasoning and encodes a radiologist-style workflow into a modular, auditable pipeline. In addition, we integrate grounding and multimodal retrieval-augmentation to verify and resolve context conflicts, resulting in outputs that are more reliable, transparent, and consistent with clinical practice.","authors":["Kai Zhang","Corey D Barrett","Jangwon Kim","Lichao Sun","Tara Taghavi","Krishnaram Kenthapadi"],"pdf_url":"","comment":"ML4H'25; Work in progress"},{"id":"http://arxiv.org/abs/2409.20302v6","updated":"2025-12-22T16:29:48Z","published":"2024-09-30T14:00:04Z","title":"OM4OV: Leveraging Ontology Matching for Ontology Versioning","summary":"Due to the dynamic nature of the Semantic Web, version control is necessary to capture time-varying information for widely used ontologies. Despite the long-standing recognition of ontology versioning (OV) as a crucial component of efficient ontology management, many approaches treat OV as similar to ontology matching (OM) and directly reuse OM systems for OV tasks. In this study, we systematically analyse the similarities and differences between OM and OV and formalise the OM4OV pipeline. The pipeline is implemented and evaluated in the state-of-the-art OM system Agent-OM. The experimental results indicate that OM systems can be reused for OV tasks, but without necessary extensions, the current OM4OV pipeline can produce skewed measurements, poor performance in detecting update entities, and limited explainability for false mappings. To tackle these issues, we propose an optimisation method called the cross-reference (CR) mechanism, building upon the existing alignments from OM to reduce the number of matching candidates and improve overall OV performance.","authors":["Zhangcheng Qiang","Kerry Taylor","Weiqing Wang"],"pdf_url":"","comment":"16 pages, 8 figures, 1 table"},{"id":"http://arxiv.org/abs/2512.19543v1","updated":"2025-12-22T16:26:15Z","published":"2025-12-22T16:26:15Z","title":"Algerian Dialect","summary":"We present Algerian Dialect, a large-scale sentiment-annotated dataset consisting of 45,000 YouTube comments written in Algerian Arabic dialect. The comments were collected from more than 30 Algerian press and media channels using the YouTube Data API. Each comment is manually annotated into one of five sentiment categories: very negative, negative, neutral, positive, and very positive. In addition to sentiment labels, the dataset includes rich metadata such as collection timestamps, like counts, video URLs, and annotation dates. This dataset addresses the scarcity of publicly available resources for Algerian dialect and aims to support research in sentiment analysis, dialectal Arabic NLP, and social media analytics. The dataset is publicly available on Mendeley Data under a CC BY 4.0 license at https://doi.org/10.17632/zzwg3nnhsz.2.","authors":["Zakaria Benmounah","Abdennour Boulesnane"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.16401v2","updated":"2025-12-22T16:22:23Z","published":"2025-12-18T10:56:27Z","title":"Navigating the Reality Gap: Privacy-Preserving Adaptation of ASR for Challenging Low-Resource Domains","summary":"Automatic Speech Recognition (ASR) holds immense potential to assist in clinical documentation and patient report generation, particularly in resource-constrained regions. However, deployment is currently hindered by a technical deadlock: a severe \"Reality Gap\" between laboratory performance and noisy, real-world clinical audio, coupled with strict privacy and resource constraints. We quantify this gap, showing that a robust multilingual model (IndicWav2Vec) degrades to a 40.94% WER on rural clinical data from India, rendering it unusable. To address this, we explore a zero-data-exfiltration framework enabling localized, continual adaptation via Low-Rank Adaptation (LoRA). We conduct a rigorous investigative study of continual learning strategies, characterizing the trade-offs between data-driven and parameter-driven stability. Our results demonstrate that multi-domain Experience Replay (ER) yields the primary performance gains, achieving a 17.1% relative improvement in target WER and reducing catastrophic forgetting by 55% compared to naive adaptation. Furthermore, we observed that standard Elastic Weight Consolidation (EWC) faced numerical stability challenges when applied to LoRA in noisy environments. Our experiments show that a stabilized, linearized formulation effectively controls gradient magnitudes and enables stable convergence. Finally, we verify via a domain-specific spot check that acoustic adaptation is a fundamental prerequisite for usability which cannot be bypassed by language models alone.","authors":["Darshil Chauhan","Adityasinh Solanki","Vansh Patel","Kanav Kapoor","Ritvik Jain","Aditya Bansal","Pratik Narang","Dhruv Kumar"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19537v1","updated":"2025-12-22T16:22:14Z","published":"2025-12-22T16:22:14Z","title":"Event Extraction in Large Language Model","summary":"Large language models (LLMs) and multimodal LLMs are changing event extraction (EE): prompting and generation can often produce structured outputs in zero shot or few shot settings. Yet LLM based pipelines face deployment gaps, including hallucinations under weak constraints, fragile temporal and causal linking over long contexts and across documents, and limited long horizon knowledge management within a bounded context window. We argue that EE should be viewed as a system component that provides a cognitive scaffold for LLM centered solutions. Event schemas and slot constraints create interfaces for grounding and verification; event centric structures act as controlled intermediate representations for stepwise reasoning; event links support relation aware retrieval with graph based RAG; and event stores offer updatable episodic and agent memory beyond the context window. This survey covers EE in text and multimodal settings, organizing tasks and taxonomy, tracing method evolution from rule based and neural models to instruction driven and generative frameworks, and summarizing formulations, decoding strategies, architectures, representations, datasets, and evaluation. We also review cross lingual, low resource, and domain specific settings, and highlight open challenges and future directions for reliable event centric systems. Finally, we outline open challenges and future directions that are central to the LLM era, aiming to evolve EE from static extraction into a structurally reliable, agent ready perception and memory layer for open world systems.","authors":["Bobo Li","Xudong Han","Jiang Liu","Yuzhe Ding","Liqiang Jing","Zhaoqi Zhang","Jinheng Li","Xinya Du","Fei Li","Meishan Zhang","Min Zhang","Aixin Sun","Philip S. Yu","Hao Fei"],"pdf_url":"","comment":"38 pages, 9 Figures, 5 Tables"},{"id":"http://arxiv.org/abs/2402.08971v3","updated":"2025-12-22T16:15:22Z","published":"2024-02-14T06:33:22Z","title":"Structured Language Generation Model: Loss Calibration and Formatted Decoding for Robust Structure Prediction and Knowledge Retrieval","summary":"Modern generative pre-trained language models excel at open-ended text generation, yet continue to underperform on structure-related tasks such as NER, relation extraction, and semantic role labeling, especially when compared to encoder-only models of similar sizes. While this gap has been attributed to limited structure knowledge, we hypothesize this is also due to the missing connection between the model's internal representations of linguistic structure and the output space used during supervised fine-tuning. We propose the Structured Language Generation Model (SLGM), a model- and task-agnostic framework that reformulates structured prediction as a classification problem through three components: (1) reinforced input formatting with structural cues, (2) loss design, and (3) format-aware decoding that constrains generation to task-valid outputs. Across 5 tasks and 13 datasets, SLGM substantially improves structure prediction without relying on dataset-specific engineering or additional model parameters, strengthening alignment between the model's internal structure representation and output. It outperforms baseline fine-tuning on models of the same size, achieves comparable performance to much larger models when used with <1B parameter models, and acts as a zero-weight adapter that reproduces the benefits of dataset-specific fine-tuning in low-resource settings.","authors":["Minho Lee","Junghyun Min","Yerang Kim","Woochul Lee","Yeonsoo Lee"],"pdf_url":"","comment":"20 pages, 4 figures. FrontierIR at AAAI 2026"},{"id":"http://arxiv.org/abs/2509.23863v2","updated":"2025-12-22T16:06:03Z","published":"2025-09-28T13:08:10Z","title":"SPELL: Self-Play Reinforcement Learning for evolving Long-Context Language Models","summary":"Progress in long-context reasoning for large language models (LLMs) has lagged behind other recent advances. This gap arises not only from the intrinsic difficulty of processing long texts, but also from the scarcity of reliable human annotations and programmatically verifiable reward signals. In this paper, we propose SPELL, a multi-role self-play reinforcement learning framework that enables scalable, label-free optimization for long-context reasoning. SPELL integrates three cyclical roles-questioner, responder, and verifier-within a single model to enable continual self-improvement. The questioner generates questions from raw documents paired with reference answers; the responder learns to solve these questions based on the documents; and the verifier evaluates semantic equivalence between the responder's output and the questioner's reference answer, producing reward signals to guide continual training. To stabilize training, we introduce an automated curriculum that gradually increases document length and a reward function that adapts question difficulty to the model's evolving capabilities. Extensive experiments on six long-context benchmarks show that SPELL consistently improves performance across diverse LLMs and outperforms equally sized models fine-tuned on large-scale annotated data. Notably, SPELL achieves an average 7.6-point gain in pass@8 on the strong reasoning model Qwen3-30B-A3B-Thinking, raising its performance ceiling and showing promise for scaling to even more capable models.","authors":["Ziyi Yang","Weizhou Shen","Chenliang Li","Ruijun Chen","Fanqi Wan","Ming Yan","Xiaojun Quan","Fei Huang"],"pdf_url":"","comment":"Preprint under review"},{"id":"http://arxiv.org/abs/2505.20063v2","updated":"2025-12-22T15:49:59Z","published":"2025-05-26T14:47:59Z","title":"SAEs Are Good for Steering -- If You Select the Right Features","summary":"Sparse Autoencoders (SAEs) have been proposed as an unsupervised approach to learn a decomposition of a model's latent space. This enables useful applications such as steering - influencing the output of a model towards a desired concept - without requiring labeled data. Current methods identify SAE features to steer by analyzing the input tokens that activate them. However, recent work has highlighted that activations alone do not fully describe the effect of a feature on the model's output. In this work, we draw a distinction between two types of features: input features, which mainly capture patterns in the model's input, and output features, which have a human-understandable effect on the model's output. We propose input and output scores to characterize and locate these types of features, and show that high values for both scores rarely co-occur in the same features. These findings have practical implications: after filtering out features with low output scores, we obtain 2-3x improvements when steering with SAEs, making them competitive with supervised methods.","authors":["Dana Arad","Aaron Mueller","Yonatan Belinkov"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19475v1","updated":"2025-12-22T15:28:55Z","published":"2025-12-22T15:28:55Z","title":"A Large-Language-Model Framework for Automated Humanitarian Situation Reporting","summary":"Timely and accurate situational reports are essential for humanitarian decision-making, yet current workflows remain largely manual, resource intensive, and inconsistent. We present a fully automated framework that uses large language models (LLMs) to transform heterogeneous humanitarian documents into structured and evidence-grounded reports. The system integrates semantic text clustering, automatic question generation, retrieval augmented answer extraction with citations, multi-level summarization, and executive summary generation, supported by internal evaluation metrics that emulate expert reasoning. We evaluated the framework across 13 humanitarian events, including natural disasters and conflicts, using more than 1,100 documents from verified sources such as ReliefWeb. The generated questions achieved 84.7 percent relevance, 84.0 percent importance, and 76.4 percent urgency. The extracted answers reached 86.3 percent relevance, with citation precision and recall both exceeding 76 percent. Agreement between human and LLM based evaluations surpassed an F1 score of 0.80. Comparative analysis shows that the proposed framework produces reports that are more structured, interpretable, and actionable than existing baselines. By combining LLM reasoning with transparent citation linking and multi-level evaluation, this study demonstrates that generative AI can autonomously produce accurate, verifiable, and operationally useful humanitarian situation reports.","authors":["Ivan Decostanzi","Yelena Mejova","Kyriaki Kalimeri"],"pdf_url":"","comment":"18 pages, 3 figures"},{"id":"http://arxiv.org/abs/2512.19466v1","updated":"2025-12-22T15:20:21Z","published":"2025-12-22T15:20:21Z","title":"Epistemological Fault Lines Between Human and Artificial Intelligence","summary":"Large language models (LLMs) are widely described as artificial intelligence, yet their epistemic profile diverges sharply from human cognition. Here we show that the apparent alignment between human and machine outputs conceals a deeper structural mismatch in how judgments are produced. Tracing the historical shift from symbolic AI and information filtering systems to large-scale generative transformers, we argue that LLMs are not epistemic agents but stochastic pattern-completion systems, formally describable as walks on high-dimensional graphs of linguistic transitions rather than as systems that form beliefs or models of the world. By systematically mapping human and artificial epistemic pipelines, we identify seven epistemic fault lines, divergences in grounding, parsing, experience, motivation, causal reasoning, metacognition, and value. We call the resulting condition Epistemia: a structural situation in which linguistic plausibility substitutes for epistemic evaluation, producing the feeling of knowing without the labor of judgment. We conclude by outlining consequences for evaluation, governance, and epistemic literacy in societies increasingly organized around generative AI.","authors":["Walter Quattrociocchi","Valerio Capraro","Matjaž Perc"],"pdf_url":"","comment":"16 pages, 1 figure"},{"id":"http://arxiv.org/abs/2512.19456v1","updated":"2025-12-22T15:01:07Z","published":"2025-12-22T15:01:07Z","title":"Activations as Features: Probing LLMs for Generalizable Essay Scoring Representations","summary":"Automated essay scoring (AES) is a challenging task in cross-prompt settings due to the diversity of scoring criteria. While previous studies have focused on the output of large language models (LLMs) to improve scoring accuracy, we believe activations from intermediate layers may also provide valuable information. To explore this possibility, we evaluated the discriminative power of LLMs' activations in cross-prompt essay scoring task. Specifically, we used activations to fit probes and further analyzed the effects of different models and input content of LLMs on this discriminative power. By computing the directions of essays across various trait dimensions under different prompts, we analyzed the variation in evaluation perspectives of large language models concerning essay types and traits. Results show that the activations possess strong discriminative power in evaluating essay quality and that LLMs can adapt their evaluation perspectives to different traits and essay types, effectively handling the diversity of scoring criteria in cross-prompt settings.","authors":["Jinwei Chi","Ke Wang","Yu Chen","Xuanye Lin","Qiang Xu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19455v1","updated":"2025-12-22T15:00:25Z","published":"2025-12-22T15:00:25Z","title":"SiamGPT: Quality-First Fine-Tuning for Stable Thai Text Generation","summary":"Open-weights large language models remain difficult to deploy for Thai due to unstable generation under complex instructions, despite strong English performance. To mitigate these limitations, We present SiamGPT-32B, an open-weights model based on Qwen3-32B, fine-tuned with a Quality-First strategy emphasizing curated supervision over data scale. The fine-tuning pipeline combines translated high-complexity English instruction data with a Thai-adapted AutoIF framework for instruction and linguistic constraints. Using supervised fine-tuning only, without continual pretraining or corpus expansion, SiamGPT-32B improves instruction adherence, multi-turn robustness, and linguistic stability. Evaluations on the SEA-HELM benchmark show that SiamGPT-32B achieves the strongest overall performance among similar-scale open-weights Thai models, with consistent gains in instruction following, multi-turn dialogue, and natural language understanding.","authors":["Thittipat Pairatsuppawat","Abhibhu Tachaapornchai","Paweekorn Kusolsomboon","Chutikan Chaiwong","Thodsaporn Chay-intr","Kobkrit Viriyayudhakorn","Nongnuch Ketui","Aslan B. Wong"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19432v1","updated":"2025-12-22T14:31:28Z","published":"2025-12-22T14:31:28Z","title":"MobileWorld: Benchmarking Autonomous Mobile Agents in Agent-User Interactive, and MCP-Augmented Environments","summary":"Among existing online mobile-use benchmarks, AndroidWorld has emerged as the dominant benchmark due to its reproducible environment and deterministic evaluation; however, recent agents achieving over 90% success rates indicate its saturation and motivate the need for a more challenging benchmark. In addition, its environment lacks key application categories, such as e-commerce and enterprise communication, and does not reflect realistic mobile-use scenarios characterized by vague user instructions and hybrid tool usage. To bridge this gap, we introduce MobileWorld, a substantially more challenging benchmark designed to better reflect real-world mobile usage, comprising 201 tasks across 20 applications, while maintaining the same level of reproducible evaluation as AndroidWorld. The difficulty of MobileWorld is twofold. First, it emphasizes long-horizon tasks with cross-application interactions: MobileWorld requires nearly twice as many task-completion steps on average (27.8 vs. 14.3) and includes far more multi-application tasks (62.2% vs. 9.5%) compared to AndroidWorld. Second, MobileWorld extends beyond standard GUI manipulation by introducing novel task categories, including agent-user interaction and MCP-augmented tasks. To ensure robust evaluation, we provide snapshot-based container environment and precise functional verifications, including backend database inspection and task callback APIs. We further develop a planner-executor agentic framework with extended action spaces to support user interactions and MCP calls. Our results reveal a sharp performance drop compared to AndroidWorld, with the best agentic framework and end-to-end model achieving 51.7% and 20.9% success rates, respectively. Our analysis shows that current models struggle significantly with user interaction and MCP calls, offering a strategic roadmap toward more robust, next-generation mobile intelligence.","authors":["Quyu Kong","Xu Zhang","Zhenyu Yang","Nolan Gao","Chen Liu","Panrong Tong","Chenglin Cai","Hanzhang Zhou","Jianan Zhang","Liangyu Chen","Zhidan Liu","Steven Hoi","Yue Wang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2505.18822v2","updated":"2025-12-22T14:30:53Z","published":"2025-05-24T18:46:50Z","title":"AdaCtrl: Towards Adaptive and Controllable Reasoning via Difficulty-Aware Budgeting","summary":"Modern large reasoning models demonstrate impressive problem-solving capabilities by employing sophisticated reasoning strategies. However, they often struggle to balance efficiency and effectiveness, frequently generating unnecessarily lengthy reasoning chains for simple problems. In this work, we propose AdaCtrl, a novel framework to support both difficulty-aware adaptive reasoning budget allocation and explicit user control over reasoning depth. AdaCtrl dynamically adjusts its reasoning length based on self-assessed problem difficulty, while also allowing users to manually control the budget to prioritize either efficiency or effectiveness. This is achieved through a two-stage training pipeline: an initial cold-start fine-tuning phase to instill the ability to self-aware difficulty and adjust reasoning budget, followed by a difficulty-aware reinforcement learning (RL) stage that refines the model's adaptive reasoning strategies and calibrates its difficulty assessments based on its evolving capabilities during online training. To enable intuitive user interaction, we design explicit length-triggered tags that function as a natural interface for budget control. Empirical results show that AdaCtrl adapts reasoning length based on estimated difficulty, compared to the standard training baseline that also incorporates fine-tuning and RL, it yields performance improvements and simultaneously reduces response length by 10.06% and 12.14% on the more challenging AIME2024 and AIME2025 datasets, which require elaborate reasoning, and by 62.05% and 91.04% on the MATH500 and GSM8K datasets, where more concise responses are sufficient. Furthermore, AdaCtrl enables precise user control over the reasoning budget, allowing for tailored responses to meet specific needs.","authors":["Shijue Huang","Hongru Wang","Wanjun Zhong","Zhaochen Su","Jiazhan Feng","Bowen Cao","Yi R. Fung"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19424v1","updated":"2025-12-22T14:27:17Z","published":"2025-12-22T14:27:17Z","title":"CodeSimpleQA: Scaling Factuality in Code Large Language Models","summary":"Large language models (LLMs) have made significant strides in code generation, achieving impressive capabilities in synthesizing code snippets from natural language instructions. However, a critical challenge remains in ensuring LLMs generate factually accurate responses about programming concepts, technical implementations, etc. Most previous code-related benchmarks focus on code execution correctness, overlooking the factual accuracy of programming knowledge. To address this gap, we present CodeSimpleQA, a comprehensive bilingual benchmark designed to evaluate the factual accuracy of code LLMs in answering code-related questions, which contains carefully curated question-answer pairs in both English and Chinese, covering diverse programming languages and major computer science domains. Further, we create CodeSimpleQA-Instruct, a large-scale instruction corpus with 66M samples, and develop a post-training framework combining supervised fine-tuning and reinforcement learning. Our comprehensive evaluation of diverse LLMs reveals that even frontier LLMs struggle with code factuality. Our proposed framework demonstrates substantial improvements over the base model, underscoring the critical importance of factuality-aware alignment in developing reliable code LLMs.","authors":["Jian Yang","Wei Zhang","Yizhi Li","Shawn Guo","Haowen Wang","Aishan Liu","Ge Zhang","Zili Wang","Zhoujun Li","Xianglong Liu","Weifeng Lv"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19414v1","updated":"2025-12-22T14:13:01Z","published":"2025-12-22T14:13:01Z","title":"From Retrieval to Reasoning: A Framework for Cyber Threat Intelligence NER with Explicit and Adaptive Instructions","summary":"The automation of Cyber Threat Intelligence (CTI) relies heavily on Named Entity Recognition (NER) to extract critical entities from unstructured text. Currently, Large Language Models (LLMs) primarily address this task through retrieval-based In-Context Learning (ICL). This paper analyzes this mainstream paradigm, revealing a fundamental flaw: its success stems not from global semantic similarity but largely from the incidental overlap of entity types within retrieved examples. This exposes the limitations of relying on unreliable implicit induction. To address this, we propose TTPrompt, a framework shifting from implicit induction to explicit instruction. TTPrompt maps the core concepts of CTI's Tactics, Techniques, and Procedures (TTPs) into an instruction hierarchy: formulating task definitions as Tactics, guiding strategies as Techniques, and annotation guidelines as Procedures. Furthermore, to handle the adaptability challenge of static guidelines, we introduce Feedback-driven Instruction Refinement (FIR). FIR enables LLMs to self-refine guidelines by learning from errors on minimal labeled data, adapting to distinct annotation dialects. Experiments on five CTI NER benchmarks demonstrate that TTPrompt consistently surpasses retrieval-based baselines. Notably, with refinement on just 1% of training data, it rivals models fine-tuned on the full dataset. For instance, on LADDER, its Micro F1 of 71.96% approaches the fine-tuned baseline, and on the complex CTINexus, its Macro F1 exceeds the fine-tuned ACLM model by 10.91%.","authors":["Jiaren Peng","Hongda Sun","Xuan Tian","Cheng Huang","Zeqing Li","Rui Yan"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.08398v2","updated":"2025-12-22T13:57:26Z","published":"2025-12-09T09:26:37Z","title":"Ontology-Based Knowledge Graph Framework for Industrial Standard Documents via Hierarchical and Propositional Structuring","summary":"Ontology-based knowledge graph (KG) construction is a core technology that enables multidimensional understanding and advanced reasoning over domain knowledge. Industrial standards, in particular, contain extensive technical information and complex rules presented in highly structured formats that combine tables, scopes of application, constraints, exceptions, and numerical calculations, making KG construction especially challenging. In this study, we propose a method that organizes such documents into a hierarchical semantic structure, decomposes sentences and tables into atomic propositions derived from conditional and numerical rules, and integrates them into an ontology-knowledge graph through LLM-based triple extraction. Our approach captures both the hierarchical and logical structures of documents, effectively representing domain-specific semantics that conventional methods fail to reflect. To verify its effectiveness, we constructed rule, table, and multi-hop QA datasets, as well as a toxic clause detection dataset, from industrial standards, and implemented an ontology-aware KG-RAG framework for comparative evaluation. Experimental results show that our method achieves significant performance improvements across all QA types compared to existing KG-RAG approaches. This study demonstrates that reliable and scalable knowledge representation is feasible even for industrial documents with intertwined conditions, constraints, and scopes, contributing to future domain-specific RAG development and intelligent document management.","authors":["Jiin Park","Hyuna Jeon","Yoonseo Lee","Jisu Hong","Misuk Kim"],"pdf_url":"","comment":"The authors have identified significant technical errors in the paper that invalidate the current findings"},{"id":"http://arxiv.org/abs/2512.19400v1","updated":"2025-12-22T13:52:33Z","published":"2025-12-22T13:52:33Z","title":"Kunnafonidilaw ka Cadeau: an ASR dataset of present-day Bambara","summary":"We present Kunkado, a 160-hour Bambara ASR dataset compiled from Malian radio archives to capture present-day spontaneous speech across a wide range of topics. It includes code-switching, disfluencies, background noise, and overlapping speakers that practical ASR systems encounter in real-world use. We finetuned Parakeet-based models on a 33.47-hour human-reviewed subset and apply pragmatic transcript normalization to reduce variability in number formatting, tags, and code-switching annotations. Evaluated on two real-world test sets, finetuning with Kunkado reduces WER from 44.47\\% to 37.12\\% on one and from 36.07\\% to 32.33\\% on the other. In human evaluation, the resulting model also outperforms a comparable system with the same architecture trained on 98 hours of cleaner, less realistic speech. We release the data and models to support robust ASR for predominantly oral languages.","authors":["Yacouba Diarra","Panga Azazia Kamate","Nouhoum Souleymane Coulibaly","Michael Leventhal"],"pdf_url":"","comment":"7 pages, 2 figures"},{"id":"http://arxiv.org/abs/2512.16229v2","updated":"2025-12-22T13:29:11Z","published":"2025-12-18T06:22:01Z","title":"LoPA: Scaling dLLM Inference via Lookahead Parallel Decoding","summary":"Diffusion Large Language Models (dLLMs) have demonstrated significant potential for high-speed inference. However, current confidence-driven decoding strategies are constrained by limited parallelism, typically achieving only 1--3 tokens per forward pass (TPF). In this work, we identify that the degree of parallelism during dLLM inference is highly sensitive to the Token Filling Order (TFO). Then, we introduce Lookahead PArallel Decoding LoPA, a training-free, plug-and-play algorithm, to identify a superior TFO and hence accelerate inference. LoPA concurrently explores distinct candidate TFOs via parallel branches, and selects the one with the highest potential for future parallelism based on branch confidence. We apply LoPA to the state-of-the-art D2F model and observe a substantial enhancement in decoding efficiency. Notably, LoPA increases the TPF of D2F-Dream to 10.1 on the GSM8K while maintaining performance superior to the Dream baseline. Furthermore, to facilitate this unprecedented degree of parallelism, we develop a specialized multi-device inference system featuring Branch Parallelism (BP), which achieves a single-sample throughput of 1073.9 tokens per second under multi-GPU deployment. The code is available at https://github.com/zhijie-group/LoPA.","authors":["Chenkai Xu","Yijie Jin","Jiajun Li","Yi Tu","Guoping Long","Dandan Tu","Mingcong Song","Hongjie Si","Tianqi Hou","Junchi Yan","Zhijie Deng"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19378v1","updated":"2025-12-22T13:23:29Z","published":"2025-12-22T13:23:29Z","title":"HATS: High-Accuracy Triple-Set Watermarking for Large Language Models","summary":"Misuse of LLM-generated text can be curbed by watermarking techniques that embed implicit signals into the output. We propose a watermark that partitions the vocabulary at each decoding step into three sets (Green/Yellow/Red) with fixed ratios and restricts sampling to the Green and Yellow sets. At detection time, we replay the same partitions, compute Green-enrichment and Red-depletion statistics, convert them to one-sided z-scores, and aggregate their p-values via Fisher's method to decide whether a passage is watermarked. We implement generation, detection, and testing on Llama 2 7B, and evaluate true-positive rate, false-positive rate, and text quality. Results show that the triple-partition scheme achieves high detection accuracy at fixed FPR while preserving readability.","authors":["Zhiqing Hu","Chenxu Zhao","Jiazhong Lu","Xiaolei Liu"],"pdf_url":"","comment":"Camera-ready version of the paper accepted for oral presentation at the 11th International Conference on Computer and Communications (ICCC 2025)"},{"id":"http://arxiv.org/abs/2506.17231v2","updated":"2025-12-22T13:04:30Z","published":"2025-05-26T08:27:51Z","title":"Efficient and Stealthy Jailbreak Attacks via Adversarial Prompt Distillation from LLMs to SLMs","summary":"As the scale and complexity of jailbreaking attacks on large language models (LLMs) continue to escalate, their efficiency and practical applicability are constrained, posing a profound challenge to LLM security. Jailbreaking techniques have advanced from manual prompt engineering to automated methodologies. Recent advances have automated jailbreaking approaches that harness LLMs to generate jailbreak instructions and adversarial examples, delivering encouraging results. Nevertheless, these methods universally include an LLM generation phase, which, due to the complexities of deploying and reasoning with LLMs, impedes effective implementation and broader adoption. To mitigate this issue, we introduce \\textbf{Adversarial Prompt Distillation}, an innovative framework that integrates masked language modeling, reinforcement learning, and dynamic temperature control to distill LLM jailbreaking prowess into smaller language models (SLMs). This methodology enables efficient, robust jailbreak attacks while maintaining high success rates and accommodating a broader range of application contexts. Empirical evaluations affirm the approach's superiority in attack efficacy, resource optimization, and cross-model versatility. Our research underscores the practicality of transferring jailbreak capabilities to SLMs, reveals inherent vulnerabilities in LLMs, and provides novel insights to advance LLM security investigations. Our code is available at: https://github.com/lxgem/Efficient_and_Stealthy_Jailbreak_Attacks_via_Adversarial_Prompt.","authors":["Xiang Li","Chong Zhang","Jia Wang","Fangyu Wu","Yushi Li","Xiaobo Jin"],"pdf_url":"","comment":"19 pages, 7 figures"},{"id":"http://arxiv.org/abs/2506.18998v3","updated":"2025-12-22T12:50:45Z","published":"2025-06-23T18:01:16Z","title":"Mirage of Mastery: Memorization Tricks LLMs into Artificially Inflated Self-Knowledge","summary":"When artificial intelligence mistakes memorization for intelligence, it creates a dangerous mirage of reasoning. Existing studies treat memorization and self-knowledge deficits in LLMs as separate issues and do not recognize an intertwining link that degrades the trustworthiness of LLM responses. In our study, we utilize a novel framework to ascertain if LLMs genuinely learn reasoning patterns from training data or merely memorize them to assume competence across problems of similar complexity focused on STEM domains. Our analysis shows a noteworthy problem in generalization: LLMs draw confidence from memorized solutions to infer a higher self-knowledge about their reasoning ability, which manifests as an over 45% inconsistency in feasibility assessments when faced with self-validated, logically coherent task perturbations. This effect is most pronounced in science and medicine domains, which tend to have maximal standardized jargon and problems, further confirming our approach. Significant wavering within the self-knowledge of LLMs also shows flaws in current architectures and training patterns, highlighting the need for techniques that ensure a balanced, consistent stance on models' perceptions of their own knowledge for maximum AI explainability and trustworthiness. Our code and results are available publicly at https://github.com/Sahil-R-Kale/mirage_of_mastery","authors":["Sahil Kale"],"pdf_url":"","comment":"12 pages, 9 figures"},{"id":"http://arxiv.org/abs/2510.02025v2","updated":"2025-12-22T12:27:15Z","published":"2025-10-02T13:57:14Z","title":"Style Over Story: A Process-Oriented Study of Authorial Creativity in Large Language Models","summary":"Evaluations of large language models (LLMs)' creativity have focused primarily on the quality of their outputs rather than the processes that shape them. This study takes a process-oriented approach, drawing on narratology to examine LLMs as computational authors. We introduce constraint-based decision-making as a lens for authorial creativity. Using controlled prompting to assign authorial personas, we analyze the creative preferences of the models. Our findings show that LLMs consistently emphasize Style over other elements, including Character, Event, and Setting. By also probing the reasoning the models provide for their choices, we show that distinctive profiles emerge across models and argue that our approach provides a novel systematic tool for analyzing AI's authorial creativity.","authors":["Donghoon Jung","Jiwoo Choi","Songeun Chae","Seohyon Jung"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19320v1","updated":"2025-12-22T12:13:17Z","published":"2025-12-22T12:13:17Z","title":"MAGIC: Achieving Superior Model Merging via Magnitude Calibration","summary":"The proliferation of pre-trained models has given rise to a wide array of specialised, fine-tuned models. Model merging aims to merge the distinct capabilities of these specialised models into a unified model, requiring minimal or even no additional training. A core objective of model merging is to ensure the merged model retains the behavioural characteristics of the specialised models, typically achieved through feature alignment. We identify that features consist of two critical components: direction and magnitude. Prior research has predominantly focused on directional alignment, while the influence of magnitude remains largely neglected, despite its pronounced vulnerability to perturbations introduced by common merging operations (e.g., parameter fusion and sparsification). Such perturbations to magnitude inevitably lead to feature deviations in the merged model from the specialised models, resulting in subsequent performance degradation. To address this, we propose MAGnItude Calibration (MAGIC), a plug-and-play framework that rectifies layer-wise magnitudes in feature and weight spaces, with three variants. Specifically, our Feature Space Calibration (FSC) realigns the merged model's features using a small set of unlabelled data, while Weight Space Calibration (WSC) extends this calibration to the weight space without requiring additional data. Combining these yields Dual Space Calibration (DSC). Comprehensive experiments demonstrate that MAGIC consistently boosts performance across diverse Computer Vision tasks (+4.3% on eight datasets) and NLP tasks (+8.0% on Llama) without additional training. Our code is available at: https://github.com/lyymuwu/MAGIC","authors":["Yayuan Li","Jian Zhang","Jintao Guo","Zihan Cheng","Lei Qi","Yinghuan Shi","Yang Gao"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19305v1","updated":"2025-12-22T11:53:01Z","published":"2025-12-22T11:53:01Z","title":"CienaLLM: Generative Climate-Impact Extraction from News Articles with Autoregressive LLMs","summary":"Understanding and monitoring the socio-economic impacts of climate hazards requires extracting structured information from heterogeneous news articles on a large scale. To that end, we have developed CienaLLM, a modular framework based on schema-guided Generative Information Extraction. CienaLLM uses open-weight Large Language Models for zero-shot information extraction from news articles, and supports configurable prompts and output schemas, multi-step pipelines, and cloud or on-premise inference. To systematically assess how the choice of LLM family, size, precision regime, and prompting strategy affect performance, we run a large factorial study in models, precisions, and prompt engineering techniques. An additional response parsing step nearly eliminates format errors while preserving accuracy; larger models deliver the strongest and most stable performance, while quantization offers substantial efficiency gains with modest accuracy trade-offs; and prompt strategies show heterogeneous, model-specific effects. CienaLLM matches or outperforms the supervised baseline in accuracy for extracting drought impacts from Spanish news, although at a higher inference cost. While evaluated in droughts, the schema-driven and model-agnostic design is suitable for adapting to related information extraction tasks (e.g., other hazards, sectors, or languages) by editing prompts and schemas rather than retraining. We release code, configurations, and schemas to support reproducible use.","authors":["Javier Vela-Tambo","Jorge Gracia","Fernando Dominguez-Castro"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.16301v2","updated":"2025-12-22T11:05:54Z","published":"2025-12-18T08:38:51Z","title":"Adaptation of Agentic AI","summary":"Cutting-edge agentic AI systems are built on foundation models that can be adapted to plan, reason, and interact with external tools to perform increasingly complex and specialized tasks. As these systems grow in capability and scope, adaptation becomes a central mechanism for improving performance, reliability, and generalization. In this paper, we unify the rapidly expanding research landscape into a systematic framework that spans both agent adaptations and tool adaptations. We further decompose these into tool-execution-signaled and agent-output-signaled forms of agent adaptation, as well as agent-agnostic and agent-supervised forms of tool adaptation. We demonstrate that this framework helps clarify the design space of adaptation strategies in agentic AI, makes their trade-offs explicit, and provides practical guidance for selecting or switching among strategies during system design. We then review the representative approaches in each category, analyze their strengths and limitations, and highlight key open challenges and future opportunities. Overall, this paper aims to offer a conceptual foundation and practical roadmap for researchers and practitioners seeking to build more capable, efficient, and reliable agentic AI systems.","authors":["Pengcheng Jiang","Jiacheng Lin","Zhiyi Shi","Zifeng Wang","Luxi He","Yichen Wu","Ming Zhong","Peiyang Song","Qizheng Zhang","Heng Wang","Xueqiang Xu","Hanwen Xu","Pengrui Han","Dylan Zhang","Jiashuo Sun","Chaoqi Yang","Kun Qian","Tian Wang","Changran Hu","Manling Li","Quanzheng Li","Hao Peng","Sheng Wang","Jingbo Shang","Chao Zhang","Jiaxuan You","Liyuan Liu","Pan Lu","Yu Zhang","Heng Ji","Yejin Choi","Dawn Song","Jimeng Sun","Jiawei Han"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19247v1","updated":"2025-12-22T10:29:51Z","published":"2025-12-22T10:29:51Z","title":"Auto-Prompting with Retrieval Guidance for Frame Detection in Logistics","summary":"Prompt engineering plays a critical role in adapting large language models (LLMs) to complex reasoning and labeling tasks without the need for extensive fine-tuning. In this paper, we propose a novel prompt optimization pipeline for frame detection in logistics texts, combining retrieval-augmented generation (RAG), few-shot prompting, chain-of-thought (CoT) reasoning, and automatic CoT synthesis (Auto-CoT) to generate highly effective task-specific prompts. Central to our approach is an LLM-based prompt optimizer agent that iteratively refines the prompts using retrieved examples, performance feedback, and internal self-evaluation. Our framework is evaluated on a real-world logistics text annotation task, where reasoning accuracy and labeling efficiency are critical. Experimental results show that the optimized prompts - particularly those enhanced via Auto-CoT and RAG - improve real-world inference accuracy by up to 15% compared to baseline zero-shot or static prompts. The system demonstrates consistent improvements across multiple LLMs, including GPT-4o, Qwen 2.5 (72B), and LLaMA 3.1 (70B), validating its generalizability and practical value. These findings suggest that structured prompt optimization is a viable alternative to full fine-tuning, offering scalable solutions for deploying LLMs in domain-specific NLP applications such as logistics.","authors":["Do Minh Duc","Quan Xuan Truong","Nguyen Tat Dat","Nguyen Van Vinh"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19240v1","updated":"2025-12-22T10:21:40Z","published":"2025-12-22T10:21:40Z","title":"ChemATP: A Training-Free Chemical Reasoning Framework for Large Language Models","summary":"Large Language Models (LLMs) exhibit strong general reasoning but struggle in molecular science due to the lack of explicit chemical priors in standard string representations. Current solutions face a fundamental dilemma. Training-based methods inject priors into parameters, but this static coupling hinders rapid knowledge updates and often compromises the model's general reasoning capabilities. Conversely, existing training-free methods avoid these issues but rely on surface-level prompting, failing to provide the fine-grained atom-level priors essential for precise chemical reasoning. To address this issue, we introduce ChemATP, a framework that decouples chemical knowledge from the reasoning engine. By constructing the first atom-level textual knowledge base, ChemATP enables frozen LLMs to explicitly retrieve and reason over this information dynamically. This architecture ensures interpretability and adaptability while preserving the LLM's intrinsic general intelligence. Experiments show that ChemATP significantly outperforms training-free baselines and rivals state-of-the-art training-based models, demonstrating that explicit prior injection is a competitive alternative to implicit parameter updates.","authors":["Mingxu Zhang","Dazhong Shen","Qi Zhang","Ying Sun"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19238v1","updated":"2025-12-22T10:20:20Z","published":"2025-12-22T10:20:20Z","title":"Identifying Features Associated with Bias Against 93 Stigmatized Groups in Language Models and Guardrail Model Safety Mitigation","summary":"Large language models (LLMs) have been shown to exhibit social bias, however, bias towards non-protected stigmatized identities remain understudied. Furthermore, what social features of stigmas are associated with bias in LLM outputs is unknown. From psychology literature, it has been shown that stigmas contain six shared social features: aesthetics, concealability, course, disruptiveness, origin, and peril. In this study, we investigate if human and LLM ratings of the features of stigmas, along with prompt style and type of stigma, have effect on bias towards stigmatized groups in LLM outputs. We measure bias against 93 stigmatized groups across three widely used LLMs (Granite 3.0-8B, Llama-3.1-8B, Mistral-7B) using SocialStigmaQA, a benchmark that includes 37 social scenarios about stigmatized identities; for example deciding wether to recommend them for an internship. We find that stigmas rated by humans to be highly perilous (e.g., being a gang member or having HIV) have the most biased outputs from SocialStigmaQA prompts (60% of outputs from all models) while sociodemographic stigmas (e.g. Asian-American or old age) have the least amount of biased outputs (11%). We test if the amount of biased outputs could be decreased by using guardrail models, models meant to identify harmful input, using each LLM's respective guardrail model (Granite Guardian 3.0, Llama Guard 3.0, Mistral Moderation API). We find that bias decreases significantly by 10.4%, 1.4%, and 7.8%, respectively. However, we show that features with significant effect on bias remain unchanged post-mitigation and that guardrail models often fail to recognize the intent of bias in prompts. This work has implications for using LLMs in scenarios involving stigmatized groups and we suggest future work towards improving guardrail models for bias mitigation.","authors":["Anna-Maria Gueorguieva","Aylin Caliskan"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2510.20647v2","updated":"2025-12-22T09:52:15Z","published":"2025-10-23T15:22:00Z","title":"The Reasoning Lingua Franca: A Double-Edged Sword for Multilingual AI","summary":"Large Reasoning Models (LRMs) achieve strong performance on mathematical, scientific, and other question-answering tasks, but their multilingual reasoning abilities remain underexplored. When presented with non-English questions, LRMs often default to reasoning in English, raising concerns about interpretability and the handling of linguistic and cultural nuances. We systematically compare an LRM's reasoning in English versus the language of the question. Our evaluation spans two tasks: MGSM and GPQA Diamond. Beyond measuring answer accuracy, we also analyze cognitive attributes in the reasoning traces. We find that English reasoning traces exhibit a substantially higher presence of these cognitive behaviors, and that reasoning in English generally yields higher final-answer accuracy, with the performance gap increasing as tasks become more complex. However, this English-centric strategy is susceptible to a key failure mode - getting \"Lost in Translation,\" where translation steps lead to errors that would have been avoided by question's language reasoning.","authors":["Alan Saji","Raj Dabre","Anoop Kunchukuttan","Ratish Puduppully"],"pdf_url":"","comment":"14 pages, 13 figures, 5 tables"},{"id":"http://arxiv.org/abs/2512.19173v1","updated":"2025-12-22T09:07:34Z","published":"2025-12-22T09:07:34Z","title":"CycleChart: A Unified Consistency-Based Learning Framework for Bidirectional Chart Understanding and Generation","summary":"Current chart-specific tasks, such as chart question answering, chart parsing, and chart generation, are typically studied in isolation, preventing models from learning the shared semantics that link chart generation and interpretation. We introduce CycleChart, a consistency-based learning framework for bidirectional chart understanding and generation. CycleChart adopts a schema-centric formulation as a common interface across tasks. We construct a consistent multi-task dataset, where each chart sample includes aligned annotations for schema prediction, data parsing, and question answering. To learn cross-directional chart semantics, CycleChart introduces a generate-parse consistency objective: the model generates a chart schema from a table and a textual query, then learns to recover the schema and data from the generated chart, enforcing semantic alignment across directions. CycleChart achieves strong results on chart generation, chart parsing, and chart question answering, demonstrating improved cross-task generalization and marking a step toward more general chart understanding models.","authors":["Dazhen Deng","Sen Yang","Yuchen He","Yuan Tian","Yingcai Wu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19171v1","updated":"2025-12-22T09:05:06Z","published":"2025-12-22T09:05:06Z","title":"JEPA-Reasoner: Decoupling Latent Reasoning from Token Generation","summary":"While Joint-Embedding Predictive Architecture (JEPA) has emerged as a powerful architecture for learning rich latent representations, it fundamentally lacks generative abilities. Meanwhile, latent space reasoning attempts for Transformer models like COCONUT do improve performance, but they ultimately rely on token-by-token generation, which still accumulates compounding error and relies on context information to gain reasoning insights. To address these limitations, we propose JEPA-Reasoner, a novel JEPA model enhanced with generative ability that reasons in latent space. We augment it with a separate action-taker model, Talker, to produce human-readable sentences. Our approach demonstrates that decoupling latent space reasoning and token generation enables JEPA-Reasoner to produce mixed latent vectors that might lay the foundation for multi-threaded reasoning, while performing autoregressive generation with superior robustness to compounding error.","authors":["Bingyang Kelvin Liu","Ziyu Patrick Chen"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19161v1","updated":"2025-12-22T08:57:16Z","published":"2025-12-22T08:57:16Z","title":"From Speech to Subtitles: Evaluating ASR Models in Subtitling Italian Television Programs","summary":"Subtitles are essential for video accessibility and audience engagement. Modern Automatic Speech Recognition (ASR) systems, built upon Encoder-Decoder neural network architectures and trained on massive amounts of data, have progressively reduced transcription errors on standard benchmark datasets. However, their performance in real-world production environments, particularly for non-English content like long-form Italian videos, remains largely unexplored. This paper presents a case study on developing a professional subtitling system for an Italian media company. To inform our system design, we evaluated four state-of-the-art ASR models (Whisper Large v2, AssemblyAI Universal, Parakeet TDT v3 0.6b, and WhisperX) on a 50-hour dataset of Italian television programs. The study highlights their strengths and limitations, benchmarking their performance against the work of professional human subtitlers. The findings indicate that, while current models cannot meet the media industry's accuracy needs for full autonomy, they can serve as highly effective tools for enhancing human productivity. We conclude that a human-in-the-loop (HITL) approach is crucial and present the production-grade, cloud-based infrastructure we designed to support this workflow.","authors":["Alessandro Lucca","Francesco Pierri"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2504.09566v3","updated":"2025-12-22T08:34:18Z","published":"2025-04-13T13:35:41Z","title":"Syzygy of Thoughts: Improving LLM CoT with the Minimal Free Resolution","summary":"Chain-of-Thought (CoT) prompting enhances the reasoning of large language models (LLMs) by decomposing problems into sequential steps, mimicking human logic and reducing errors. However, complex tasks with vast solution spaces and vague constraints often exceed the capacity of a single reasoning chain. Inspired by Minimal Free Resolution (MFR) in commutative algebra and algebraic geometry, we propose Syzygy of Thoughts (SoT)-a novel framework that extends CoT by introducing auxiliary, interrelated reasoning paths. SoT captures deeper logical dependencies, enabling more robust and structured problem-solving. MFR decomposes a module into a sequence of free modules with minimal rank, providing a structured analytical approach to complex systems. This method introduces the concepts of \"Module\", \"Betti numbers\",\"Freeness\", \"Mapping\", \"Exactness\" and \"Minimality\", enabling the systematic decomposition of the original complex problem into logically complete minimal subproblems while preserving key problem features and reducing reasoning length. We tested SoT across diverse datasets (e.g., GSM8K, MATH) and models (e.g., GPT-4o-mini, Qwen2.5), achieving inference accuracy that matches or surpasses mainstream CoTs standards. Additionally, by aligning the sampling process with algebraic constraints, our approach enhances the scalability of inference time in LLMs, ensuring both transparent reasoning and high performance. Our code will be publicly available at https://github.com/dlMARiA/Syzygy-of-thoughts.","authors":["Chenghao Li","Chaoning Zhang","Yi Lu","Jiaquan Zhang","Qigan Sun","Xudong Wang","Jiwei Wei","Guoqing Wang","Yang Yang","Heng Tao Shen"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19134v1","updated":"2025-12-22T08:28:05Z","published":"2025-12-22T08:28:05Z","title":"QuCo-RAG: Quantifying Uncertainty from the Pre-training Corpus for Dynamic Retrieval-Augmented Generation","summary":"Dynamic Retrieval-Augmented Generation adaptively determines when to retrieve during generation to mitigate hallucinations in large language models (LLMs). However, existing methods rely on model-internal signals (e.g., logits, entropy), which are fundamentally unreliable because LLMs are typically ill-calibrated and often exhibit high confidence in erroneous outputs. We propose QuCo-RAG, which shifts from subjective confidence to objective statistics computed from pre-training data. Our method quantifies uncertainty through two stages: (1) before generation, we identify low-frequency entities indicating long-tail knowledge gaps; (2) during generation, we verify entity co-occurrence in the pre-training corpus, where zero co-occurrence often signals hallucination risk. Both stages leverage Infini-gram for millisecond-latency queries over 4 trillion tokens, triggering retrieval when uncertainty is high. Experiments on multi-hop QA benchmarks show QuCo-RAG achieves EM gains of 5--12 points over state-of-the-art baselines with OLMo-2 models, and transfers effectively to models with undisclosed pre-training data (Llama, Qwen, GPT), improving EM by up to 14 points. Domain generalization on biomedical QA further validates the robustness of our paradigm. These results establish corpus-grounded verification as a principled, practically model-agnostic paradigm for dynamic RAG. Our code is publicly available at https://github.com/ZhishanQ/QuCo-RAG.","authors":["Dehai Min","Kailin Zhang","Tongtong Wu","Lu Cheng"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19126v1","updated":"2025-12-22T08:07:00Z","published":"2025-12-22T08:07:00Z","title":"AWPO: Enhancing Tool-Use of Large Language Models through Explicit Integration of Reasoning Rewards","summary":"While reinforcement learning (RL) shows promise in training tool-use large language models (LLMs) using verifiable outcome rewards, existing methods largely overlook the potential of explicit reasoning rewards to bolster reasoning and tool utilization. Furthermore, natively combining reasoning and outcome rewards may yield suboptimal performance or conflict with the primary optimization objective. To address this, we propose advantage-weighted policy optimization (AWPO) -- a principled RL framework that effectively integrates explicit reasoning rewards to enhance tool-use capability. AWPO incorporates variance-aware gating and difficulty-aware weighting to adaptively modulate advantages from reasoning signals based on group-relative statistics, alongside a tailored clipping mechanism for stable optimization. Extensive experiments demonstrate that AWPO achieves state-of-the-art performance across standard tool-use benchmarks, significantly outperforming strong baselines and leading closed-source models in challenging multi-turn scenarios. Notably, with exceptional parameter efficiency, our 4B model surpasses Grok-4 by 16.0 percent in multi-turn accuracy while preserving generalization capability on the out-of-distribution MMLU-Pro benchmark.","authors":["Zihan Lin","Xiaohan Wang","Hexiong Yang","Jiajun Chai","Jie Cao","Guojun Yin","Wei Lin","Ran He"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19125v1","updated":"2025-12-22T08:05:01Z","published":"2025-12-22T08:05:01Z","title":"SAP: Syntactic Attention Pruning for Transformer-based Language Models","summary":"This paper introduces Syntactic Attention Pruning (SAP), a novel method for effectively pruning attention heads in Transformer models. Unlike conventional approaches that rely solely on mathematical analysis of model weights and activations, SAP incorporates both the syntactic structure and attention patterns of sentences to guide the pruning process. By leveraging these linguistic features, SAP not only achieves performance comparable to state-of-the-art methods but also enhances the interpretability of model behavior. To further improve robustness, we propose Candidate Filtering (CF), a mechanism that prioritizes heads based on their contribution to model performance, mitigating degradation during pruning. Experimental results indicate that SAP effectively preserves critical heads of a high density of strong attention values, outperforming existing head pruning strategies in retrain-free settings. These findings position SAP as a promising foundation for a new direction in model compression research, offering high flexibility for pruning across all transformer-based language models.","authors":["Tzu-Yun Lee","Ding-Yong Hong","Jan-Jan Wu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19122v1","updated":"2025-12-22T07:53:16Z","published":"2025-12-22T07:53:16Z","title":"BanglaForge: LLM Collaboration with Self-Refinement for Bangla Code Generation","summary":"Bangla is a low-resource language for code generation, lacking large-scale annotated datasets and tools to transform natural language specifications into executable programs. This makes Bangla-to-code generation a challenging task requiring innovative solutions. To address this, we introduce BanglaForge, a novel framework for generating code from Bangla function descriptions. BanglaForge leverages a retrieval-augmented dual-model collaboration paradigm with self-refinement, combining in-context learning, llm-based translation, systematic prompt engineering, and iterative self-refinement based on execution feedback, where a coder generates initial solutions and a reviewer enhances them for robustness. On the BLP-2025 Bangla Code Generation benchmark, BanglaForge achieves a competitive Pass@1 accuracy of 84.00%, demonstrating the effectiveness of retrieval, model collaboration, and self-refinement for low-resource Bangla code generation.","authors":["Mahir Labib Dihan","Sadif Ahmed","Md Nafiu Rahman"],"pdf_url":"","comment":"Accepted at BLP Workshop @ IJCNLP-AACL 2025. Code is available at https://github.com/mahirlabibdihan/BanglaForge"},{"id":"http://arxiv.org/abs/2512.19117v1","updated":"2025-12-22T07:43:43Z","published":"2025-12-22T07:43:43Z","title":"Stop saying LLM: Large Discourse Models (LDM) and Artificial Discursive Agent (ADA)?","summary":"This paper proposes an epistemological shift in the analysis of large generative models, replacing the category ''Large Language Models'' (LLM) with that of ''Large Discourse Models'' (LDM), and then with that of Artificial Discursive Agent (ADA). The theoretical framework is based on an ontological triad distinguishing three regulatory instances: the apprehension of the phenomenal regularities of the referential world, the structuring of embodied cognition, and the structural-linguistic sedimentation of the utterance within a socio-historical context. LDMs, operating on the product of these three instances (the document), model the discursive projection of a portion of human experience reified by the learning corpus. The proposed program aims to replace the ''fascination/fear'' dichotomy with public trials and procedures that make the place, uses, and limits of artificial discursive agents in contemporary social space decipherable, situating this approach within a perspective of governance and co-regulation involving the State, industry, civil society, and academia.","authors":["Amar Lakel"],"pdf_url":"","comment":"in French language"},{"id":"http://arxiv.org/abs/2512.19092v1","updated":"2025-12-22T07:01:05Z","published":"2025-12-22T07:01:05Z","title":"A Large Language Model Based Method for Complex Logical Reasoning over Knowledge Graphs","summary":"Reasoning over knowledge graphs (KGs) with first-order logic (FOL) queries is challenging due to the inherent incompleteness of real-world KGs and the compositional complexity of logical query structures. Most existing methods rely on embedding entities and relations into continuous geometric spaces and answer queries via differentiable set operations. While effective for simple query patterns, these approaches often struggle to generalize to complex queries involving multiple operators, deeper reasoning chains, or heterogeneous KG schemas. We propose ROG (Reasoning Over knowledge Graphs with large language models), an ensemble-style framework that combines query-aware KG neighborhood retrieval with large language model (LLM)-based chain-of-thought reasoning. ROG decomposes complex FOL queries into sequences of simpler sub-queries, retrieves compact, query-relevant subgraphs as contextual evidence, and performs step-by-step logical inference using an LLM, avoiding the need for task-specific embedding optimization. Experiments on standard KG reasoning benchmarks demonstrate that ROG consistently outperforms strong embedding-based baselines in terms of mean reciprocal rank (MRR), with particularly notable gains on high-complexity query types. These results suggest that integrating structured KG retrieval with LLM-driven logical reasoning offers a robust and effective alternative for complex KG reasoning tasks.","authors":["Ziyan Zhang","Chao Wang","Zhuo Chen","Lei Chen","Chiyi Li","Kai Song"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2505.22255v3","updated":"2025-12-22T06:48:29Z","published":"2025-05-28T11:41:11Z","title":"Kronecker Factorization Improves Efficiency and Interpretability of Sparse Autoencoders","summary":"Sparse Autoencoders (SAEs) have demonstrated significant promise in interpreting the hidden states of language models by decomposing them into interpretable latent directions. However, training and interpreting SAEs at scale remains challenging, especially when large dictionary sizes are used. While decoders can leverage sparse-aware kernels for efficiency, encoders still require computationally intensive linear operations with large output dimensions. To address this, we propose KronSAE, a novel architecture that factorizes the latent representation via Kronecker product decomposition, drastically reducing memory and computational overhead. Furthermore, we introduce mAND, a differentiable activation function approximating the binary AND operation, which improves interpretability and performance in our factorized framework.","authors":["Vadim Kurochkin","Yaroslav Aksenov","Daniil Laptev","Daniil Gavrilov","Nikita Balagansky"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19070v1","updated":"2025-12-22T06:20:53Z","published":"2025-12-22T06:20:53Z","title":"Watch Closely: Mitigating Object Hallucinations in Large Vision-Language Models with Disentangled Decoding","summary":"Large Vision-Language Models (LVLMs) bridge the gap between visual and linguistic modalities, demonstrating strong potential across a variety of domains. However, despite significant progress, LVLMs still suffer from severe hallucination issues in object recognition tasks. These models often fail to accurately identify certain objects, leading to text generation that appears fluent but does not correspond to the visual content, which can have serious consequences in real-world applications. Recently, several methods have been proposed to alleviate LVLM hallucinations, but most focus solely on reducing hallucinations in the language modality. To mitigate hallucinations in both the language and visual modalities, we introduce Hallucination Disentangled Decoding (HDD) method that requires no training. HDD enhances the original image by segmenting it and selecting images that augment the original, while also utilizing a blank image to eliminate language prior hallucinations in both the original and segmented images. This design not only reduces the model's dependence on language priors but also enhances its visual performance. (Code: https://github.com/rickeyhhh/Hallucination-Disentangled-Decoding)","authors":["Ruiqi Ma","Yu Yan","Chunhong Zhang","Minghao Yin","XinChao Liu","Zhihong Jin","Zheng Hu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2411.07019v7","updated":"2025-12-22T05:48:26Z","published":"2024-11-11T14:22:42Z","title":"UniHR: Hierarchical Representation Learning for Unified Knowledge Graph Link Prediction","summary":"Real-world knowledge graphs (KGs) contain not only standard triple-based facts, but also more complex, heterogeneous types of facts, such as hyper-relational facts with auxiliary key-value pairs, temporal facts with additional timestamps, and nested facts that imply relationships between facts. These richer forms of representation have attracted significant attention due to their enhanced expressiveness and capacity to model complex semantics in real-world scenarios. However, most existing studies suffer from two main limitations: (1) they typically focus on modeling only specific types of facts, thus making it difficult to generalize to real-world scenarios with multiple fact types; and (2) they struggle to achieve generalizable hierarchical (inter-fact and intra-fact) modeling due to the complexity of these representations. To overcome these limitations, we propose UniHR, a Unified Hierarchical Representation learning framework, which consists of a learning-optimized Hierarchical Data Representation (HiDR) module and a unified Hierarchical Structure Learning (HiSL) module. The HiDR module unifies hyper-relational KGs, temporal KGs, and nested factual KGs into triple-based representations. Then HiSL incorporates intra-fact and inter-fact message passing, focusing on enhancing both semantic information within individual facts and enriching the structural information between facts. To go beyond the unified method itself, we further explore the potential of unified representation in complex real-world scenarios. Extensive experiments on 9 datasets across 5 types of KGs demonstrate the effectiveness of UniHR and highlight the strong potential of unified representations. Code and data are available at https://github.com/zjukg/UniHR.","authors":["Zhiqiang Liu","Yin Hua","Mingyang Chen","Yichi Zhang","Zhuo Chen","Lei Liang","Wen Zhang"],"pdf_url":"","comment":"AAAI 2026 (oral)"},{"id":"http://arxiv.org/abs/2512.19012v1","updated":"2025-12-22T04:03:01Z","published":"2025-12-22T04:03:01Z","title":"DramaBench: A Six-Dimensional Evaluation Framework for Drama Script Continuation","summary":"Drama script continuation requires models to maintain character consistency, advance plot coherently, and preserve dramatic structurecapabilities that existing benchmarks fail to evaluate comprehensively. We present DramaBench, the first large-scale benchmark for evaluating drama script continuation across six independent dimensions: Format Standards, Narrative Efficiency, Character Consistency, Emotional Depth, Logic Consistency, and Conflict Handling. Our framework combines rulebased analysis with LLM-based labeling and statistical metrics, ensuring objective and reproducible evaluation. We conduct comprehensive evaluation of 8 state-of-the-art language models on 1,103 scripts (8,824 evaluations total), with rigorous statistical significance testing (252 pairwise comparisons, 65.9% significant) and human validation (188 scripts, substantial agreement on 3/5 dimensions). Our ablation studies confirm all six dimensions capture independent quality aspects (mean | r | = 0.020). DramaBench provides actionable, dimensionspecific feedback for model improvement and establishes a rigorous standard for creative writing evaluation.","authors":["Shijian Ma","Yunqi Huang","Yan Lin"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19011v1","updated":"2025-12-22T04:00:35Z","published":"2025-12-22T04:00:35Z","title":"Efficient Jailbreak Mitigation Using Semantic Linear Classification in a Multi-Staged Pipeline","summary":"Prompt injection and jailbreaking attacks pose persistent security challenges to large language model (LLM)-based systems. We present an efficient and systematically evaluated defense architecture that mitigates these threats through a lightweight, multi-stage pipeline. Its core component is a semantic filter based on text normalization, TF-IDF representations, and a Linear SVM classifier. Despite its simplicity, this module achieves 93.4% accuracy and 96.5% specificity on held-out data, substantially reducing attack throughput while incurring negligible computational overhead.\n  Building on this efficient foundation, the full pipeline integrates complementary detection and mitigation mechanisms that operate at successive stages, providing strong robustness with minimal latency. In comparative experiments, our SVM-based configuration improves overall accuracy from 35.1% to 93.4% while reducing average time to completion from approximately 450s to 47s, yielding over 10 times lower latency than ShieldGemma. These results demonstrate that the proposed design simultaneously advances defensive precision and efficiency, addressing a core limitation of current model-based moderators.\n  Evaluation across a curated corpus of over 30,000 labeled prompts, including benign, jailbreak, and application-layer injections, confirms that staged, resource-efficient defenses can robustly secure modern LLM-driven applications.","authors":["Akshaj Prashanth Rao","Advait Singh","Saumya Kumaar Saksena","Dhruv Kumar"],"pdf_url":"","comment":"Under Review"},{"id":"http://arxiv.org/abs/2512.19004v1","updated":"2025-12-22T03:45:04Z","published":"2025-12-22T03:45:04Z","title":"Context-Aware Initialization for Reducing Generative Path Length in Diffusion Language Models","summary":"Diffusion Large Language Models (DLLMs) enable fully parallel token decoding but often remain impractical at inference time due to the many denoising iterations required to refine an information-free, fully masked initialization into coherent text. Most existing acceleration methods focus on traversing this generative trajectory more efficiently via improved solvers or sampling strategies. We advance a complementary perspective: shorten the trajectory itself by starting closer to the target distribution through context-aware initialization.\n  We propose a training-free interface that injects prompt-conditioned priors from a lightweight auxiliary model into the diffusion initialization, and instantiate it with two mechanisms: discrete token injection and representation-level embedding interpolation. Because injected priors can be imperfect and unmask-only decoding can over-commit early, we also introduce a simple confidence-based remasking mechanism as a form of prior skepticism. Preliminary evidence on GSM8K suggests that context-aware initialization can substantially reduce denoising iterations (about 35\\% fewer function evaluations in our setting), while also exposing a key open challenge: naive warm-starting can degrade final accuracy relative to strong diffusion baselines. We use these findings to motivate a research agenda around calibration, revision mechanisms, and representation alignment for reliable warm-started diffusion decoding.","authors":["Tongyuan Miao","Gary Huang","Kai Jun Han","Annie Jiang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.14554v3","updated":"2025-12-22T03:45:03Z","published":"2025-12-16T16:28:32Z","title":"VLegal-Bench: Cognitively Grounded Benchmark for Vietnamese Legal Reasoning of Large Language Models","summary":"The rapid advancement of large language models (LLMs) has enabled new possibilities for applying artificial intelligence within the legal domain. Nonetheless, the complexity, hierarchical organization, and frequent revisions of Vietnamese legislation pose considerable challenges for evaluating how well these models interpret and utilize legal knowledge. To address this gap, Vietnamese Legal Benchmark (VLegal-Bench) is introduced, the first comprehensive benchmark designed to systematically assess LLMs on Vietnamese legal tasks. Informed by Bloom's cognitive taxonomy, VLegal-Bench encompasses multiple levels of legal understanding through tasks designed to reflect practical usage scenarios. The benchmark comprises 10,450 samples generated through a rigorous annotation pipeline, where legal experts label and cross-validate each instance using our annotation system to ensure every sample is grounded in authoritative legal documents and mirrors real-world legal assistant workflows, including general legal questions and answers, retrieval-augmented generation, multi-step reasoning, and scenario-based problem solving tailored to Vietnamese law. By providing a standardized, transparent, and cognitively informed evaluation framework, VLegal-Bench establishes a solid foundation for assessing LLM performance in Vietnamese legal contexts and supports the development of more reliable, interpretable, and ethically aligned AI-assisted legal systems.","authors":["Nguyen Tien Dong","Minh-Anh Nguyen","Thanh Dat Hoang","Nguyen Tuan Ngoc","Dao Xuan Quang Minh","Phan Phi Hai","Nguyen Thi Ngoc Anh","Dang Van Tu","Binh Vu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.18999v1","updated":"2025-12-22T03:33:43Z","published":"2025-12-22T03:33:43Z","title":"Evaluating the Challenges of LLMs in Real-world Medical Follow-up: A Comparative Study and An Optimized Framework","summary":"When applied directly in an end-to-end manner to medical follow-up tasks, Large Language Models (LLMs) often suffer from uncontrolled dialog flow and inaccurate information extraction due to the complexity of follow-up forms. To address this limitation, we designed and compared two follow-up chatbot systems: an end-to-end LLM-based system (control group) and a modular pipeline with structured process control (experimental group). Experimental results show that while the end-to-end approach frequently fails on lengthy and complex forms, our modular method-built on task decomposition, semantic clustering, and flow management-substantially improves dialog stability and extraction accuracy. Moreover, it reduces the number of dialogue turns by 46.73% and lowers token consumption by 80% to 87.5%. These findings highlight the necessity of integrating external control mechanisms when deploying LLMs in high-stakes medical follow-up scenarios.","authors":["Jinyan Liu","Zikang Chen","Qinchuan Wang","Tan Xie","Heming Zheng","Xudong Lv"],"pdf_url":"","comment":"10 pages,3 figures,conference ICCBB2025"},{"id":"http://arxiv.org/abs/2512.12608v2","updated":"2025-12-22T03:19:42Z","published":"2025-12-14T09:12:09Z","title":"Human-Inspired Learning for Large Language Models via Obvious Record and Maximum-Entropy Method Discovery","summary":"Large Language Models (LLMs) excel at extracting common patterns from large-scale corpora, yet they struggle with rare, low-resource, or previously unseen scenarios-such as niche hardware deployment issues or irregular IoT device behaviors-because such cases are sparsely represented in training data. Moreover, LLMs rely primarily on implicit parametric memory, which limits their ability to explicitly acquire, recall, and refine methods, causing them to behave predominantly as intuition-driven predictors rather than deliberate, method-oriented learners. Inspired by how humans learn from rare experiences, this paper proposes a human-inspired learning framework that integrates two complementary mechanisms. The first, Obvious Record, explicitly stores cause--result (or question--solution) relationships as symbolic memory, enabling persistent learning even from single or infrequent encounters. The second, Maximum-Entropy Method Discovery, prioritizes and preserves methods with high semantic dissimilarity, allowing the system to capture diverse and underrepresented strategies that are typically overlooked by next-token prediction. Verification on a benchmark of 60 semantically diverse question--solution pairs demonstrates that the proposed entropy-guided approach achieves stronger coverage of unseen questions and significantly greater internal diversity than a random baseline, confirming its effectiveness in discovering more generalizable and human-inspired methods.","authors":["Hong Su"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.18987v1","updated":"2025-12-22T02:55:25Z","published":"2025-12-22T02:55:25Z","title":"Affordance RAG: Hierarchical Multimodal Retrieval with Affordance-Aware Embodied Memory for Mobile Manipulation","summary":"In this study, we address the problem of open-vocabulary mobile manipulation, where a robot is required to carry a wide range of objects to receptacles based on free-form natural language instructions. This task is challenging, as it involves understanding visual semantics and the affordance of manipulation actions. To tackle these challenges, we propose Affordance RAG, a zero-shot hierarchical multimodal retrieval framework that constructs Affordance-Aware Embodied Memory from pre-explored images. The model retrieves candidate targets based on regional and visual semantics and reranks them with affordance scores, allowing the robot to identify manipulation options that are likely to be executable in real-world environments. Our method outperformed existing approaches in retrieval performance for mobile manipulation instruction in large-scale indoor environments. Furthermore, in real-world experiments where the robot performed mobile manipulation in indoor environments based on free-form instructions, the proposed method achieved a task success rate of 85%, outperforming existing methods in both retrieval performance and overall task success.","authors":["Ryosuke Korekata","Quanting Xie","Yonatan Bisk","Komei Sugiura"],"pdf_url":"","comment":"Accepted to IEEE RA-L, with presentation at ICRA 2026"},{"id":"http://arxiv.org/abs/2403.15676v5","updated":"2025-12-22T02:52:50Z","published":"2024-03-23T01:44:57Z","title":"AC4: Algebraic Computation Checker for Circuit Constraints in ZKPs","summary":"Zero-knowledge proof (ZKP) systems have surged attention and held a fundamental role in contemporary cryptography. Zero-knowledge succinct non-interactive argument of knowledge (zk-SNARK) protocols dominate the ZKP usage, implemented through arithmetic circuit programming paradigm. However, underconstrained or overconstrained circuits may lead to bugs. The former refers to circuits that lack the necessary constraints, resulting in unexpected solutions and causing the verifier to accept a bogus witness, and the latter refers to circuits that are constrained excessively, resulting in lacking necessary solutions and causing the verifier to accept no witness. This paper introduces a novel approach for pinpointing two distinct types of bugs in ZKP circuits. The method involves encoding the arithmetic circuit constraints to polynomial equation systems and solving them over finite fields by the computer algebra system. The classification of verification results is refined, greatly enhancing the expressive power of the system. A tool, AC4, is proposed to represent the implementation of the method. Experiments show that AC4 demonstrates a increase in the solved rate, showing a 29% improvement over Picus and CIVER, and a slight improvement over halo2-analyzer, a checker for halo2 circuits. Within a solvable range, the checking time has also exhibited noticeable improvement, demonstrating a magnitude increase compared to previous efforts.","authors":["Qizhe Yang","Boxuan Liang","Hao Chen","Guoqiang Li"],"pdf_url":"","comment":"26 pages, 5 figures"},{"id":"http://arxiv.org/abs/2512.01457v3","updated":"2025-12-22T02:26:36Z","published":"2025-12-01T09:44:31Z","title":"Zero-Overhead Introspection for Adaptive Test-Time Compute","summary":"Large language models excel at reasoning but lack key aspects of introspection, including anticipating their own success and the computation required to achieve it. Humans use real-time introspection to decide how much effort to invest, when to make multiple attempts, when to stop, and when to signal success or failure. Without this, LLMs struggle to make intelligent meta-cognition decisions. Test-time scaling methods like Best-of-N drive up cost and latency by using a fixed budget of samples regardless of the marginal benefit of each one at any point in generation, and the absence of confidence signals can mislead people, prevent appropriate escalation to better tools, and undermine trustworthiness. Learned verifiers or reward models can provide confidence estimates, but do not enable adaptive inference and add substantial cost by requiring extra models or forward passes. We present ZIP-RC, an adaptive inference method that equips models with zero-overhead inference-time predictions of reward and cost. At every token, ZIP-RC reuses reserved or unused logits in the same forward pass as next-token prediction to output a joint distribution over final reward and remaining length -- no extra models, architecture change, or inference overhead. This full joint distribution is used to compute a sampling utility which is the linear combination of the expected maximum reward, total compute, and latency of set of samples if generated to completion. During inference, we maximize this utility with meta-actions that determine which prefix of tokens to continue or initiate sampling from. On mixed-difficulty mathematical benchmarks, ZIP-RC improves accuracy by up to 12% over majority voting at equal or lower average cost, and traces smooth Pareto frontiers between quality, compute, and latency. By providing real-time reward-cost introspection, ZIP-RC enables adaptive, efficient reasoning.","authors":["Rohin Manvi","Joey Hong","Tim Seyde","Maxime Labonne","Mathias Lechner","Sergey Levine"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2505.15210v3","updated":"2025-12-22T02:26:22Z","published":"2025-05-21T07:38:45Z","title":"Deliberation on Priors: Trustworthy Reasoning of Large Language Models on Knowledge Graphs","summary":"Knowledge graph-based retrieval-augmented generation seeks to mitigate hallucinations in Large Language Models (LLMs) caused by insufficient or outdated knowledge. However, existing methods often fail to fully exploit the prior knowledge embedded in knowledge graphs (KGs), particularly their structural information and explicit or implicit constraints. The former can enhance the faithfulness of LLMs' reasoning, while the latter can improve the reliability of response generation. Motivated by these, we propose a trustworthy reasoning framework, termed Deliberation over Priors (DP), which sufficiently utilizes the priors contained in KGs. Specifically, DP adopts a progressive knowledge distillation strategy that integrates structural priors into LLMs through a combination of supervised fine-tuning and Kahneman-Tversky optimization, thereby improving the faithfulness of relation path generation. Furthermore, our framework employs a reasoning-introspection strategy, which guides LLMs to perform refined reasoning verification based on extracted constraint priors, ensuring the reliability of response generation. Extensive experiments on three benchmark datasets demonstrate that DP achieves new state-of-the-art performance, especially a Hit@1 improvement of 13% on the ComplexWebQuestions dataset, and generates highly trustworthy responses. We also conduct various analyses to verify its flexibility and practicality. The code is available at https://github.com/reml-group/Deliberation-on-Priors.","authors":["Jie Ma","Ning Qu","Zhitao Gao","Rui Xing","Jun Liu","Hongbin Pei","Jiang Xie","Linyun Song","Pinghui Wang","Jing Tao","Zhou Su"],"pdf_url":"","comment":"Accepted by NeurIPS 2025"},{"id":"http://arxiv.org/abs/2408.15510v5","updated":"2025-12-22T02:12:55Z","published":"2024-08-28T03:45:49Z","title":"How Reliable are Causal Probing Interventions?","summary":"Causal probing aims to analyze foundation models by examining how intervening on their representation of various latent properties impacts their outputs. Recent works have cast doubt on the theoretical basis of several leading causal probing methods, but it has been unclear how to systematically evaluate the effectiveness of these methods in practice. To address this, we define two key causal probing desiderata: completeness (how thoroughly the representation of the target property has been transformed) and selectivity (how little non-targeted properties have been impacted). We find that there is an inherent tradeoff between the two, which we define as reliability, their harmonic mean. We introduce an empirical analysis framework to measure and evaluate these quantities, allowing us to make the first direct comparisons between different families of leading causal probing methods (e.g., linear vs. nonlinear, or concept removal vs. counterfactual interventions). We find that: (1) all methods show a clear tradeoff between completeness and selectivity; (2) more complete and reliable methods have a greater impact on LLM behavior; and (3) nonlinear interventions are almost always more reliable than linear interventions.\n  Our project webpage is available at: https://ahdavies6.github.io/causal_probing_reliability/","authors":["Marc Canby","Adam Davies","Chirag Rastogi","Julia Hockenmaier"],"pdf_url":"","comment":"In Proceedings of IJCNLP-AACL, 2025"},{"id":"http://arxiv.org/abs/2512.18940v1","updated":"2025-12-22T01:19:50Z","published":"2025-12-22T01:19:50Z","title":"FASTRIC: Prompt Specification Language for Verifiable LLM Interactions","summary":"Large Language Models (LLMs) execute complex multi-turn interaction protocols but lack formal specifications to verify execution against designer intent. We introduce FASTRIC, a Prompt Specification Language that makes implicit Finite State Machines (FSMs) explicit in natural language prompts, enabling conformance verification through execution trace analysis. The LLM serves as intelligent execution agent: interpreting designer-encoded FSMs to execute specified behavioral roles. Unlike symbolic specification languages requiring parsers and compilers, FASTRIC leverages LLMs as unified infrastructure-simultaneously parser, interpreter, runtime environment, and development assistant. FASTRIC guides designers to articulate seven FSM elements (Final States, Agents, States, Triggers, Roles, Initial State, Constraints) structuring multi-turn interactions. Specification formality-ranging from implicit descriptions that frontier models infer to explicit step-by-step instructions for weaker models-serves as a design parameter. We introduce procedural conformance as verification metric measuring execution adherence to FSM specifications. Testing a 3-state kindergarten tutoring FSM across four formality levels and three model scales (14.7B, 685B, 1T+ parameters) reveals optimal specification formality is a function of model capacity. DeepSeek-V3.2 (685B) achieves perfect conformance (1.00) at L2-L4; ChatGPT-5 (~1T) peaks at L3 (0.90) before collapsing at L4 (0.39); Phi4 (14.7B) shows no stable optimum with high variance (SD=0.16-0.36). These findings reveal model-specific formality ranges-\"Goldilocks zones\"-where specifications provide sufficient structure without over-constraint, establishing Prompt Specification Engineering for creating verifiable interaction protocols, transforming multi-turn interaction design from heuristic art to systematic engineering with measurable procedural guarantees.","authors":["Wen-Long Jin"],"pdf_url":"","comment":"13 pages, 3 figures. Supplementary materials at https://doi.org/10.17605/OSF.IO/PV6R3"},{"id":"http://arxiv.org/abs/2512.19933v1","updated":"2025-12-22T23:31:49Z","published":"2025-12-22T23:31:49Z","title":"PRISM: A Personality-Driven Multi-Agent Framework for Social Media Simulation","summary":"Traditional agent-based models (ABMs) of opinion dynamics often fail to capture the psychological heterogeneity driving online polarization due to simplistic homogeneity assumptions. This limitation obscures the critical interplay between individual cognitive biases and information propagation, thereby hindering a mechanistic understanding of how ideological divides are amplified. To address this challenge, we introduce the Personality-Refracted Intelligent Simulation Model (PRISM), a hybrid framework coupling stochastic differential equations (SDE) for continuous emotional evolution with a personality-conditional partially observable Markov decision process (PC-POMDP) for discrete decision-making. In contrast to continuous trait approaches, PRISM assigns distinct Myers-Briggs Type Indicator (MBTI) based cognitive policies to multimodal large language model (MLLM) agents, initialized via data-driven priors from large-scale social media datasets. PRISM achieves superior personality consistency aligned with human ground truth, significantly outperforming standard homogeneous and Big Five benchmarks. This framework effectively replicates emergent phenomena such as rational suppression and affective resonance, offering a robust tool for analyzing complex social media ecosystems.","authors":["Zhixiang Lu","Xueyuan Deng","Yiran Liu","Yulong Li","Qiang Yan","Imran Razzak","Jionglong Su"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19908v1","updated":"2025-12-22T22:22:46Z","published":"2025-12-22T22:22:46Z","title":"Counterfactual LLM-based Framework for Measuring Rhetorical Style","summary":"The rise of AI has fueled growing concerns about ``hype'' in machine learning papers, yet a reliable way to quantify rhetorical style independently of substantive content has remained elusive. Because bold language can stem from either strong empirical results or mere rhetorical style, it is often difficult to distinguish between the two. To disentangle rhetorical style from substantive content, we introduce a counterfactual, LLM-based framework: multiple LLM rhetorical personas generate counterfactual writings from the same substantive content, an LLM judge compares them through pairwise evaluations, and the outcomes are aggregated using a Bradley--Terry model. Applying this method to 8,485 ICLR submissions sampled from 2017 to 2025, we generate more than 250,000 counterfactual writings and provide a large-scale quantification of rhetorical style in ML papers. We find that visionary framing significantly predicts downstream attention, including citations and media attention, even after controlling for peer-review evaluations. We also observe a sharp rise in rhetorical strength after 2023, and provide empirical evidence showing that this increase is largely driven by the adoption of LLM-based writing assistance. The reliability of our framework is validated by its robustness to the choice of personas and the high correlation between LLM judgments and human annotations. Our work demonstrates that LLMs can serve as instruments to measure and improve scientific evaluation.","authors":["Jingyi Qiu","Hong Chen","Zongyi Li"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19903v1","updated":"2025-12-22T22:08:32Z","published":"2025-12-22T22:08:32Z","title":"How well do Large Language Models Recognize Instructional Moves? Establishing Baselines for Foundation Models in Educational Discourse","summary":"Large language models (LLMs) are increasingly adopted in educational technologies for a variety of tasks, from generating instructional materials and assisting with assessment design to tutoring. While prior work has investigated how models can be adapted or optimized for specific tasks, far less is known about how well LLMs perform at interpreting authentic educational scenarios without significant customization. As LLM-based systems become widely adopted by learners and educators in everyday academic contexts, understanding their out-of-the-box capabilities is increasingly important for setting expectations and benchmarking. We compared six LLMs to estimate their baseline performance on a simple but important task: classifying instructional moves in authentic classroom transcripts. We evaluated typical prompting methods: zero-shot, one-shot, and few-shot prompting. We found that while zero-shot performance was moderate, providing comprehensive examples (few-shot prompting) significantly improved performance for state-of-the-art models, with the strongest configuration reaching Cohen's Kappa = 0.58 against expert-coded annotations. At the same time, improvements were neither uniform nor complete: performance varied considerably by instructional move, and higher recall frequently came at the cost of increased false positives. Overall, these findings indicate that foundation models demonstrate meaningful yet limited capacity to interpret instructional discourse, with prompt design helping to surface capability but not eliminating fundamental reliability constraints.","authors":["Kirk Vanacore","Rene F. Kizilcec"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19864v1","updated":"2025-12-22T20:38:30Z","published":"2025-12-22T20:38:30Z","title":"HARMON-E: Hierarchical Agentic Reasoning for Multimodal Oncology Notes to Extract Structured Data","summary":"Unstructured notes within the electronic health record (EHR) contain rich clinical information vital for cancer treatment decision making and research, yet reliably extracting structured oncology data remains challenging due to extensive variability, specialized terminology, and inconsistent document formats. Manual abstraction, although accurate, is prohibitively costly and unscalable. Existing automated approaches typically address narrow scenarios - either using synthetic datasets, restricting focus to document-level extraction, or isolating specific clinical variables (e.g., staging, biomarkers, histology) - and do not adequately handle patient-level synthesis across the large number of clinical documents containing contradictory information. In this study, we propose an agentic framework that systematically decomposes complex oncology data extraction into modular, adaptive tasks. Specifically, we use large language models (LLMs) as reasoning agents, equipped with context-sensitive retrieval and iterative synthesis capabilities, to exhaustively and comprehensively extract structured clinical variables from real-world oncology notes. Evaluated on a large-scale dataset of over 400,000 unstructured clinical notes and scanned PDF reports spanning 2,250 cancer patients, our method achieves an average F1-score of 0.93, with 100 out of 103 oncology-specific clinical variables exceeding 0.85, and critical variables (e.g., biomarkers and medications) surpassing 0.95. Moreover, integration of the agentic system into a data curation workflow resulted in 0.94 direct manual approval rate, significantly reducing annotation costs. To our knowledge, this constitutes the first exhaustive, end-to-end application of LLM-based agents for structured oncology data extraction at scale","authors":["Shashi Kant Gupta","Arijeet Pramanik","Jerrin John Thomas","Regina Schwind","Lauren Wiener","Avi Raju","Jeremy Kornbluth","Yanshan Wang","Zhaohui Su","Hrituraj Singh"],"pdf_url":"","comment":"39 Pages, Supplementary Included"},{"id":"http://arxiv.org/abs/2509.10534v2","updated":"2025-12-22T20:13:10Z","published":"2025-09-05T14:22:27Z","title":"Decoupling the \"What\" and \"Where\" With Polar Coordinate Positional Embeddings","summary":"The attention mechanism in a Transformer architecture matches key to query based on both content -- the what -- and position in a sequence -- the where. We present an analysis indicating that what and where are entangled in the popular RoPE rotary position embedding. This entanglement can impair performance particularly when decisions require independent matches on these two factors. We propose an improvement to RoPE, which we call Polar Coordinate Position Embeddings or PoPE, that eliminates the what-where confound. PoPE is far superior on a diagnostic task requiring indexing solely by position or by content. On autoregressive sequence modeling in music, genomic, and natural language domains, Transformers using PoPE as the positional encoding scheme outperform baselines using RoPE with respect to evaluation loss (perplexity) and downstream task performance. On language modeling, these gains persist across model scale, from 124M to 774M parameters. Crucially, PoPE shows strong zero-shot length extrapolation capabilities compared not only to RoPE but even a method designed for extrapolation, YaRN, which requires additional fine tuning and frequency interpolation.","authors":["Anand Gopalakrishnan","Robert Csordás","Jürgen Schmidhuber","Michael C. Mozer"],"pdf_url":"","comment":"Comparison to YaRN added + additional bias visualization + model ablation"},{"id":"http://arxiv.org/abs/2506.00920v2","updated":"2025-12-22T19:46:45Z","published":"2025-06-01T09:20:44Z","title":"Position as Probability: Self-Supervised Transformers that Think Past Their Training for Length Extrapolation","summary":"Deep sequence models typically degrade in accuracy when test sequences significantly exceed their training lengths, yet many critical tasks--such as algorithmic reasoning, multi-step arithmetic, and compositional generalization--require robust length extrapolation. We introduce PRISM, a Probabilistic Relative-position Implicit Superposition Model, a novel positional encoding mechanism that enables Transformers to extrapolate accurately up to 10x beyond their training length. PRISM learns continuous relative positions through a differentiable histogram-filter update, preserving position uncertainty via a probabilistic superposition rather than conventional deterministic embeddings. Empirically, PRISM achieves state-of-the-art length extrapolation, successfully generalizing to previously intractable sequence lengths across algorithmic benchmarks--including arithmetic (addition, multiplication), SCAN compositionality tasks, and complex copy variants derived from DeepMind's recent datasets. Our analysis demonstrates that PRISM's stochastic positional encoding maintains sharp and interpretable internal states, providing a theoretical basis for reliable length generalization. These results advance the goal of neural sequence models that remain algorithmically robust at lengths far exceeding their training horizon.","authors":["Philip Heejun Lee"],"pdf_url":"","comment":"Note: v2: working paper; code, additional baselines, ablations, will follow in v3"},{"id":"http://arxiv.org/abs/2507.16003v3","updated":"2025-12-22T19:30:19Z","published":"2025-07-21T18:44:35Z","title":"Learning without training: The implicit dynamics of in-context learning","summary":"One of the most striking features of Large Language Models (LLMs) is their ability to learn in-context. Namely at inference time an LLM is able to learn new patterns without any additional weight update when these patterns are presented in the form of examples in the prompt, even if these patterns were not seen during training. The mechanisms through which this can happen are still largely unknown. In this work, we show that the stacking of a self-attention layer with an MLP, allows the transformer block to implicitly modify the weights of the MLP layer according to the context. We argue through theory and experimentation that this simple mechanism may be the reason why LLMs can learn in-context and not only during training. Specifically, we show how a transformer block implicitly transforms a context into a low-rank weight-update of its MLP layer.","authors":["Benoit Dherin","Michael Munn","Hanna Mazzawi","Michael Wunder","Javier Gonzalvo"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2409.19916v5","updated":"2025-12-22T18:59:31Z","published":"2024-09-30T03:37:10Z","title":"Deep Learning and Machine Learning, Advancing Big Data Analytics and Management: Object-Oriented Programming","summary":"Object-Oriented Programming (OOP) has become a crucial paradigm for managing the growing complexity of modern software systems, particularly in fields like machine learning, deep learning, large language models (LLM), and data analytics. This work provides a comprehensive introduction to the integration of OOP techniques within these domains, with a focus on improving code modularity, maintainability, and scalability. We begin by outlining the evolution of computing and the rise of OOP, followed by an in-depth discussion of key OOP principles such as encapsulation, inheritance, polymorphism, and abstraction. The practical application of these principles is demonstrated using Python, a widely adopted language in AI and data science. Furthermore, we examine how design patterns and modular programming can be employed to enhance the structure and efficiency of machine learning systems. In subsequent sections, we apply these OOP concepts to real-world AI tasks, including the encapsulation of preprocessing workflows, machine learning model training, and evaluation. Detailed examples illustrate how OOP can be used to build reusable, scalable machine learning systems while maintaining code clarity and reducing redundancy.This work is intended to serve as a bridge for both beginners and experienced developers, equipping them with the necessary knowledge to apply OOP methodologies in AI-driven projects, ultimately fostering the development of more robust and maintainable systems.","authors":["Tianyang Wang","Ziqian Bi","Keyu Chen","Jiawei Xu","Qian Niu","Junyu Liu","Benji Peng","Ming Li","Sen Zhang","Xuanhe Pan","Jinlang Wang","Pohsun Feng","Yizhu Wen","Xinyuan Song","Ming Liu"],"pdf_url":"","comment":"49pages"},{"id":"http://arxiv.org/abs/2512.19399v1","updated":"2025-12-22T13:51:03Z","published":"2025-12-22T13:51:03Z","title":"Brain-Grounded Axes for Reading and Steering LLM States","summary":"Interpretability methods for large language models (LLMs) typically derive directions from textual supervision, which can lack external grounding. We propose using human brain activity not as a training signal but as a coordinate system for reading and steering LLM states. Using the SMN4Lang MEG dataset, we construct a word-level brain atlas of phase-locking value (PLV) patterns and extract latent axes via ICA. We validate axes with independent lexica and NER-based labels (POS/log-frequency used as sanity checks), then train lightweight adapters that map LLM hidden states to these brain axes without fine-tuning the LLM. Steering along the resulting brain-derived directions yields a robust lexical (frequency-linked) axis in a mid TinyLlama layer, surviving perplexity-matched controls, and a brain-vs-text probe comparison shows larger log-frequency shifts (relative to the text probe) with lower perplexity for the brain axis. A function/content axis (axis 13) shows consistent steering in TinyLlama, Qwen2-0.5B, and GPT-2, with PPL-matched text-level corroboration. Layer-4 effects in TinyLlama are large but inconsistent, so we treat them as secondary (Appendix). Axis structure is stable when the atlas is rebuilt without GPT embedding-change features or with word2vec embeddings (|r|=0.64-0.95 across matched axes), reducing circularity concerns. Exploratory fMRI anchoring suggests potential alignment for embedding change and log frequency, but effects are sensitive to hemodynamic modeling assumptions and are treated as population-level evidence only. These results support a new interface: neurophysiology-grounded axes provide interpretable and controllable handles for LLM behavior.","authors":["Sandro Andric"],"pdf_url":"","comment":"10 pages, 4 figures. Code: https://github.com/sandroandric/Brain-Grounded-Axes-for-Reading-and-Steering-LLM-States"},{"id":"http://arxiv.org/abs/2512.20687v1","updated":"2025-12-22T19:26:59Z","published":"2025-12-22T19:26:59Z","title":"PHOTON: Hierarchical Autoregressive Modeling for Lightspeed and Memory-Efficient Language Generation","summary":"Transformers operate as horizontal token-by-token scanners; at each generation step, the model attends to an ever-growing sequence of token-level states. This access pattern increases prefill latency and makes long-context decoding increasingly memory-bound, as KV-cache reads and writes dominate inference throughput rather than arithmetic computation. We propose Parallel Hierarchical Operation for Top-down Networks (PHOTON), a hierarchical autoregressive model that replaces flat scanning with vertical, multi-resolution context access. PHOTON maintains a hierarchy of latent streams: a bottom-up encoder progressively compresses tokens into low-rate contextual states, while lightweight top-down decoders reconstruct fine-grained token representations. Experimental results show that PHOTON is superior to competitive Transformer-based language models regarding the throughput-quality trade-off, offering significant advantages in long-context and multi-query tasks. This reduces decode-time KV-cache traffic, yielding up to $10^{3}\\times$ higher throughput per unit memory.","authors":["Yuma Ichikawa","Naoya Takagi","Takumi Nakagawa","Yuzi Kanazawa","Akira Sakai"],"pdf_url":"","comment":"12 pages, 5 figures"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2512.19693v1","updated":"2025-12-22T18:59:57Z","published":"2025-12-22T18:59:57Z","title":"The Prism Hypothesis: Harmonizing Semantic and Pixel Representations via Unified Autoencoding","summary":"Deep representations across modalities are inherently intertwined. In this paper, we systematically analyze the spectral characteristics of various semantic and pixel encoders. Interestingly, our study uncovers a highly inspiring and rarely explored correspondence between an encoder's feature spectrum and its functional role: semantic encoders primarily capture low-frequency components that encode abstract meaning, whereas pixel encoders additionally retain high-frequency information that conveys fine-grained detail. This heuristic finding offers a unifying perspective that ties encoder behavior to its underlying spectral structure. We define it as the Prism Hypothesis, where each data modality can be viewed as a projection of the natural world onto a shared feature spectrum, just like the prism. Building on this insight, we propose Unified Autoencoding (UAE), a model that harmonizes semantic structure and pixel details via an innovative frequency-band modulator, enabling their seamless coexistence. Extensive experiments on ImageNet and MS-COCO benchmarks validate that our UAE effectively unifies semantic abstraction and pixel-level fidelity into a single latent space with state-of-the-art performance.","authors":["Weichen Fan","Haiwen Diao","Quan Wang","Dahua Lin","Ziwei Liu"],"pdf_url":"","comment":"Code link: https://github.com/WeichenFan/UAE"},{"id":"http://arxiv.org/abs/2512.19692v1","updated":"2025-12-22T18:59:50Z","published":"2025-12-22T18:59:50Z","title":"Interact2Ar: Full-Body Human-Human Interaction Generation via Autoregressive Diffusion Models","summary":"Generating realistic human-human interactions is a challenging task that requires not only high-quality individual body and hand motions, but also coherent coordination among all interactants. Due to limitations in available data and increased learning complexity, previous methods tend to ignore hand motions, limiting the realism and expressivity of the interactions. Additionally, current diffusion-based approaches generate entire motion sequences simultaneously, limiting their ability to capture the reactive and adaptive nature of human interactions. To address these limitations, we introduce Interact2Ar, the first end-to-end text-conditioned autoregressive diffusion model for generating full-body, human-human interactions. Interact2Ar incorporates detailed hand kinematics through dedicated parallel branches, enabling high-fidelity full-body generation. Furthermore, we introduce an autoregressive pipeline coupled with a novel memory technique that facilitates adaptation to the inherent variability of human interactions using efficient large context windows. The adaptability of our model enables a series of downstream applications, including temporal motion composition, real-time adaptation to disturbances, and extension beyond dyadic to multi-person scenarios. To validate the generated motions, we introduce a set of robust evaluators and extended metrics designed specifically for assessing full-body interactions. Through quantitative and qualitative experiments, we demonstrate the state-of-the-art performance of Interact2Ar.","authors":["Pablo Ruiz-Ponce","Sergio Escalera","José García-Rodríguez","Jiankang Deng","Rolandos Alexandros Potamias"],"pdf_url":"","comment":"Project Page: https://pabloruizponce.com/papers/Interact2Ar"},{"id":"http://arxiv.org/abs/2512.19687v1","updated":"2025-12-22T18:59:07Z","published":"2025-12-22T18:59:07Z","title":"Pushing the Frontier of Audiovisual Perception with Large-Scale Multimodal Correspondence Learning","summary":"We introduce Perception Encoder Audiovisual, PE-AV, a new family of encoders for audio and video understanding trained with scaled contrastive learning. Built on PE, PE-AV makes several key contributions to extend representations to audio, and natively support joint embeddings across audio-video, audio-text, and video-text modalities. PE-AV's unified cross-modal embeddings enable novel tasks such as speech retrieval, and set a new state of the art across standard audio and video benchmarks. We unlock this by building a strong audiovisual data engine that synthesizes high-quality captions for O(100M) audio-video pairs, enabling large-scale supervision consistent across modalities. Our audio data includes speech, music, and general sound effects-avoiding single-domain limitations common in prior work. We exploit ten pairwise contrastive objectives, showing that scaling cross-modality and caption-type pairs strengthens alignment and improves zero-shot performance. We further develop PE-A-Frame by fine-tuning PE-AV with frame-level contrastive objectives, enabling fine-grained audio-frame-to-text alignment for tasks such as sound event detection.","authors":["Apoorv Vyas","Heng-Jui Chang","Cheng-Fu Yang","Po-Yao Huang","Luya Gao","Julius Richter","Sanyuan Chen","Matt Le","Piotr Dollár","Christoph Feichtenhofer","Ann Lee","Wei-Ning Hsu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19686v1","updated":"2025-12-22T18:59:03Z","published":"2025-12-22T18:59:03Z","title":"Visual-Aware CoT: Achieving High-Fidelity Visual Consistency in Unified Models","summary":"Recently, the introduction of Chain-of-Thought (CoT) has largely improved the generation ability of unified models. However, it is observed that the current thinking process during generation mainly focuses on the text consistency with the text prompt, ignoring the \\textbf{visual context consistency} with the visual reference images during the multi-modal generation, e.g., multi-reference generation. The lack of such consistency results in the failure in maintaining key visual features (like human ID, object attribute, style). To this end, we integrate the visual context consistency into the reasoning of unified models, explicitly motivating the model to sustain such consistency by 1) Adaptive Visual Planning: generating structured visual check list to figure out the visual element of needed consistency keeping, and 2) Iterative Visual Correction: performing self-reflection with the guidance of check lists and refining the generated result in an iterative manner. To achieve this, we use supervised finetuning to teach the model how to plan the visual checking, conduct self-reflection and self-refinement, and use flow-GRPO to further enhance the visual consistency through a customized visual checking reward. The experiments show that our method outperforms both zero-shot unified models and those with text CoTs in multi-modal generation, demonstrating higher visual context consistency.","authors":["Zixuan Ye","Quande Liu","Cong Wei","Yuanxing Zhang","Xintao Wang","Pengfei Wan","Kun Gai","Wenhan Luo"],"pdf_url":"","comment":"Project Page: https://zixuan-ye.github.io/VACoT/"},{"id":"http://arxiv.org/abs/2512.19684v1","updated":"2025-12-22T18:58:29Z","published":"2025-12-22T18:58:29Z","title":"Zero-shot Reconstruction of In-Scene Object Manipulation from Video","summary":"We build the first system to address the problem of reconstructing in-scene object manipulation from a monocular RGB video. It is challenging due to ill-posed scene reconstruction, ambiguous hand-object depth, and the need for physically plausible interactions. Existing methods operate in hand centric coordinates and ignore the scene, hindering metric accuracy and practical use. In our method, we first use data-driven foundation models to initialize the core components, including the object mesh and poses, the scene point cloud, and the hand poses. We then apply a two-stage optimization that recovers a complete hand-object motion from grasping to interaction, which remains consistent with the scene information observed in the input video.","authors":["Dixuan Lin","Tianyou Wang","Zhuoyang Pan","Yufu Wang","Lingjie Liu","Kostas Daniilidis"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19683v1","updated":"2025-12-22T18:58:12Z","published":"2025-12-22T18:58:12Z","title":"From Indoor to Open World: Revealing the Spatial Reasoning Gap in MLLMs","summary":"While Multimodal Large Language Models (MLLMs) have achieved impressive performance on semantic tasks, their spatial intelligence--crucial for robust and grounded AI systems--remains underdeveloped. Existing benchmarks fall short of diagnosing this limitation: they either focus on overly simplified qualitative reasoning or rely on domain-specific indoor data, constrained by the lack of outdoor datasets with verifiable metric ground truth. To bridge this gap, we introduce a large-scale benchmark built from pedestrian-perspective videos captured with synchronized stereo cameras, LiDAR, and IMU/GPS sensors. This dataset provides metrically precise 3D information, enabling the automatic generation of spatial reasoning questions that span a hierarchical spectrum--from qualitative relational reasoning to quantitative metric and kinematic understanding. Evaluations reveal that the performance gains observed in structured indoor benchmarks vanish in open-world settings. Further analysis using synthetic abnormal scenes and blinding tests confirms that current MLLMs depend heavily on linguistic priors instead of grounded visual reasoning. Our benchmark thus provides a principled platform for diagnosing these limitations and advancing physically grounded spatial intelligence.","authors":["Mingrui Wu","Zhaozhi Wang","Fangjinhua Wang","Jiaolong Yang","Marc Pollefeys","Tong Zhang"],"pdf_url":"","comment":"Project page: https://harmlesssr.github.io/openbench/"},{"id":"http://arxiv.org/abs/2508.20072v3","updated":"2025-12-22T18:57:39Z","published":"2025-08-27T17:39:11Z","title":"Discrete Diffusion VLA: Bringing Discrete Diffusion to Action Decoding in Vision-Language-Action Policies","summary":"Vision-Language-Action (VLA) models adapt large vision-language backbones to map images and instructions into robot actions. However, prevailing VLAs either generate actions auto-regressively in a fixed left-to-right order or attach separate MLP or diffusion heads outside the backbone, leading to fragmented information pathways and specialized training requirements that hinder a unified, scalable architecture. We present Discrete Diffusion VLA, a unified-transformer policy that models discretized action chunks with discrete diffusion. The design retains diffusion's progressive refinement paradigm while remaining natively compatible with the discrete token interface of VLMs. Our method achieves an adaptive decoding order that resolves easy action elements before harder ones and uses secondary re-masking to revisit uncertain predictions across refinement rounds, which improves consistency and enables robust error correction. This unified decoder preserves pre-trained vision-language priors, supports parallel decoding, breaks the autoregressive bottleneck, and reduces the number of function evaluations. Discrete Diffusion VLA achieves 96.3% avg. success rates on LIBERO, 71.2% visual matching on SimplerEnv-Fractal and 54.2% overall on SimplerEnv-Bridge. We also provide ablation study on vision-language ability retention on LIBERO-OOD (Out-of-Distribution) benchmark, with our method improving over autoregressive, MLP decoder and continuous diffusion baselines. These findings indicate that discrete-diffusion VLA supports precise action modeling and consistent training, laying groundwork for scaling VLA to larger models and datasets. Our code is available at https://github.com/Liang-ZX/DiscreteDiffusionVLA/tree/libero.","authors":["Zhixuan Liang","Yizhuo Li","Tianshuo Yang","Chengyue Wu","Sitong Mao","Tian Nian","Liuao Pei","Shunbo Zhou","Xiaokang Yang","Jiangmiao Pang","Yao Mu","Ping Luo"],"pdf_url":"","comment":"New experiments on VL retention and new ablations. 18 pages"},{"id":"http://arxiv.org/abs/2512.19680v1","updated":"2025-12-22T18:54:30Z","published":"2025-12-22T18:54:30Z","title":"VA-$π$: Variational Policy Alignment for Pixel-Aware Autoregressive Generation","summary":"Autoregressive (AR) visual generation relies on tokenizers to map images to and from discrete sequences. However, tokenizers are trained to reconstruct clean images from ground-truth tokens, while AR generators are optimized only for token likelihood. This misalignment leads to generated token sequences that may decode into low-quality images, without direct supervision from the pixel space. We propose VA-$π$, a lightweight post-training framework that directly optimizes AR models with a principled pixel-space objective. VA-$π$ formulates the generator-tokenizer alignment as a variational optimization, deriving an evidence lower bound (ELBO) that unifies pixel reconstruction and autoregressive modeling. To optimize under the discrete token space, VA-$π$ introduces a reinforcement-based alignment strategy that treats the AR generator as a policy, uses pixel-space reconstruction quality as its intrinsic reward. The reward is measured by how well the predicted token sequences can reconstruct the original image under teacher forcing, giving the model direct pixel-level guidance without expensive free-running sampling. The regularization term of the ELBO serves as a natural regularizer, maintaining distributional consistency of tokens. VA-$π$ enables rapid adaptation of existing AR generators, without neither tokenizer retraining nor external reward models. With only 1% ImageNet-1K data and 25 minutes of tuning, it reduces FID from 14.36 to 7.65 and improves IS from 86.55 to 116.70 on LlamaGen-XXL, while also yielding notable gains in the text-to-image task on GenEval for both visual generation model (LlamaGen: from 0.306 to 0.339) and unified multi-modal model (Janus-Pro: from 0.725 to 0.744). Code is available at https://github.com/Lil-Shake/VA-Pi.","authors":["Xinyao Liao","Qiyuan He","Kai Xu","Xiaoye Qu","Yicong Li","Wei Wei","Angela Yao"],"pdf_url":"","comment":"21 pages, 24 figures"},{"id":"http://arxiv.org/abs/2512.19678v1","updated":"2025-12-22T18:53:50Z","published":"2025-12-22T18:53:50Z","title":"WorldWarp: Propagating 3D Geometry with Asynchronous Video Diffusion","summary":"Generating long-range, geometrically consistent video presents a fundamental dilemma: while consistency demands strict adherence to 3D geometry in pixel space, state-of-the-art generative models operate most effectively in a camera-conditioned latent space. This disconnect causes current methods to struggle with occluded areas and complex camera trajectories. To bridge this gap, we propose WorldWarp, a framework that couples a 3D structural anchor with a 2D generative refiner. To establish geometric grounding, WorldWarp maintains an online 3D geometric cache built via Gaussian Splatting (3DGS). By explicitly warping historical content into novel views, this cache acts as a structural scaffold, ensuring each new frame respects prior geometry. However, static warping inevitably leaves holes and artifacts due to occlusions. We address this using a Spatio-Temporal Diffusion (ST-Diff) model designed for a \"fill-and-revise\" objective. Our key innovation is a spatio-temporal varying noise schedule: blank regions receive full noise to trigger generation, while warped regions receive partial noise to enable refinement. By dynamically updating the 3D cache at every step, WorldWarp maintains consistency across video chunks. Consequently, it achieves state-of-the-art fidelity by ensuring that 3D logic guides structure while diffusion logic perfects texture. Project page: \\href{https://hyokong.github.io/worldwarp-page/}{https://hyokong.github.io/worldwarp-page/}.","authors":["Hanyang Kong","Xingyi Yang","Xiaoxu Zheng","Xinchao Wang"],"pdf_url":"","comment":"Project page: https://hyokong.github.io/worldwarp-page/"},{"id":"http://arxiv.org/abs/2512.19676v1","updated":"2025-12-22T18:53:13Z","published":"2025-12-22T18:53:13Z","title":"Efficient Vision Mamba for MRI Super-Resolution via Hybrid Selective Scanning","summary":"Background: High-resolution MRI is critical for diagnosis, but long acquisition times limit clinical use. Super-resolution (SR) can enhance resolution post-scan, yet existing deep learning methods face fidelity-efficiency trade-offs. Purpose: To develop a computationally efficient and accurate deep learning framework for MRI SR that preserves anatomical detail for clinical integration. Materials and Methods: We propose a novel SR framework combining multi-head selective state-space models (MHSSM) with a lightweight channel MLP. The model uses 2D patch extraction with hybrid scanning to capture long-range dependencies. Each MambaFormer block integrates MHSSM, depthwise convolutions, and gated channel mixing. Evaluation used 7T brain T1 MP2RAGE maps (n=142) and 1.5T prostate T2w MRI (n=334). Comparisons included Bicubic interpolation, GANs (CycleGAN, Pix2pix, SPSR), transformers (SwinIR), Mamba (MambaIR), and diffusion models (I2SB, Res-SRDiff). Results: Our model achieved superior performance with exceptional efficiency. For 7T brain data: SSIM=0.951+-0.021, PSNR=26.90+-1.41 dB, LPIPS=0.076+-0.022, GMSD=0.083+-0.017, significantly outperforming all baselines (p<0.001). For prostate data: SSIM=0.770+-0.049, PSNR=27.15+-2.19 dB, LPIPS=0.190+-0.095, GMSD=0.087+-0.013. The framework used only 0.9M parameters and 57 GFLOPs, reducing parameters by 99.8% and computation by 97.5% versus Res-SRDiff, while outperforming SwinIR and MambaIR in accuracy and efficiency. Conclusion: The proposed framework provides an efficient, accurate MRI SR solution, delivering enhanced anatomical detail across datasets. Its low computational demand and state-of-the-art performance show strong potential for clinical translation.","authors":["Mojtaba Safari","Shansong Wang","Vanessa L Wildman","Mingzhe Hu","Zach Eidex","Chih-Wei Chang","Erik H Middlebrooks","Richard L. J Qiu","Pretesh Patel","Ashesh B. Jania","Hui Mao","Zhen Tian","Xiaofeng Yang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19675v1","updated":"2025-12-22T18:53:03Z","published":"2025-12-22T18:53:03Z","title":"Multimodal LLMs for Historical Dataset Construction from Archival Image Scans: German Patents (1877-1918)","summary":"We leverage multimodal large language models (LLMs) to construct a dataset of 306,070 German patents (1877-1918) from 9,562 archival image scans using our LLM-based pipeline powered by Gemini-2.5-Pro and Gemini-2.5-Flash-Lite. Our benchmarking exercise provides tentative evidence that multimodal LLMs can create higher quality datasets than our research assistants, while also being more than 795 times faster and 205 times cheaper in constructing the patent dataset from our image corpus. About 20 to 50 patent entries are embedded on each page, arranged in a double-column format and printed in Gothic and Roman fonts. The font and layout complexity of our primary source material suggests to us that multimodal LLMs are a paradigm shift in how datasets are constructed in economic history. We open-source our benchmarking and patent datasets as well as our LLM-based data pipeline, which can be easily adapted to other image corpora using LLM-assisted coding tools, lowering the barriers for less technical researchers. Finally, we explain the economics of deploying LLMs for historical dataset construction and conclude by speculating on the potential implications for the field of economic history.","authors":["Niclas Griesshaber","Jochen Streb"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2505.10999v3","updated":"2025-12-22T18:48:24Z","published":"2025-05-16T08:47:16Z","title":"DDAE++: Enhancing Diffusion Models Towards Unified Generative and Discriminative Learning","summary":"While diffusion models excel at image synthesis, useful representations have been shown to emerge from generative pre-training, suggesting a path towards unified generative and discriminative learning. However, suboptimal semantic flow within current architectures can hinder this potential: features encoding the richest high-level semantics are underutilized and diluted when propagating through decoding layers, impeding the formation of an explicit semantic bottleneck layer. To address this, we introduce self-conditioning, a lightweight mechanism that reshapes the model's layer-wise semantic hierarchy without external guidance. By aggregating and rerouting intermediate features to guide subsequent decoding layers, our method concentrates more high-level semantics, concurrently strengthening global generative guidance and forming more discriminative representations. This simple approach yields a dual-improvement trend across pixel-space UNet, UViT and latent-space DiT models with minimal overhead. Crucially, it creates an architectural semantic bridge that propagates discriminative improvements into generation and accommodates further techniques such as contrastive self-distillation. Experiments show that our enhanced models, especially self-conditioned DiT, are powerful dual learners that yield strong and transferable representations on image and dense classification tasks, surpassing various generative self-supervised models in linear probing while also improving or maintaining high generation quality.","authors":["Weilai Xiang","Hongyu Yang","Di Huang","Yunhong Wang"],"pdf_url":"","comment":"Updated version. Code available at https://github.com/FutureXiang/ddae_plus_plus"},{"id":"http://arxiv.org/abs/2512.19663v1","updated":"2025-12-22T18:41:45Z","published":"2025-12-22T18:41:45Z","title":"Beyond CLIP: Knowledge-Enhanced Multimodal Transformers for Cross-Modal Alignment in Diabetic Retinopathy Diagnosis","summary":"Diabetic retinopathy (DR) is a leading cause of preventable blindness worldwide, demanding accurate automated diagnostic systems. While general-domain vision-language models like Contrastive Language-Image Pre-Training (CLIP) perform well on natural image tasks, they struggle in medical domain applications, particularly in cross-modal retrieval for ophthalmological images. We propose a novel knowledge-enhanced joint embedding framework that integrates retinal fundus images, clinical text, and structured patient data through a multimodal transformer architecture to address the critical gap in medical image-text alignment. Our approach employs separate encoders for each modality: a Vision Transformer (ViT-B/16) for retinal images, Bio-ClinicalBERT for clinical narratives, and a multilayer perceptron for structured demographic and clinical features. These modalities are fused through a joint transformer with modality-specific embeddings, trained using multiple objectives including contrastive losses between modality pairs, reconstruction losses for images and text, and classification losses for DR severity grading according to ICDR and SDRG schemes. Experimental results on the Brazilian Multilabel Ophthalmological Dataset (BRSET) demonstrate significant improvements over baseline models. Our framework achieves near-perfect text-to-image retrieval performance with Recall@1 of 99.94% compared to fine-tuned CLIP's 1.29%, while maintaining state-of-the-art classification accuracy of 97.05% for SDRG and 97.97% for ICDR. Furthermore, zero-shot evaluation on the unseen DeepEyeNet dataset validates strong generalizability with 93.95% Recall@1 versus 0.22% for fine-tuned CLIP. These results demonstrate that our multimodal training approach effectively captures cross-modal relationships in the medical domain, establishing both superior retrieval capabilities and robust diagnostic performance.","authors":["Argha Kamal Samanta","Harshika Goyal","Vasudha Joshi","Tushar Mungle","Pabitra Mitra"],"pdf_url":"","comment":"14 pages, 14 figures"},{"id":"http://arxiv.org/abs/2512.19661v1","updated":"2025-12-22T18:39:58Z","published":"2025-12-22T18:39:58Z","title":"Over++: Generative Video Compositing for Layer Interaction Effects","summary":"In professional video compositing workflows, artists must manually create environmental interactions-such as shadows, reflections, dust, and splashes-between foreground subjects and background layers. Existing video generative models struggle to preserve the input video while adding such effects, and current video inpainting methods either require costly per-frame masks or yield implausible results. We introduce augmented compositing, a new task that synthesizes realistic, semi-transparent environmental effects conditioned on text prompts and input video layers, while preserving the original scene. To address this task, we present Over++, a video effect generation framework that makes no assumptions about camera pose, scene stationarity, or depth supervision. We construct a paired effect dataset tailored for this task and introduce an unpaired augmentation strategy that preserves text-driven editability. Our method also supports optional mask control and keyframe guidance without requiring dense annotations. Despite training on limited data, Over++ produces diverse and realistic environmental effects and outperforms existing baselines in both effect generation and scene preservation.","authors":["Luchao Qi","Jiaye Wu","Jun Myeong Choi","Cary Phillips","Roni Sengupta","Dan B Goldman"],"pdf_url":"","comment":"Project page: https://overplusplus.github.io/"},{"id":"http://arxiv.org/abs/2509.12715v2","updated":"2025-12-22T18:22:20Z","published":"2025-09-16T06:16:05Z","title":"AsyMoE: Leveraging Modal Asymmetry for Enhanced Expert Specialization in Large Vision-Language Models","summary":"Large Vision-Language Models (LVLMs) have demonstrated impressive performance on multimodal tasks through scaled architectures and extensive training. However, existing Mixture of Experts (MoE) approaches face challenges due to the asymmetry between visual and linguistic processing. Visual information is spatially complete, while language requires maintaining sequential context. As a result, MoE models struggle to balance modality-specific features and cross-modal interactions. Through systematic analysis, we observe that language experts in deeper layers progressively lose contextual grounding and rely more on parametric knowledge rather than utilizing the provided visual and linguistic information. To address this, we propose AsyMoE, a novel architecture that models this asymmetry using three specialized expert groups. We design intra-modality experts for modality-specific processing, hyperbolic inter-modality experts for hierarchical cross-modal interactions, and evidence-priority language experts to suppress parametric biases and maintain contextual grounding. Extensive experiments demonstrate that AsyMoE achieves 26.58% and 15.45% accuracy improvements over vanilla MoE and modality-specific MoE respectively, with 25.45% fewer activated parameters than dense models.","authors":["Heng Zhang","Haichuan Hu","Yaomin Shen","Weihao Yu","Yilei Yuan","Haochen You","Guo Cheng","Zijian Zhang","Lubin Gan","Huihui Wei","Hao Zhang","Jin Huang"],"pdf_url":"","comment":"This submission has been withdrawn by the authors due to a fundamental error in the methodology that affects the validity of the main results"},{"id":"http://arxiv.org/abs/2511.00908v2","updated":"2025-12-22T18:21:18Z","published":"2025-11-02T11:58:55Z","title":"GraphGeo: Multi-Agent Debate Framework for Visual Geo-localization with Heterogeneous Graph Neural Networks","summary":"Visual geo-localization requires extensive geographic knowledge and sophisticated reasoning to determine image locations without GPS metadata. Traditional retrieval methods are constrained by database coverage and quality. Recent Large Vision-Language Models (LVLMs) enable direct location reasoning from image content, yet individual models struggle with diverse geographic regions and complex scenes. Existing multi-agent systems improve performance through model collaboration but treat all agent interactions uniformly. They lack mechanisms to handle conflicting predictions effectively. We propose \\textbf{GraphGeo}, a multi-agent debate framework using heterogeneous graph neural networks for visual geo-localization. Our approach models diverse debate relationships through typed edges, distinguishing supportive collaboration, competitive argumentation, and knowledge transfer. We introduce a dual-level debate mechanism combining node-level refinement and edge-level argumentation modeling. A cross-level topology refinement strategy enables co-evolution between graph structure and agent representations. Experiments on multiple benchmarks demonstrate GraphGeo significantly outperforms state-of-the-art methods. Our framework transforms cognitive conflicts between agents into enhanced geo-localization accuracy through structured debate.","authors":["Heng Zheng","Yuling Shi","Xiaodong Gu","Haochen You","Zijian Zhang","Lubin Gan","Hao Zhang","Wenjun Huang","Jin Huang"],"pdf_url":"","comment":"This submission has been withdrawn by the authors due to a fundamental error in the methodology that affects the validity of the main results"},{"id":"http://arxiv.org/abs/2512.19648v1","updated":"2025-12-22T18:20:29Z","published":"2025-12-22T18:20:29Z","title":"4D Gaussian Splatting as a Learned Dynamical System","summary":"We reinterpret 4D Gaussian Splatting as a continuous-time dynamical system, where scene motion arises from integrating a learned neural dynamical field rather than applying per-frame deformations. This formulation, which we call EvoGS, treats the Gaussian representation as an evolving physical system whose state evolves continuously under a learned motion law. This unlocks capabilities absent in deformation-based approaches:(1) sample-efficient learning from sparse temporal supervision by modeling the underlying motion law; (2) temporal extrapolation enabling forward and backward prediction beyond observed time ranges; and (3) compositional dynamics that allow localized dynamics injection for controllable scene synthesis. Experiments on dynamic scene benchmarks show that EvoGS achieves better motion coherence and temporal consistency compared to deformation-field baselines while maintaining real-time rendering","authors":["Arnold Caleb Asiimwe","Carl Vondrick"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2509.00767v2","updated":"2025-12-22T18:14:12Z","published":"2025-08-31T09:38:59Z","title":"InterPose: Learning to Generate Human-Object Interactions from Large-Scale Web Videos","summary":"Human motion generation has shown great advances thanks to the recent diffusion models trained on large-scale motion capture data. Most of existing works, however, currently target animation of isolated people in empty scenes. Meanwhile, synthesizing realistic human-object interactions in complex 3D scenes remains a critical challenge in computer graphics and robotics. One obstacle towards generating versatile high-fidelity human-object interactions is the lack of large-scale datasets with diverse object manipulations. Indeed, existing motion capture data is typically restricted to single people and manipulations of limited sets of objects. To address this issue, we propose an automatic motion extraction pipeline and use it to collect interaction-rich human motions. Our new dataset InterPose contains 73.8K sequences of 3D human motions and corresponding text captions automatically obtained from 45.8K videos with human-object interactions. We perform extensive experiments and demonstrate InterPose to bring significant improvements to state-of-the-art methods for human motion generation. Moreover, using InterPose we develop an LLM-based agent enabling zero-shot animation of people interacting with diverse objects and scenes.","authors":["Yangsong Zhang","Abdul Ahad Butt","Gül Varol","Ivan Laptev"],"pdf_url":"","comment":"Accepted to 3DV 2026. Project page: https://mael-zys.github.io/InterPose/"},{"id":"http://arxiv.org/abs/2512.19632v1","updated":"2025-12-22T18:07:08Z","published":"2025-12-22T18:07:08Z","title":"Generative diffusion models for agricultural AI: plant image generation, indoor-to-outdoor translation, and expert preference alignment","summary":"The success of agricultural artificial intelligence depends heavily on large, diverse, and high-quality plant image datasets, yet collecting such data in real field conditions is costly, labor intensive, and seasonally constrained. This paper investigates diffusion-based generative modeling to address these challenges through plant image synthesis, indoor-to-outdoor translation, and expert preference aligned fine tuning. First, a Stable Diffusion model is fine tuned on captioned indoor and outdoor plant imagery to generate realistic, text conditioned images of canola and soybean. Evaluation using Inception Score, Frechet Inception Distance, and downstream phenotype classification shows that synthetic images effectively augment training data and improve accuracy. Second, we bridge the gap between high resolution indoor datasets and limited outdoor imagery using DreamBooth-based text inversion and image guided diffusion, generating translated images that enhance weed detection and classification with YOLOv8. Finally, a preference guided fine tuning framework trains a reward model on expert scores and applies reward weighted updates to produce more stable and expert aligned outputs. Together, these components demonstrate a practical pathway toward data efficient generative pipelines for agricultural AI.","authors":["Da Tan","Michael Beck","Christopher P. Bidinosti","Robert H. Gulden","Christopher J. Henry"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19629v1","updated":"2025-12-22T18:03:08Z","published":"2025-12-22T18:03:08Z","title":"LoGoPlanner: Localization Grounded Navigation Policy with Metric-aware Visual Geometry","summary":"Trajectory planning in unstructured environments is a fundamental and challenging capability for mobile robots. Traditional modular pipelines suffer from latency and cascading errors across perception, localization, mapping, and planning modules. Recent end-to-end learning methods map raw visual observations directly to control signals or trajectories, promising greater performance and efficiency in open-world settings. However, most prior end-to-end approaches still rely on separate localization modules that depend on accurate sensor extrinsic calibration for self-state estimation, thereby limiting generalization across embodiments and environments. We introduce LoGoPlanner, a localization-grounded, end-to-end navigation framework that addresses these limitations by: (1) finetuning a long-horizon visual-geometry backbone to ground predictions with absolute metric scale, thereby providing implicit state estimation for accurate localization; (2) reconstructing surrounding scene geometry from historical observations to supply dense, fine-grained environmental awareness for reliable obstacle avoidance; and (3) conditioning the policy on implicit geometry bootstrapped by the aforementioned auxiliary tasks, thereby reducing error propagation.We evaluate LoGoPlanner in both simulation and real-world settings, where its fully end-to-end design reduces cumulative error while metric-aware geometry memory enhances planning consistency and obstacle avoidance, leading to more than a 27.3\\% improvement over oracle-localization baselines and strong generalization across embodiments and environments. The code and models have been made publicly available on the \\href{https://steinate.github.io/logoplanner.github.io/}{project page}.","authors":["Jiaqi Peng","Wenzhe Cai","Yuqiang Yang","Tai Wang","Yuan Shen","Jiangmiao Pang"],"pdf_url":"","comment":"Project page:https://steinate.github.io/logoplanner.github.io/"},{"id":"http://arxiv.org/abs/2412.11154v3","updated":"2025-12-22T17:52:35Z","published":"2024-12-15T11:08:49Z","title":"From Easy to Hard: Progressive Active Learning Framework for Infrared Small Target Detection with Single Point Supervision","summary":"Recently, single-frame infrared small target (SIRST) detection with single point supervision has drawn wide-spread attention. However, the latest label evolution with single point supervision (LESPS) framework suffers from instability, excessive label evolution, and difficulty in exerting embedded network performance. Inspired by organisms gradually adapting to their environment and continuously accumulating knowledge, we construct an innovative Progressive Active Learning (PAL) framework, which drives the existing SIRST detection networks progressively and actively recognizes and learns harder samples. Specifically, to avoid the early low-performance model leading to the wrong selection of hard samples, we propose a model pre-start concept, which focuses on automatically selecting a portion of easy samples and helping the model have basic task-specific learning capabilities. Meanwhile, we propose a refined dual-update strategy, which can promote reasonable learning of harder samples and continuous refinement of pseudo-labels. In addition, to alleviate the risk of excessive label evolution, a decay factor is reasonably introduced, which helps to achieve a dynamic balance between the expansion and contraction of target annotations. Extensive experiments show that existing SIRST detection networks equipped with our PAL framework have achieved state-of-the-art (SOTA) results on multiple public datasets. Furthermore, our PAL framework can build an efficient and stable bridge between full supervision and single point supervision tasks. Our code is available at https://github.com/YuChuang1205/PAL","authors":["Chuang Yu","Jinmiao Zhao","Yunpeng Liu","Sicheng Zhao","Yimian Dai","Xiangyu Yue"],"pdf_url":"","comment":"Accepted by ICCV 2025"},{"id":"http://arxiv.org/abs/2512.19609v1","updated":"2025-12-22T17:45:39Z","published":"2025-12-22T17:45:39Z","title":"MapTrace: Scalable Data Generation for Route Tracing on Maps","summary":"While Multimodal Large Language Models have achieved human-like performance on many visual and textual reasoning tasks, their proficiency in fine-grained spatial understanding, such as route tracing on maps remains limited. Unlike humans, who can quickly learn to parse and navigate maps, current models often fail to respect fundamental path constraints, in part due to the prohibitive cost and difficulty of collecting large-scale, pixel-accurate path annotations. To address this, we introduce a scalable synthetic data generation pipeline that leverages synthetic map images and pixel-level parsing to automatically produce precise annotations for this challenging task. Using this pipeline, we construct a fine-tuning dataset of 23k path samples across 4k maps, enabling models to acquire more human-like spatial capabilities. Using this dataset, we fine-tune both open-source and proprietary MLLMs. Results on MapBench show that finetuning substantially improves robustness, raising success rates by up to 6.4 points, while also reducing path-tracing error (NDTW). These gains highlight that fine-grained spatial reasoning, absent in pretrained models, can be explicitly taught with synthetic supervision.","authors":["Artemis Panagopoulou","Aveek Purohit","Achin Kulshrestha","Soroosh Yazdani","Mohit Goyal"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19605v1","updated":"2025-12-22T17:41:26Z","published":"2025-12-22T17:41:26Z","title":"KerJEPA: Kernel Discrepancies for Euclidean Self-Supervised Learning","summary":"Recent breakthroughs in self-supervised Joint-Embedding Predictive Architectures (JEPAs) have established that regularizing Euclidean representations toward isotropic Gaussian priors yields provable gains in training stability and downstream generalization. We introduce a new, flexible family of KerJEPAs, self-supervised learning algorithms with kernel-based regularizers. One instance of this family corresponds to the recently-introduced LeJEPA Epps-Pulley regularizer which approximates a sliced maximum mean discrepancy (MMD) with a Gaussian prior and Gaussian kernel. By expanding the class of viable kernels and priors and computing the closed-form high-dimensional limit of sliced MMDs, we develop alternative KerJEPAs with a number of favorable properties including improved training stability and design flexibility.","authors":["Eric Zimmermann","Harley Wiltzer","Justin Szeto","David Alvarez-Melis","Lester Mackey"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19602v1","updated":"2025-12-22T17:35:32Z","published":"2025-12-22T17:35:32Z","title":"No Data? No Problem: Robust Vision-Tabular Learning with Missing Values","summary":"Large-scale medical biobanks provide imaging data complemented by extensive tabular information, such as demographics or clinical measurements. However, this abundance of tabular attributes does not reflect real-world datasets, where only a subset of attributes may be available. This discrepancy calls for methods that can leverage all the tabular data during training while remaining robust to missing values at inference. To address this challenge, we propose RoVTL (Robust Vision-Tabular Learning), a framework designed to handle any level of tabular data availability, from 0% to 100%. RoVTL comprises two key stages: contrastive pretraining, where we introduce tabular attribute missingness as data augmentation to promote robustness, and downstream task tuning using a gated cross-attention module for multimodal fusion. During fine-tuning, we employ a novel Tabular More vs. Fewer loss that ranks performance based on the amount of available tabular data. Combined with disentangled gradient learning, this enables consistent performance across all tabular data completeness scenarios. We evaluate RoVTL on cardiac MRI scans from the UK Biobank, demonstrating superior robustness to missing tabular data compared to prior methods. Furthermore, RoVTL successfully generalizes to an external cardiac MRI dataset for multimodal disease classification, and extends to the natural images domain, achieving robust performance on a car advertisements dataset. The code is available at https://github.com/marteczkah/RoVTL.","authors":["Marta Hasny","Laura Daza","Keno Bressem","Maxime Di Folco","Julia Schnabel"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19584v1","updated":"2025-12-22T17:11:33Z","published":"2025-12-22T17:11:33Z","title":"Patlak Parametric Image Estimation from Dynamic PET Using Diffusion Model Prior","summary":"Dynamic PET enables the quantitative estimation of physiology-related parameters and is widely utilized in research and increasingly adopted in clinical settings. Parametric imaging in dynamic PET requires kinetic modeling to estimate voxel-wise physiological parameters based on specific kinetic models. However, parametric images estimated through kinetic model fitting often suffer from low image quality due to the inherently ill-posed nature of the fitting process and the limited counts resulting from non-continuous data acquisition across multiple bed positions in whole-body PET. In this work, we proposed a diffusion model-based kinetic modeling framework for parametric image estimation, using the Patlak model as an example. The score function of the diffusion model was pre-trained on static total-body PET images and served as a prior for both Patlak slope and intercept images by leveraging their patch-wise similarity. During inference, the kinetic model was incorporated as a data-consistency constraint to guide the parametric image estimation. The proposed framework was evaluated on total-body dynamic PET datasets with different dose levels, demonstrating the feasibility and promising performance of the proposed framework in improving parametric image quality.","authors":["Ziqian Huang","Boxiao Yu","Siqi Li","Savas Ozdemir","Sangjin Bae","Jae Sung Lee","Guobao Wang","Kuang Gong"],"pdf_url":"","comment":"10 pages, 9 figures"},{"id":"http://arxiv.org/abs/2512.19577v1","updated":"2025-12-22T17:03:07Z","published":"2025-12-22T17:03:07Z","title":"Deep Learning for Primordial $B$-mode Extraction","summary":"The search for primordial gravitational waves is a central goal of cosmic microwave background (CMB) surveys. Isolating the characteristic $B$-mode polarization signal sourced by primordial gravitational waves is challenging for several reasons: the amplitude of the signal is inherently small; astrophysical foregrounds produce $B$-mode polarization contaminating the signal; and secondary $B$-mode polarization fluctuations are produced via the conversion of $E$ modes. Current and future low-noise, multi-frequency observations enable sufficient precision to address the first two of these challenges such that secondary $B$ modes will become the bottleneck for improved constraints on the amplitude of primordial gravitational waves. The dominant source of secondary $B$-mode polarization is gravitational lensing by large scale structure. Various strategies have been developed to estimate the lensing deflection and to reverse its effects the CMB, thus reducing confusion from lensing $B$ modes in the search for primordial gravitational waves. However, a few complications remain. First, there may be additional sources of secondary $B$-mode polarization, for example from patchy reionization or from cosmic polarization rotation. Second, the statistics of delensed CMB maps can become complicated and non-Gaussian, especially when advanced lensing reconstruction techniques are applied. We previously demonstrated how a deep learning network, ResUNet-CMB, can provide nearly optimal simultaneous estimates of multiple sources of secondary $B$-mode polarization. In this paper, we show how deep learning can be applied to estimate and remove multiple sources of secondary $B$-mode polarization, and we further show how this technique can be used in a likelihood analysis to produce nearly optimal, unbiased estimates of the amplitude of primordial gravitational waves.","authors":["Eric Guzman","Joel Meyers"],"pdf_url":"","comment":"12 pages, 8 figures. Code available from https://github.com/EEmGuzman/resunet-cmb"},{"id":"http://arxiv.org/abs/2509.20490v3","updated":"2025-12-22T17:02:44Z","published":"2025-09-24T19:08:01Z","title":"RadAgents: Multimodal Agentic Reasoning for Chest X-ray Interpretation with Radiologist-like Workflows","summary":"Agentic systems offer a potential path to solve complex clinical tasks through collaboration among specialized agents, augmented by tool use and external knowledge bases. Nevertheless, for chest X-ray (CXR) interpretation, prevailing methods remain limited: (i) reasoning is frequently neither clinically interpretable nor aligned with guidelines, reflecting mere aggregation of tool outputs; (ii) multimodal evidence is insufficiently fused, yielding text-only rationales that are not visually grounded; and (iii) systems rarely detect or resolve cross-tool inconsistencies and provide no principled verification mechanisms. To bridge the above gaps, we present RadAgents, a multi-agent framework that couples clinical priors with task-aware multimodal reasoning and encodes a radiologist-style workflow into a modular, auditable pipeline. In addition, we integrate grounding and multimodal retrieval-augmentation to verify and resolve context conflicts, resulting in outputs that are more reliable, transparent, and consistent with clinical practice.","authors":["Kai Zhang","Corey D Barrett","Jangwon Kim","Lichao Sun","Tara Taghavi","Krishnaram Kenthapadi"],"pdf_url":"","comment":"ML4H'25; Work in progress"},{"id":"http://arxiv.org/abs/2512.19560v1","updated":"2025-12-22T16:42:58Z","published":"2025-12-22T16:42:58Z","title":"BabyFlow: 3D modeling of realistic and expressive infant faces","summary":"Early detection of developmental disorders can be aided by analyzing infant craniofacial morphology, but modeling infant faces is challenging due to limited data and frequent spontaneous expressions. We introduce BabyFlow, a generative AI model that disentangles facial identity and expression, enabling independent control over both. Using normalizing flows, BabyFlow learns flexible, probabilistic representations that capture the complex, non-linear variability of expressive infant faces without restrictive linear assumptions. To address scarce and uncontrolled expressive data, we perform cross-age expression transfer, adapting expressions from adult 3D scans to enrich infant datasets with realistic and systematic expressive variants. As a result, BabyFlow improves 3D reconstruction accuracy, particularly in highly expressive regions such as the mouth, eyes, and nose, and supports synthesis and modification of infant expressions while preserving identity. Additionally, by integrating with diffusion models, BabyFlow generates high-fidelity 2D infant images with consistent 3D geometry, providing powerful tools for data augmentation and early facial analysis.","authors":["Antonia Alomar","Mireia Masias","Marius George Linguraru","Federico M. Sukno","Gemma Piella"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19546v1","updated":"2025-12-22T16:28:27Z","published":"2025-12-22T16:28:27Z","title":"ActAvatar: Temporally-Aware Precise Action Control for Talking Avatars","summary":"Despite significant advances in talking avatar generation, existing methods face critical challenges: insufficient text-following capability for diverse actions, lack of temporal alignment between actions and audio content, and dependency on additional control signals such as pose skeletons. We present ActAvatar, a framework that achieves phase-level precision in action control through textual guidance by capturing both action semantics and temporal context. Our approach introduces three core innovations: (1) Phase-Aware Cross-Attention (PACA), which decomposes prompts into a global base block and temporally-anchored phase blocks, enabling the model to concentrate on phase-relevant tokens for precise temporal-semantic alignment; (2) Progressive Audio-Visual Alignment, which aligns modality influence with the hierarchical feature learning process-early layers prioritize text for establishing action structure while deeper layers emphasize audio for refining lip movements, preventing modality interference; (3) A two-stage training strategy that first establishes robust audio-visual correspondence on diverse data, then injects action control through fine-tuning on structured annotations, maintaining both audio-visual alignment and the model's text-following capabilities. Extensive experiments demonstrate that ActAvatar significantly outperforms state-of-the-art methods in both action control and visual quality.","authors":["Ziqiao Peng","Yi Chen","Yifeng Ma","Guozhen Zhang","Zhiyao Sun","Zixiang Zhou","Youliang Zhang","Zhengguang Zhou","Zhaoxin Fan","Hongyan Liu","Yuan Zhou","Qinglin Lu","Jun He"],"pdf_url":"","comment":"Project Page: https://ziqiaopeng.github.io/ActAvatar/"},{"id":"http://arxiv.org/abs/2410.24116v2","updated":"2025-12-22T16:25:04Z","published":"2024-10-31T16:46:23Z","title":"AIDOVECL: AI-generated Dataset of Outpainted Vehicles for Eye-level Classification and Localization","summary":"Image labeling is a critical bottleneck in the development of computer vision technologies, often constraining the potential of machine learning models due to the time-intensive nature of manual annotations. This work introduces a novel approach that leverages outpainting to mitigate the problem of annotated data scarcity by generating artificial contexts and annotations, significantly reducing manual labeling efforts. We apply this technique to a particularly acute challenge in autonomous driving, urban planning, and environmental monitoring: the lack of diverse, eye-level vehicle images in desired classes. Our dataset comprises AI-generated vehicle images obtained by detecting and cropping vehicles from manually selected seed images, which are then outpainted onto larger canvases to simulate varied real-world conditions. The outpainted images include detailed annotations, providing high-quality ground truth data. Advanced outpainting techniques and image quality assessments ensure visual fidelity and contextual relevance. Ablation results show that incorporating AIDOVECL improves overall detection performance by up to 10%, and delivers gains of up to 40% in settings with greater diversity of context, object scale, and placement, with underrepresented classes achieving up to 50% higher true positives. AIDOVECL enhances vehicle detection by augmenting real training data and supporting evaluation across diverse scenarios. By demonstrating outpainting as an automatic annotation paradigm, it offers a practical and versatile solution for building fine-grained datasets with reduced labeling effort across multiple machine learning domains. The code and links to datasets used in this study are available for further research and replication at https://github.com/amir-kazemi/aidovecl .","authors":["Amir Kazemi","Qurat ul ain Fatima","Volodymyr Kindratenko","Christopher Tessum"],"pdf_url":"","comment":"34 pages, 10 figures, 5 tables"},{"id":"http://arxiv.org/abs/2512.19539v1","updated":"2025-12-22T16:23:24Z","published":"2025-12-22T16:23:24Z","title":"StoryMem: Multi-shot Long Video Storytelling with Memory","summary":"Visual storytelling requires generating multi-shot videos with cinematic quality and long-range consistency. Inspired by human memory, we propose StoryMem, a paradigm that reformulates long-form video storytelling as iterative shot synthesis conditioned on explicit visual memory, transforming pre-trained single-shot video diffusion models into multi-shot storytellers. This is achieved by a novel Memory-to-Video (M2V) design, which maintains a compact and dynamically updated memory bank of keyframes from historical generated shots. The stored memory is then injected into single-shot video diffusion models via latent concatenation and negative RoPE shifts with only LoRA fine-tuning. A semantic keyframe selection strategy, together with aesthetic preference filtering, further ensures informative and stable memory throughout generation. Moreover, the proposed framework naturally accommodates smooth shot transitions and customized story generation applications. To facilitate evaluation, we introduce ST-Bench, a diverse benchmark for multi-shot video storytelling. Extensive experiments demonstrate that StoryMem achieves superior cross-shot consistency over previous methods while preserving high aesthetic quality and prompt adherence, marking a significant step toward coherent minute-long video storytelling.","authors":["Kaiwen Zhang","Liming Jiang","Angtian Wang","Jacob Zhiyuan Fang","Tiancheng Zhi","Qing Yan","Hao Kang","Xin Lu","Xingang Pan"],"pdf_url":"","comment":"Project page: https://kevin-thu.github.io/StoryMem"},{"id":"http://arxiv.org/abs/2512.19535v1","updated":"2025-12-22T16:21:39Z","published":"2025-12-22T16:21:39Z","title":"CASA: Cross-Attention via Self-Attention for Efficient Vision-Language Fusion","summary":"Vision-language models (VLMs) are commonly trained by inserting image tokens from a pretrained vision encoder into the textual stream of a language model. This allows text and image information to fully attend to one another within the model, but becomes extremely costly for high-resolution images, long conversations, or streaming videos, both in memory and compute. VLMs leveraging cross-attention are an efficient alternative to token insertion but exhibit a clear performance gap, in particular on tasks involving fine-grained visual details. We find that a key to improving such models is to also enable local text-to-text interaction in the dedicated cross-attention layers. Building on this, we propose CASA, Cross-Attention via Self-Attention, a simple and efficient paradigm which substantially reduces the gap with full token insertion on common image understanding benchmarks, while enjoying the same scalability as cross-attention models when applied to long-context multimodal tasks such as streaming video captioning. For samples and code, please see our project page at https://kyutai.org/casa .","authors":["Moritz Böhle","Amélie Royer","Juliette Marrie","Edouard Grave","Patrick Pérez"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19534v1","updated":"2025-12-22T16:21:29Z","published":"2025-12-22T16:21:29Z","title":"SlicerOrbitSurgerySim: An Open-Source Platform for Virtual Registration and Quantitative Comparison of Preformed Orbital Plates","summary":"Poor adaptation of orbital implants remains a major contributor to postoperative complications and revision surgery. Although preformed orbital plates are widely used to reduce cost and operative time compared with customized implants, surgeons currently lack publicly available tools and standardized metrics to quantitatively compare plate fit across vendors, sizes, and patient anatomy. We developed SlicerOrbitSurgerySim, an open-source extension for the 3D Slicer platform that enables interactive virtual registration, evaluation, and comparison of multiple preformed orbital plates in a patient-specific virtual planning environment. The software generates reproducible quantitative plate-to-orbit distance metrics and visualization tools that support both patient-specific planning and population-level statistical analysis of plate adaptability. By facilitating objective comparison of implant designs and placement strategies, this tool aims to improve preoperative decision-making, reduce intraoperative plate modification, and promote collaborative research and surgical education. Pilot studies, sample datasets, and detailed tutorials are provided to support testing, transparency, and reproducibility.","authors":["Chi Zhang","Braedon Gunn","Andrew M. Read-Fuller"],"pdf_url":"","comment":"12 pages, 8 figures. Submitted to Journal of Oral and Maxillofacial Surgery. Code: https://github.com/chz31/SlicerOrbitSurgerySim/tree/main"},{"id":"http://arxiv.org/abs/2512.19528v1","updated":"2025-12-22T16:18:45Z","published":"2025-12-22T16:18:45Z","title":"Multi-Modal Soccer Scene Analysis with Masked Pre-Training","summary":"In this work we propose a multi-modal architecture for analyzing soccer scenes from tactical camera footage, with a focus on three core tasks: ball trajectory inference, ball state classification, and ball possessor identification. To this end, our solution integrates three distinct input modalities (player trajectories, player types and image crops of individual players) into a unified framework that processes spatial and temporal dynamics using a cascade of sociotemporal transformer blocks. Unlike prior methods, which rely heavily on accurate ball tracking or handcrafted heuristics, our approach infers the ball trajectory without direct access to its past or future positions, and robustly identifies the ball state and ball possessor under noisy or occluded conditions from real top league matches. We also introduce CropDrop, a modality-specific masking pre-training strategy that prevents over-reliance on image features and encourages the model to rely on cross-modal patterns during pre-training. We show the effectiveness of our approach on a large-scale dataset providing substantial improvements over state-of-the-art baselines in all tasks. Our results highlight the benefits of combining structured and visual cues in a transformer-based architecture, and the importance of realistic masking strategies in multi-modal learning.","authors":["Marc Peral","Guillem Capellera","Luis Ferraz","Antonio Rubio","Antonio Agudo"],"pdf_url":"","comment":"10 pages, 2 figures. WACV 2026"},{"id":"http://arxiv.org/abs/2512.19522v1","updated":"2025-12-22T16:16:13Z","published":"2025-12-22T16:16:13Z","title":"A Convolutional Neural Deferred Shader for Physics Based Rendering","summary":"Recent advances in neural rendering have achieved impressive results on photorealistic shading and relighting, by using a multilayer perceptron (MLP) as a regression model to learn the rendering equation from a real-world dataset. Such methods show promise for photorealistically relighting real-world objects, which is difficult to classical rendering, as there is no easy-obtained material ground truth. However, significant challenges still remain the dense connections in MLPs result in a large number of parameters, which requires high computation resources, complicating the training, and reducing performance during rendering. Data driven approaches require large amounts of training data for generalization; unbalanced data might bias the model to ignore the unusual illumination conditions, e.g. dark scenes. This paper introduces pbnds+: a novel physics-based neural deferred shading pipeline utilizing convolution neural networks to decrease the parameters and improve the performance in shading and relighting tasks; Energy regularization is also proposed to restrict the model reflection during dark illumination. Extensive experiments demonstrate that our approach outperforms classical baselines, a state-of-the-art neural shading model, and a diffusion-based method.","authors":["Zhuo He","Yingdong Ru","Qianying Liu","Paul Henderson","Nicolas Pugeault"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19512v1","updated":"2025-12-22T16:06:36Z","published":"2025-12-22T16:06:36Z","title":"Anatomy-R1: Enhancing Anatomy Reasoning in Multimodal Large Language Models via Anatomical Similarity Curriculum and Group Diversity Augmentation","summary":"Multimodal Large Language Models (MLLMs) have achieved impressive progress in natural image reasoning, yet their potential in medical imaging remains underexplored, especially in clinical anatomical surgical images. Anatomy understanding tasks demand precise understanding and clinically coherent answers, which are difficult to achieve due to the complexity of medical data and the scarcity of high-quality expert annotations. These challenges limit the effectiveness of conventional Supervised Fine-Tuning (SFT) strategies. While recent work has demonstrated that Group Relative Policy Optimization (GRPO) can enhance reasoning in MLLMs without relying on large amounts of data, we find two weaknesses that hinder GRPO's reasoning performance in anatomy recognition: 1) knowledge cannot be effectively shared between different anatomical structures, resulting in uneven information gain and preventing the model from converging, and 2) the model quickly converges to a single reasoning path, suppressing the exploration of diverse strategies. To overcome these challenges, we propose two novel methods. First, we implement a progressive learning strategy called Anatomical Similarity Curriculum Learning by controlling question difficulty via the similarity of answer choices, enabling the model to master complex problems incrementally. Second, we utilize question augmentation referred to as Group Diversity Question Augmentation to expand the model's search space for difficult queries, mitigating the tendency to produce uniform responses. Comprehensive experiments on the SGG-VQA and OmniMedVQA benchmarks show our method achieves a significant improvement across the two benchmarks, demonstrating its effectiveness in enhancing the medical reasoning capabilities of MLLMs. The code can be found in https://github.com/tomato996/Anatomy-R1","authors":["Ziyang Song","Zelin Zang","Zuyao Chen","Xusheng Liang","Dong Yi","Jinlin Wu","Hongbin Liu","Jiebo Luo"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2508.00493v2","updated":"2025-12-22T16:00:24Z","published":"2025-08-01T10:16:26Z","title":"SAMSA 2.0: Prompting Segment Anything with Spectral Angles for Hyperspectral Interactive Medical Image Segmentation","summary":"We present SAMSA 2.0, an interactive segmentation framework for hyperspectral medical imaging that introduces spectral angle prompting to guide the Segment Anything Model (SAM) using spectral similarity alongside spatial cues. This early fusion of spectral information enables more accurate and robust segmentation across diverse spectral datasets. Without retraining, SAMSA 2.0 achieves up to +3.8% higher Dice scores compared to RGB-only models and up to +3.1% over prior spectral fusion methods. Our approach enhances few-shot and zero-shot performance, demonstrating strong generalization in challenging low-data and noisy scenarios common in clinical imaging.","authors":["Alfie Roddan","Tobias Czempiel","Chi Xu","Daniel S. Elson","Stamatia Giannarou"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19504v1","updated":"2025-12-22T15:59:37Z","published":"2025-12-22T15:59:37Z","title":"FusionNet: Physics-Aware Representation Learning for Multi-Spectral and Thermal Data via Trainable Signal-Processing Priors","summary":"Modern deep learning models operating on multi-modal visual signals often rely on inductive biases that are poorly aligned with the physical processes governing signal formation, leading to brittle performance under cross-spectral and real-world conditions. In particular, approaches that prioritise direct thermal cues struggle to capture indirect yet persistent environmental alterations induced by sustained heat emissions.\n  This work introduces a physics-aware representation learning framework that leverages multi-spectral information to model stable signatures of long-term physical processes. Specifically, a geological Short Wave Infrared (SWIR) ratio sensitive to soil property changes is integrated with Thermal Infrared (TIR) data through an intermediate fusion architecture, instantiated as FusionNet. The proposed backbone embeds trainable differential signal-processing priors within convolutional layers, combines mixed pooling strategies, and employs wider receptive fields to enhance robustness across spectral modalities.\n  Systematic ablations show that each architectural component contributes to performance gains, with DGCNN achieving 88.7% accuracy on the SWIR ratio and FusionNet reaching 90.6%, outperforming state-of-the-art baselines across five spectral configurations. Transfer learning experiments further show that ImageNet pretraining degrades TIR performance, highlighting the importance of modality-aware training for cross-spectral learning.\n  Evaluated on real-world data, the results demonstrate that combining physics-aware feature selection with principled deep learning architectures yields robust and generalisable representations, illustrating how first-principles signal modelling can improve multi-spectral learning under challenging conditions.","authors":["Georgios Voulgaris"],"pdf_url":"","comment":"Preprint. Under review at IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing (JSTARS)"},{"id":"http://arxiv.org/abs/2512.19489v1","updated":"2025-12-22T15:43:59Z","published":"2025-12-22T15:43:59Z","title":"Rethinking Coupled Tensor Analysis for Hyperspectral Super-Resolution: Recoverable Modeling Under Endmember Variability","summary":"This work revisits the hyperspectral super-resolution (HSR) problem, i.e., fusing a pair of spatially co-registered hyperspectral (HSI) and multispectral (MSI) images to recover a super-resolution image (SRI) that enhances the spatial resolution of the HSI. Coupled tensor decomposition (CTD)-based methods have gained traction in this domain, offering recoverability guarantees under various assumptions. Existing models such as canonical polyadic decomposition (CPD) and Tucker decomposition provide strong expressive power but lack physical interpretability. The block-term decomposition model with rank-$(L_r, L_r, 1)$ terms (the LL1 model) yields interpretable factors under the linear mixture model (LMM) of spectral images, but LMM assumptions are often violated in practice -- primarily due to nonlinear effects such as endmember variability (EV). To address this, we propose modeling spectral images using a more flexible block-term tensor decomposition with rank-$(L_r, M_r, N_r)$ terms (the LMN model). This modeling choice retains interpretability, subsumes CPD, Tucker, and LL1 as special cases, and robustly accounts for non-ideal effects such as EV, offering a balanced tradeoff between expressiveness and interpretability for HSR. Importantly, under the LMN model for HSI and MSI, recoverability of the SRI can still be established under proper conditions -- providing strong theoretical support. Extensive experiments on synthetic and real datasets further validate the effectiveness and robustness of the proposed method compared with existing CTD-based approaches.","authors":["Meng Ding","Xiao Fu"],"pdf_url":"","comment":"The paper was accepted by SIAM Journal on Imaging Sciences"},{"id":"http://arxiv.org/abs/2512.19486v1","updated":"2025-12-22T15:43:23Z","published":"2025-12-22T15:43:23Z","title":"Dynamic Stream Network for Combinatorial Explosion Problem in Deformable Medical Image Registration","summary":"Combinatorial explosion problem caused by dual inputs presents a critical challenge in Deformable Medical Image Registration (DMIR). Since DMIR processes two images simultaneously as input, the combination relationships between features has grown exponentially, ultimately the model considers more interfering features during the feature modeling process. Introducing dynamics in the receptive fields and weights of the network enable the model to eliminate the interfering features combination and model the potential feature combination relationships. In this paper, we propose the Dynamic Stream Network (DySNet), which enables the receptive fields and weights to be dynamically adjusted. This ultimately enables the model to ignore interfering feature combinations and model the potential feature relationships. With two key innovations: 1) Adaptive Stream Basin (AdSB) module dynamically adjusts the shape of the receptive field, thereby enabling the model to focus on the feature relationships with greater correlation. 2) Dynamic Stream Attention (DySA) mechanism generates dynamic weights to search for more valuable feature relationships. Extensive experiments have shown that DySNet consistently outperforms the most advanced DMIR methods, highlighting its outstanding generalization ability. Our code will be released on the website: https://github.com/ShaochenBi/DySNet.","authors":["Shaochen Bi","Yuting He","Weiming Wang","Hao Chen"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2505.15925v3","updated":"2025-12-22T15:37:49Z","published":"2025-05-21T18:24:36Z","title":"VERDI: VLM-Embedded Reasoning for Autonomous Driving","summary":"While autonomous driving (AD) stacks struggle with decision making under partial observability and real-world complexity, human drivers are capable of commonsense reasoning to make near-optimal decisions with limited information. Recent work has attempted to leverage finetuned Vision-Language Models (VLMs) for trajectory planning at inference time to emulate human behavior. Despite their success in benchmark evaluations, these methods are often impractical to deploy (a 70B parameter VLM inference at merely 8 tokens per second requires more than 160G of memory), and their monolithic network structure prohibits safety decomposition. To bridge this gap, we propose VLM-Embedded Reasoning for autonomous Driving (VERDI), a training-time framework that distills the reasoning process and commonsense knowledge of VLMs into the AD stack. VERDI augments modular differentiable end-to-end (e2e) AD models by aligning intermediate module outputs at the perception, prediction, and planning stages with text features explaining the driving reasoning process produced by VLMs. By encouraging alignment in latent space, VERDI enables the modular AD stack to internalize structured reasoning, without incurring the inference-time costs of large VLMs. We validate VERDI in both open-loop (NuScenes and Bench2Drive benchmarks) and closed-loop (HugSim Simulator) settings. We find that VERDI outperforms existing e2e methods that do not embed reasoning by up to 11% in $\\ell_{2}$ distance and 11% in driving performance, while maintaining real-time inference speed.","authors":["Bowen Feng","Zhiting Mei","Baiang Li","Julian Ost","Filippo Ghilotti","Roger Girgis","Anirudha Majumdar","Felix Heide"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2511.10412v2","updated":"2025-12-22T15:36:44Z","published":"2025-11-13T15:33:48Z","title":"3DFETUS: Deep Learning-Based Standardization of Facial Planes in 3D Ultrasound","summary":"The automatic localization and standardization of anatomical planes in 3D medical imaging remains a challenging problem due to variability in object pose, appearance, and image quality. In 3D ultrasound, these challenges are exacerbated by speckle noise and limited contrast, particularly in fetal imaging.\n  To address these challenges in the context of facial assessment, we present: 1) GT++, a robust algorithm that estimates standard facial planes from 3D US volumes using annotated anatomical landmarks; and 2) 3DFETUS, a deep learning model that automates and standardizes their localization in 3D fetal US volumes.\n  We evaluated our methods both qualitatively, through expert clinical review, and quantitatively. The proposed approach achieved a mean translation error of 3.21 $\\pm$ 1.98mm and a mean rotation error of 5.31 $\\pm$ 3.945$^\\circ$ per plane, outperforming other state-of-the-art methods on 3D US volumes. Clinical assessments further confirmed the effectiveness of both GT++ and 3DFETUS, demonstrating statistically significant improvements in plane estimation accuracy.","authors":["Alomar Antonia","Rubio Ricardo","Albaiges Gerard","Salort-Benejam Laura","Caminal Julia","Prat Maria","Rueda Carolina","Cortes Berta","Piella Gemma","Sukno Federico"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19479v1","updated":"2025-12-22T15:32:18Z","published":"2025-12-22T15:32:18Z","title":"Emotion-Director: Bridging Affective Shortcut in Emotion-Oriented Image Generation","summary":"Image generation based on diffusion models has demonstrated impressive capability, motivating exploration into diverse and specialized applications. Owing to the importance of emotion in advertising, emotion-oriented image generation has attracted increasing attention. However, current emotion-oriented methods suffer from an affective shortcut, where emotions are approximated to semantics. As evidenced by two decades of research, emotion is not equivalent to semantics. To this end, we propose Emotion-Director, a cross-modal collaboration framework consisting of two modules. First, we propose a cross-Modal Collaborative diffusion model, abbreviated as MC-Diffusion. MC-Diffusion integrates visual prompts with textual prompts for guidance, enabling the generation of emotion-oriented images beyond semantics. Further, we improve the DPO optimization by a negative visual prompt, enhancing the model's sensitivity to different emotions under the same semantics. Second, we propose MC-Agent, a cross-Modal Collaborative Agent system that rewrites textual prompts to express the intended emotions. To avoid template-like rewrites, MC-Agent employs multi-agents to simulate human subjectivity toward emotions, and adopts a chain-of-concept workflow that improves the visual expressiveness of the rewritten prompts. Extensive qualitative and quantitative experiments demonstrate the superiority of Emotion-Director in emotion-oriented image generation.","authors":["Guoli Jia","Junyao Hu","Xinwei Long","Kai Tian","Kaiyan Zhang","KaiKai Zhao","Ning Ding","Bowen Zhou"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2510.16416v2","updated":"2025-12-22T15:14:59Z","published":"2025-10-18T09:22:40Z","title":"SSL4RL: Revisiting Self-supervised Learning as Intrinsic Reward for Visual-Language Reasoning","summary":"Vision-language models (VLMs) have shown remarkable abilities by integrating large language models with visual inputs. However, they often fail to utilize visual evidence adequately, either depending on linguistic priors in vision-centric tasks or resorting to textual shortcuts during reasoning. Although reinforcement learning (RL) can align models with desired behaviors, its application to VLMs has been hindered by the lack of scalable and reliable reward mechanisms. To overcome this challenge, we propose SSL4RL, a novel framework that leverages self-supervised learning (SSL) tasks as a source of verifiable rewards for RL-based fine-tuning. Our approach reformulates SSL objectives-such as predicting image rotation or reconstructing masked patches-into dense, automatic reward signals, eliminating the need for human preference data or unreliable AI evaluators. Experiments show that SSL4RL substantially improves performance on both vision-centric and vision-language reasoning benchmarks. Furthermore, through systematic ablations, we identify key factors-such as task difficulty, model scale, and semantic alignment with the target domain-that influence the effectiveness of SSL4RL tasks, offering new design principles for future work. We also demonstrate the framework's generality by applying it to graph learning, where it yields significant gains. SSL4RL establishes a versatile and effective paradigm for aligning multimodal models using verifiable, self-supervised objectives.","authors":["Xiaojun Guo","Runyu Zhou","Yifei Wang","Qi Zhang","Chenheng Zhang","Stefanie Jegelka","Xiaohan Wang","Jiajun Chai","Guojun Yin","Wei Lin","Yisen Wang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19451v1","updated":"2025-12-22T14:55:54Z","published":"2025-12-22T14:55:54Z","title":"Sign Language Recognition using Parallel Bidirectional Reservoir Computing","summary":"Sign language recognition (SLR) facilitates communication between deaf and hearing communities. Deep learning based SLR models are commonly used but require extensive computational resources, making them unsuitable for deployment on edge devices. To address these limitations, we propose a lightweight SLR system that combines parallel bidirectional reservoir computing (PBRC) with MediaPipe. MediaPipe enables real-time hand tracking and precise extraction of hand joint coordinates, which serve as input features for the PBRC architecture. The proposed PBRC architecture consists of two echo state network (ESN) based bidirectional reservoir computing (BRC) modules arranged in parallel to capture temporal dependencies, thereby creating a rich feature representation for classification. We trained our PBRC-based SLR system on the Word-Level American Sign Language (WLASL) video dataset, achieving top-1, top-5, and top-10 accuracies of 60.85%, 85.86%, and 91.74%, respectively. Training time was significantly reduced to 18.67 seconds due to the intrinsic properties of reservoir computing, compared to over 55 minutes for deep learning based methods such as Bi-GRU. This approach offers a lightweight, cost-effective solution for real-time SLR on edge devices.","authors":["Nitin Kumar Singh","Arie Rachmad Syulistyo","Yuichiro Tanaka","Hakaru Tamukoh"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2511.14396v4","updated":"2025-12-22T14:45:53Z","published":"2025-11-18T12:01:06Z","title":"Continuous Vision-Language-Action Co-Learning with Semantic-Physical Alignment for Behavioral Cloning","summary":"Language-conditioned manipulation facilitates human-robot interaction via behavioral cloning (BC), which learns control policies from human demonstrations and serves as a cornerstone of embodied AI. Overcoming compounding errors in sequential action decisions remains a central challenge to improving BC performance. Existing approaches mitigate compounding errors through data augmentation, expressive representation, or temporal abstraction. However, they suffer from physical discontinuities and semantic-physical misalignment, leading to inaccurate action cloning and intermittent execution. In this paper, we present Continuous vision-language-action Co-Learning with Semantic-Physical Alignment (CCoL), a novel BC framework that ensures temporally consistent execution and fine-grained semantic grounding. It generates robust and smooth action execution trajectories through continuous co-learning across vision, language, and proprioceptive inputs (e.g., robot internal states). Meanwhile, we anchor language semantics to visuomotor representations by a bidirectional cross-attention to learn contextual information for action generation, successfully overcoming the problem of semantic-physical misalignment. Extensive experiments show that CCoL achieves an average 8.0% relative improvement across three simulation suites, with up to 19.2% relative gain in human-demonstrated bimanual insertion tasks. Real-world tests on a 7-DoF robot further confirm CCoL's generalization under unseen and noisy object states.","authors":["Xiuxiu Qi","Yu Yang","Jiannong Cao","Luyao Bai","Chongshan Fan","Chengtai Cao","Hongpeng Wang"],"pdf_url":"","comment":"Accepted at AAAI 2026, the Project website is available at https://qhemu.github.io/CCoL/"},{"id":"http://arxiv.org/abs/2512.19443v1","updated":"2025-12-22T14:42:31Z","published":"2025-12-22T14:42:31Z","title":"D2Pruner: Debiased Importance and Structural Diversity for MLLM Token Pruning","summary":"Processing long visual token sequences poses a significant computational burden on Multimodal Large Language Models (MLLMs). While token pruning offers a path to acceleration, we find that current methods, while adequate for general understanding, catastrophically fail on fine-grained localization tasks. We attribute this failure to the inherent flaws of the two prevailing strategies: importance-based methods suffer from a strong positional bias, an inherent model artifact that distracts from semantic content, while diversity-based methods exhibit structural blindness, disregarding the user's prompt and spatial redundancy. To address this, we introduce D2Pruner, a framework that rectifies these issues by uniquely combining debiased importance with a structural pruning mechanism. Our method first secures a core set of the most critical tokens as pivots based on a debiased attention score. It then performs a Maximal Independent Set (MIS) selection on the remaining tokens, which are modeled on a hybrid graph where edges signify spatial proximity and semantic similarity. This process iteratively preserves the most important and available token while removing its neighbors, ensuring that the supplementary tokens are chosen to maximize importance and diversity. Extensive experiments demonstrate that D2Pruner has exceptional efficiency and fidelity. Applied to LLaVA-1.5-7B for general understanding tasks, it reduces FLOPs by 74.2\\% while retaining 99.2\\% of its original performance. Furthermore, in challenging localization benchmarks with InternVL-2.5-8B, it maintains 85.7\\% performance at a 90\\% token reduction rate, marking a significant advancement with up to 63. 53\\% improvement over existing methods.","authors":["Evelyn Zhang","Fufu Yu","Aoqi Wu","Zichen Wen","Ke Yan","Shouhong Ding","Biqing Qi","Linfeng Zhang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19438v1","updated":"2025-12-22T14:36:08Z","published":"2025-12-22T14:36:08Z","title":"MT-Mark: Rethinking Image Watermarking via Mutual-Teacher Collaboration with Adaptive Feature Modulation","summary":"Existing deep image watermarking methods follow a fixed embedding-distortion-extraction pipeline, where the embedder and extractor are weakly coupled through a final loss and optimized in isolation. This design lacks explicit collaboration, leaving no structured mechanism for the embedder to incorporate decoding-aware cues or for the extractor to guide embedding during training. To address this architectural limitation, we rethink deep image watermarking by reformulating embedding and extraction as explicitly collaborative components. To realize this reformulation, we introduce a Collaborative Interaction Mechanism (CIM) that establishes direct, bidirectional communication between the embedder and extractor, enabling a mutual-teacher training paradigm and coordinated optimization. Built upon this explicitly collaborative architecture, we further propose an Adaptive Feature Modulation Module (AFMM) to support effective interaction. AFMM enables content-aware feature regulation by decoupling modulation structure and strength, guiding watermark embedding toward stable image features while suppressing host interference during extraction. Under CIM, the AFMMs on both sides form a closed-loop collaboration that aligns embedding behavior with extraction objectives. This architecture-level redesign changes how robustness is learned in watermarking systems. Rather than relying on exhaustive distortion simulation, robustness emerges from coordinated representation learning between embedding and extraction. Experiments on real-world and AI-generated datasets demonstrate that the proposed method consistently outperforms state-of-the-art approaches in watermark extraction accuracy while maintaining high perceptual quality, showing strong robustness and generalization.","authors":["Fei Ge","Ying Huang","Jie Liu","Guixuan Zhang","Zhi Zeng","Shuwu Zhang","Hu Guan"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19433v1","updated":"2025-12-22T14:31:58Z","published":"2025-12-22T14:31:58Z","title":"dMLLM-TTS: Self-Verified and Efficient Test-Time Scaling for Diffusion Multi-Modal Large Language Models","summary":"Diffusion Multi-modal Large Language Models (dMLLMs) have recently emerged as a novel architecture unifying image generation and understanding. However, developing effective and efficient Test-Time Scaling (TTS) methods to unlock their full generative potential remains an underexplored challenge. To address this, we propose dMLLM-TTS, a novel framework operating on two complementary scaling axes: (1) trajectory exploration scaling to enhance the diversity of generated hypotheses, and (2) iterative refinement scaling for stable generation. Conventional TTS approaches typically perform linear search across these two dimensions, incurring substantial computational costs of O(NT) and requiring an external verifier for best-of-N selection. To overcome these limitations, we propose two innovations. First, we design an efficient hierarchical search algorithm with O(N+T) complexity that adaptively expands and prunes sampling trajectories. Second, we introduce a self-verified feedback mechanism that leverages the dMLLMs' intrinsic image understanding capabilities to assess text-image alignment, eliminating the need for external verifier. Extensive experiments on the GenEval benchmark across three representative dMLLMs (e.g., Lumina-DiMOO, MMaDA, Muddit) show that our framework substantially improves generation quality while achieving up to 6x greater efficiency than linear search. Project page: https://github.com/Alpha-VLLM/Lumina-DiMOO.","authors":["Yi Xin","Siqi Luo","Qi Qin","Haoxing Chen","Kaiwen Zhu","Zhiwei Zhang","Yangfan He","Rongchao Zhang","Jinbin Bai","Shuo Cao","Bin Fu","Junjun He","Yihao Liu","Yuewen Cao","Xiaohong Liu"],"pdf_url":"","comment":"Project page: https://github.com/Alpha-VLLM/Lumina-DiMOO"},{"id":"http://arxiv.org/abs/2511.07329v2","updated":"2025-12-22T14:21:27Z","published":"2025-11-10T17:31:39Z","title":"Preparation of Fractal-Inspired Computational Architectures for Advanced Large Language Model Analysis","summary":"It introduces FractalNet, a fractal-inspired computational architectures for advanced large language model analysis that mainly challenges model diversity on a large scale in an efficient manner. The new set-up involves a template-driven generator, runner, and evaluation framework that, through systematic permutations of convolutional, normalization, activation, and dropout layers, can create more than 1,200 variants of neural networks. Fractal templates allow for structural recursion and multi-column pathways, thus, models become deeper and wider in a balanced way. Training utilizes PyTorch, Automatic Mixed Precision (AMP), and gradient checkpointing and is carried out on the CIFAR-10 dataset for five epochs. The outcomes show that fractal-based architectures are capable of strong performance and are computationally efficient. The paper positions fractal design as a feasible and resource-efficient method of automated architecture exploration.","authors":["Yash Mittal","Dmitry Ignatov","Radu Timofte"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2511.20431v3","updated":"2025-12-22T14:19:18Z","published":"2025-11-25T16:03:38Z","title":"BRIC: Bridging Kinematic Plans and Physical Control at Test Time","summary":"We propose BRIC, a novel test-time adaptation (TTA) framework that enables long-term human motion generation by resolving execution discrepancies between diffusion-based kinematic motion planners and reinforcement learning-based physics controllers. While diffusion models can generate diverse and expressive motions conditioned on text and scene context, they often produce physically implausible outputs, leading to execution drift during simulation. To address this, BRIC dynamically adapts the physics controller to noisy motion plans at test time, while preserving pre-trained skills via a loss function that mitigates catastrophic forgetting. In addition, BRIC introduces a lightweight test-time guidance mechanism that steers the diffusion model in the signal space without updating its parameters. By combining both adaptation strategies, BRIC ensures consistent and physically plausible long-term executions across diverse environments in an effective and efficient manner. We validate the effectiveness of BRIC on a variety of long-term tasks, including motion composition, obstacle avoidance, and human-scene interaction, achieving state-of-the-art performance across all tasks.","authors":["Dohun Lim","Minji Kim","Jaewoon Lim","Sungchan Kim"],"pdf_url":"","comment":"Accepted to AAAI'26"},{"id":"http://arxiv.org/abs/2512.19415v1","updated":"2025-12-22T14:17:35Z","published":"2025-12-22T14:17:35Z","title":"Non-Contrast CT Esophageal Varices Grading through Clinical Prior-Enhanced Multi-Organ Analysis","summary":"Esophageal varices (EV) represent a critical complication of portal hypertension, affecting approximately 60% of cirrhosis patients with a significant bleeding risk of ~30%. While traditionally diagnosed through invasive endoscopy, non-contrast computed tomography (NCCT) presents a potential non-invasive alternative that has yet to be fully utilized in clinical practice. We present Multi-Organ-COhesion Network++ (MOON++), a novel multimodal framework that enhances EV assessment through comprehensive analysis of NCCT scans. Inspired by clinical evidence correlating organ volumetric relationships with liver disease severity, MOON++ synthesizes imaging characteristics of the esophagus, liver, and spleen through multimodal learning. We evaluated our approach using 1,631 patients, those with endoscopically confirmed EV were classified into four severity grades. Validation in 239 patient cases and independent testing in 289 cases demonstrate superior performance compared to conventional single organ methods, achieving an AUC of 0.894 versus 0.803 for the severe grade EV classification (G3 versus <G3) and 0.921 versus 0.793 for the differentiation of moderate to severe grades (>=G2 versus <G2). We conducted a reader study involving experienced radiologists to further validate the performance of MOON++. To our knowledge, MOON++ represents the first comprehensive multi-organ NCCT analysis framework incorporating clinical knowledge priors for EV assessment, potentially offering a promising non-invasive diagnostic alternative.","authors":["Xiaoming Zhang","Chunli Li","Jiacheng Hao","Yuan Gao","Danyang Tu","Jianyi Qiao","Xiaoli Yin","Le Lu","Ling Zhang","Ke Yan","Yang Hou","Yu Shi"],"pdf_url":"","comment":"Medical Image Analysis"},{"id":"http://arxiv.org/abs/2512.07651v2","updated":"2025-12-22T14:15:04Z","published":"2025-12-08T15:44:24Z","title":"Liver Fibrosis Quantification and Analysis: The LiQA Dataset and Baseline Method","summary":"Liver fibrosis represents a significant global health burden, necessitating accurate staging for effective clinical management. This report introduces the LiQA (Liver Fibrosis Quantification and Analysis) dataset, established as part of the CARE 2024 challenge. Comprising $440$ patients with multi-phase, multi-center MRI scans, the dataset is curated to benchmark algorithms for Liver Segmentation (LiSeg) and Liver Fibrosis Staging (LiFS) under complex real-world conditions, including domain shifts, missing modalities, and spatial misalignment. We further describe the challenge's top-performing methodology, which integrates a semi-supervised learning framework with external data for robust segmentation, and utilizes a multi-view consensus approach with Class Activation Map (CAM)-based regularization for staging. Evaluation of this baseline demonstrates that leveraging multi-source data and anatomical constraints significantly enhances model robustness in clinical settings.","authors":["Yuanye Liu","Hanxiao Zhang","Jiyao Liu","Nannan Shi","Yuxin Shi","Arif Mahmood","Murtaza Taj","Xiahai Zhuang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2407.12538v3","updated":"2025-12-22T14:04:56Z","published":"2024-07-17T13:21:31Z","title":"High Frequency Matters: Uncertainty Guided Image Compression with Wavelet Diffusion","summary":"Diffusion probabilistic models have recently achieved remarkable success in generating high-quality images. However, balancing high perceptual quality and low distortion remains challenging in application of diffusion models in image compression. To address this issue, we propose a novel Uncertainty-Guided image compression approach with wavelet Diffusion (UGDiff). Our approach focuses on high frequency compression via the wavelet transform, since high frequency components are crucial for reconstructing image details. We introduce a wavelet conditional diffusion model for high frequency prediction, followed by a residual codec that compresses and transmits prediction residuals to the decoder. This diffusion prediction-then-residual compression paradigm effectively addresses the low fidelity issue common in direct reconstructions by existing diffusion models. Considering the uncertainty from the random sampling of the diffusion model, we further design an uncertainty-weighted rate-distortion (R-D) loss tailored for residual compression, providing a more rational trade-off between rate and distortion. Comprehensive experiments on two benchmark datasets validate the effectiveness of UGDiff, surpassing state-of-the-art image compression methods in R-D performance, perceptual quality, subjective quality, and inference time. Our code is available at: https://github.com/hejiaxiang1/Wavelet-Diffusion/tree/main.","authors":["Juan Song","Jiaxiang He","Lijie Yang","Mingtao Feng","Keyan Wang"],"pdf_url":"","comment":"Revised version for IEEE TMM submission"},{"id":"http://arxiv.org/abs/2511.13031v3","updated":"2025-12-22T14:03:37Z","published":"2025-11-17T06:28:26Z","title":"Towards 3D Object-Centric Feature Learning for Semantic Scene Completion","summary":"Vision-based 3D Semantic Scene Completion (SSC) has received growing attention due to its potential in autonomous driving. While most existing approaches follow an ego-centric paradigm by aggregating and diffusing features over the entire scene, they often overlook fine-grained object-level details, leading to semantic and geometric ambiguities, especially in complex environments. To address this limitation, we propose Ocean, an object-centric prediction framework that decomposes the scene into individual object instances to enable more accurate semantic occupancy prediction. Specifically, we first employ a lightweight segmentation model, MobileSAM, to extract instance masks from the input image. Then, we introduce a 3D Semantic Group Attention module that leverages linear attention to aggregate object-centric features in 3D space. To handle segmentation errors and missing instances, we further design a Global Similarity-Guided Attention module that leverages segmentation features for global interaction. Finally, we propose an Instance-aware Local Diffusion module that improves instance features through a generative process and subsequently refines the scene representation in the BEV space. Extensive experiments on the SemanticKITTI and SSCBench-KITTI360 benchmarks demonstrate that Ocean achieves state-of-the-art performance, with mIoU scores of 17.40 and 20.28, respectively.","authors":["Weihua Wang","Yubo Cui","Xiangru Lin","Zhiheng Li","Zheng Fang"],"pdf_url":"","comment":"Accepted to AAAI-2026"},{"id":"http://arxiv.org/abs/2512.19402v1","updated":"2025-12-22T13:53:25Z","published":"2025-12-22T13:53:25Z","title":"Real2Edit2Real: Generating Robotic Demonstrations via a 3D Control Interface","summary":"Recent progress in robot learning has been driven by large-scale datasets and powerful visuomotor policy architectures, yet policy robustness remains limited by the substantial cost of collecting diverse demonstrations, particularly for spatial generalization in manipulation tasks. To reduce repetitive data collection, we present Real2Edit2Real, a framework that generates new demonstrations by bridging 3D editability with 2D visual data through a 3D control interface. Our approach first reconstructs scene geometry from multi-view RGB observations with a metric-scale 3D reconstruction model. Based on the reconstructed geometry, we perform depth-reliable 3D editing on point clouds to generate new manipulation trajectories while geometrically correcting the robot poses to recover physically consistent depth, which serves as a reliable condition for synthesizing new demonstrations. Finally, we propose a multi-conditional video generation model guided by depth as the primary control signal, together with action, edge, and ray maps, to synthesize spatially augmented multi-view manipulation videos. Experiments on four real-world manipulation tasks demonstrate that policies trained on data generated from only 1-5 source demonstrations can match or outperform those trained on 50 real-world demonstrations, improving data efficiency by up to 10-50x. Moreover, experimental results on height and texture editing demonstrate the framework's flexibility and extensibility, indicating its potential to serve as a unified data generation framework.","authors":["Yujie Zhao","Hongwei Fan","Di Chen","Shengcong Chen","Liliang Chen","Xiaoqi Li","Guanghui Ren","Hao Dong"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19390v1","updated":"2025-12-22T13:38:11Z","published":"2025-12-22T13:38:11Z","title":"TwinAligner: Visual-Dynamic Alignment Empowers Physics-aware Real2Sim2Real for Robotic Manipulation","summary":"The robotics field is evolving towards data-driven, end-to-end learning, inspired by multimodal large models. However, reliance on expensive real-world data limits progress. Simulators offer cost-effective alternatives, but the gap between simulation and reality challenges effective policy transfer. This paper introduces TwinAligner, a novel Real2Sim2Real system that addresses both visual and dynamic gaps. The visual alignment module achieves pixel-level alignment through SDF reconstruction and editable 3DGS rendering, while the dynamic alignment module ensures dynamic consistency by identifying rigid physics from robot-object interaction. TwinAligner improves robot learning by providing scalable data collection and establishing a trustworthy iterative cycle, accelerating algorithm development. Quantitative evaluations highlight TwinAligner's strong capabilities in visual and dynamic real-to-sim alignment. This system enables policies trained in simulation to achieve strong zero-shot generalization to the real world. The high consistency between real-world and simulated policy performance underscores TwinAligner's potential to advance scalable robot learning. Code and data will be released on https://twin-aligner.github.io","authors":["Hongwei Fan","Hang Dai","Jiyao Zhang","Jinzhou Li","Qiyang Yan","Yujie Zhao","Mingju Gao","Jinghang Wu","Hao Tang","Hao Dong"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19387v1","updated":"2025-12-22T13:36:26Z","published":"2025-12-22T13:36:26Z","title":"DSTED: Decoupling Temporal Stabilization and Discriminative Enhancement for Surgical Workflow Recognition","summary":"Purpose: Surgical workflow recognition enables context-aware assistance and skill assessment in computer-assisted interventions. Despite recent advances, current methods suffer from two critical challenges: prediction jitter across consecutive frames and poor discrimination of ambiguous phases. This paper aims to develop a stable framework by selectively propagating reliable historical information and explicitly modeling uncertainty for hard sample enhancement.\n  Methods: We propose a dual-pathway framework DSTED with Reliable Memory Propagation (RMP) and Uncertainty-Aware Prototype Retrieval (UPR). RMP maintains temporal coherence by filtering and fusing high-confidence historical features through multi-criteria reliability assessment. UPR constructs learnable class-specific prototypes from high-uncertainty samples and performs adaptive prototype matching to refine ambiguous frame representations. Finally, a confidence-driven gate dynamically balances both pathways based on prediction certainty.\n  Results: Our method achieves state-of-the-art performance on AutoLaparo-hysterectomy with 84.36% accuracy and 65.51% F1-score, surpassing the second-best method by 3.51% and 4.88% respectively. Ablations reveal complementary gains from RMP (2.19%) and UPR (1.93%), with synergistic effects when combined. Extensive analysis confirms substantial reduction in temporal jitter and marked improvement on challenging phase transitions.\n  Conclusion: Our dual-pathway design introduces a novel paradigm for stable workflow recognition, demonstrating that decoupling the modeling of temporal consistency and phase ambiguity yields superior performance and clinical applicability.","authors":["Yueyao Chen","Kai-Ni Wang","Dario Tayupo","Arnaud Huaulm'e","Krystel Nyangoh Timoh","Pierre Jannin","Qi Dou"],"pdf_url":"","comment":"Early accepted to IPCAI 2026"},{"id":"http://arxiv.org/abs/2406.05491v4","updated":"2025-12-22T13:30:54Z","published":"2024-06-08T15:01:54Z","title":"One Perturbation is Enough: On Generating Universal Adversarial Perturbations against Vision-Language Pre-training Models","summary":"Vision-Language Pre-training (VLP) models have exhibited unprecedented capability in many applications by taking full advantage of the multimodal alignment. However, previous studies have shown they are vulnerable to maliciously crafted adversarial samples. Despite recent success, these methods are generally instance-specific and require generating perturbations for each input sample. In this paper, we reveal that VLP models are also vulnerable to the instance-agnostic universal adversarial perturbation (UAP). Specifically, we design a novel Contrastive-training Perturbation Generator with Cross-modal conditions (C-PGC) to achieve the attack. In light that the pivotal multimodal alignment is achieved through the advanced contrastive learning technique, we devise to turn this powerful weapon against themselves, i.e., employ a malicious version of contrastive learning to train the C-PGC based on our carefully crafted positive and negative image-text pairs for essentially destroying the alignment relationship learned by VLP models. Besides, C-PGC fully utilizes the characteristics of Vision-and-Language (V+L) scenarios by incorporating both unimodal and cross-modal information as effective guidance. Extensive experiments show that C-PGC successfully forces adversarial samples to move away from their original area in the VLP model's feature space, thus essentially enhancing attacks across various victim models and V+L tasks. The GitHub repository is available at https://github.com/ffhibnese/CPGC_VLP_Universal_Attacks.","authors":["Hao Fang","Jiawei Kong","Wenbo Yu","Bin Chen","Jiawei Li","Hao Wu","Shutao Xia","Ke Xu"],"pdf_url":"","comment":"Accepted by ICCV-2025"},{"id":"http://arxiv.org/abs/2512.17436v2","updated":"2025-12-22T13:27:24Z","published":"2025-12-19T10:43:37Z","title":"Xiaomi MiMo-VL-Miloco Technical Report","summary":"We open-source MiMo-VL-Miloco-7B and its quantized variant MiMo-VL-Miloco-7B-GGUF, a pair of home-centric vision-language models that achieve strong performance on both home-scenario understanding and general multimodal reasoning. Built on the MiMo-VL-7B backbone, MiMo-VL-Miloco-7B is specialized for smart-home environments, attaining leading F1 scores on gesture recognition and common home-scenario understanding, while also delivering consistent gains across video benchmarks such as Video-MME, Video-MMMU, and Charades-STA, as well as language understanding benchmarks including MMMU-Pro and MMLU-Pro. In our experiments, MiMo-VL-Miloco-7B outperforms strong closed-source and open-source baselines on home-scenario understanding and several multimodal reasoning benchmarks. To balance specialization and generality, we design a two-stage training pipeline that combines supervised fine-tuning with reinforcement learning based on Group Relative Policy Optimization, leveraging efficient multi-domain data. We further incorporate chain-of-thought supervision and token-budget-aware reasoning, enabling the model to learn knowledge in a data-efficient manner while also performing reasoning efficiently. Our analysis shows that targeted home-scenario training not only enhances activity and gesture understanding, but also improves text-only reasoning with only modest trade-offs on document-centric tasks. Model checkpoints, quantized GGUF weights, and our home-scenario evaluation toolkit are publicly available at https://github.com/XiaoMi/xiaomi-mimo-vl-miloco to support research and deployment in real-world smart-home applications.","authors":["Jiaze Li","Jingyang Chen","Yuxun Qu","Shijie Xu","Zhenru Lin","Junyou Zhu","Boshen Xu","Wenhui Tan","Pei Fu","Jianzhong Ju","Zhenbo Luo","Jian Luan"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2508.17061v2","updated":"2025-12-22T13:11:30Z","published":"2025-08-23T15:28:05Z","title":"REGEN: Real-Time Photorealism Enhancement in Games via a Dual-Stage Generative Network Framework","summary":"Photorealism is an important aspect of modern video games since it can shape player experience and impact immersion, narrative engagement, and visual fidelity. To achieve photorealism, beyond traditional rendering pipelines, generative models have been increasingly adopted as an effective approach for bridging the gap between the visual realism of synthetic and real worlds. However, under real-time constraints of video games, existing generative approaches continue to face a tradeoff between visual quality and runtime efficiency. In this work, we present a framework for enhancing the photorealism of rendered game frames using generative networks. We propose REGEN, which first employs a robust unpaired image-to-image translation model to generate semantically consistent photorealistic frames. These generated frames are then used to create a paired dataset, which transforms the problem to a simpler unpaired image-to-image translation. This enables training with a lightweight method, achieving real-time inference without compromising visual quality. We evaluate REGEN on Unreal Engine, showing, by employing the CMMD metric, that it achieves comparable or slightly improved visual quality compared to the robust method, while improving the frame rate by 12x. Additional experiments also validate that REGEN adheres to the semantic preservation of the initial robust image-to-image translation method and maintains temporal consistency. Code, pre-trained models, and demos for this work are available at: https://github.com/stefanos50/REGEN","authors":["Stefanos Pasios","Nikos Nikolaidis"],"pdf_url":"","comment":"8 pages"},{"id":"http://arxiv.org/abs/2506.15849v2","updated":"2025-12-22T13:08:59Z","published":"2025-06-18T19:59:50Z","title":"PRISM-Loc: a Lightweight Long-range LiDAR Localization in Urban Environments with Topological Maps","summary":"We propose PRISM-Loc - a lightweight and robust approach for localization in large outdoor environments that combines a compact topological representation with a novel scan-matching and curb-detection module operating on raw LiDAR scans. The method is designed for resource-constrained platforms and emphasizes real-time performance and resilience to common urban sensing challenges. It provides accurate localization in compact topological maps using global place recognition and an original scan matching technique. Experiments on standard benchmarks and on an embedded platform demonstrate the effectiveness of our approach. Our method achieves a 99\\% success rate on the large-scale ITLP-Campus dataset while running at 150 ms per localization and using a 20 MB map for localization. We highlight three main contributions: (1) a compact representation for city-scale localization; (2) a novel curb detection and scan matching pipeline operating directly on raw LiDAR points; (3) a thorough evaluation of our method with performance analysis.","authors":["Kirill Muravyev","Artem Kobozev","Vasily Yuryev","Alexander Melekhin","Oleg Bulichev","Dmitry Yudin","Konstantin Yakovlev"],"pdf_url":"","comment":"This version was submitted to ICRA 2026 conference"},{"id":"http://arxiv.org/abs/2512.19365v1","updated":"2025-12-22T13:07:04Z","published":"2025-12-22T13:07:04Z","title":"Efficient Spike-driven Transformer for High-performance Drone-View Geo-Localization","summary":"Traditional drone-view geo-localization (DVGL) methods based on artificial neural networks (ANNs) have achieved remarkable performance. However, ANNs rely on dense computation, which results in high power consumption. In contrast, spiking neural networks (SNNs), which benefit from spike-driven computation, inherently provide low power consumption. Regrettably, the potential of SNNs for DVGL has yet to be thoroughly investigated. Meanwhile, the inherent sparsity of spike-driven computation for representation learning scenarios also results in loss of critical information and difficulties in learning long-range dependencies when aligning heterogeneous visual data sources. To address these, we propose SpikeViMFormer, the first SNN framework designed for DVGL. In this framework, a lightweight spike-driven transformer backbone is adopted to extract coarse-grained features. To mitigate the loss of critical information, the spike-driven selective attention (SSA) block is designed, which uses a spike-driven gating mechanism to achieve selective feature enhancement and highlight discriminative regions. Furthermore, a spike-driven hybrid state space (SHS) block is introduced to learn long-range dependencies using a hybrid state space. Moreover, only the backbone is utilized during the inference stage to reduce computational cost. To ensure backbone effectiveness, a novel hierarchical re-ranking alignment learning (HRAL) strategy is proposed. It refines features via neighborhood re-ranking and maintains cross-batch consistency to directly optimize the backbone. Experimental results demonstrate that SpikeViMFormer outperforms state-of-the-art SNNs. Compared with advanced ANNs, it also achieves competitive performance.Our code is available at https://github.com/ISChenawei/SpikeViMFormer","authors":["Zhongwei Chen","Hai-Jun Rong","Zhao-Xu Yang","Guoqi Li"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19354v1","updated":"2025-12-22T12:54:26Z","published":"2025-12-22T12:54:26Z","title":"ReasonCD: A Multimodal Reasoning Large Model for Implicit Change-of-Interest Semantic Mining","summary":"Remote sensing image change detection is one of the fundamental tasks in remote sensing intelligent interpretation. Its core objective is to identify changes within change regions of interest (CRoI). Current multimodal large models encode rich human semantic knowledge, which is utilized for guidance in tasks such as remote sensing change detection. However, existing methods that use semantic guidance for detecting users' CRoI overly rely on explicit textual descriptions of CRoI, leading to the problem of near-complete performance failure when presented with implicit CRoI textual descriptions. This paper proposes a multimodal reasoning change detection model named ReasonCD, capable of mining users' implicit task intent. The model leverages the powerful reasoning capabilities of pre-trained large language models to mine users' implicit task intents and subsequently obtains different change detection results based on these intents. Experiments on public datasets demonstrate that the model achieves excellent change detection performance, with an F1 score of 92.1\\% on the BCDD dataset. Furthermore, to validate its superior reasoning functionality, this paper annotates a subset of reasoning data based on the SECOND dataset. Experimental results show that the model not only excels at basic reasoning-based change detection tasks but can also explain the reasoning process to aid human decision-making.","authors":["Zhenyang Huang","Xiao Yu","Yi Zhang","Decheng Wang","Hang Ruan"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2412.14579v2","updated":"2025-12-22T12:47:43Z","published":"2024-12-19T06:57:37Z","title":"GSRender: Deduplicated Occupancy Prediction via Weakly Supervised 3D Gaussian Splatting","summary":"Weakly-supervised 3D occupancy perception is crucial for vision-based autonomous driving in outdoor environments. Previous methods based on NeRF often face a challenge in balancing the number of samples used. Too many samples can decrease efficiency, while too few can compromise accuracy, leading to variations in the mean Intersection over Union (mIoU) by 5-10 points. Furthermore, even with surrounding-view image inputs, only a single image is rendered from each viewpoint at any given moment. This limitation leads to duplicated predictions, which significantly impacts the practicality of the approach. However, this issue has largely been overlooked in existing research. To address this, we propose GSRender, which uses 3D Gaussian Splatting for weakly-supervised occupancy estimation, simplifying the sampling process. Additionally, we introduce the Ray Compensation module, which reduces duplicated predictions by compensating for features from adjacent frames. Finally, we redesign the dynamic loss to remove the influence of dynamic objects from adjacent frames. Extensive experiments show that our approach achieves SOTA results in RayIoU (+6.0), while also narrowing the gap with 3D- supervised methods. This work lays a solid foundation for weakly-supervised occupancy perception. The code is available at https://github.com/Jasper-sudo-Sun/GSRender.","authors":["Qianpu Sun","Changyong Shu","Sifan Zhou","Runxi Cheng","Yongxian Wei","Zichen Yu","Dawei Yang","Sirui Han","Yuan Chun"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.02789v2","updated":"2025-12-22T12:38:09Z","published":"2025-12-02T14:04:30Z","title":"TrackNetV5: Residual-Driven Spatio-Temporal Refinement and Motion Direction Decoupling for Fast Object Tracking","summary":"The TrackNet series has established a strong baseline for fast-moving small object tracking in sports. However, existing iterations face significant limitations: V1-V3 struggle with occlusions due to a reliance on purely visual cues, while TrackNetV4, despite introducing motion inputs, suffers from directional ambiguity as its absolute difference method discards motion polarity. To overcome these bottlenecks, we propose TrackNetV5, a robust architecture integrating two novel mechanisms. First, to recover lost directional priors, we introduce the Motion Direction Decoupling (MDD) module. Unlike V4, MDD decomposes temporal dynamics into signed polarity fields, explicitly encoding both movement occurrence and trajectory direction. Second, we propose the Residual-Driven Spatio-Temporal Refinement (R-STR) head. Operating on a coarse-to-fine paradigm, this Transformer-based module leverages factorized spatio-temporal contexts to estimate a corrective residual, effectively recovering occluded targets. Extensive experiments on the TrackNetV2 dataset demonstrate that TrackNetV5 achieves a new state-of-the-art F1-score of 0.9859 and an accuracy of 0.9733, significantly outperforming previous versions. Notably, this performance leap is achieved with a marginal 3.7% increase in FLOPs compared to V4, maintaining real-time inference capabilities while delivering superior tracking precision.","authors":["Tang Haonan","Chen Yanjun","Jiang Lezhi"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.16504v2","updated":"2025-12-22T12:36:43Z","published":"2025-12-18T13:15:52Z","title":"Skeleton-Snippet Contrastive Learning with Multiscale Feature Fusion for Action Localization","summary":"The self-supervised pretraining paradigm has achieved great success in learning 3D action representations for skeleton-based action recognition using contrastive learning. However, learning effective representations for skeleton-based temporal action localization remains challenging and underexplored. Unlike video-level {action} recognition, detecting action boundaries requires temporally sensitive features that capture subtle differences between adjacent frames where labels change. To this end, we formulate a snippet discrimination pretext task for self-supervised pretraining, which densely projects skeleton sequences into non-overlapping segments and promotes features that distinguish them across videos via contrastive learning. Additionally, we build on strong backbones of skeleton-based action recognition models by fusing intermediate features with a U-shaped module to enhance feature resolution for frame-level localization. Our approach consistently improves existing skeleton-based contrastive learning methods for action localization on BABEL across diverse subsets and evaluation protocols. We also achieve state-of-the-art transfer learning performance on PKUMMD with pretraining on NTU RGB+D and BABEL.","authors":["Qiushuo Cheng","Jingjing Liu","Catherine Morgan","Alan Whone","Majid Mirmehdi"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2509.26070v2","updated":"2025-12-22T12:34:40Z","published":"2025-09-30T10:45:10Z","title":"Geometric Learning of Canonical Parameterizations of $2D$-curves","summary":"Most datasets encountered in computer vision and medical applications present symmetries that should be taken into account in classification tasks. A typical example is the symmetry by rotation and/or scaling in object detection. A common way to build neural networks that learn the symmetries is to use data augmentation. In order to avoid data augmentation and build more sustainable algorithms, we present an alternative method to mod out symmetries based on the notion of section of a principal fiber bundle. This framework allows the use of simple metrics on the space of objects in order to measure dissimilarities between orbits of objects under the symmetry group. Moreover, the section used can be optimized to maximize separation of classes. We illustrate this methodology on a dataset of contours of objects for the groups of translations, rotations, scalings and reparameterizations. In particular, we present a $2$-parameter family of canonical parameterizations of curves, containing the constant-speed parameterization as a special case, which we believe is interesting in its own right. We hope that this simple application will serve to convey the geometric concepts underlying this method, which have a wide range of possible applications. The code is available at the following link: $\\href{https://github.com/GiLonga/Geometric-Learning}{https://github.com/GiLonga/Geometric-Learning}$. A tutorial notebook showcasing an application of the code to a specific dataset is available at the following link: $\\href{https://github.com/ioanaciuclea/geometric-learning-notebook}{https://github.com/ioanaciuclea/geometric-learning-notebook}$","authors":["Ioana Ciuclea","Giorgio Longari","Alice Barbara Tumpach"],"pdf_url":"","comment":"33 pages, 20 figures"},{"id":"http://arxiv.org/abs/2512.19336v1","updated":"2025-12-22T12:32:16Z","published":"2025-12-22T12:32:16Z","title":"GANeXt: A Fully ConvNeXt-Enhanced Generative Adversarial Network for MRI- and CBCT-to-CT Synthesis","summary":"The synthesis of computed tomography (CT) from magnetic resonance imaging (MRI) and cone-beam CT (CBCT) plays a critical role in clinical treatment planning by enabling accurate anatomical representation in adaptive radiotherapy. In this work, we propose GANeXt, a 3D patch-based, fully ConvNeXt-powered generative adversarial network for unified CT synthesis across different modalities and anatomical regions. Specifically, GANeXt employs an efficient U-shaped generator constructed from stacked 3D ConvNeXt blocks with compact convolution kernels, while the discriminator adopts a conditional PatchGAN. To improve synthesis quality, we incorporate a combination of loss functions, including mean absolute error (MAE), perceptual loss, segmentation-based masked MAE, and adversarial loss and a combination of Dice loss and cross-entropy for multi-head segmentation discriminator. For both tasks, training is performed with a batch size of 8 using two separate AdamW optimizers for the generator and discriminator, each equipped with a warmup and cosine decay scheduler, with learning rates of $5\\times10^{-4}$ and $1\\times10^{-3}$, respectively. Data preprocessing includes deformable registration, foreground cropping, percentile normalization for the input modality, and linear normalization of the CT to the range $[-1024, 1000]$. Data augmentation involves random zooming within $(0.8, 1.3)$ (for MRI-to-CT only), fixed-size cropping to $32\\times160\\times192$ for MRI-to-CT and $32\\times128\\times128$ for CBCT-to-CT, and random flipping. During inference, we apply a sliding-window approach with $0.8$ overlap and average folding to reconstruct the full-size sCT, followed by inversion of the CT normalization. After joint training on all regions without any fine-tuning, the final models are selected at the end of 3000 epochs for MRI-to-CT and 1000 epochs for CBCT-to-CT using the full training dataset.","authors":["Siyuan Mei","Yan Xia","Fuxin Fan"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.07984v3","updated":"2025-12-22T12:27:35Z","published":"2025-12-08T19:15:08Z","title":"Restrictive Hierarchical Semantic Segmentation for Stratified Tooth Layer Detection","summary":"Accurate understanding of anatomical structures is essential for reliably staging certain dental diseases. A way of introducing this within semantic segmentation models is by utilising hierarchy-aware methodologies. However, existing hierarchy-aware segmentation methods largely encode anatomical structure through the loss functions, providing weak and indirect supervision. We introduce a general framework that embeds an explicit anatomical hierarchy into semantic segmentation by coupling a recurrent, level-wise prediction scheme with restrictive output heads and top-down feature conditioning. At each depth of the class tree, the backbone is re-run on the original image concatenated with logits from the previous level. Child class features are conditioned using Feature-wise Linear Modulation of their parent class probabilities, to modulate child feature spaces for fine grained detection. A probabilistic composition rule enforces consistency between parent and descendant classes. Hierarchical loss combines per-level class weighted Dice and cross entropy loss and a consistency term loss, ensuring parent predictions are the sum of their children. We validate our approach on our proposed dataset, TL-pano, containing 194 panoramic radiographs with dense instance and semantic segmentation annotations, of tooth layers and alveolar bone. Utilising UNet and HRNet as donor models across a 5-fold cross validation scheme, the hierarchical variants consistently increase IoU, Dice, and recall, particularly for fine-grained anatomies, and produce more anatomically coherent masks. However, hierarchical variants also demonstrated increased recall over precision, implying increased false positives. The results demonstrate that explicit hierarchical structuring improves both performance and clinical plausibility, especially in low data dental imaging regimes.","authors":["Ryan Banks","Camila Lindoni Azevedo","Hongying Tang","Yunpeng Li"],"pdf_url":"","comment":"Incorrect initial draft was submitted by mistake. Method, results and citations are incorrect"},{"id":"http://arxiv.org/abs/2512.19331v1","updated":"2025-12-22T12:27:12Z","published":"2025-12-22T12:27:12Z","title":"DeltaMIL: Gated Memory Integration for Efficient and Discriminative Whole Slide Image Analysis","summary":"Whole Slide Images (WSIs) are typically analyzed using multiple instance learning (MIL) methods. However, the scale and heterogeneity of WSIs generate highly redundant and dispersed information, making it difficult to identify and integrate discriminative signals. Existing MIL methods either fail to discard uninformative cues effectively or have limited ability to consolidate relevant features from multiple patches, which restricts their performance on large and heterogeneous WSIs. To address this issue, we propose DeltaMIL, a novel MIL framework that explicitly selects semantically relevant regions and integrates the discriminative information from WSIs. Our method leverages the gated delta rule to efficiently filter and integrate information through a block combining forgetting and memory mechanisms. The delta mechanism dynamically updates the memory by removing old values and inserting new ones according to their correlation with the current patch. The gating mechanism further enables rapid forgetting of irrelevant signals. Additionally, DeltaMIL integrates a complementary local pattern mixing mechanism to retain fine-grained pathological locality. Our design enhances the extraction of meaningful cues and suppresses redundant or noisy information, which improves the model's robustness and discriminative power. Experiments demonstrate that DeltaMIL achieves state-of-the-art performance. Specifically, for survival prediction, DeltaMIL improves performance by 3.69\\% using ResNet-50 features and 2.36\\% using UNI features. For slide-level classification, it increases accuracy by 3.09\\% with ResNet-50 features and 3.75\\% with UNI features. These results demonstrate the strong and consistent performance of DeltaMIL across diverse WSI tasks.","authors":["Yueting Zhu","Yuehao Song","Shuai Zhang","Wenyu Liu","Xinggang Wang"],"pdf_url":"","comment":"11 pages,7 figures,8 tables"},{"id":"http://arxiv.org/abs/2512.19327v1","updated":"2025-12-22T12:25:50Z","published":"2025-12-22T12:25:50Z","title":"Extended OpenTT Games Dataset: A table tennis dataset for fine-grained shot type and point outcome","summary":"Automatically detecting and classifying strokes in table tennis video can streamline training workflows, enrich broadcast overlays, and enable fine-grained performance analytics. For this to be possible, annotated video data of table tennis is needed. We extend the public OpenTTGames dataset with highly detailed, frame-accurate shot type annotations (forehand, backhand with subtypes), player posture labels (body lean and leg stance), and rally outcome tags at point end. OpenTTGames is a set of recordings from the side of the table with official labels for bounces, when the ball is above the net, or hitting the net. The dataset already contains ball coordinates near events, which are either \"bounce\", \"net\", or \"empty_event\" in the original OpenTTGames dataset, and semantic masks (humans, table, scoreboard). Our extension adds the types of stroke to the events and a per-player taxonomy so models can move beyond event spotting toward tactical understanding (e.g., whether a stroke is likely to win the point or set up an advantage). We provide a compact coding scheme and code-assisted labeling procedure to support reproducible annotations and baselines for fine-grained stroke understanding in racket sports. This fills a practical gap in the community, where many prior video resources are either not publicly released or carry restrictive/unclear licenses that hinder reuse and benchmarking. Our annotations are released under the same CC BY-NC-SA 4.0 license as OpenTTGames, allowing free non-commercial use, modification, and redistribution, with appropriate attribution.","authors":["Moamal Fadhil Abdul","Jonas Bruun Hubrechts","Thomas Martini Jørgensen","Emil Hovad"],"pdf_url":"","comment":"Thomas Martini Jørgensen and Emil Hovad contributed equally and share last authorship"},{"id":"http://arxiv.org/abs/2512.19320v1","updated":"2025-12-22T12:13:17Z","published":"2025-12-22T12:13:17Z","title":"MAGIC: Achieving Superior Model Merging via Magnitude Calibration","summary":"The proliferation of pre-trained models has given rise to a wide array of specialised, fine-tuned models. Model merging aims to merge the distinct capabilities of these specialised models into a unified model, requiring minimal or even no additional training. A core objective of model merging is to ensure the merged model retains the behavioural characteristics of the specialised models, typically achieved through feature alignment. We identify that features consist of two critical components: direction and magnitude. Prior research has predominantly focused on directional alignment, while the influence of magnitude remains largely neglected, despite its pronounced vulnerability to perturbations introduced by common merging operations (e.g., parameter fusion and sparsification). Such perturbations to magnitude inevitably lead to feature deviations in the merged model from the specialised models, resulting in subsequent performance degradation. To address this, we propose MAGnItude Calibration (MAGIC), a plug-and-play framework that rectifies layer-wise magnitudes in feature and weight spaces, with three variants. Specifically, our Feature Space Calibration (FSC) realigns the merged model's features using a small set of unlabelled data, while Weight Space Calibration (WSC) extends this calibration to the weight space without requiring additional data. Combining these yields Dual Space Calibration (DSC). Comprehensive experiments demonstrate that MAGIC consistently boosts performance across diverse Computer Vision tasks (+4.3% on eight datasets) and NLP tasks (+8.0% on Llama) without additional training. Our code is available at: https://github.com/lyymuwu/MAGIC","authors":["Yayuan Li","Jian Zhang","Jintao Guo","Zihan Cheng","Lei Qi","Yinghuan Shi","Yang Gao"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19316v1","updated":"2025-12-22T12:07:05Z","published":"2025-12-22T12:07:05Z","title":"Neural Implicit Heart Coordinates: 3D cardiac shape reconstruction from sparse segmentations","summary":"Accurate reconstruction of cardiac anatomy from sparse clinical images remains a major challenge in patient-specific modeling. While neural implicit functions have previously been applied to this task, their application to mapping anatomical consistency across subjects has been limited. In this work, we introduce Neural Implicit Heart Coordinates (NIHCs), a standardized implicit coordinate system, based on universal ventricular coordinates, that provides a common anatomical reference frame for the human heart. Our method predicts NIHCs directly from a limited number of 2D segmentations (sparse acquisition) and subsequently decodes them into dense 3D segmentations and high-resolution meshes at arbitrary output resolution. Trained on a large dataset of 5,000 cardiac meshes, the model achieves high reconstruction accuracy on clinical contours, with mean Euclidean surface errors of 2.51$\\pm$0.33 mm in a diseased cohort (n=4549) and 2.3$\\pm$0.36 mm in a healthy cohort (n=5576). The NIHC representation enables anatomically coherent reconstruction even under severe slice sparsity and segmentation noise, faithfully recovering complex structures such as the valve planes. Compared with traditional pipelines, inference time is reduced from over 60 s to 5-15 s. These results demonstrate that NIHCs constitute a robust and efficient anatomical representation for patient-specific 3D cardiac reconstruction from minimal input data.","authors":["Marica Muffoletto","Uxio Hermida","Charlène Mauger","Avan Suinesiaputra","Yiyang Xu","Richard Burns","Lisa Pankewitz","Andrew D McCulloch","Steffen E Petersen","Daniel Rueckert","Alistair A Young"],"pdf_url":"","comment":"42 pages, 8 figures"},{"id":"http://arxiv.org/abs/2512.19311v1","updated":"2025-12-22T12:00:12Z","published":"2025-12-22T12:00:12Z","title":"MixFlow Training: Alleviating Exposure Bias with Slowed Interpolation Mixture","summary":"This paper studies the training-testing discrepancy (a.k.a. exposure bias) problem for improving the diffusion models. During training, the input of a prediction network at one training timestep is the corresponding ground-truth noisy data that is an interpolation of the noise and the data, and during testing, the input is the generated noisy data. We present a novel training approach, named MixFlow, for improving the performance. Our approach is motivated by the Slow Flow phenomenon: the ground-truth interpolation that is the nearest to the generated noisy data at a given sampling timestep is observed to correspond to a higher-noise timestep (termed slowed timestep), i.e., the corresponding ground-truth timestep is slower than the sampling timestep. MixFlow leverages the interpolations at the slowed timesteps, named slowed interpolation mixture, for post-training the prediction network for each training timestep. Experiments over class-conditional image generation (including SiT, REPA, and RAE) and text-to-image generation validate the effectiveness of our approach. Our approach MixFlow over the RAE models achieve strong generation results on ImageNet: 1.43 FID (without guidance) and 1.10 (with guidance) at 256 x 256, and 1.55 FID (without guidance) and 1.10 (with guidance) at 512 x 512.","authors":["Hui Li","Jiayue Lyu","Fu-Yun Wang","Kaihui Cheng","Siyu Zhu","Jingdong Wang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19302v1","updated":"2025-12-22T11:46:42Z","published":"2025-12-22T11:46:42Z","title":"Bridging Semantics and Geometry: A Decoupled LVLM-SAM Framework for Reasoning Segmentation in Remote Sensing","summary":"Large Vision-Language Models (LVLMs) hold great promise for advancing remote sensing (RS) analysis, yet existing reasoning segmentation frameworks couple linguistic reasoning and pixel prediction through end-to-end supervised fine-tuning, leading to weak geometric grounding and limited generalization across tasks. To address this, we developed Think2Seg-RS, a decoupled framework that trains an LVLM prompter to control a frozen Segment Anything Model (SAM) via structured geometric prompts. Through a mask-only reinforcement learning objective, the LVLM learns to translate abstract semantic reasoning into spatially grounded actions, achieving state-of-the-art performance on the EarthReason dataset. Remarkably, the learned prompting policy generalizes zero-shot to multiple referring segmentation benchmarks, exposing a distinct divide between semantic-level and instance-level grounding. We further found that compact segmenters outperform larger ones under semantic-level supervision, and that negative prompts are ineffective in heterogeneous aerial backgrounds. Together, these findings establish semantic-level reasoning segmentation as a new paradigm for geospatial understanding, opening the way toward unified, interpretable LVLM-driven Earth observation. Our code and model are available at https://github.com/Ricardo-XZ/Think2Seg-RS.","authors":["Xu Zhang","Junyao Ge","Yang Zheng","Kaitai Guo","Jimin Liang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19300v1","updated":"2025-12-22T11:44:32Z","published":"2025-12-22T11:44:32Z","title":"RMLer: Synthesizing Novel Objects across Diverse Categories via Reinforcement Mixing Learning","summary":"Novel object synthesis by integrating distinct textual concepts from diverse categories remains a significant challenge in Text-to-Image (T2I) generation. Existing methods often suffer from insufficient concept mixing, lack of rigorous evaluation, and suboptimal outputs-manifesting as conceptual imbalance, superficial combinations, or mere juxtapositions. To address these limitations, we propose Reinforcement Mixing Learning (RMLer), a framework that formulates cross-category concept fusion as a reinforcement learning problem: mixed features serve as states, mixing strategies as actions, and visual outcomes as rewards. Specifically, we design an MLP-policy network to predict dynamic coefficients for blending cross-category text embeddings. We further introduce visual rewards based on (1) semantic similarity and (2) compositional balance between the fused object and its constituent concepts, optimizing the policy via proximal policy optimization. At inference, a selection strategy leverages these rewards to curate the highest-quality fused objects. Extensive experiments demonstrate RMLer's superiority in synthesizing coherent, high-fidelity objects from diverse categories, outperforming existing methods. Our work provides a robust framework for generating novel visual concepts, with promising applications in film, gaming, and design.","authors":["Jun Li","Zikun Chen","Haibo Chen","Shuo Chen","Jian Yang"],"pdf_url":"","comment":"accepted by AAAI2026"},{"id":"http://arxiv.org/abs/2512.19283v1","updated":"2025-12-22T11:26:41Z","published":"2025-12-22T11:26:41Z","title":"Hand-Aware Egocentric Motion Reconstruction with Sequence-Level Context","summary":"Egocentric vision systems are becoming widely available, creating new opportunities for human-computer interaction. A core challenge is estimating the wearer's full-body motion from first-person videos, which is crucial for understanding human behavior. However, this task is difficult since most body parts are invisible from the egocentric view. Prior approaches mainly rely on head trajectories, leading to ambiguity, or assume continuously tracked hands, which is unrealistic for lightweight egocentric devices. In this work, we present HaMoS, the first hand-aware, sequence-level diffusion framework that directly conditions on both head trajectory and intermittently visible hand cues caused by field-of-view limitations and occlusions, as in real-world egocentric devices. To overcome the lack of datasets pairing diverse camera views with human motion, we introduce a novel augmentation method that models such real-world conditions. We also demonstrate that sequence-level contexts such as body shape and field-of-view are crucial for accurate motion reconstruction, and thus employ local attention to infer long sequences efficiently. Experiments on public benchmarks show that our method achieves state-of-the-art accuracy and temporal smoothness, demonstrating a practical step toward reliable in-the-wild egocentric 3D motion understanding.","authors":["Kyungwon Cho","Hanbyul Joo"],"pdf_url":"","comment":"Project Page: https://kyungwoncho.github.io/HaMoS/"},{"id":"http://arxiv.org/abs/2512.19275v1","updated":"2025-12-22T11:19:46Z","published":"2025-12-22T11:19:46Z","title":"Is Visual Realism Enough? Evaluating Gait Biometric Fidelity in Generative AI Human Animation","summary":"Generative AI (GenAI) models have revolutionized animation, enabling the synthesis of humans and motion patterns with remarkable visual fidelity. However, generating truly realistic human animation remains a formidable challenge, where even minor inconsistencies can make a subject appear unnatural. This limitation is particularly critical when AI-generated videos are evaluated for behavioral biometrics, where subtle motion cues that define identity are easily lost or distorted. The present study investigates whether state-of-the-art GenAI human animation models can preserve the subtle spatio-temporal details needed for person identification through gait biometrics. Specifically, we evaluate four different GenAI models across two primary evaluation tasks to assess their ability to i) restore gait patterns from reference videos under varying conditions of complexity, and ii) transfer these gait patterns to different visual identities. Our results show that while visual quality is mostly high, biometric fidelity remains low in tasks focusing on identification, suggesting that current GenAI models struggle to disentangle identity from motion. Furthermore, through an identity transfer task, we expose a fundamental flaw in appearance-based gait recognition: when texture is disentangled from motion, identification collapses, proving current GenAI models rely on visual attributes rather than temporal dynamics.","authors":["Ivan DeAndres-Tame","Chengwei Ye","Ruben Tolosana","Ruben Vera-Rodriguez","Shiqi Yu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19271v1","updated":"2025-12-22T11:07:27Z","published":"2025-12-22T11:07:27Z","title":"3SGen: Unified Subject, Style, and Structure-Driven Image Generation with Adaptive Task-specific Memory","summary":"Recent image generation approaches often address subject, style, and structure-driven conditioning in isolation, leading to feature entanglement and limited task transferability. In this paper, we introduce 3SGen, a task-aware unified framework that performs all three conditioning modes within a single model. 3SGen employs an MLLM equipped with learnable semantic queries to align text-image semantics, complemented by a VAE branch that preserves fine-grained visual details. At its core, an Adaptive Task-specific Memory (ATM) module dynamically disentangles, stores, and retrieves condition-specific priors, such as identity for subjects, textures for styles, and spatial layouts for structures, via a lightweight gating mechanism along with several scalable memory items. This design mitigates inter-task interference and naturally scales to compositional inputs. In addition, we propose 3SGen-Bench, a unified image-driven generation benchmark with standardized metrics for evaluating cross-task fidelity and controllability. Extensive experiments on our proposed 3SGen-Bench and other public benchmarks demonstrate our superior performance across diverse image-driven generation tasks.","authors":["Xinyang Song","Libin Wang","Weining Wang","Zhiwei Li","Jianxin Sun","Dandan Zheng","Jingdong Chen","Qi Li","Zhenan Sun"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2508.17054v3","updated":"2025-12-22T10:54:05Z","published":"2025-08-23T15:06:59Z","title":"DeltaFlow: An Efficient Multi-frame Scene Flow Estimation Method","summary":"Previous dominant methods for scene flow estimation focus mainly on input from two consecutive frames, neglecting valuable information in the temporal domain. While recent trends shift towards multi-frame reasoning, they suffer from rapidly escalating computational costs as the number of frames grows. To leverage temporal information more efficiently, we propose DeltaFlow ($Δ$Flow), a lightweight 3D framework that captures motion cues via a $Δ$ scheme, extracting temporal features with minimal computational cost, regardless of the number of frames. Additionally, scene flow estimation faces challenges such as imbalanced object class distributions and motion inconsistency. To tackle these issues, we introduce a Category-Balanced Loss to enhance learning across underrepresented classes and an Instance Consistency Loss to enforce coherent object motion, improving flow accuracy. Extensive evaluations on the Argoverse 2, Waymo and nuScenes datasets show that $Δ$Flow achieves state-of-the-art performance with up to 22% lower error and $2\\times$ faster inference compared to the next-best multi-frame supervised method, while also demonstrating a strong cross-domain generalization ability. The code is open-sourced at https://github.com/Kin-Zhang/DeltaFlow along with trained model weights.","authors":["Qingwen Zhang","Xiaomeng Zhu","Yushan Zhang","Yixi Cai","Olov Andersson","Patric Jensfelt"],"pdf_url":"","comment":"NeurIPS 2025 Spotlight, 18 pages (10 main pages + 8 supp materail), 11 figures, code at https://github.com/Kin-Zhang/DeltaFlow"},{"id":"http://arxiv.org/abs/2507.17860v4","updated":"2025-12-22T10:41:08Z","published":"2025-07-23T18:33:27Z","title":"Towards Facilitated Fairness Assessment of AI-based Skin Lesion Classifiers Through GenAI-based Image Synthesis","summary":"Recent advances in deep learning and on-device inference could transform routine screening for skin cancers. Along with the anticipated benefits of this technology, potential dangers arise from unforeseen and inherent biases. A significant obstacle is building evaluation datasets that accurately reflect key demographics, including sex, age, and race, as well as other underrepresented groups. To address this, we train a state-of-the-art generative model to generate synthetic data in a controllable manner to assess the fairness of publicly available skin cancer classifiers. To evaluate whether synthetic images can be used as a fairness testing dataset, we prepare a real-image dataset (MILK10K) as a benchmark and compare the True Positive Rate result of three models (DeepGuide, MelaNet, and SkinLesionDensnet). As a result, the classification tendencies observed in each model when tested on real and generated images showed similar patterns across different attribute data sets. We confirm that highly realistic synthetic images facilitate model fairness verification.","authors":["Ko Watanabe","Stanislav Frolov","Aya Hassan","David Dembinsky","Adriano Lucieri","Andreas Dengel"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19253v1","updated":"2025-12-22T10:40:03Z","published":"2025-12-22T10:40:03Z","title":"Machine Unlearning in the Era of Quantum Machine Learning: An Empirical Study","summary":"We present the first comprehensive empirical study of machine unlearning (MU) in hybrid quantum-classical neural networks. While MU has been extensively explored in classical deep learning, its behavior within variational quantum circuits (VQCs) and quantum-augmented architectures remains largely unexplored. First, we adapt a broad suite of unlearning methods to quantum settings, including gradient-based, distillation-based, regularization-based and certified techniques. Second, we introduce two new unlearning strategies tailored to hybrid models. Experiments across Iris, MNIST, and Fashion-MNIST, under both subset removal and full-class deletion, reveal that quantum models can support effective unlearning, but outcomes depend strongly on circuit depth, entanglement structure, and task complexity. Shallow VQCs display high intrinsic stability with minimal memorization, whereas deeper hybrid models exhibit stronger trade-offs between utility, forgetting strength, and alignment with retrain oracle. We find that certain methods, e.g. EU-k, LCA, and Certified Unlearning, consistently provide the best balance across metrics. These findings establish baseline empirical insights into quantum machine unlearning and highlight the need for quantum-aware algorithms and theoretical guarantees, as quantum machine learning systems continue to expand in scale and capability. We publicly release our code at: https://github.com/CrivoiCarla/HQML.","authors":["Carla Crivoi","Radu Tudor Ionescu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19243v1","updated":"2025-12-22T10:25:38Z","published":"2025-12-22T10:25:38Z","title":"VisionDirector: Vision-Language Guided Closed-Loop Refinement for Generative Image Synthesis","summary":"Generative models can now produce photorealistic imagery, yet they still struggle with the long, multi-goal prompts that professional designers issue. To expose this gap and better evaluate models' performance in real-world settings, we introduce Long Goal Bench (LGBench), a 2,000-task suite (1,000 T2I and 1,000 I2I) whose average instruction contains 18 to 22 tightly coupled goals spanning global layout, local object placement, typography, and logo fidelity. We find that even state-of-the-art models satisfy fewer than 72 percent of the goals and routinely miss localized edits, confirming the brittleness of current pipelines. To address this, we present VisionDirector, a training-free vision-language supervisor that (i) extracts structured goals from long instructions, (ii) dynamically decides between one-shot generation and staged edits, (iii) runs micro-grid sampling with semantic verification and rollback after every edit, and (iv) logs goal-level rewards. We further fine-tune the planner with Group Relative Policy Optimization, yielding shorter edit trajectories (3.1 versus 4.2 steps) and stronger alignment. VisionDirector achieves new state of the art on GenEval (plus 7 percent overall) and ImgEdit (plus 0.07 absolute) while producing consistent qualitative improvements on typography, multi-object scenes, and pose editing.","authors":["Meng Chu","Senqiao Yang","Haoxuan Che","Suiyun Zhang","Xichen Zhang","Shaozuo Yu","Haokun Gui","Zhefan Rao","Dandan Tu","Rui Liu","Jiaya Jia"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.00086v2","updated":"2025-12-22T10:06:08Z","published":"2025-11-26T09:46:09Z","title":"Multi-modal On-Device Learning for Monocular Depth Estimation on Ultra-low-power MCUs","summary":"Monocular depth estimation (MDE) plays a crucial role in enabling spatially-aware applications in Ultra-low-power (ULP) Internet-of-Things (IoT) platforms. However, the limited number of parameters of Deep Neural Networks for the MDE task, designed for IoT nodes, results in severe accuracy drops when the sensor data observed in the field shifts significantly from the training dataset. To address this domain shift problem, we present a multi-modal On-Device Learning (ODL) technique, deployed on an IoT device integrating a Greenwaves GAP9 MicroController Unit (MCU), a 80 mW monocular camera and a 8 x 8 pixel depth sensor, consuming $\\approx$300mW. In its normal operation, this setup feeds a tiny 107 k-parameter $μ$PyD-Net model with monocular images for inference. The depth sensor, usually deactivated to minimize energy consumption, is only activated alongside the camera to collect pseudo-labels when the system is placed in a new environment. Then, the fine-tuning task is performed entirely on the MCU, using the new data. To optimize our backpropagation-based on-device training, we introduce a novel memory-driven sparse update scheme, which minimizes the fine-tuning memory to 1.2 MB, 2.2x less than a full update, while preserving accuracy (i.e., only 2% and 1.5% drops on the KITTI and NYUv2 datasets). Our in-field tests demonstrate, for the first time, that ODL for MDE can be performed in 17.8 minutes on the IoT node, reducing the root mean squared error from 4.9 to 0.6m with only 3 k self-labeled samples, collected in a real-life deployment scenario.","authors":["Davide Nadalini","Manuele Rusci","Elia Cereda","Luca Benini","Francesco Conti","Daniele Palossi"],"pdf_url":"","comment":"14 pages, 9 figures, 3 tables. Associated open-source release available at: https://github.com/idsia-robotics/ultralow-power-monocular-depth-ondevice-learning"},{"id":"http://arxiv.org/abs/2512.19225v1","updated":"2025-12-22T10:05:37Z","published":"2025-12-22T10:05:37Z","title":"Selective Phase-Aware Training of nnU-Net for Robust Breast Cancer Segmentation in Multi-Center DCE-MRI","summary":"Breast cancer remains the most common cancer among women and is a leading cause of female mortality. Dynamic contrast-enhanced MRI (DCE-MRI) is a powerful imaging tool for evaluating breast tumors, yet the field lacks a standardized benchmark for analyzing treatment responses and guiding personalized care. We participated in the MAMA-MIA Challenge's Primary Tumor Segmentation task and this work presents a proposed selective, phase-aware training framework for the nnU-Net architecture, emphasizing quality-focused data selection to strengthen model robustness and generalization. We employed the No New Net (nnU-Net) framework with a selective training strategy that systematically analyzed the impact of image quality and center-specific variability on segmentation performance. Controlled experiments on the DUKE, NACT, ISPY1, and ISPY2 datasets revealed that including ISPY scans with motion artifacts and reduced contrast impaired segmentation performance, even with advanced preprocessing, such as contrast-limited adaptive histogram equalization (CLAHE). In contrast, training on DUKE and NACT data, which exhibited clearer contrast and fewer motion artifacts despite varying resolutions, with early phase images (0000-0002) provided more stable training conditions. Our results demonstrate the importance of phase-sensitive and quality-aware training strategies in achieving reliable segmentation performance in heterogeneous clinical datasets, highlighting the limitations of the expansion of naive datasets and motivating the need for future automation of quality-based data selection strategies.","authors":["Beyza Zayim","Aissiou Ikram","Boukhiar Naima"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19221v1","updated":"2025-12-22T10:02:53Z","published":"2025-12-22T10:02:53Z","title":"From Pixels to Predicates Structuring urban perception with scene graphs","summary":"Perception research is increasingly modelled using streetscapes, yet many approaches still rely on pixel features or object co-occurrence statistics, overlooking the explicit relations that shape human perception. This study proposes a three stage pipeline that transforms street view imagery (SVI) into structured representations for predicting six perceptual indicators. In the first stage, each image is parsed using an open-set Panoptic Scene Graph model (OpenPSG) to extract object predicate object triplets. In the second stage, compact scene-level embeddings are learned through a heterogeneous graph autoencoder (GraphMAE). In the third stage, a neural network predicts perception scores from these embeddings. We evaluate the proposed approach against image-only baselines in terms of accuracy, precision, and cross-city generalization. Results indicate that (i) our approach improves perception prediction accuracy by an average of 26% over baseline models, and (ii) maintains strong generalization performance in cross-city prediction tasks. Additionally, the structured representation clarifies which relational patterns contribute to lower perception scores in urban scenes, such as graffiti on wall and car parked on sidewalk. Overall, this study demonstrates that graph-based structure provides expressive, generalizable, and interpretable signals for modelling urban perception, advancing human-centric and context-aware urban analytics.","authors":["Yunlong Liu","Shuyang Li","Pengyuan Liu","Yu Zhang","Rudi Stouffs"],"pdf_url":"","comment":"10 pages, CAADRIA2026 presentation forthcoming"},{"id":"http://arxiv.org/abs/2512.19219v1","updated":"2025-12-22T10:02:10Z","published":"2025-12-22T10:02:10Z","title":"Towards Minimal Fine-Tuning of VLMs","summary":"We introduce Image-LoRA, a lightweight parameter efficient fine-tuning (PEFT) recipe for transformer-based vision-language models (VLMs). Image-LoRA applies low-rank adaptation only to the value path of attention layers within the visual-token span, reducing adapter-only training FLOPs roughly in proportion to the visual-token fraction. We further adapt only a subset of attention heads, selected using head influence scores estimated with a rank-1 Image-LoRA, and stabilize per-layer updates via selection-size normalization. Across screen-centric grounding and referring benchmarks spanning text-heavy to image-heavy regimes, Image-LoRA matches or closely approaches standard LoRA accuracy while using fewer trainable parameters and lower adapter-only training FLOPs. The method also preserves the pure-text reasoning performance of VLMs before and after fine-tuning, as further shown on GSM8K.","authors":["Tiange Luo","Lajanugen Logeswaran","Jaekyeom Kim","Justin Johnson","Honglak Lee"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19214v1","updated":"2025-12-22T09:53:55Z","published":"2025-12-22T09:53:55Z","title":"HippMetric: A skeletal-representation-based framework for cross-sectional and longitudinal hippocampal substructural morphometry","summary":"Accurate characterization of hippocampal substructure is crucial for detecting subtle structural changes and identifying early neurodegenerative biomarkers. However, high inter-subject variability and complex folding pattern of human hippocampus hinder consistent cross-subject and longitudinal analysis. Most existing approaches rely on subject-specific modelling and lack a stable intrinsic coordinate system to accommodate anatomical variability, which limits their ability to establish reliable inter- and intra-individual correspondence. To address this, we propose HippMetric, a skeletal representation (s-rep)-based framework for hippocampal substructural morphometry and point-wise correspondence across individuals and scans. HippMetric builds on the Axis-Referenced Morphometric Model (ARMM) and employs a deformable skeletal coordinate system aligned with hippocampal anatomy and function, providing a biologically grounded reference for correspondence. Our framework comprises two core modules: a skeletal-based coordinate system that respects the hippocampus' conserved longitudinal lamellar architecture, in which functional units (lamellae) are stacked perpendicular to the long-axis, enabling anatomically consistent localization across subjects and time; and individualized s-reps generated through surface reconstruction, deformation, and geometrically constrained spoke refinement, enforcing boundary adherence, orthogonality and non-intersection to produce mathematically valid skeletal geometry. Extensive experiments on two international cohorts demonstrate that HippMetric achieves higher accuracy, reliability, and correspondence stability compared to existing shape models.","authors":["Na Gao","Chenfei Ye","Yanwu Yang","Anqi Li","Zhengbo He","Li Liang","Zhiyuan Liu","Xingyu Hao","Ting Ma","Tengfei Guo"],"pdf_url":"","comment":"35 pages, 8 figures"},{"id":"http://arxiv.org/abs/2512.19213v1","updated":"2025-12-22T09:53:38Z","published":"2025-12-22T09:53:38Z","title":"InvCoSS: Inversion-driven Continual Self-supervised Learning in Medical Multi-modal Image Pre-training","summary":"Continual self-supervised learning (CSSL) in medical imaging trains a foundation model sequentially, alleviating the need for collecting multi-modal images for joint training and offering promising improvements in downstream performance while preserving data privacy. However, most existing methods still rely on replaying data from previous stages to prevent catastrophic forgetting, which compromises privacy and limits their applicability in real-world scenarios where data transfer across sites is often restricted. In this work, we propose InvCoSS, an inversion-driven continual self-supervised learning framework for medical multi-modal image pre-training. Specifically, after training on a previous task, InvCoSS inverts the pre-trained self-supervised model to generate synthetic images that approximate the original training distribution. These synthetic images are then combined with data from the new task for joint optimization, which effectively mitigates catastrophic forgetting while strictly adhering to the constraint of no access to previous real data. Furthermore, to improve the fidelity of synthetic images, we introduce a novel InvUNet with a multi-scale fusion architecture to restore both high- and low-frequency components of the inverted images. To enhance diversity and prevent mode collapse, we design a repulsive representation-learning mechanism that encourages a diverse feature space for synthetic images without class guidance. Extensive experiments across nine downstream tasks validate the effectiveness of InvCoSS, achieving performance comparable to or even superior to prior data-replay methods while significantly reducing storage requirements and eliminating data privacy constraints.","authors":["Zihao Luo","Shaohao Rui","Zhenyu Tang","Guotai Wang","Xiaosong Wang"],"pdf_url":"","comment":"16 pages, 10 figures, 5 tables"},{"id":"http://arxiv.org/abs/2505.08438v3","updated":"2025-12-22T09:39:55Z","published":"2025-05-13T11:04:04Z","title":"A Survey of 3D Reconstruction with Event Cameras","summary":"Event cameras are rapidly emerging as powerful vision sensors for 3D reconstruction, uniquely capable of asynchronously capturing per-pixel brightness changes. Compared to traditional frame-based cameras, event cameras produce sparse yet temporally dense data streams, enabling robust and accurate 3D reconstruction even under challenging conditions such as high-speed motion, low illumination, and extreme dynamic range scenarios. These capabilities offer substantial promise for transformative applications across various fields, including autonomous driving, robotics, aerial navigation, and immersive virtual reality. In this survey, we present the first comprehensive review exclusively dedicated to event-based 3D reconstruction. Existing approaches are systematically categorised based on input modality into stereo, monocular, and multimodal systems, and further classified according to reconstruction methodologies, including geometry-based techniques, deep learning approaches, and neural rendering techniques such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS). Within each category, methods are chronologically organised to highlight the evolution of key concepts and advancements. Furthermore, we provide a detailed summary of publicly available datasets specifically suited to event-based reconstruction tasks. Finally, we discuss significant open challenges in dataset availability, standardised evaluation, effective representation, and dynamic scene reconstruction, outlining insightful directions for future research. This survey aims to serve as an essential reference and provides a clear and motivating roadmap toward advancing the state of the art in event-driven 3D reconstruction.","authors":["Chuanzhi Xu","Haoxian Zhou","Langyi Chen","Haodong Chen","Zeke Zexi Hu","Zhicheng Lu","Ying Zhou","Vera Chung","Qiang Qu","Weidong Cai"],"pdf_url":"","comment":"This survey has been accepted for publication in the Computational Visual Media Journal"},{"id":"http://arxiv.org/abs/2512.19190v1","updated":"2025-12-22T09:28:23Z","published":"2025-12-22T09:28:23Z","title":"PEDESTRIAN: An Egocentric Vision Dataset for Obstacle Detection on Pavements","summary":"Walking has always been a primary mode of transportation and is recognized as an essential activity for maintaining good health. Despite the need for safe walking conditions in urban environments, sidewalks are frequently obstructed by various obstacles that hinder free pedestrian movement. Any object obstructing a pedestrian's path can pose a safety hazard. The advancement of pervasive computing and egocentric vision techniques offers the potential to design systems that can automatically detect such obstacles in real time, thereby enhancing pedestrian safety. The development of effective and efficient identification algorithms relies on the availability of comprehensive and well-balanced datasets of egocentric data. In this work, we introduce the PEDESTRIAN dataset, comprising egocentric data for 29 different obstacles commonly found on urban sidewalks. A total of 340 videos were collected using mobile phone cameras, capturing a pedestrian's point of view. Additionally, we present the results of a series of experiments that involved training several state-of-the-art deep learning algorithms using the proposed dataset, which can be used as a benchmark for obstacle detection and recognition tasks. The dataset can be used for training pavement obstacle detectors to enhance the safety of pedestrians in urban areas.","authors":["Marios Thoma","Zenonas Theodosiou","Harris Partaourides","Vassilis Vassiliades","Loizos Michael","Andreas Lanitis"],"pdf_url":"","comment":"24 pages, 7 figures, 9 tables, Dataset: https://doi.org/10.5281/zenodo.10907945, Code: https://github.com/CYENS/PEDESTRIAN"},{"id":"http://arxiv.org/abs/2512.17098v2","updated":"2025-12-22T09:17:40Z","published":"2025-12-18T21:52:43Z","title":"Predictive Modeling of Maritime Radar Data Using Transformer Architecture","summary":"Maritime autonomous systems require robust predictive capabilities to anticipate vessel motion and environmental dynamics. While transformer architectures have revolutionized AIS-based trajectory prediction and demonstrated feasibility for sonar frame forecasting, their application to maritime radar frame prediction remains unexplored, creating a critical gap given radar's all-weather reliability for navigation. This survey systematically reviews predictive modeling approaches relevant to maritime radar, with emphasis on transformer architectures for spatiotemporal sequence forecasting, where existing representative methods are analyzed according to data type, architecture, and prediction horizon. Our review shows that, while the literature has demonstrated transformer-based frame prediction for sonar sensing, no prior work addresses transformer-based maritime radar frame prediction, thereby defining a clear research gap and motivating a concrete research direction for future work in this area.","authors":["Bjorna Qesaraku","Jan Steckel"],"pdf_url":"","comment":"9 pages, 2 figures, 1 table"},{"id":"http://arxiv.org/abs/2512.19173v1","updated":"2025-12-22T09:07:34Z","published":"2025-12-22T09:07:34Z","title":"CycleChart: A Unified Consistency-Based Learning Framework for Bidirectional Chart Understanding and Generation","summary":"Current chart-specific tasks, such as chart question answering, chart parsing, and chart generation, are typically studied in isolation, preventing models from learning the shared semantics that link chart generation and interpretation. We introduce CycleChart, a consistency-based learning framework for bidirectional chart understanding and generation. CycleChart adopts a schema-centric formulation as a common interface across tasks. We construct a consistent multi-task dataset, where each chart sample includes aligned annotations for schema prediction, data parsing, and question answering. To learn cross-directional chart semantics, CycleChart introduces a generate-parse consistency objective: the model generates a chart schema from a table and a textual query, then learns to recover the schema and data from the generated chart, enforcing semantic alignment across directions. CycleChart achieves strong results on chart generation, chart parsing, and chart question answering, demonstrating improved cross-task generalization and marking a step toward more general chart understanding models.","authors":["Dazhen Deng","Sen Yang","Yuchen He","Yuan Tian","Yingcai Wu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2502.13385v2","updated":"2025-12-22T09:07:34Z","published":"2025-02-19T02:50:51Z","title":"SNN-Driven Multimodal Human Action Recognition via Sparse Spatial-Temporal Data Fusion","summary":"Multimodal human action recognition based on RGB and skeleton data fusion, while effective, is constrained by significant limitations such as high computational complexity, excessive memory consumption, and substantial energy demands, particularly when implemented with Artificial Neural Networks (ANN). These limitations restrict its applicability in resource-constrained scenarios. To address these challenges, we propose a novel Spiking Neural Network (SNN)-driven framework for multimodal human action recognition, utilizing event camera and skeleton data. Our framework is centered on two key innovations: (1) a novel multimodal SNN architecture that employs distinct backbone networks for each modality-an SNN-based Mamba for event camera data and a Spiking Graph Convolutional Network (SGN) for skeleton data-combined with a spiking semantic extraction module to capture deep semantic representations; and (2) a pioneering SNN-based discretized information bottleneck mechanism for modality fusion, which effectively balances the preservation of modality-specific semantics with efficient information compression. To validate our approach, we propose a novel method for constructing a multimodal dataset that integrates event camera and skeleton data, enabling comprehensive evaluation. Extensive experiments demonstrate that our method achieves superior performance in both recognition accuracy and energy efficiency, offering a promising solution for practical applications.","authors":["Naichuan Zheng","Hailun Xia","Zeyu Liang","Yuchen Du"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2504.09377v3","updated":"2025-12-22T09:07:03Z","published":"2025-04-12T23:52:59Z","title":"Gradient as Conditions: Rethinking HOG for All-in-one Image Restoration","summary":"All-in-one image restoration (AIR) aims to address diverse degradations within a unified model by leveraging informative degradation conditions to guide the restoration process. However, existing methods often rely on implicitly learned priors, which may entangle feature representations and hinder performance in complex or unseen scenarios. Histogram of Oriented Gradients (HOG) as a classical gradient representation, we observe that it has strong discriminative capability across diverse degradations, making it a powerful and interpretable prior for AIR. Based on this insight, we propose HOGformer, a Transformer-based model that integrates learnable HOG features for degradation-aware restoration. The core of HOGformer is a Dynamic HOG-aware Self-Attention (DHOGSA) mechanism, which adaptively models long-range spatial dependencies conditioned on degradation-specific cues encoded by HOG descriptors. To further adapt the heterogeneity of degradations in AIR, we propose a Dynamic Interaction Feed-Forward (DIFF) module that facilitates channel-spatial interactions, enabling robust feature transformation under diverse degradations. Besides, we propose a HOG loss to explicitly enhance structural fidelity and edge sharpness. Extensive experiments on a variety of benchmarks, including adverse weather and natural degradations, demonstrate that HOGformer achieves state-of-the-art performance and generalizes well to complex real-world scenarios.Code is available at https://github.com/Fire-friend/HOGformer.","authors":["Jiawei Wu","Zhifei Yang","Zhe Wang","Zhi Jin"],"pdf_url":"","comment":"AAAI2026"},{"id":"http://arxiv.org/abs/2512.19159v1","updated":"2025-12-22T08:55:23Z","published":"2025-12-22T08:55:23Z","title":"OmniMoGen: Unifying Human Motion Generation via Learning from Interleaved Text-Motion Instructions","summary":"Large language models (LLMs) have unified diverse linguistic tasks within a single framework, yet such unification remains unexplored in human motion generation. Existing methods are confined to isolated tasks, limiting flexibility for free-form and omni-objective generation. To address this, we propose OmniMoGen, a unified framework that enables versatile motion generation through interleaved text-motion instructions. Built upon a concise RVQ-VAE and transformer architecture, OmniMoGen supports end-to-end instruction-driven motion generation. We construct X2Mo, a large-scale dataset of over 137K interleaved text-motion instructions, and introduce AnyContext, a benchmark for evaluating interleaved motion generation. Experiments show that OmniMoGen achieves state-of-the-art performance on text-to-motion, motion editing, and AnyContext, exhibiting emerging capabilities such as compositional editing, self-reflective generation, and knowledge-informed generation. These results mark a step toward the next intelligent motion generation. Project Page: https://OmniMoGen.github.io/.","authors":["Wendong Bu","Kaihang Pan","Yuze Lin","Jiacheng Li","Kai Shen","Wenqiao Zhang","Juncheng Li","Jun Xiao","Siliang Tang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19150v1","updated":"2025-12-22T08:46:59Z","published":"2025-12-22T08:46:59Z","title":"AMap: Distilling Future Priors for Ahead-Aware Online HD Map Construction","summary":"Online High-Definition (HD) map construction is pivotal for autonomous driving. While recent approaches leverage historical temporal fusion to improve performance, we identify a critical safety flaw in this paradigm: it is inherently ``spatially backward-looking.\" These methods predominantly enhance map reconstruction in traversed areas, offering minimal improvement for the unseen road ahead. Crucially, our analysis of downstream planning tasks reveals a severe asymmetry: while rearward perception errors are often tolerable, inaccuracies in the forward region directly precipitate hazardous driving maneuvers. To bridge this safety gap, we propose AMap, a novel framework for Ahead-aware online HD Mapping. We pioneer a ``distill-from-future\" paradigm, where a teacher model with privileged access to future temporal contexts guides a lightweight student model restricted to the current frame. This process implicitly compresses prospective knowledge into the student model, endowing it with ``look-ahead\" capabilities at zero inference-time cost. Technically, we introduce a Multi-Level BEV Distillation strategy with spatial masking and an Asymmetric Query Adaptation module to effectively transfer future-aware representations to the student's static queries. Extensive experiments on the nuScenes and Argoverse 2 benchmark demonstrate that AMap significantly enhances current-frame perception. Most notably, it outperforms state-of-the-art temporal models in critical forward regions while maintaining the efficiency of single current frame inference.","authors":["Ruikai Li","Xinrun Li","Mengwei Xie","Hao Shan","Shoumeng Qiu","Xinyuan Chang","Yizhe Fan","Feng Xiong","Han Jiang","Yilong Ren","Haiyang Yu","Mu Xu","Yang Long","Varun Ojha","Zhiyong Cui"],"pdf_url":"","comment":"19 pages, 11 figures"},{"id":"http://arxiv.org/abs/2512.19133v1","updated":"2025-12-22T08:27:44Z","published":"2025-12-22T08:27:44Z","title":"WorldRFT: Latent World Model Planning with Reinforcement Fine-Tuning for Autonomous Driving","summary":"Latent World Models enhance scene representation through temporal self-supervised learning, presenting a perception annotation-free paradigm for end-to-end autonomous driving. However, the reconstruction-oriented representation learning tangles perception with planning tasks, leading to suboptimal optimization for planning. To address this challenge, we propose WorldRFT, a planning-oriented latent world model framework that aligns scene representation learning with planning via a hierarchical planning decomposition and local-aware interactive refinement mechanism, augmented by reinforcement learning fine-tuning (RFT) to enhance safety-critical policy performance. Specifically, WorldRFT integrates a vision-geometry foundation model to improve 3D spatial awareness, employs hierarchical planning task decomposition to guide representation optimization, and utilizes local-aware iterative refinement to derive a planning-oriented driving policy. Furthermore, we introduce Group Relative Policy Optimization (GRPO), which applies trajectory Gaussianization and collision-aware rewards to fine-tune the driving policy, yielding systematic improvements in safety. WorldRFT achieves state-of-the-art (SOTA) performance on both open-loop nuScenes and closed-loop NavSim benchmarks. On nuScenes, it reduces collision rates by 83% (0.30% -> 0.05%). On NavSim, using camera-only sensors input, it attains competitive performance with the LiDAR-based SOTA method DiffusionDrive (87.8 vs. 88.1 PDMS).","authors":["Pengxuan Yang","Ben Lu","Zhongpu Xia","Chao Han","Yinfeng Gao","Teng Zhang","Kun Zhan","XianPeng Lang","Yupeng Zheng","Qichao Zhang"],"pdf_url":"","comment":"AAAI 2026, first version"},{"id":"http://arxiv.org/abs/2506.10609v2","updated":"2025-12-22T08:24:53Z","published":"2025-06-12T11:54:13Z","title":"MSTAR: Box-free Multi-query Scene Text Retrieval with Attention Recycling","summary":"Scene text retrieval has made significant progress with the assistance of accurate text localization. However, existing approaches typically require costly bounding box annotations for training. Besides, they mostly adopt a customized retrieval strategy but struggle to unify various types of queries to meet diverse retrieval needs. To address these issues, we introduce Muti-query Scene Text retrieval with Attention Recycling (MSTAR), a box-free approach for scene text retrieval. It incorporates progressive vision embedding to dynamically capture the multi-grained representation of texts and harmonizes free-style text queries with style-aware instructions. Additionally, a multi-instance matching module is integrated to enhance vision-language alignment. Furthermore, we build the Multi-Query Text Retrieval (MQTR) dataset, the first benchmark designed to evaluate the multi-query scene text retrieval capability of models, comprising four query types and 16k images. Extensive experiments demonstrate the superiority of our method across seven public datasets and the MQTR dataset. Notably, MSTAR marginally surpasses the previous state-of-the-art model by 6.4% in MAP on Total-Text while eliminating box annotation costs. Moreover, on the MQTR benchmark, MSTAR significantly outperforms the previous models by an average of 8.5%. The code and datasets are available at https://github.com/yingift/MSTAR.","authors":["Liang Yin","Xudong Xie","Zhang Li","Xiang Bai","Yuliang Liu"],"pdf_url":"","comment":"Neurips 2025"},{"id":"http://arxiv.org/abs/2508.15553v2","updated":"2025-12-22T07:57:02Z","published":"2025-08-21T13:35:11Z","title":"Deep Equilibrium Convolutional Sparse Coding for Hyperspectral Image Denoising","summary":"Hyperspectral images (HSIs) play a crucial role in remote sensing but are often degraded by complex noise patterns. Ensuring the physical property of the denoised HSIs is vital for robust HSI denoising, giving the rise of deep unfolding-based methods. However, these methods map the optimization of a physical model to a learnable network with a predefined depth, which lacks convergence guarantees. In contrast, Deep Equilibrium (DEQ) models treat the hidden layers of deep networks as the solution to a fixed-point problem and models them as infinite-depth networks, naturally consistent with the optimization. Under the framework of DEQ, we propose a Deep Equilibrium Convolutional Sparse Coding (DECSC) framework that unifies local spatial-spectral correlations, nonlocal spatial self-similarities, and global spatial consistency for robust HSI denoising. Within the convolutional sparse coding (CSC) framework, we enforce shared 2D convolutional sparse representation to ensure global spatial consistency across bands, while unshared 3D convolutional sparse representation captures local spatial-spectral details. To further exploit nonlocal self-similarities, a transformer block is embedded after the 2D CSC. Additionally, a detail enhancement module is integrated with the 3D CSC to promote image detail preservation. We formulate the proximal gradient descent of the CSC model as a fixed-point problem and transform the iterative updates into a learnable network architecture within the framework of DEQ. Experimental results demonstrate that our DECSC method achieves superior denoising performance compared to state-of-the-art methods.","authors":["Jin Ye","Jingran Wang","Fengchao Xiong","Jingzhou Chen","Yuntao Qian"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.15647v2","updated":"2025-12-22T07:47:31Z","published":"2025-12-17T17:54:20Z","title":"Hard Labels In! Rethinking the Role of Hard Labels in Mitigating Local Semantic Drift","summary":"Soft labels generated by teacher models have become a dominant paradigm for knowledge transfer and recent large-scale dataset distillation such as SRe2L, RDED, LPLD, offering richer supervision than conventional hard labels. However, we observe that when only a limited number of crops per image are used, soft labels are prone to local semantic drift: a crop may visually resemble another class, causing its soft embedding to deviate from the ground-truth semantics of the original image. This mismatch between local visual content and global semantic meaning introduces systematic errors and distribution misalignment between training and testing. In this work, we revisit the overlooked role of hard labels and show that, when appropriately integrated, they provide a powerful content-agnostic anchor to calibrate semantic drift. We theoretically characterize the emergence of drift under few soft-label supervision and demonstrate that hybridizing soft and hard labels restores alignment between visual content and semantic supervision. Building on this insight, we propose a new training paradigm, Hard Label for Alleviating Local Semantic Drift (HALD), which leverages hard labels as intermediate corrective signals while retaining the fine-grained advantages of soft labels. Extensive experiments on dataset distillation and large-scale conventional classification benchmarks validate our approach, showing consistent improvements in generalization. On ImageNet-1K, we achieve 42.7% with only 285M storage for soft labels, outperforming prior state-of-the-art LPLD by 9.0%. Our findings re-establish the importance of hard labels as a complementary tool, and call for a rethinking of their role in soft-label-dominated training.","authors":["Jiacheng Cui","Bingkui Tong","Xinyue Bi","Xiaohan Zhao","Jiacheng Liu","Zhiqiang Shen"],"pdf_url":"","comment":"Code at: https://github.com/Jiacheng8/HALD"},{"id":"http://arxiv.org/abs/2512.19115v1","updated":"2025-12-22T07:36:20Z","published":"2025-12-22T07:36:20Z","title":"Generative Giants, Retrieval Weaklings: Why do Multimodal Large Language Models Fail at Multimodal Retrieval?","summary":"Despite the remarkable success of multimodal large language models (MLLMs) in generative tasks, we observe that they exhibit a counterintuitive deficiency in the zero-shot multimodal retrieval task. In this work, we investigate the underlying mechanisms that hinder MLLMs from serving as effective retrievers. With the help of sparse autoencoders (SAEs), we decompose MLLM output representations into interpretable semantic concepts to probe their intrinsic behavior. Our analysis reveals that the representation space of MLLMs is overwhelmingly dominated by textual semantics; the visual information essential for multimodal retrieval only constitutes a small portion. This imbalance is compounded by the heavy focus of MLLMs on bridging image-text modalities, which facilitates generation but homogenizes embeddings and finally diminishes the discriminative power required for multimodal retrieval. We further discover that the specific feature components that contribute most to the similarity computations for MLLMs are in fact distractors that actively degrade retrieval performance. Overall, our work provides the first in-depth interpretability analysis of MLLM representations in the context of multimodal retrieval and offers possible directions for enhancing the multimodal retrieval capabilities of MLLMs.","authors":["Hengyi Feng","Zeang Sheng","Meiyi Qiang","Wentao Zhang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19110v1","updated":"2025-12-22T07:26:40Z","published":"2025-12-22T07:26:40Z","title":"Trifocal Tensor and Relative Pose Estimation with Known Vertical Direction","summary":"This work presents two novel solvers for estimating the relative poses among views with known vertical directions. The vertical directions of camera views can be easily obtained using inertial measurement units (IMUs) which have been widely used in autonomous vehicles, mobile phones, and unmanned aerial vehicles (UAVs). Given the known vertical directions, our lgorithms only need to solve for two rotation angles and two translation vectors. In this paper, a linear closed-form solution has been described, requiring only four point correspondences in three views. We also propose a minimal solution with three point correspondences using the latest Gröbner basis solver. Since the proposed methods require fewer point correspondences, they can be efficiently applied within the RANSAC framework for outliers removal and pose estimation in visual odometry. The proposed method has been tested on both synthetic data and real-world scenes from KITTI. The experimental results show that the accuracy of the estimated poses is superior to other alternative methods.","authors":["Tao Li","Zhenbao Yu","Banglei Guan","Jianli Han","Weimin Lv","Friedrich Fraundorfer"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19108v1","updated":"2025-12-22T07:22:39Z","published":"2025-12-22T07:22:39Z","title":"GaussianImage++: Boosted Image Representation and Compression with 2D Gaussian Splatting","summary":"Implicit neural representations (INRs) have achieved remarkable success in image representation and compression, but they require substantial training time and memory. Meanwhile, recent 2D Gaussian Splatting (GS) methods (\\textit{e.g.}, GaussianImage) offer promising alternatives through efficient primitive-based rendering. However, these methods require excessive Gaussian primitives to maintain high visual fidelity. To exploit the potential of GS-based approaches, we present GaussianImage++, which utilizes limited Gaussian primitives to achieve impressive representation and compression performance. Firstly, we introduce a distortion-driven densification mechanism. It progressively allocates Gaussian primitives according to signal intensity. Secondly, we employ context-aware Gaussian filters for each primitive, which assist in the densification to optimize Gaussian primitives based on varying image content. Thirdly, we integrate attribute-separated learnable scalar quantizers and quantization-aware training, enabling efficient compression of primitive attributes. Experimental results demonstrate the effectiveness of our method. In particular, GaussianImage++ outperforms GaussianImage and INRs-based COIN in representation and compression performance while maintaining real-time decoding and low memory usage.","authors":["Tiantian Li","Xinjie Zhang","Xingtong Ge","Tongda Xu","Dailan He","Jun Zhang","Yan Wang"],"pdf_url":"","comment":"Accepted to AAAI 2026.Code URL:https://github.com/Sweethyh/GaussianImage_plus.git"},{"id":"http://arxiv.org/abs/2507.02314v4","updated":"2025-12-22T07:17:17Z","published":"2025-07-03T04:54:37Z","title":"MAGIC: Few-Shot Mask-Guided Anomaly Inpainting with Prompt Perturbation, Spatially Adaptive Guidance, and Context Awareness","summary":"Few-shot anomaly generation is a key challenge in industrial quality control. Although diffusion models are promising, existing methods struggle: global prompt-guided approaches corrupt normal regions, and existing inpainting-based methods often lack the in-distribution diversity essential for robust downstream models. We propose MAGIC, a fine-tuned inpainting framework that generates high-fidelity anomalies that strictly adhere to the mask while maximizing this diversity. MAGIC introduces three complementary components: (i) Gaussian prompt perturbation, which prevents model overfitting in the few-shot setting by learning and sampling from a smooth manifold of realistic anomalies, (ii) spatially adaptive guidance that applies distinct guidance strengths to the anomaly and background regions, and (iii) context-aware mask alignment to relocate masks for plausible placement within the host object. Under consistent identical evaluation protocol, MAGIC outperforms state-of-the-art methods on diverse anomaly datasets in downstream tasks","authors":["JaeHyuck Choi","MinJun Kim","Je Hyeong Hong"],"pdf_url":"","comment":"46 pages, 47 figures. Code: https://github.com/SpatialAILab/MAGIC-Anomaly-generation"},{"id":"http://arxiv.org/abs/2512.19095v1","updated":"2025-12-22T07:06:34Z","published":"2025-12-22T07:06:34Z","title":"Mamba-Based Modality Disentanglement Network for Multi-Contrast MRI Reconstruction","summary":"Magnetic resonance imaging (MRI) is a cornerstone of modern clinical diagnosis, offering unparalleled soft-tissue contrast without ionizing radiation. However, prolonged scan times remain a major barrier to patient throughput and comfort. Existing accelerated MRI techniques often struggle with two key challenges: (1) failure to effectively utilize inherent K-space prior information, leading to persistent aliasing artifacts from zero-filled inputs; and (2) contamination of target reconstruction quality by irrelevant information when employing multi-contrast fusion strategies. To overcome these challenges, we present MambaMDN, a dual-domain framework for multi-contrast MRI reconstruction. Our approach first employs fully-sampled reference K-space data to complete the undersampled target data, generating structurally aligned but modality-mixed inputs. Subsequently, we develop a Mamba-based modality disentanglement network to extract and remove reference-specific features from the mixed representation. Furthermore, we introduce an iterative refinement mechanism to progressively enhance reconstruction accuracy through repeated feature purification. Extensive experiments demonstrate that MambaMDN can significantly outperform existing multi-contrast reconstruction methods.","authors":["Weiyi Lyu","Xinming Fang","Jun Wang","Jun Shi","Guixu Zhang","Juncheng Li"],"pdf_url":"","comment":"12 pages, 11 figures, 6 tables"},{"id":"http://arxiv.org/abs/2512.19091v1","updated":"2025-12-22T07:00:49Z","published":"2025-12-22T07:00:49Z","title":"Auditing Significance, Metric Choice, and Demographic Fairness in Medical AI Challenges","summary":"Open challenges have become the de facto standard for comparative ranking of medical AI methods. Despite their importance, medical AI leaderboards exhibit three persistent limitations: (1) score gaps are rarely tested for statistical significance, so rank stability is unknown; (2) single averaged metrics are applied to every organ, hiding clinically important boundary errors; (3) performance across intersecting demographics is seldom reported, masking fairness and equity gaps. We introduce RankInsight, an open-source toolkit that seeks to address these limitations. RankInsight (1) computes pair-wise significance maps that show the nnU-Net family outperforms Vision-Language and MONAI submissions with high statistical certainty; (2) recomputes leaderboards with organ-appropriate metrics, reversing the order of the top four models when Dice is replaced by NSD for tubular structures; and (3) audits intersectional fairness, revealing that more than half of the MONAI-based entries have the largest gender-race discrepancy on our proprietary Johns Hopkins Hospital dataset. The RankInsight toolkit is publicly released and can be directly applied to past, ongoing, and future challenges. It enables organizers and participants to publish rankings that are statistically sound, clinically meaningful, and demographically fair.","authors":["Ariel Lubonja","Pedro R. A. S. Bassi","Wenxuan Li","Hualin Qiao","Randal Burns","Alan L. Yuille","Zongwei Zhou"],"pdf_url":"","comment":"MICCAI 2025 Workshop on Machine Learning in Medical Imaging"},{"id":"http://arxiv.org/abs/2512.19088v1","updated":"2025-12-22T06:57:42Z","published":"2025-12-22T06:57:42Z","title":"Retrieving Objects from 3D Scenes with Box-Guided Open-Vocabulary Instance Segmentation","summary":"Locating and retrieving objects from scene-level point clouds is a challenging problem with broad applications in robotics and augmented reality. This task is commonly formulated as open-vocabulary 3D instance segmentation. Although recent methods demonstrate strong performance, they depend heavily on SAM and CLIP to generate and classify 3D instance masks from images accompanying the point cloud, leading to substantial computational overhead and slow processing that limit their deployment in real-world settings. Open-YOLO 3D alleviates this issue by using a real-time 2D detector to classify class-agnostic masks produced directly from the point cloud by a pretrained 3D segmenter, eliminating the need for SAM and CLIP and significantly reducing inference time. However, Open-YOLO 3D often fails to generalize to object categories that appear infrequently in the 3D training data. In this paper, we propose a method that generates 3D instance masks for novel objects from RGB images guided by a 2D open-vocabulary detector. Our approach inherits the 2D detector's ability to recognize novel objects while maintaining efficient classification, enabling fast and accurate retrieval of rare instances from open-ended text queries. Our code will be made available at https://github.com/ndkhanh360/BoxOVIS.","authors":["Khanh Nguyen","Dasith de Silva Edirimuni","Ghulam Mubashar Hassan","Ajmal Mian"],"pdf_url":"","comment":"Accepted to AAAI 2026 Workshop on New Frontiers in Information Retrieval"},{"id":"http://arxiv.org/abs/2504.04034v2","updated":"2025-12-22T06:52:02Z","published":"2025-04-05T03:05:04Z","title":"UCS: A Universal Model for Curvilinear Structure Segmentation","summary":"Curvilinear structure segmentation (CSS) is essential in various domains, including medical imaging, landscape analysis, industrial surface inspection, and plant analysis. While existing methods achieve high performance within specific domains, their generalizability is limited. On the other hand, large-scale models such as Segment Anything Model (SAM) exhibit strong generalization but are not optimized for curvilinear structures. Existing adaptations of SAM primarily focus on general object segmentation and lack specialized design for CSS tasks. To bridge this gap, we propose the Universal Curvilinear structure Segmentation (UCS) model, which adapts SAM to CSS tasks while further enhancing its cross-domain generalization. UCS features a novel encoder architecture integrating a pretrained SAM encoder with two innovations: a Sparse Adapter, strategically inserted to inherit the pre-trained SAM encoder's generalization capability while minimizing the number of fine-tuning parameters, and a Prompt Generation module, which leverages Fast Fourier Transform with a high-pass filter to generate curve-specific prompts. Furthermore, the UCS incorporates a mask decoder that eliminates reliance on manual interaction through a dual-compression module: a Hierarchical Feature Compression module, which aggregates the outputs of the sampled encoder to enhance detail preservation, and a Guidance Feature Compression module, which extracts and compresses image-driven guidance features. Evaluated on a comprehensive multi-domain dataset, including an in-house dataset covering eight natural curvilinear structures, UCS demonstrates state-of-the-art generalization and open-set segmentation performance across medical, engineering, natural, and plant imagery, establishing a new benchmark for universal CSS. The source code is available at https://github.com/kylechuuuuu/UCS.","authors":["Kai Zhu","Li Chen","Dianshuo Li","Yunxiang Cao","Jun Cheng"],"pdf_url":"","comment":"13 pages, 13 figures"},{"id":"http://arxiv.org/abs/2512.19070v1","updated":"2025-12-22T06:20:53Z","published":"2025-12-22T06:20:53Z","title":"Watch Closely: Mitigating Object Hallucinations in Large Vision-Language Models with Disentangled Decoding","summary":"Large Vision-Language Models (LVLMs) bridge the gap between visual and linguistic modalities, demonstrating strong potential across a variety of domains. However, despite significant progress, LVLMs still suffer from severe hallucination issues in object recognition tasks. These models often fail to accurately identify certain objects, leading to text generation that appears fluent but does not correspond to the visual content, which can have serious consequences in real-world applications. Recently, several methods have been proposed to alleviate LVLM hallucinations, but most focus solely on reducing hallucinations in the language modality. To mitigate hallucinations in both the language and visual modalities, we introduce Hallucination Disentangled Decoding (HDD) method that requires no training. HDD enhances the original image by segmenting it and selecting images that augment the original, while also utilizing a blank image to eliminate language prior hallucinations in both the original and segmented images. This design not only reduces the model's dependence on language priors but also enhances its visual performance. (Code: https://github.com/rickeyhhh/Hallucination-Disentangled-Decoding)","authors":["Ruiqi Ma","Yu Yan","Chunhong Zhang","Minghao Yin","XinChao Liu","Zhihong Jin","Zheng Hu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19058v1","updated":"2025-12-22T05:49:57Z","published":"2025-12-22T05:49:57Z","title":"6DAttack: Backdoor Attacks in the 6DoF Pose Estimation","summary":"Deep learning advances have enabled accurate six-degree-of-freedom (6DoF) object pose estimation, widely used in robotics, AR/VR, and autonomous systems. However, backdoor attacks pose significant security risks. While most research focuses on 2D vision, 6DoF pose estimation remains largely unexplored. Unlike traditional backdoors that only change classes, 6DoF attacks must control continuous parameters like translation and rotation, rendering 2D methods inapplicable. We propose 6DAttack, a framework using 3D object triggers to induce controlled erroneous poses while maintaining normal behavior. Evaluations on PVNet, DenseFusion, and PoseDiffusion across LINEMOD, YCB-Video, and CO3D show high attack success rates (ASRs) without compromising clean performance. Backdoored models achieve up to 100% clean ADD accuracy and 100% ASR, with triggered samples reaching 97.70% ADD-P. Furthermore, a representative defense remains ineffective. Our findings reveal a serious, underexplored threat to 6DoF pose estimation.","authors":["Jihui Guo","Zongmin Zhang","Zhen Sun","Yuhao Yang","Jinlin Wu","Fu Zhang","Xinlei He"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19048v1","updated":"2025-12-22T05:33:59Z","published":"2025-12-22T05:33:59Z","title":"WaTeRFlow: Watermark Temporal Robustness via Flow Consistency","summary":"Image watermarking supports authenticity and provenance, yet many schemes are still easy to bypass with various distortions and powerful generative edits. Deep learning-based watermarking has improved robustness to diffusion-based image editing, but a gap remains when a watermarked image is converted to video by image-to-video (I2V), in which per-frame watermark detection weakens. I2V has quickly advanced from short, jittery clips to multi-second, temporally coherent scenes, and it now serves not only content creation but also world-modeling and simulation workflows, making cross-modal watermark recovery crucial. We present WaTeRFlow, a framework tailored for robustness under I2V. It consists of (i) FUSE (Flow-guided Unified Synthesis Engine), which exposes the encoder-decoder to realistic distortions via instruction-driven edits and a fast video diffusion proxy during training, (ii) optical-flow warping with a Temporal Consistency Loss (TCL) that stabilizes per-frame predictions, and (iii) a semantic preservation loss that maintains the conditioning signal. Experiments across representative I2V models show accurate watermark recovery from frames, with higher first-frame and per-frame bit accuracy and resilience when various distortions are applied before or after video generation.","authors":["Utae Jeong","Sumin In","Hyunju Ryu","Jaewan Choi","Feng Yang","Jongheon Jeong","Seungryong Kim","Sangpil Kim"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19049v1","updated":"2025-12-22T05:33:59Z","published":"2025-12-22T05:33:59Z","title":"Decoupled Generative Modeling for Human-Object Interaction Synthesis","summary":"Synthesizing realistic human-object interaction (HOI) is essential for 3D computer vision and robotics, underpinning animation and embodied control. Existing approaches often require manually specified intermediate waypoints and place all optimization objectives on a single network, which increases complexity, reduces flexibility, and leads to errors such as unsynchronized human and object motion or penetration. To address these issues, we propose Decoupled Generative Modeling for Human-Object Interaction Synthesis (DecHOI), which separates path planning and action synthesis. A trajectory generator first produces human and object trajectories without prescribed waypoints, and an action generator conditions on these paths to synthesize detailed motions. To further improve contact realism, we employ adversarial training with a discriminator that focuses on the dynamics of distal joints. The framework also models a moving counterpart and supports responsive, long-sequence planning in dynamic scenes, while preserving plan consistency. Across two benchmarks, FullBodyManipulation and 3D-FUTURE, DecHOI surpasses prior methods on most quantitative metrics and qualitative evaluations, and perceptual studies likewise prefer our results.","authors":["Hwanhee Jung","Seunggwan Lee","Jeongyoon Yoon","SeungHyeon Kim","Giljoo Nam","Qixing Huang","Sangpil Kim"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19036v1","updated":"2025-12-22T05:13:58Z","published":"2025-12-22T05:13:58Z","title":"Distinguishing Visually Similar Actions: Prompt-Guided Semantic Prototype Modulation for Few-Shot Action Recognition","summary":"Few-shot action recognition aims to enable models to quickly learn new action categories from limited labeled samples, addressing the challenge of data scarcity in real-world applications. Current research primarily addresses three core challenges: (1) temporal modeling, where models are prone to interference from irrelevant static background information and struggle to capture the essence of dynamic action features; (2) visual similarity, where categories with subtle visual differences are difficult to distinguish; and (3) the modality gap between visual-textual support prototypes and visual-only queries, which complicates alignment within a shared embedding space. To address these challenges, this paper proposes a CLIP-SPM framework, which includes three components: (1) the Hierarchical Synergistic Motion Refinement (HSMR) module, which aligns deep and shallow motion features to improve temporal modeling by reducing static background interference; (2) the Semantic Prototype Modulation (SPM) strategy, which generates query-relevant text prompts to bridge the modality gap and integrates them with visual features, enhancing the discriminability between similar actions; and (3) the Prototype-Anchor Dual Modulation (PADM) method, which refines support prototypes and aligns query features with a global semantic anchor, improving consistency across support and query samples. Comprehensive experiments across standard benchmarks, including Kinetics, SSv2-Full, SSv2-Small, UCF101, and HMDB51, demonstrate that our CLIP-SPM achieves competitive performance under 1-shot, 3-shot, and 5-shot settings. Extensive ablation studies and visual analyses further validate the effectiveness of each component and its contributions to addressing the core challenges. The source code and models are publicly available at GitHub.","authors":["Xiaoyang Li","Mingming Lu","Ruiqi Wang","Hao Li","Zewei Le"],"pdf_url":"","comment":"19 pages, 7 figures. Preprint under review for journal submission"},{"id":"http://arxiv.org/abs/2512.19032v1","updated":"2025-12-22T05:08:52Z","published":"2025-12-22T05:08:52Z","title":"Automatic Neuronal Activity Segmentation in Fast Four Dimensional Spatio-Temporal Fluorescence Imaging using Bayesian Approach","summary":"Fluorescence Microcopy Calcium Imaging is a fundamental tool to in-vivo record and analyze large scale neuronal activities simultaneously at a single cell resolution. Automatic and precise detection of behaviorally relevant neuron activity from the recordings is critical to study the mapping of brain activity in organisms. However a perpetual bottleneck to this problem is the manual segmentation which is time and labor intensive and lacks generalizability. To this end, we present a Bayesian Deep Learning Framework to detect neuronal activities in 4D spatio-temporal data obtained by light sheet microscopy. Our approach accounts for the use of temporal information by calculating pixel wise correlation maps and combines it with spatial information given by the mean summary image. The Bayesian framework not only produces probability segmentation maps but also models the uncertainty pertaining to active neuron detection. To evaluate the accuracy of our framework we implemented the test of reproducibility to assert the generalization of the network to detect neuron activity. The network achieved a mean Dice Score of 0.81 relative to the synthetic Ground Truth obtained by Otsu's method and a mean Dice Score of 0.79 between the first and second run for test of reproducibility. Our method successfully deployed can be used for rapid detection of active neuronal activities for behavioural studies.","authors":["Ran Li","Pan Xiao","Kaushik Dutta","Youdong Guo"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2507.09885v3","updated":"2025-12-22T05:02:30Z","published":"2025-07-14T03:46:06Z","title":"MCGA: Mixture of Codebooks Hyperspectral Reconstruction via Grayscale-Aware Attention","summary":"Reconstructing hyperspectral images (HSIs) from RGB inputs provides a cost-effective alternative to hyperspectral cameras, but reconstructing high-dimensional spectra from three channels is inherently ill-posed. Existing methods typically directly regress RGB-to-HSI mappings using large attention networks, which are computationally expensive and handle ill-posedness only implicitly. We propose MCGA, a Mixture-of-Codebooks with Grayscale-aware Attention framework that explicitly addresses these challenges using spectral priors and photometric consistency. MCGA first learns transferable spectral priors via a mixture-of-codebooks (MoC) from heterogeneous HSI datasets, then aligns RGB features with these priors through grayscale-aware photometric attention (GANet). Efficiency and robustness are further improved via top-K attention design and test-time adaptation (TTA). Experiments on multiple real-world benchmarks demonstrate the state-of-the-art accuracy, strong cross-dataset generalization, and 4-5x faster inference. Codes will be available once acceptance at https://github.com/Fibonaccirabbit/MCGA.","authors":["Zhanjiang Yang","Lijun Sun","Jiawei Dong","Xiaoxin An","Yang Liu","Meng Li"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2508.19291v2","updated":"2025-12-22T05:01:02Z","published":"2025-08-25T13:04:04Z","title":"Modeling spectral filtering effects on color-matching functions: Implications for observer variability","summary":"This study investigates the impact of spectral filtering on color-matching functions (CMFs) and its implications for observer variability modeling. We conducted color matching experiments with two observers, both with and without a spectral filter in front of a bipartite field. Using a novel computational approach, we estimated the filter transmittance and transformation matrix necessary to convert unfiltered CMFs to filtered CMFs. Statistical analysis revealed good agreement between estimated and measured filter characteristics, particularly in central wavelength regions. Applying this methodology to compare between Stiles and Burch 1955 (SB1955) mean observer CMFs and our previously published \"ICVIO\" mean observer CMFs, we identified a \"yellow\" (short-wavelength suppressing) filter that effectively transforms between these datasets. This finding aligns with our hypothesis that observed differences between the CMF sets are attributable to age-related lens yellowing (average observer age: 49 years in ICVIO versus 30 years in SB1955). Our approach enables efficient representation of observer variability through a single filter rather than three separate functions, offering potentially reduced experimental overhead while maintaining accuracy in characterizing individual color vision differences.","authors":["Luvin Munish Ragoo","Ivar Farup","Casper F. Andersen","Graham Finlayson"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19026v1","updated":"2025-12-22T04:53:40Z","published":"2025-12-22T04:53:40Z","title":"Finer-Personalization Rank: Fine-Grained Retrieval Examines Identity Preservation for Personalized Generation","summary":"The rise of personalized generative models raises a central question: how should we evaluate identity preservation? Given a reference image (e.g., one's pet), we expect the generated image to retain precise details attached to the subject's identity. However, current generative evaluation metrics emphasize the overall semantic similarity between the reference and the output, and overlook these fine-grained discriminative details. We introduce Finer-Personalization Rank, an evaluation protocol tailored to identity preservation. Instead of pairwise similarity, Finer-Personalization Rank adopts a ranking view: it treats each generated image as a query against an identity-labeled gallery consisting of visually similar real images. Retrieval metrics (e.g., mean average precision) measure performance, where higher scores indicate that identity-specific details (e.g., a distinctive head spot) are preserved. We assess identity at multiple granularities -- from fine-grained categories (e.g., bird species, car models) to individual instances (e.g., re-identification). Across CUB, Stanford Cars, and animal Re-ID benchmarks, Finer-Personalization Rank more faithfully reflects identity retention than semantic-only metrics and reveals substantial identity drift in several popular personalization methods. These results position the gallery-based protocol as a principled and practical evaluation for personalized generation.","authors":["Connor Kilrain","David Carlyn","Julia Chae","Sara Beery","Wei-Lun Chao","Jianyang Gu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2412.05277v2","updated":"2025-12-22T04:43:31Z","published":"2024-12-06T18:59:51Z","title":"Text to Blind Motion","summary":"People who are blind perceive the world differently than those who are sighted, which can result in distinct motion characteristics. For instance, when crossing at an intersection, blind individuals may have different patterns of movement, such as veering more from a straight path or using touch-based exploration around curbs and obstacles. These behaviors may appear less predictable to motion models embedded in technologies such as autonomous vehicles. Yet, the ability of 3D motion models to capture such behavior has not been previously studied, as existing datasets for 3D human motion currently lack diversity and are biased toward people who are sighted. In this work, we introduce BlindWays, the first multimodal motion benchmark for pedestrians who are blind. We collect 3D motion data using wearable sensors with 11 blind participants navigating eight different routes in a real-world urban setting. Additionally, we provide rich textual descriptions that capture the distinctive movement characteristics of blind pedestrians and their interactions with both the navigation aid (e.g., a white cane or a guide dog) and the environment. We benchmark state-of-the-art 3D human prediction models, finding poor performance with off-the-shelf and pre-training-based methods for our novel task. To contribute toward safer and more reliable systems that can seamlessly reason over diverse human movements in their environments, our text-and-motion benchmark is available at https://blindways.github.io.","authors":["Hee Jae Kim","Kathakoli Sengupta","Masaki Kuribayashi","Hernisa Kacorri","Eshed Ohn-Bar"],"pdf_url":"","comment":"Accepted at NeurIPS 2024"},{"id":"http://arxiv.org/abs/2510.11496v3","updated":"2025-12-22T04:40:04Z","published":"2025-10-13T15:04:38Z","title":"AndesVL Technical Report: An Efficient Mobile-side Multimodal Large Language Model","summary":"In recent years, while cloud-based MLLMs such as QwenVL, InternVL, GPT-4o, Gemini, and Claude Sonnet have demonstrated outstanding performance with enormous model sizes reaching hundreds of billions of parameters, they significantly surpass the limitations in memory, power consumption, and computing capacity of edge devices such as mobile phones. This paper introduces AndesVL, a suite of mobile-side MLLMs with 0.6B to 4B parameters based on Qwen3's LLM and various visual encoders. We comprehensively outline the model architectures, training pipeline, and training data of AndesVL, which achieves first-tier performance across a wide range of open-source benchmarks, including fields such as text-rich image understanding, reasoning and math, multi-image comprehension, general VQA, hallucination mitigation, multilingual understanding, and GUI-related tasks when compared with state-of-the-art models of a similar scale. Furthermore, we introduce a 1+N LoRA architecture alongside a Quantization-Aware LoRA Fine-Tuning (QALFT) framework to facilitate efficient task adaptation and model compression during mobile-side deployment of AndesVL. Moreover, utilizing our cache eviction algorithm -- OKV -- along with customized speculative decoding and compression strategies, we achieve a 6.7x peak decoding speedup ratio, up to 30.9% memory reduction, and 1.8 bits-per-weight when deploying AndesVL-4B on MediaTek Dimensity 9500 chips. We release all models on https://huggingface.co/OPPOer.","authors":["Zhiwei Jin","Xiaohui Song","Nan Wang","Yafei Liu","Chao Li","Xin Li","Ruichen Wang","Zhihao Li","Qi Qi","Long Cheng","Dongze Hao","Quanlong Zheng","Yanhao Zhang","Haobo Ji","Jian Ma","Zhitong Zheng","Zhenyi Lin","Haolin Deng","Xin Zou","Xiaojie Yin","Ruilin Wang","Liankai Cai","Haijing Liu","Yuqing Qiu","Ke Chen","Zixian Li","Chi Xie","Huafei Li","Chenxing Li","Chuangchuang Wang","Kai Tang","Zhiguang Zhu","Kai Tang","Wenmei Gao","Rui Wang","Jun Wu","Chao Liu","Qin Xie","Chen Chen","Haonan Lu"],"pdf_url":"","comment":"Tech report of OPPO AndesVL Team"},{"id":"http://arxiv.org/abs/2512.19022v1","updated":"2025-12-22T04:30:11Z","published":"2025-12-22T04:30:11Z","title":"Steering Vision-Language Pre-trained Models for Incremental Face Presentation Attack Detection","summary":"Face Presentation Attack Detection (PAD) demands incremental learning (IL) to combat evolving spoofing tactics and domains. Privacy regulations, however, forbid retaining past data, necessitating rehearsal-free IL (RF-IL). Vision-Language Pre-trained (VLP) models, with their prompt-tunable cross-modal representations, enable efficient adaptation to new spoofing styles and domains. Capitalizing on this strength, we propose \\textbf{SVLP-IL}, a VLP-based RF-IL framework that balances stability and plasticity via \\textit{Multi-Aspect Prompting} (MAP) and \\textit{Selective Elastic Weight Consolidation} (SEWC). MAP isolates domain dependencies, enhances distribution-shift sensitivity, and mitigates forgetting by jointly exploiting universal and domain-specific cues. SEWC selectively preserves critical weights from previous tasks, retaining essential knowledge while allowing flexibility for new adaptations. Comprehensive experiments across multiple PAD benchmarks show that SVLP-IL significantly reduces catastrophic forgetting and enhances performance on unseen domains. SVLP-IL offers a privacy-compliant, practical solution for robust lifelong PAD deployment in RF-IL settings.","authors":["Haoze Li","Jie Zhang","Guoying Zhao","Stephen Lin","Shiguang Shan"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19021v1","updated":"2025-12-22T04:27:26Z","published":"2025-12-22T04:27:26Z","title":"VLNVerse: A Benchmark for Vision-Language Navigation with Versatile, Embodied, Realistic Simulation and Evaluation","summary":"Despite remarkable progress in Vision-Language Navigation (VLN), existing benchmarks remain confined to fixed, small-scale datasets with naive physical simulation. These shortcomings limit the insight that the benchmarks provide into sim-to-real generalization, and create a significant research gap. Furthermore, task fragmentation prevents unified/shared progress in the area, while limited data scales fail to meet the demands of modern LLM-based pretraining. To overcome these limitations, we introduce VLNVerse: a new large-scale, extensible benchmark designed for Versatile, Embodied, Realistic Simulation, and Evaluation. VLNVerse redefines VLN as a scalable, full-stack embodied AI problem. Its Versatile nature unifies previously fragmented tasks into a single framework and provides an extensible toolkit for researchers. Its Embodied design moves beyond intangible and teleporting \"ghost\" agents that support full-kinematics in a Realistic Simulation powered by a robust physics engine. We leverage the scale and diversity of VLNVerse to conduct a comprehensive Evaluation of existing methods, from classic models to MLLM-based agents. We also propose a novel unified multi-task model capable of addressing all tasks within the benchmark. VLNVerse aims to narrow the gap between simulated navigation and real-world generalization, providing the community with a vital tool to boost research towards scalable, general-purpose embodied locomotion agents.","authors":["Sihao Lin","Zerui Li","Xunyi Zhao","Gengze Zhou","Liuyi Wang","Rong Wei","Rui Tang","Juncheng Li","Hanqing Wang","Jiangmiao Pang","Anton van den Hengel","Jiajun Liu","Qi Wu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19020v1","updated":"2025-12-22T04:21:39Z","published":"2025-12-22T04:21:39Z","title":"CETCAM: Camera-Controllable Video Generation via Consistent and Extensible Tokenization","summary":"Achieving precise camera control in video generation remains challenging, as existing methods often rely on camera pose annotations that are difficult to scale to large and dynamic datasets and are frequently inconsistent with depth estimation, leading to train-test discrepancies. We introduce CETCAM, a camera-controllable video generation framework that eliminates the need for camera annotations through a consistent and extensible tokenization scheme. CETCAM leverages recent advances in geometry foundation models, such as VGGT, to estimate depth and camera parameters and converts them into unified, geometry-aware tokens. These tokens are seamlessly integrated into a pretrained video diffusion backbone via lightweight context blocks. Trained in two progressive stages, CETCAM first learns robust camera controllability from diverse raw video data and then refines fine-grained visual quality using curated high-fidelity datasets. Extensive experiments across multiple benchmarks demonstrate state-of-the-art geometric consistency, temporal stability, and visual realism. Moreover, CETCAM exhibits strong adaptability to additional control modalities, including inpainting and layout control, highlighting its flexibility beyond camera control. The project page is available at https://sjtuytc.github.io/CETCam_project_page.github.io/.","authors":["Zelin Zhao","Xinyu Gong","Bangya Liu","Ziyang Song","Jun Zhang","Suhui Wu","Yongxin Chen","Hao Zhang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.06802v3","updated":"2025-12-22T04:06:01Z","published":"2025-12-07T11:31:00Z","title":"VDOT: Efficient Unified Video Creation via Optimal Transport Distillation","summary":"The rapid development of generative models has significantly advanced image and video applications. Among these, video creation, aimed at generating videos under various conditions, has gained substantial attention. However, existing video creation models either focus solely on a few specific conditions or suffer from excessively long generation times due to complex model inference, making them impractical for real-world applications. To mitigate these issues, we propose an efficient unified video creation model, named VDOT. Concretely, we model the training process with the distribution matching distillation (DMD) paradigm. Instead of using the Kullback-Leibler (KL) minimization, we additionally employ a novel computational optimal transport (OT) technique to optimize the discrepancy between the real and fake score distributions. The OT distance inherently imposes geometric constraints, mitigating potential zero-forcing or gradient collapse issues that may arise during KL-based distillation within the few-step generation scenario, and thus, enhances the efficiency and stability of the distillation process. Further, we integrate a discriminator to enable the model to perceive real video data, thereby enhancing the quality of generated videos. To support training unified video creation models, we propose a fully automated pipeline for video data annotation and filtering that accommodates multiple video creation tasks. Meanwhile, we curate a unified testing benchmark, UVCBench, to standardize evaluation. Experiments demonstrate that our 4-step VDOT outperforms or matches other baselines with 100 denoising steps.","authors":["Yutong Wang","Haiyu Zhang","Tianfan Xue","Yu Qiao","Yaohui Wang","Chang Xu","Xinyuan Chen"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.18994v1","updated":"2025-12-22T03:20:05Z","published":"2025-12-22T03:20:05Z","title":"Towards AI-Guided Open-World Ecological Taxonomic Classification","summary":"AI-guided classification of ecological families, genera, and species underpins global sustainability efforts such as biodiversity monitoring, conservation planning, and policy-making. Progress toward this goal is hindered by long-tailed taxonomic distributions from class imbalance, along with fine-grained taxonomic variations, test-time spatiotemporal domain shifts, and closed-set assumptions that can only recognize previously seen taxa. We introduce the Open-World Ecological Taxonomy Classification, a unified framework that captures the co-occurrence of these challenges in realistic ecological settings. To address them, we propose TaxoNet, an embedding-based encoder with a dual-margin penalization loss that strengthens learning signals from rare underrepresented taxa while mitigating the dominance of overrepresented ones, directly confronting interrelated challenges. We evaluate our method on diverse ecological domains: Google Auto-Arborist (urban trees), iNat-Plantae (Plantae observations from various ecosystems in iNaturalist-2019), and NAFlora-Mini (a curated herbarium collection). Our model consistently outperforms baselines, particularly for rare taxa, establishing a strong foundation for open-world plant taxonomic monitoring. Our findings further show that general-purpose multimodal foundation models remain constrained in plant-domain applications.","authors":["Cheng Yaw Low","Heejoon Koo","Jaewoo Park","Kaleb Mesfin Asfaw","Meeyoung Cha"],"pdf_url":"","comment":"4 figures, 11 tables, and 15 pages"},{"id":"http://arxiv.org/abs/2402.06118v4","updated":"2025-12-22T03:14:10Z","published":"2024-02-09T01:00:14Z","title":"ViGoR: Improving Visual Grounding of Large Vision Language Models with Fine-Grained Reward Modeling","summary":"By combining natural language understanding, generation capabilities, and breadth of knowledge of large language models with image perception, recent large vision language models (LVLMs) have shown unprecedented visual reasoning capabilities. However, the generated text often suffers from inaccurate grounding in the visual input, resulting in errors such as hallucination of nonexistent scene elements, missing significant parts of the scene, and inferring incorrect attributes of and relationships between objects. To address these issues, we introduce a novel framework, ViGoR (Visual Grounding Through Fine-Grained Reward Modeling) that utilizes fine-grained reward modeling to significantly enhance the visual grounding of LVLMs over pre-trained baselines. This improvement is efficiently achieved using much cheaper human evaluations instead of full supervisions, as well as automated methods. We show the effectiveness of our approach through a variety of evaluation methods and benchmarks. Additionally, we released our human annotation (https://github.com/amazon-science/vigor) comprising 15,440 images and generated text pairs with fine-grained evaluations to contribute to related research in the community.","authors":["Siming Yan","Min Bai","Weifeng Chen","Xiong Zhou","Qixing Huang","Li Erran Li"],"pdf_url":"","comment":"Accepted by ECCV 2024"},{"id":"http://arxiv.org/abs/2512.18991v1","updated":"2025-12-22T03:13:08Z","published":"2025-12-22T03:13:08Z","title":"ICP-4D: Bridging Iterative Closest Point and LiDAR Panoptic Segmentation","summary":"Dominant paradigms for 4D LiDAR panoptic segmentation are usually required to train deep neural networks with large superimposed point clouds or design dedicated modules for instance association. However, these approaches perform redundant point processing and consequently become computationally expensive, yet still overlook the rich geometric priors inherently provided by raw point clouds. To this end, we introduce ICP-4D, a simple yet effective training-free framework that unifies spatial and temporal reasoning through geometric relations among instance-level point sets. Specifically, we apply the Iterative Closest Point (ICP) algorithm to directly associate temporally consistent instances by aligning the source and target point sets through the estimated transformation. To stabilize association under noisy instance predictions, we introduce a Sinkhorn-based soft matching. This exploits the underlying instance distribution to obtain accurate point-wise correspondences, resulting in robust geometric alignment. Furthermore, our carefully designed pipeline, which considers three instance types-static, dynamic, and missing-offers computational efficiency and occlusion-aware matching. Our extensive experiments across both SemanticKITTI and panoptic nuScenes demonstrate that our method consistently outperforms state-of-the-art approaches, even without additional training or extra point cloud inputs.","authors":["Gyeongrok Oh","Youngdong Jang","Jonghyun Choi","Suk-Ju Kang","Guang Lin","Sangpil Kim"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17012v2","updated":"2025-12-22T03:08:53Z","published":"2025-12-18T19:13:44Z","title":"4D-RGPT: Toward Region-level 4D Understanding via Perceptual Distillation","summary":"Despite advances in Multimodal LLMs (MLLMs), their ability to reason over 3D structures and temporal dynamics remains limited, constrained by weak 4D perception and temporal understanding. Existing 3D and 4D Video Question Answering (VQA) benchmarks also emphasize static scenes and lack region-level prompting. We tackle these issues by introducing: (a) 4D-RGPT, a specialized MLLM designed to capture 4D representations from video inputs with enhanced temporal perception; (b) Perceptual 4D Distillation (P4D), a training framework that transfers 4D representations from a frozen expert model into 4D-RGPT for comprehensive 4D perception; and (c) R4D-Bench, a benchmark for depth-aware dynamic scenes with region-level prompting, built via a hybrid automated and human-verified pipeline. Our 4D-RGPT achieves notable improvements on both existing 4D VQA benchmarks and the proposed R4D-Bench benchmark.","authors":["Chiao-An Yang","Ryo Hachiuma","Sifei Liu","Subhashree Radhakrishnan","Raymond A. Yeh","Yu-Chiang Frank Wang","Min-Hung Chen"],"pdf_url":"","comment":"Project page: https://ca-joe-yang.github.io/resource/projects/4D_RGPT"},{"id":"http://arxiv.org/abs/2512.18987v1","updated":"2025-12-22T02:55:25Z","published":"2025-12-22T02:55:25Z","title":"Affordance RAG: Hierarchical Multimodal Retrieval with Affordance-Aware Embodied Memory for Mobile Manipulation","summary":"In this study, we address the problem of open-vocabulary mobile manipulation, where a robot is required to carry a wide range of objects to receptacles based on free-form natural language instructions. This task is challenging, as it involves understanding visual semantics and the affordance of manipulation actions. To tackle these challenges, we propose Affordance RAG, a zero-shot hierarchical multimodal retrieval framework that constructs Affordance-Aware Embodied Memory from pre-explored images. The model retrieves candidate targets based on regional and visual semantics and reranks them with affordance scores, allowing the robot to identify manipulation options that are likely to be executable in real-world environments. Our method outperformed existing approaches in retrieval performance for mobile manipulation instruction in large-scale indoor environments. Furthermore, in real-world experiments where the robot performed mobile manipulation in indoor environments based on free-form instructions, the proposed method achieved a task success rate of 85%, outperforming existing methods in both retrieval performance and overall task success.","authors":["Ryosuke Korekata","Quanting Xie","Yonatan Bisk","Komei Sugiura"],"pdf_url":"","comment":"Accepted to IEEE RA-L, with presentation at ICRA 2026"},{"id":"http://arxiv.org/abs/2504.11734v2","updated":"2025-12-22T02:54:33Z","published":"2025-04-16T03:22:06Z","title":"Recent Advances in 3D Object and Scene Generation: A Survey","summary":"In recent years, the demand for 3D content has grown exponentially with the intelligent upgrade of interactive media, extended reality (XR), and Metaverse industries. In order to overcome the limitations of traditional manual modeling approaches, such as labor-intensive workflows and prolonged production cycles, revolutionary advances have been achieved through the convergence of novel 3D representation paradigms and artificial intelligence generative technologies. In this survey, we conduct a systematic review of the cutting-edge achievements in static 3D object and scene generation, as well as establish a comprehensive technical framework through systematic categorization. We start our analysis with mainstream 3D object representations. Subsequently, we delve into the technical pathways of 3D object generation based on four mainstream deep generative models: Variational Autoencoders, Generative Adversarial Networks, Autoregressive Models, and Diffusion Models. Regarding scene generation, we focus on three dominant paradigms: layout-guided generation, lifting based on 2D priors, and rule-driven modeling. Finally, we critically examine persistent challenges in 3D generation and propose potential research directions for future investigation. This survey aims to provide readers with a structured understanding of state-of-the-art 3D generation technologies while inspiring researchers to undertake more exploration in this domain.","authors":["Xiang Tang","Ruotong Li","Xiaopeng Fan"],"pdf_url":"","comment":"35 pages, 7 figures, 6 tables, Project page: https://github.com/xdlbw/Awesome-3D-Object-and-Scene-Generation"},{"id":"http://arxiv.org/abs/2512.17296v2","updated":"2025-12-22T02:49:56Z","published":"2025-12-19T07:25:13Z","title":"Towards Pixel-Wise Anomaly Location for High-Resolution PCBA via Self-Supervised Image Reconstruction","summary":"Automated defect inspection of assembled Printed Circuit Board Assemblies (PCBA) is quite challenging due to the insufficient labeled data, micro-defects with just a few pixels in visually-complex and high-resolution images. To address these challenges, we present HiSIR-Net, a High resolution, Self-supervised Reconstruction framework for pixel-wise PCBA localization. Our design combines two lightweight modules that make this practical on real 4K-resolution boards: (i) a Selective Input-Reconstruction Gate (SIR-Gate) that lets the model decide where to trust reconstruction versus the original input, thereby reducing irrelevant reconstruction artifacts and false alarms; and (ii) a Region-level Optimized Patch Selection (ROPS) scheme with positional cues to select overlapping patch reconstructions coherently across arbitrary resolutions. Organically integrating these mechanisms yields clean, high-resolution anomaly maps with low false positive (FP) rate. To bridge the gap in high-resolution PCBA datasets, we further contribute a self-collected dataset named SIPCBA-500 of 500 images. We conduct extensive experiments on our SIPCBA-500 as well as public benchmarks, demonstrating the superior localization performance of our method while running at practical speed. Full code and dataset will be made available upon acceptance.","authors":["Wuyi Liu","Le Jin","Junxian Yang","Yuanchao Yu","Zishuo Peng","Jinfeng Xu","Xianzhi Li","Jun Zhou"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2503.07982v3","updated":"2025-12-22T02:38:35Z","published":"2025-03-11T02:34:33Z","title":"TRACE: Your Diffusion Model is Secretly an Instance Edge Detector","summary":"High-quality instance and panoptic segmentation has traditionally relied on dense instance-level annotations such as masks, boxes, or points, which are costly, inconsistent, and difficult to scale. Unsupervised and weakly-supervised approaches reduce this burden but remain constrained by semantic backbone constraints and human bias, often producing merged or fragmented outputs. We present TRACE (TRAnsforming diffusion Cues to instance Edges), showing that text-to-image diffusion models secretly function as instance edge annotators. TRACE identifies the Instance Emergence Point (IEP) where object boundaries first appear in self-attention maps, extracts boundaries through Attention Boundary Divergence (ABDiv), and distills them into a lightweight one-step edge decoder. This design removes the need for per-image diffusion inversion, achieving 81x faster inference while producing sharper and more connected boundaries. On the COCO benchmark, TRACE improves unsupervised instance segmentation by +5.1 AP, and in tag-supervised panoptic segmentation it outperforms point-supervised baselines by +1.7 PQ without using any instance-level labels. These results reveal that diffusion models encode hidden instance boundary priors, and that decoding these signals offers a practical and scalable alternative to costly manual annotation. Code is available at https://github.com/shjo-april/DiffEGG.","authors":["Sanghyun Jo","Ziseok Lee","Wooyeol Lee","Jonghyun Choi","Jaesik Park","Kyungsu Kim"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2501.05904v2","updated":"2025-12-22T02:34:43Z","published":"2025-01-10T12:00:11Z","title":"Binary Event-Driven Spiking Transformer","summary":"Transformer-based Spiking Neural Networks (SNNs) introduce a novel event-driven self-attention paradigm that combines the high performance of Transformers with the energy efficiency of SNNs. However, the larger model size and increased computational demands of the Transformer structure limit their practicality in resource-constrained scenarios. In this paper, we integrate binarization techniques into Transformer-based SNNs and propose the Binary Event-Driven Spiking Transformer, i.e. BESTformer. The proposed BESTformer can significantly reduce storage and computational demands by representing weights and attention maps with a mere 1-bit. However, BESTformer suffers from a severe performance drop from its full-precision counterpart due to the limited representation capability of binarization. To address this issue, we propose a Coupled Information Enhancement (CIE) method, which consists of a reversible framework and information enhancement distillation. By maximizing the mutual information between the binary model and its full-precision counterpart, the CIE method effectively mitigates the performance degradation of the BESTformer. Extensive experiments on static and neuromorphic datasets demonstrate that our method achieves superior performance to other binary SNNs, showcasing its potential as a compact yet high-performance model for resource-limited edge devices. The repository of this paper is available at https://github.com/CaoHLin/BESTFormer.","authors":["Honglin Cao","Zijian Zhou","Wenjie Wei","Ammar Belatreche","Yu Liang","Dehao Zhang","Malu Zhang","Yang Yang","Haizhou Li"],"pdf_url":"","comment":"9 pages, 5 figures"},{"id":"http://arxiv.org/abs/2512.17817v2","updated":"2025-12-22T02:33:28Z","published":"2025-12-19T17:22:35Z","title":"Chorus: Multi-Teacher Pretraining for Holistic 3D Gaussian Scene Encoding","summary":"While 3DGS has emerged as a high-fidelity scene representation, encoding rich, general-purpose features directly from its primitives remains under-explored. We address this gap by introducing Chorus, a multi-teacher pretraining framework that learns a holistic feed-forward 3D Gaussian Splatting (3DGS) scene encoder by distilling complementary signals from 2D foundation models. Chorus employs a shared 3D encoder and teacher-specific projectors to learn from language-aligned, generalist, and object-aware teachers, encouraging a shared embedding space that captures signals from high-level semantics to fine-grained structure.\n  We evaluate Chorus on a wide range of tasks: open-vocabulary semantic and instance segmentation, linear and decoder probing, as well as data-efficient supervision. Besides 3DGS, we also test Chorus on several benchmarks that only support point clouds by pretraining a variant using only Gaussians' centers, colors, estimated normals as inputs. Interestingly, this encoder shows strong transfer and outperforms the point clouds baseline while using 39.9 times fewer training scenes. Finally, we propose a render-and-distill adaptation that facilitates out-of-domain finetuning. Our code and model will be released upon publication.","authors":["Yue Li","Qi Ma","Runyi Yang","Mengjiao Ma","Bin Ren","Nikola Popovic","Nicu Sebe","Theo Gevers","Luc Van Gool","Danda Pani Paudel","Martin R. Oswald"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.18969v1","updated":"2025-12-22T02:30:19Z","published":"2025-12-22T02:30:19Z","title":"Self-Attention with State-Object Weighted Combination for Compositional Zero Shot Learning","summary":"Object recognition has become prevalent across various industries. However, most existing applications are limited to identifying objects alone, without considering their associated states. The ability to recognize both the state and object simultaneously remains less common. One approach to address this is by treating state and object as a single category during training. However, this approach poses challenges in data collection and training since it requires comprehensive data for all possible combinations. Compositional Zero-shot Learning (CZSL) emerges as a viable solution by treating the state and object as distinct categories during training. CZSL facilitates the identification of novel compositions even in the absence of data for every conceivable combination. The current state-of-the-art method, KG-SP, addresses this issue by training distinct classifiers for states and objects, while leveraging a semantic model to evaluate the plausibility of composed compositions. However, KG-SP's accuracy in state and object recognition can be further improved, and it fails to consider the weighting of states and objects during composition. In this study, we propose SASOW, an enhancement of KG-SP that considers the weighting of states and objects while improving composition recognition accuracy. First, we introduce self-attention mechanisms into the classifiers for states and objects, leading to enhanced accuracy in recognizing both. Additionally, we incorporate the weighting of states and objects during composition to generate more reasonable and accurate compositions. Our validation process involves testing SASOW on three established benchmark datasets. Experimental outcomes affirm when compared against OW-CZSL approach, KG-SP, SASOW showcases improvements of 2.1%, 1.7%, and 0.4% in terms of accuracy for unseen compositions across the MIT-States, UT Zappos, and C-GQA datasets, respectively.","authors":["Cheng-Hong Chang","Pei-Hsuan Tsai"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.18968v1","updated":"2025-12-22T02:29:43Z","published":"2025-12-22T02:29:43Z","title":"Total Curvature Regularization and its_Minimization for Surface and Image Smoothing","summary":"We introduce a novel formulation for curvature regularization by penalizing normal curvatures from multiple directions. This total normal curvature regularization is capable of producing solutions with sharp edges and precise isotropic properties. To tackle the resulting high-order nonlinear optimization problem, we reformulate it as the task of finding the steady-state solution of a time-dependent partial differential equation (PDE) system. Time discretization is achieved through operator splitting, where each subproblem at the fractional steps either has a closed-form solution or can be efficiently solved using advanced algorithms. Our method circumvents the need for complex parameter tuning and demonstrates robustness to parameter choices. The efficiency and effectiveness of our approach have been rigorously validated in the context of surface and image smoothing problems.","authors":["Tianle Lu","Ke Chen","Yuping Duan"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.18964v1","updated":"2025-12-22T02:25:05Z","published":"2025-12-22T02:25:05Z","title":"DVI: Disentangling Semantic and Visual Identity for Training-Free Personalized Generation","summary":"Recent tuning-free identity customization methods achieve high facial fidelity but often overlook visual context, such as lighting, skin texture, and environmental tone. This limitation leads to ``Semantic-Visual Dissonance,'' where accurate facial geometry clashes with the input's unique atmosphere, causing an unnatural ``sticker-like'' effect. We propose **DVI (Disentangled Visual-Identity)**, a zero-shot framework that orthogonally disentangles identity into fine-grained semantic and coarse-grained visual streams. Unlike methods relying solely on semantic vectors, DVI exploits the inherent statistical properties of the VAE latent space, utilizing mean and variance as lightweight descriptors for global visual atmosphere. We introduce a **Parameter-Free Feature Modulation** mechanism that adaptively modulates semantic embeddings with these visual statistics, effectively injecting the reference's ``visual soul'' without training. Furthermore, a **Dynamic Temporal Granularity Scheduler** aligns with the diffusion process, prioritizing visual atmosphere in early denoising stages while refining semantic details later. Extensive experiments demonstrate that DVI significantly enhances visual consistency and atmospheric fidelity without parameter fine-tuning, maintaining robust identity preservation and outperforming state-of-the-art methods in IBench evaluations.","authors":["Guandong Li","Yijun Ding"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.18954v1","updated":"2025-12-22T02:05:45Z","published":"2025-12-22T02:05:45Z","title":"VOIC: Visible-Occluded Decoupling for Monocular 3D Semantic Scene Completion","summary":"Camera-based 3D Semantic Scene Completion (SSC) is a critical task for autonomous driving and robotic scene understanding. It aims to infer a complete 3D volumetric representation of both semantics and geometry from a single image. Existing methods typically focus on end-to-end 2D-to-3D feature lifting and voxel completion. However, they often overlook the interference between high-confidence visible-region perception and low-confidence occluded-region reasoning caused by single-image input, which can lead to feature dilution and error propagation.\n  To address these challenges, we introduce an offline Visible Region Label Extraction (VRLE) strategy that explicitly separates and extracts voxel-level supervision for visible regions from dense 3D ground truth. This strategy purifies the supervisory space for two complementary sub-tasks: visible-region perception and occluded-region reasoning. Building on this idea, we propose the Visible-Occluded Interactive Completion Network (VOIC), a novel dual-decoder framework that explicitly decouples SSC into visible-region semantic perception and occluded-region scene completion. VOIC first constructs a base 3D voxel representation by fusing image features with depth-derived occupancy. The visible decoder focuses on generating high-fidelity geometric and semantic priors, while the occlusion decoder leverages these priors together with cross-modal interaction to perform coherent global scene reasoning.\n  Extensive experiments on the SemanticKITTI and SSCBench-KITTI360 benchmarks demonstrate that VOIC outperforms existing monocular SSC methods in both geometric completion and semantic segmentation accuracy, achieving state-of-the-art performance.","authors":["Zaidao Han","Risa Higashita","Jiang Liu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.18953v1","updated":"2025-12-22T02:05:02Z","published":"2025-12-22T02:05:02Z","title":"Symmetrization of 3D Generative Models","summary":"We propose a novel data-centric approach to promote symmetry in 3D generative models by modifying the training data rather than the model architecture. Our method begins with an analysis of reflectional symmetry in both real-world 3D shapes and samples generated by state-of-the-art models. We hypothesize that training a generative model exclusively on half-objects, obtained by reflecting one half of the shapes along the x=0 plane, enables the model to learn a rich distribution of partial geometries which, when reflected during generation, yield complete shapes that are both visually plausible and geometrically symmetric. To test this, we construct a new dataset of half-objects from three ShapeNet classes (Airplane, Car, and Chair) and train two generative models. Experiments demonstrate that the generated shapes are symmetrical and consistent, compared with the generated objects from the original model and the original dataset objects.","authors":["Nicolas Caytuiro","Ivan Sipiran"],"pdf_url":"","comment":"10 pages, 5 figures"},{"id":"http://arxiv.org/abs/2504.10834v2","updated":"2025-12-22T01:49:26Z","published":"2025-04-15T03:25:39Z","title":"LightFormer: A lightweight and efficient decoder for remote sensing image segmentation","summary":"Deep learning techniques have achieved remarkable success in the semantic segmentation of remote sensing images and in land-use change detection. Nevertheless, their real-time deployment on edge platforms remains constrained by decoder complexity. Herein, we introduce LightFormer, a lightweight decoder for time-critical tasks that involve unstructured targets, such as disaster assessment, unmanned aerial vehicle search-and-rescue, and cultural heritage monitoring. LightFormer employs a feature-fusion and refinement module built on channel processing and a learnable gating mechanism to aggregate multi-scale, multi-range information efficiently, which drastically curtails model complexity. Furthermore, we propose a spatial information selection module (SISM) that integrates long-range attention with a detail preservation branch to capture spatial dependencies across multiple scales, thereby substantially improving the recognition of unstructured targets in complex scenes. On the ISPRS Vaihingen benchmark, LightFormer attains 99.9% of GLFFNet's mIoU (83.9% vs. 84.0%) while requiring only 14.7% of its FLOPs and 15.9% of its parameters, thus achieving an excellent accuracy-efficiency trade-off. Consistent results on LoveDA, ISPRS Potsdam, RescueNet, and FloodNet further demonstrate its robustness and superior perception of unstructured objects. These findings highlight LightFormer as a practical solution for remote sensing applications where both computational economy and high-precision segmentation are imperative.","authors":["Sihang Chen","Lijun Yun","Ze Liu","JianFeng Zhu","Jie Chen","Hui Wang","Yueping Nie"],"pdf_url":"","comment":"18 pages,42 figures"},{"id":"http://arxiv.org/abs/2510.14836v2","updated":"2025-12-22T01:30:40Z","published":"2025-10-16T16:11:18Z","title":"QDepth-VLA: Quantized Depth Prediction as Auxiliary Supervision for Vision-Language-Action Models","summary":"Spatial perception and reasoning are crucial for Vision-Language-Action (VLA) models to accomplish fine-grained manipulation tasks. However, existing approaches often lack the ability to understand and reason over the essential 3D structures necessary for precise control. To address this limitation, we propose QDepth-VLA, a general framework that augments VLA models with an auxiliary depth prediction task. A dedicated depth expert is designed to predict quantized latent tokens of depth maps obtained from a VQ-VAE encoder, enabling the model to learn depth-aware representations that capture critical geometric cues. Experimental results on the simulation benchmarks and real-world tasks demonstrate that QDepth-VLA yields strong spatial reasoning and competitive performance on manipulation tasks.","authors":["Yixuan Li","Yuhui Chen","Mingcai Zhou","Haoran Li","Zhengtao Zhang","Dongbin Zhao"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2506.22678v2","updated":"2025-12-22T01:00:59Z","published":"2025-06-27T23:06:06Z","title":"3D Shape Generation: A Survey","summary":"Recent advances in deep learning have significantly transformed the field of 3D shape generation, enabling the synthesis of complex, diverse, and semantically meaningful 3D objects. This survey provides a comprehensive overview of the current state-of-the-art in 3D shape generation, organizing the discussion around three core components: shape representations, generative modeling approaches, and evaluation protocols. We begin by categorizing 3D representations into explicit, implicit, and hybrid setups, highlighting their structural properties, advantages, and limitations. Next, we review a wide range of generation methods, focusing on feedforward architectures. We further summarize commonly used datasets and evaluation metrics that assess fidelity, diversity, and realism of generated shapes. Finally, we identify open challenges and outline future research directions that could drive progress in controllable, efficient, and high-quality 3D shape generation. This survey aims to serve as a valuable reference for researchers and practitioners seeking a structured and in-depth understanding of this rapidly evolving field.","authors":["Nicolas Caytuiro","Ivan Sipiran"],"pdf_url":"","comment":"24 pages, 6 figures"},{"id":"http://arxiv.org/abs/2512.18933v1","updated":"2025-12-22T00:44:19Z","published":"2025-12-22T00:44:19Z","title":"Point What You Mean: Visually Grounded Instruction Policy","summary":"Vision-Language-Action (VLA) models align vision and language with embodied control, but their object referring ability remains limited when relying solely on text prompt, especially in cluttered or out-of-distribution (OOD) scenes. In this study, we introduce the Point-VLA, a plug-and-play policy that augments language instructions with explicit visual cues (e.g., bounding boxes) to resolve referential ambiguity and enable precise object-level grounding. To efficiently scale visually grounded datasets, we further develop an automatic data annotation pipeline requiring minimal human effort. We evaluate Point-VLA on diverse real-world referring tasks and observe consistently stronger performance than text-only instruction VLAs, particularly in cluttered or unseen-object scenarios, with robust generalization. These results demonstrate that Point-VLA effectively resolves object referring ambiguity through pixel-level visual grounding, achieving more generalizable embodied control.","authors":["Hang Yu","Juntu Zhao","Yufeng Liu","Kaiyu Li","Cheng Ma","Di Zhang","Yingdong Hu","Guang Chen","Junyuan Xie","Junliang Guo","Junqiao Zhao","Yang Gao"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.18930v1","updated":"2025-12-22T00:36:22Z","published":"2025-12-22T00:36:22Z","title":"LouvreSAE: Sparse Autoencoders for Interpretable and Controllable Style Transfer","summary":"Artistic style transfer in generative models remains a significant challenge, as existing methods often introduce style only via model fine-tuning, additional adapters, or prompt engineering, all of which can be computationally expensive and may still entangle style with subject matter. In this paper, we introduce a training- and inference-light, interpretable method for representing and transferring artistic style. Our approach leverages an art-specific Sparse Autoencoder (SAE) on top of latent embeddings of generative image models. Trained on artistic data, our SAE learns an emergent, largely disentangled set of stylistic and compositional concepts, corresponding to style-related elements pertaining brushwork, texture, and color palette, as well as semantic and structural concepts. We call it LouvreSAE and use it to construct style profiles: compact, decomposable steering vectors that enable style transfer without any model updates or optimization. Unlike prior concept-based style transfer methods, our method requires no fine-tuning, no LoRA training, and no additional inference passes, enabling direct steering of artistic styles from only a few reference images. We validate our method on ArtBench10, achieving or surpassing existing methods on style evaluations (VGG Style Loss and CLIP Score Style) while being 1.7-20x faster and, critically, interpretable.","authors":["Raina Panda","Daniel Fein","Arpita Singhal","Mark Fiore","Maneesh Agrawala","Matyas Bohacek"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2207.09775v3","updated":"2025-12-22T00:21:03Z","published":"2022-07-20T09:28:51Z","title":"Rethinking Open-Set Object Detection: Issues, a New Formulation, and Taxonomy","summary":"Open-set object detection (OSOD), a task involving the detection of unknown objects while accurately detecting known objects, has recently gained attention. However, we identify a fundamental issue with the problem formulation employed in current OSOD studies. Inherent to object detection is knowing \"what to detect,\" which contradicts the idea of identifying \"unknown\" objects. This sets OSOD apart from open-set recognition (OSR). This contradiction complicates a proper evaluation of methods' performance, a fact that previous studies have overlooked. Next, we propose a novel formulation wherein detectors are required to detect both known and unknown classes within specified super-classes of object classes. This new formulation is free from the aforementioned issues and has practical applications. Finally, we design benchmark tests utilizing existing datasets and report the experimental evaluation of existing OSOD methods. The results show that existing methods fail to accurately detect unknown objects due to misclassification of known and unknown classes rather than incorrect bounding box prediction. As a byproduct, we introduce a taxonomy of OSOD, resolving confusion prevalent in the literature. We anticipate that our study will encourage the research community to reconsider OSOD and facilitate progress in the right direction.","authors":["Yusuke Hosoya","Masanori Suganuma","Takayuki Okatani"],"pdf_url":"","comment":"Accepted to IJCV"},{"id":"http://arxiv.org/abs/2512.19934v1","updated":"2025-12-22T23:42:45Z","published":"2025-12-22T23:42:45Z","title":"Vehicle-centric Perception via Multimodal Structured Pre-training","summary":"Vehicle-centric perception plays a crucial role in many intelligent systems, including large-scale surveillance systems, intelligent transportation, and autonomous driving. Existing approaches lack effective learning of vehicle-related knowledge during pre-training, resulting in poor capability for modeling general vehicle perception representations. To handle this problem, we propose VehicleMAE-V2, a novel vehicle-centric pre-trained large model. By exploring and exploiting vehicle-related multimodal structured priors to guide the masked token reconstruction process, our approach can significantly enhance the model's capability to learn generalizable representations for vehicle-centric perception. Specifically, we design the Symmetry-guided Mask Module (SMM), Contour-guided Representation Module (CRM) and Semantics-guided Representation Module (SRM) to incorporate three kinds of structured priors into token reconstruction including symmetry, contour and semantics of vehicles respectively. SMM utilizes the vehicle symmetry constraints to avoid retaining symmetric patches and can thus select high-quality masked image patches and reduce information redundancy. CRM minimizes the probability distribution divergence between contour features and reconstructed features and can thus preserve holistic vehicle structure information during pixel-level reconstruction. SRM aligns image-text features through contrastive learning and cross-modal distillation to address the feature confusion caused by insufficient semantic understanding during masked reconstruction. To support the pre-training of VehicleMAE-V2, we construct Autobot4M, a large-scale dataset comprising approximately 4 million vehicle images and 12,693 text descriptions. Extensive experiments on five downstream tasks demonstrate the superior performance of VehicleMAE-V2.","authors":["Wentao Wu","Xiao Wang","Chenglong Li","Jin Tang","Bin Luo"],"pdf_url":"","comment":"Journal extension of VehicleMAE (AAAI 2024)"},{"id":"http://arxiv.org/abs/2512.19928v1","updated":"2025-12-22T23:05:26Z","published":"2025-12-22T23:05:26Z","title":"Unified Brain Surface and Volume Registration","summary":"Accurate registration of brain MRI scans is fundamental for cross-subject analysis in neuroscientific studies. This involves aligning both the cortical surface of the brain and the interior volume. Traditional methods treat volumetric and surface-based registration separately, which often leads to inconsistencies that limit downstream analyses. We propose a deep learning framework, NeurAlign, that registers $3$D brain MRI images by jointly aligning both cortical and subcortical regions through a unified volume-and-surface-based representation. Our approach leverages an intermediate spherical coordinate space to bridge anatomical surface topology with volumetric anatomy, enabling consistent and anatomically accurate alignment. By integrating spherical registration into the learning, our method ensures geometric coherence between volume and surface domains. In a series of experiments on both in-domain and out-of-domain datasets, our method consistently outperforms both classical and machine learning-based registration methods -- improving the Dice score by up to 7 points while maintaining regular deformation fields. Additionally, it is orders of magnitude faster than the standard method for this task, and is simpler to use because it requires no additional inputs beyond an MRI scan. With its superior accuracy, fast inference, and ease of use, NeurAlign sets a new standard for joint cortical and subcortical registration.","authors":["S. Mazdak Abulnaga","Andrew Hoopes","Malte Hoffmann","Robin Magnet","Maks Ovsjanikov","Lilla Zöllei","John Guttag","Bruce Fischl","Adrian Dalca"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19918v1","updated":"2025-12-22T22:45:39Z","published":"2025-12-22T22:45:39Z","title":"Widget2Code: From Visual Widgets to UI Code via Multimodal LLMs","summary":"User interface to code (UI2Code) aims to generate executable code that can faithfully reconstruct a given input UI. Prior work focuses largely on web pages and mobile screens, leaving app widgets underexplored. Unlike web or mobile UIs with rich hierarchical context, widgets are compact, context-free micro-interfaces that summarize key information through dense layouts and iconography under strict spatial constraints. Moreover, while (image, code) pairs are widely available for web or mobile UIs, widget designs are proprietary and lack accessible markup. We formalize this setting as the Widget-to-Code (Widget2Code) and introduce an image-only widget benchmark with fine-grained, multi-dimensional evaluation metrics. Benchmarking shows that although generalized multimodal large language models (MLLMs) outperform specialized UI2Code methods, they still produce unreliable and visually inconsistent code. To address these limitations, we develop a baseline that jointly advances perceptual understanding and structured code generation. At the perceptual level, we follow widget design principles to assemble atomic components into complete layouts, equipped with icon retrieval and reusable visualization modules. At the system level, we design an end-to-end infrastructure, WidgetFactory, which includes a framework-agnostic widget-tailored domain-specific language (WidgetDSL) and a compiler that translates it into multiple front-end implementations (e.g., React, HTML/CSS). An adaptive rendering module further refines spatial dimensions to satisfy compactness constraints. Together, these contributions substantially enhance visual fidelity, establishing a strong baseline and unified infrastructure for future Widget2Code research.","authors":["Houston H. Zhang","Tao Zhang","Baoze Lin","Yuanqi Xue","Yincheng Zhu","Huan Liu","Li Gu","Linfeng Ye","Ziqiang Wang","Xinxin Zuo","Yang Wang","Yuanhao Yu","Zhixiang Chi"],"pdf_url":"","comment":"Code: https://github.com/Djanghao/widget2code"},{"id":"http://arxiv.org/abs/2407.04258v2","updated":"2025-12-22T22:17:14Z","published":"2024-07-05T05:08:06Z","title":"Reinforcement Learning for Unsupervised Video Summarization with Reward Generator Training","summary":"This paper presents a novel approach for unsupervised video summarization using reinforcement learning (RL), addressing limitations like unstable adversarial training and reliance on heuristic-based reward functions. The method operates on the principle that reconstruction fidelity serves as a proxy for informativeness, correlating summary quality with reconstruction ability. The summarizer model assigns importance scores to frames to generate the final summary. For training, RL is coupled with a unique reward generation pipeline that incentivizes improved reconstructions. This pipeline uses a generator model to reconstruct the full video from the selected summary frames; the similarity between the original and reconstructed video provides the reward signal. The generator itself is pre-trained self-supervisedly to reconstruct randomly masked frames. This two-stage training process enhances stability compared to adversarial architectures. Experimental results show strong alignment with human judgments and promising F-scores, validating the reconstruction objective.","authors":["Mehryar Abbasi","Hadi Hadizadeh","Parvaneh Saeedi"],"pdf_url":"","comment":"in IEEE Transactions on Circuits and Systems for Video Technology"},{"id":"http://arxiv.org/abs/2511.13891v2","updated":"2025-12-22T21:37:32Z","published":"2025-11-17T20:29:44Z","title":"Weakly Supervised Ephemeral Gully Detection In Remote Sensing Images Using Vision Language Models","summary":"Among soil erosion problems, Ephemeral Gullies are one of the most concerning phenomena occurring in agricultural fields. Their short temporal cycles increase the difficulty in automatically detecting them using classical computer vision approaches and remote sensing. Also, due to scarcity of and the difficulty in producing accurate labeled data, automatic detection of ephemeral gullies using Machine Learning is limited to zero-shot approaches which are hard to implement. To overcome these challenges, we present the first weakly supervised pipeline for detection of ephemeral gullies. Our method relies on remote sensing and uses Vision Language Models (VLMs) to drastically reduce the labor-intensive task of manual labeling. In order to achieve that, the method exploits: 1) the knowledge embedded in the VLM's pretraining; 2) a teacher-student model where the teacher learns from noisy labels coming from the VLMs, and the student learns by weak supervision using teacher-generate labels and a noise-aware loss function. We also make available the first-of-its-kind dataset for semi-supervised detection of ephemeral gully from remote-sensed images. The dataset consists of a number of locations labeled by a group of soil and plant scientists, as well as a large number of unlabeled locations. The dataset represent more than 18,000 high-resolution remote-sensing images obtained over the course of 13 years. Our experimental results demonstrate the validity of our approach by showing superior performances compared to VLMs and the label model itself when using weak supervision to train an student model. The code and dataset for this work are made publicly available.","authors":["Seyed Mohamad Ali Tousi","Ramy Farag","John A. Lory","G. N. DeSouza"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19871v1","updated":"2025-12-22T20:59:15Z","published":"2025-12-22T20:59:15Z","title":"HyGE-Occ: Hybrid View-Transformation with 3D Gaussian and Edge Priors for 3D Panoptic Occupancy Prediction","summary":"3D Panoptic Occupancy Prediction aims to reconstruct a dense volumetric scene map by predicting the semantic class and instance identity of every occupied region in 3D space. Achieving such fine-grained 3D understanding requires precise geometric reasoning and spatially consistent scene representation across complex environments. However, existing approaches often struggle to maintain precise geometry and capture the precise spatial range of 3D instances critical for robust panoptic separation. To overcome these limitations, we introduce HyGE-Occ, a novel framework that leverages a hybrid view-transformation branch with 3D Gaussian and edge priors to enhance both geometric consistency and boundary awareness in 3D panoptic occupancy prediction. HyGE-Occ employs a hybrid view-transformation branch that fuses a continuous Gaussian-based depth representation with a discretized depth-bin formulation, producing BEV features with improved geometric consistency and structural coherence. In parallel, we extract edge maps from BEV features and use them as auxiliary information to learn edge cues. In our extensive experiments on the Occ3D-nuScenes dataset, HyGE-Occ outperforms existing work, demonstrating superior 3D geometric reasoning.","authors":["Jong Wook Kim","Wonseok Roh","Ha Dam Baek","Pilhyeon Lee","Jonghyun Choi","Sangpil Kim"],"pdf_url":"","comment":"11 pages, 6 figures"},{"id":"http://arxiv.org/abs/2503.07940v3","updated":"2025-12-22T20:52:40Z","published":"2025-03-11T00:40:45Z","title":"BUFFER-X: Towards Zero-Shot Point Cloud Registration in Diverse Scenes","summary":"Recent advances in deep learning-based point cloud registration have improved generalization, yet most methods still require retraining or manual parameter tuning for each new environment. In this paper, we identify three key factors limiting generalization: (a) reliance on environment-specific voxel size and search radius, (b) poor out-of-domain robustness of learning-based keypoint detectors, and (c) raw coordinate usage, which exacerbates scale discrepancies. To address these issues, we present a zero-shot registration pipeline called BUFFER-X by (a) adaptively determining voxel size/search radii, (b) using farthest point sampling to bypass learned detectors, and (c) leveraging patch-wise scale normalization for consistent coordinate bounds. In particular, we present a multi-scale patch-based descriptor generation and a hierarchical inlier search across scales to improve robustness in diverse scenes. We also propose a novel generalizability benchmark using 11 datasets that cover various indoor/outdoor scenarios and sensor modalities, demonstrating that BUFFER-X achieves substantial generalization without prior information or manual parameter tuning for the test datasets. Our code is available at https://github.com/MIT-SPARK/BUFFER-X.","authors":["Minkyun Seo","Hyungtae Lim","Kanghee Lee","Luca Carlone","Jaesik Park"],"pdf_url":"","comment":"20 pages, 14 figures. Accepted as a highlight paper at ICCV 2025"},{"id":"http://arxiv.org/abs/2512.19850v1","updated":"2025-12-22T20:08:46Z","published":"2025-12-22T20:08:46Z","title":"RANSAC Scoring Functions: Analysis and Reality Check","summary":"We revisit the problem of assigning a score (a quality of fit) to candidate geometric models -- one of the key components of RANSAC for robust geometric fitting. In a non-robust setting, the ``gold standard'' scoring function, known as the geometric error, follows from a probabilistic model with Gaussian noises. We extend it to spherical noises. In a robust setting, we consider a mixture with uniformly distributed outliers and show that a threshold-based parameterization leads to a unified view of likelihood-based and robust M-estimators and associated local optimization schemes.\n  Next we analyze MAGSAC++ which stands out for two reasons. First, it achieves the best results according to existing benchmarks. Second, it makes quite different modeling assumptions and derivation steps. We discovered, however that the derivation does not correspond to sound principles and the resulting score function is in fact numerically equivalent to a simple Gaussian-uniform likelihood, a basic model within the proposed framework.\n  Finally, we propose an experimental methodology for evaluating scoring functions: assuming either a large validation set, or a small random validation set in expectation. We find that all scoring functions, including using a learned inlier distribution, perform identically. In particular, MAGSAC++ score is found to be neither better performing than simple contenders nor less sensitive to the choice of the threshold hyperparameter.\n  Our theoretical and experimental analysis thus comprehensively revisit the state-of-the-art, which is critical for any future research seeking to improve the methods or apply them to other robust fitting problems.","authors":["A. Shekhovtsov"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19823v1","updated":"2025-12-22T19:29:57Z","published":"2025-12-22T19:29:57Z","title":"Learning to Refocus with Video Diffusion Models","summary":"Focus is a cornerstone of photography, yet autofocus systems often fail to capture the intended subject, and users frequently wish to adjust focus after capture. We introduce a novel method for realistic post-capture refocusing using video diffusion models. From a single defocused image, our approach generates a perceptually accurate focal stack, represented as a video sequence, enabling interactive refocusing and unlocking a range of downstream applications. We release a large-scale focal stack dataset acquired under diverse real-world smartphone conditions to support this work and future research. Our method consistently outperforms existing approaches in both perceptual quality and robustness across challenging scenarios, paving the way for more advanced focus-editing capabilities in everyday photography. Code and data are available at www.learn2refocus.github.io","authors":["SaiKiran Tedla","Zhoutong Zhang","Xuaner Zhang","Shumian Xin"],"pdf_url":"","comment":"Code and data are available at https://www.learn2refocus.github.io . SIGGRAPH Asia 2025, Dec. 2025"},{"id":"http://arxiv.org/abs/2412.16631v2","updated":"2025-12-22T19:13:55Z","published":"2024-12-21T13:53:15Z","title":"Deep Learning for Spatio-Temporal Fusion in Land Surface Temperature Estimation: A Comprehensive Survey, Experimental Analysis, and Future Trends","summary":"Land Surface Temperature (LST) plays a key role in climate monitoring, urban heat assessment, and land-atmosphere interactions. However, current thermal infrared satellite sensors cannot simultaneously achieve high spatial and temporal resolution. Spatio-temporal fusion (STF) techniques address this limitation by combining complementary satellite data, one with high spatial but low temporal resolution, and another with high temporal but low spatial resolution. Existing STF techniques, from classical models to modern deep learning (DL) architectures, were primarily developed for surface reflectance (SR). Their application to thermal data remains limited and often overlooks LST-specific spatial and temporal variability. This study provides a focused review of DL-based STF methods for LST. We present a formal mathematical definition of the thermal fusion task, propose a refined taxonomy of relevant DL methods, and analyze the modifications required when adapting SR-oriented models to LST. To support reproducibility and benchmarking, we introduce a new dataset comprising 51 Terra MODIS-Landsat LST pairs from 2013 to 2024, and evaluate representative models to explore their behavior on thermal data. The analysis highlights performance gaps, architecture sensitivities, and open research challenges. The dataset and accompanying resources are publicly available at https://github.com/Sofianebouaziz1/STF-LST.","authors":["Sofiane Bouaziz","Adel Hafiane","Raphael Canals","Rachid Nedjai"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19817v1","updated":"2025-12-22T19:12:33Z","published":"2025-12-22T19:12:33Z","title":"Generating the Past, Present and Future from a Motion-Blurred Image","summary":"We seek to answer the question: what can a motion-blurred image reveal about a scene's past, present, and future? Although motion blur obscures image details and degrades visual quality, it also encodes information about scene and camera motion during an exposure. Previous techniques leverage this information to estimate a sharp image from an input blurry one, or to predict a sequence of video frames showing what might have occurred at the moment of image capture. However, they rely on handcrafted priors or network architectures to resolve ambiguities in this inverse problem, and do not incorporate image and video priors on large-scale datasets. As such, existing methods struggle to reproduce complex scene dynamics and do not attempt to recover what occurred before or after an image was taken. Here, we introduce a new technique that repurposes a pre-trained video diffusion model trained on internet-scale datasets to recover videos revealing complex scene dynamics during the moment of capture and what might have occurred immediately into the past or future. Our approach is robust and versatile; it outperforms previous methods for this task, generalizes to challenging in-the-wild images, and supports downstream tasks such as recovering camera trajectories, object motion, and dynamic 3D scene structure. Code and data are available at https://blur2vid.github.io","authors":["SaiKiran Tedla","Kelly Zhu","Trevor Canham","Felix Taubner","Michael S. Brown","Kiriakos N. Kutulakos","David B. Lindell"],"pdf_url":"","comment":"Code and data are available at https://blur2vid.github.io"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2409.20302v6","updated":"2025-12-22T16:29:48Z","published":"2024-09-30T14:00:04Z","title":"OM4OV: Leveraging Ontology Matching for Ontology Versioning","summary":"Due to the dynamic nature of the Semantic Web, version control is necessary to capture time-varying information for widely used ontologies. Despite the long-standing recognition of ontology versioning (OV) as a crucial component of efficient ontology management, many approaches treat OV as similar to ontology matching (OM) and directly reuse OM systems for OV tasks. In this study, we systematically analyse the similarities and differences between OM and OV and formalise the OM4OV pipeline. The pipeline is implemented and evaluated in the state-of-the-art OM system Agent-OM. The experimental results indicate that OM systems can be reused for OV tasks, but without necessary extensions, the current OM4OV pipeline can produce skewed measurements, poor performance in detecting update entities, and limited explainability for false mappings. To tackle these issues, we propose an optimisation method called the cross-reference (CR) mechanism, building upon the existing alignments from OM to reduce the number of matching candidates and improve overall OV performance.","authors":["Zhangcheng Qiang","Kerry Taylor","Weiqing Wang"],"pdf_url":"","comment":"16 pages, 8 figures, 1 table"},{"id":"http://arxiv.org/abs/2512.08398v2","updated":"2025-12-22T13:57:26Z","published":"2025-12-09T09:26:37Z","title":"Ontology-Based Knowledge Graph Framework for Industrial Standard Documents via Hierarchical and Propositional Structuring","summary":"Ontology-based knowledge graph (KG) construction is a core technology that enables multidimensional understanding and advanced reasoning over domain knowledge. Industrial standards, in particular, contain extensive technical information and complex rules presented in highly structured formats that combine tables, scopes of application, constraints, exceptions, and numerical calculations, making KG construction especially challenging. In this study, we propose a method that organizes such documents into a hierarchical semantic structure, decomposes sentences and tables into atomic propositions derived from conditional and numerical rules, and integrates them into an ontology-knowledge graph through LLM-based triple extraction. Our approach captures both the hierarchical and logical structures of documents, effectively representing domain-specific semantics that conventional methods fail to reflect. To verify its effectiveness, we constructed rule, table, and multi-hop QA datasets, as well as a toxic clause detection dataset, from industrial standards, and implemented an ontology-aware KG-RAG framework for comparative evaluation. Experimental results show that our method achieves significant performance improvements across all QA types compared to existing KG-RAG approaches. This study demonstrates that reliable and scalable knowledge representation is feasible even for industrial documents with intertwined conditions, constraints, and scopes, contributing to future domain-specific RAG development and intelligent document management.","authors":["Jiin Park","Hyuna Jeon","Yoonseo Lee","Jisu Hong","Misuk Kim"],"pdf_url":"","comment":"The authors have identified significant technical errors in the paper that invalidate the current findings"},{"id":"http://arxiv.org/abs/2512.19360v1","updated":"2025-12-22T12:59:23Z","published":"2025-12-22T12:59:23Z","title":"Generative vector search to improve pathology foundation models across multimodal vision-language tasks","summary":"Retrieval-augmented generation improves large language models by grounding outputs in external knowledge sources, reducing hallucinations and addressing knowledge cutoffs. However, standard embedding-based retrieval fails to capture the complexity of multi-concept queries, particularly in domains like biomedicine, where biological data are inherently high-dimensional. For example,omics datasets, and clinical reports simultaneously exhibit numerous molecular, cellular, and physiological features. We present Stochastic Latent Matching (STHLM), a generative vector search method that samples query-conditioned embeddings from text or image inputs to enhance retrieval performance. Analogous to how Chain-of-Thought reasoning enables language models to \"think longer\" on complex problems, STHLM allows retrieval systems to \"search wider\" through iterative sampling. STHLM demonstrates critical improvements over classical vector retrieval across diverse benchmarks, including scientific literature, clinical notes, and tissue images, boosting retrieval performance by 10-30% through test-time compute (trading latency for accuracy), while enabling up to a 10-fold compression of embedding dimensions.","authors":["Markus Ekvall","Ludvig Bergenstråhle","Patrick Truong","Ben Murrell","Joakim Lundeberg"],"pdf_url":"","comment":"13 pages main (54 total), 2 main figures (9 total)"},{"id":"http://arxiv.org/abs/2511.11646v2","updated":"2025-12-22T10:22:37Z","published":"2025-11-10T08:50:03Z","title":"What-If Decision Support for Product Line Extension Using Conditional Deep Generative Models","summary":"Product line extension is a strategically important managerial decision that requires anticipating how consumer segments and purchasing contexts may respond to hypothetical product designs that do not yet exist in the market. Such decisions are inherently uncertain because managers must infer future outcomes from historical purchase data without direct market observations. This study addresses this challenge by proposing a data-driven decision support framework that enables forward-looking what-if analysis based on historical transaction data. We introduce a Conditional Tabular Variational Autoencoder (CTVAE) that learns the conditional joint distribution of product attributes and consumer characteristics from large-scale tabular data. By conditioning the generative process on controllable design variables such as container type, volume, flavor, and calorie content, the proposed model generates synthetic consumer attribute distributions for hypothetical line-extended products. This enables systematic exploration of alternative design scenarios without costly market pretests. The framework is evaluated using home-scan panel data covering more than 20,000 consumers and 700 soft drink products. Empirical results show that the CTVAE outperforms existing tabular generative models in capturing conditional consumer attribute distributions. Simulation-based analyses further demonstrate that the generated synthetic data support knowledge-driven reasoning for assessing cannibalization risks and identifying potential target segments. These findings highlight the value of conditional deep generative models as core components of decision support systems for product line extension planning.","authors":["Yinxing Li","Tsukasa Ishigaki"],"pdf_url":"","comment":"25 pages"},{"id":"http://arxiv.org/abs/2512.19134v1","updated":"2025-12-22T08:28:05Z","published":"2025-12-22T08:28:05Z","title":"QuCo-RAG: Quantifying Uncertainty from the Pre-training Corpus for Dynamic Retrieval-Augmented Generation","summary":"Dynamic Retrieval-Augmented Generation adaptively determines when to retrieve during generation to mitigate hallucinations in large language models (LLMs). However, existing methods rely on model-internal signals (e.g., logits, entropy), which are fundamentally unreliable because LLMs are typically ill-calibrated and often exhibit high confidence in erroneous outputs. We propose QuCo-RAG, which shifts from subjective confidence to objective statistics computed from pre-training data. Our method quantifies uncertainty through two stages: (1) before generation, we identify low-frequency entities indicating long-tail knowledge gaps; (2) during generation, we verify entity co-occurrence in the pre-training corpus, where zero co-occurrence often signals hallucination risk. Both stages leverage Infini-gram for millisecond-latency queries over 4 trillion tokens, triggering retrieval when uncertainty is high. Experiments on multi-hop QA benchmarks show QuCo-RAG achieves EM gains of 5--12 points over state-of-the-art baselines with OLMo-2 models, and transfers effectively to models with undisclosed pre-training data (Llama, Qwen, GPT), improving EM by up to 14 points. Domain generalization on biomedical QA further validates the robustness of our paradigm. These results establish corpus-grounded verification as a principled, practically model-agnostic paradigm for dynamic RAG. Our code is publicly available at https://github.com/ZhishanQ/QuCo-RAG.","authors":["Dehai Min","Kailin Zhang","Tongtong Wu","Lu Cheng"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2407.13349v8","updated":"2025-12-22T07:21:15Z","published":"2024-07-18T09:49:13Z","title":"FCN: Fusing Exponential and Linear Cross Network for Click-Through Rate Prediction","summary":"As an important modeling paradigm in click-through rate (CTR) prediction, the Deep & Cross Network (DCN) and its derivative models have gained widespread recognition primarily due to their success in a trade-off between computational cost and performance. This paradigm employs a cross network to explicitly model feature interactions with linear growth, while leveraging deep neural networks (DNN) to implicitly capture higher-order feature interactions. However, these models still face several key limitations: (1) The performance of existing explicit feature interaction methods lags behind that of implicit DNN, resulting in overall model performance being dominated by the DNN; (2) While these models claim to capture high-order feature interactions, they often overlook potential noise within these interactions; (3) The learning process for different interaction network branches lacks appropriate supervision signals; and (4) The high-order feature interactions captured by these models are often implicit and non-interpretable due to their reliance on DNN.\n  To address the identified limitations, this paper proposes a novel model, called Fusing Cross Network (FCN), along with two sub-networks: Linear Cross Network (LCN) and Exponential Cross Network (ECN). FCN explicitly captures feature interactions with both linear and exponential growth, eliminating the need to rely on implicit DNN. Moreover, we introduce the Self-Mask operation to filter noise layer by layer and reduce the number of parameters in the cross network by half. To effectively train these two cross networks, we propose a simple yet effective loss function called Tri-BCE, which provides tailored supervision signals for each network. We evaluate the effectiveness, efficiency, and interpretability of FCN on six benchmark datasets. Furthermore, by integrating LCN and ECN, FCN achieves a new state-of-the-art performance.","authors":["Honghao Li","Yiwen Zhang","Yi Zhang","Hanwei Li","Lei Sang","Jieming Zhu"],"pdf_url":"","comment":"KDD'26 accepted"},{"id":"http://arxiv.org/abs/2512.18996v1","updated":"2025-12-22T03:24:11Z","published":"2025-12-22T03:24:11Z","title":"Modular Layout Synthesis (MLS): Front-end Code via Structure Normalization and Constrained Generation","summary":"Automated front-end engineering drastically reduces development cycles and minimizes manual coding overhead. While Generative AI has shown promise in translating designs to code, current solutions often produce monolithic scripts, failing to natively support modern ecosystems like React, Vue, or Angular. Furthermore, the generated code frequently suffers from poor modularity, making it difficult to maintain. To bridge this gap, we introduce Modular Layout Synthesis (MLS), a hierarchical framework that merges visual understanding with structural normalization. Initially, a visual-semantic encoder maps the screen capture into a serialized tree topology, capturing the essential layout hierarchy. Instead of simple parsing, we apply heuristic deduplication and pattern recognition to isolate reusable blocks, creating a framework-agnostic schema. Finally, a constraint-based generation protocol guides the LLM to synthesize production-ready code with strict typing and component props. Evaluations show that MLS significantly outperforms existing baselines, ensuring superior code reusability and structural integrity across multiple frameworks","authors":["Chong Liu","Ming Zhang","Fei Li","Hao Zhou","Xiaoshuang Chen","Ye Yuan"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2503.04162v5","updated":"2025-12-22T03:07:50Z","published":"2025-03-06T07:25:19Z","title":"Semantic Retrieval Augmented Contrastive Learning for Sequential Recommendation","summary":"Contrastive learning has shown effectiveness in improving sequential recommendation models. However, existing methods still face challenges in generating high-quality contrastive pairs: they either rely on random perturbations that corrupt user preference patterns or depend on sparse collaborative data that generates unreliable contrastive pairs. Furthermore, existing approaches typically require predefined selection rules that impose strong assumptions, limiting the model's ability to autonomously learn optimal contrastive pairs. To address these limitations, we propose a novel approach named Semantic Retrieval Augmented Contrastive Learning (SRA-CL). SRA-CL leverages the semantic understanding and reasoning capabilities of LLMs to generate expressive embeddings that capture both user preferences and item characteristics. These semantic embeddings enable the construction of candidate pools for inter-user and intra-user contrastive learning through semantic-based retrieval. To further enhance the quality of the contrastive samples, we introduce a learnable sample synthesizer that optimizes the contrastive sample generation process during model training. SRA-CL adopts a plug-and-play design, enabling seamless integration with existing sequential recommendation architectures. Extensive experiments on four public datasets demonstrate the effectiveness and model-agnostic nature of our approach.","authors":["Ziqiang Cui","Yunpeng Weng","Xing Tang","Xiaokun Zhang","Shiwei Li","Peiyang Liu","Bowei He","Dugang Liu","Weihong Luo","Xiuqiang He","Chen Ma"],"pdf_url":"","comment":"Accepted by NeurIPS 2025. Code is available at: https://github.com/ziqiangcui/SRA-CL"},{"id":"http://arxiv.org/abs/2505.15210v3","updated":"2025-12-22T02:26:22Z","published":"2025-05-21T07:38:45Z","title":"Deliberation on Priors: Trustworthy Reasoning of Large Language Models on Knowledge Graphs","summary":"Knowledge graph-based retrieval-augmented generation seeks to mitigate hallucinations in Large Language Models (LLMs) caused by insufficient or outdated knowledge. However, existing methods often fail to fully exploit the prior knowledge embedded in knowledge graphs (KGs), particularly their structural information and explicit or implicit constraints. The former can enhance the faithfulness of LLMs' reasoning, while the latter can improve the reliability of response generation. Motivated by these, we propose a trustworthy reasoning framework, termed Deliberation over Priors (DP), which sufficiently utilizes the priors contained in KGs. Specifically, DP adopts a progressive knowledge distillation strategy that integrates structural priors into LLMs through a combination of supervised fine-tuning and Kahneman-Tversky optimization, thereby improving the faithfulness of relation path generation. Furthermore, our framework employs a reasoning-introspection strategy, which guides LLMs to perform refined reasoning verification based on extracted constraint priors, ensuring the reliability of response generation. Extensive experiments on three benchmark datasets demonstrate that DP achieves new state-of-the-art performance, especially a Hit@1 improvement of 13% on the ComplexWebQuestions dataset, and generates highly trustworthy responses. We also conduct various analyses to verify its flexibility and practicality. The code is available at https://github.com/reml-group/Deliberation-on-Priors.","authors":["Jie Ma","Ning Qu","Zhitao Gao","Rui Xing","Jun Liu","Hongbin Pei","Jiang Xie","Linyun Song","Pinghui Wang","Jing Tao","Zhou Su"],"pdf_url":"","comment":"Accepted by NeurIPS 2025"},{"id":"http://arxiv.org/abs/2512.19834v1","updated":"2025-12-22T19:44:01Z","published":"2025-12-22T19:44:01Z","title":"Towards a point-to-point CV-QKD system: Implementation challenges and perspectives","summary":"This article presents an analysis of the practical challenges and implementation perspectives of point-to-point continuous-variable quantum key distribution (CV-QKD) systems over optical fiber. The study addresses the physical layer, including the design of transmitters, quantum channels, and receivers, with emphasis on impairments such as attenuation, chromatic dispersion, polarization fluctuations, and coexistence with classical channels. We further examine the role of digital signal processing (DSP) as the bridge between quantum state transmission and classical post-processing, highlighting its impact on excess noise mitigation, covariance matrix estimation, and reconciliation efficiency. The post-processing pipeline is detailed with a focus on parameter estimation in the finite-size regime, information reconciliation using LDPC-based codes optimized for low-SNR conditions, and privacy amplification employing large-block universal hashing. From a hardware perspective, we discuss modular digital architectures that integrate dedicated accelerators with programmable processors, supported by a reference software framework (CV-QKD-ModSim) for algorithm validation and hardware co-design. Finally, we outline perspectives for the deployment of CV-QKD in Brazil, starting from metropolitan testbeds and extending toward hybrid fiber/FSO and space-based infrastructures. The work establishes the foundations for the first point-to-point CV-QKD system in Brazil, while providing a roadmap for scalable and interoperable quantum communication networks.","authors":["Davi Juvêncio Gomes de Sousa","Nelson Alves Ferreira Neto","Christiano M. S. Nascimento","Lucas Q. Galvão","Mauro Queiroz Nooblath Neto","Micael Andrade Dias","Cássio de Castro Silva","Braian Pinheiro da Silva","Alexandre B. Tacla","Valéria Loureiro da Silva"],"pdf_url":"","comment":"33 pages with 8 figures"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2512.19687v1","updated":"2025-12-22T18:59:07Z","published":"2025-12-22T18:59:07Z","title":"Pushing the Frontier of Audiovisual Perception with Large-Scale Multimodal Correspondence Learning","summary":"We introduce Perception Encoder Audiovisual, PE-AV, a new family of encoders for audio and video understanding trained with scaled contrastive learning. Built on PE, PE-AV makes several key contributions to extend representations to audio, and natively support joint embeddings across audio-video, audio-text, and video-text modalities. PE-AV's unified cross-modal embeddings enable novel tasks such as speech retrieval, and set a new state of the art across standard audio and video benchmarks. We unlock this by building a strong audiovisual data engine that synthesizes high-quality captions for O(100M) audio-video pairs, enabling large-scale supervision consistent across modalities. Our audio data includes speech, music, and general sound effects-avoiding single-domain limitations common in prior work. We exploit ten pairwise contrastive objectives, showing that scaling cross-modality and caption-type pairs strengthens alignment and improves zero-shot performance. We further develop PE-A-Frame by fine-tuning PE-AV with frame-level contrastive objectives, enabling fine-grained audio-frame-to-text alignment for tasks such as sound event detection.","authors":["Apoorv Vyas","Heng-Jui Chang","Cheng-Fu Yang","Po-Yao Huang","Luya Gao","Julius Richter","Sanyuan Chen","Matt Le","Piotr Dollár","Christoph Feichtenhofer","Ann Lee","Wei-Ning Hsu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2508.20072v3","updated":"2025-12-22T18:57:39Z","published":"2025-08-27T17:39:11Z","title":"Discrete Diffusion VLA: Bringing Discrete Diffusion to Action Decoding in Vision-Language-Action Policies","summary":"Vision-Language-Action (VLA) models adapt large vision-language backbones to map images and instructions into robot actions. However, prevailing VLAs either generate actions auto-regressively in a fixed left-to-right order or attach separate MLP or diffusion heads outside the backbone, leading to fragmented information pathways and specialized training requirements that hinder a unified, scalable architecture. We present Discrete Diffusion VLA, a unified-transformer policy that models discretized action chunks with discrete diffusion. The design retains diffusion's progressive refinement paradigm while remaining natively compatible with the discrete token interface of VLMs. Our method achieves an adaptive decoding order that resolves easy action elements before harder ones and uses secondary re-masking to revisit uncertain predictions across refinement rounds, which improves consistency and enables robust error correction. This unified decoder preserves pre-trained vision-language priors, supports parallel decoding, breaks the autoregressive bottleneck, and reduces the number of function evaluations. Discrete Diffusion VLA achieves 96.3% avg. success rates on LIBERO, 71.2% visual matching on SimplerEnv-Fractal and 54.2% overall on SimplerEnv-Bridge. We also provide ablation study on vision-language ability retention on LIBERO-OOD (Out-of-Distribution) benchmark, with our method improving over autoregressive, MLP decoder and continuous diffusion baselines. These findings indicate that discrete-diffusion VLA supports precise action modeling and consistent training, laying groundwork for scaling VLA to larger models and datasets. Our code is available at https://github.com/Liang-ZX/DiscreteDiffusionVLA/tree/libero.","authors":["Zhixuan Liang","Yizhuo Li","Tianshuo Yang","Chengyue Wu","Sitong Mao","Tian Nian","Liuao Pei","Shunbo Zhou","Xiaokang Yang","Jiangmiao Pang","Yao Mu","Ping Luo"],"pdf_url":"","comment":"New experiments on VL retention and new ablations. 18 pages"},{"id":"http://arxiv.org/abs/2510.09595v2","updated":"2025-12-22T18:56:01Z","published":"2025-10-10T17:54:24Z","title":"LiveOIBench: Can Large Language Models Outperform Human Contestants in Informatics Olympiads?","summary":"Competitive programming problems increasingly serve as valuable benchmarks to evaluate the coding capabilities of large language models (LLMs) due to their complexity and ease of verification. Yet, current coding benchmarks face limitations such as lack of exceptionally challenging problems, insufficient test case coverage, reliance on online platform APIs that limit accessibility. To address these issues, we introduce LiveOIBench, a comprehensive benchmark featuring 403 expert-curated Olympiad-level competitive programming problems, each with an average of 60 expert-designed test cases. The problems are sourced directly from 72 official contests of 14 Informatics Olympiads in different regions conducted between 2023 and 2025. LiveOIBench distinguishes itself through four key features: (1) meticulously curated high-quality tasks with detailed subtask rubrics and extensive private test cases; (2) direct integration of elite contestant performance data to enable informative comparison against top-performing humans; (3) planned continuous, contamination-free updates from newly released Olympiad problems; and (4) a self-contained evaluation system facilitating offline and easy-to-reproduce assessments. Benchmarking 34 popular general-purpose and reasoning LLMs, we find that GPT-5 achieves a notable 81.76th percentile, a strong result that nonetheless falls short of top human contestants, who usually place above 90th. In contrast, among open-weight reasoning models, GPT-OSS-120B achieves only a 60th percentile, underscoring significant capability disparities from frontier closed models. Detailed analyses indicate that robust reasoning models prioritize precise problem analysis over excessive exploration, suggesting future models should emphasize structured analysis and minimize unnecessary exploration. All data, code, and leaderboard results are publicly available on our website.","authors":["Kaijian Zou","Aaron Xiong","Yunxiang Zhang","Frederick Zhang","Yueqi Ren","Jirong Yang","Ayoung Lee","Shitanshu Bhushan","Lu Wang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19673v1","updated":"2025-12-22T18:51:48Z","published":"2025-12-22T18:51:48Z","title":"Bottom-up Policy Optimization: Your Language Model Policy Secretly Contains Internal Policies","summary":"Existing reinforcement learning (RL) approaches treat large language models (LLMs) as a single unified policy, overlooking their internal mechanisms. Understanding how policy evolves across layers and modules is therefore crucial for enabling more targeted optimization and raveling out complex reasoning mechanisms. In this paper, we decompose the language model policy by leveraging the intrinsic split of the Transformer residual stream and the equivalence between the composition of hidden states with the unembedding matrix and the resulting samplable policy. This decomposition reveals Internal Layer Policies, corresponding to contributions from individual layers, and Internal Modular Policies, which align with the self-attention and feed-forward network (FFN) components within each layer. By analyzing the entropy of internal policy, we find that: (a) Early layers keep high entropy for exploration, top layers converge to near-zero entropy for refinement, with convergence patterns varying across model series. (b) LLama's prediction space rapidly converges in the final layer, whereas Qwen-series models, especially Qwen3, exhibit a more human-like, progressively structured reasoning pattern. Motivated by these findings, we propose Bottom-up Policy Optimization (BuPO), a novel RL paradigm that directly optimizes the internal layer policy during early training. By aligning training objective at lower layer, BuPO reconstructs foundational reasoning capabilities and achieves superior performance. Extensive experiments on complex reasoning benchmarks demonstrates the effectiveness of our method. Our code is available at https://github.com/Trae1ounG/BuPO.","authors":["Yuqiao Tan","Minzheng Wang","Shizhu He","Huanxuan Liao","Chengfeng Zhao","Qiunan Lu","Tian Liang","Jun Zhao","Kang Liu"],"pdf_url":"","comment":"Preprint. Our code is available at https://github.com/Trae1ounG/BuPO"},{"id":"http://arxiv.org/abs/2506.22552v7","updated":"2025-12-22T18:48:57Z","published":"2025-06-27T18:04:36Z","title":"Probing forced responses and causality in data-driven climate emulators: conceptual limitations and the role of reduced-order models","summary":"A central challenge in climate science and applied mathematics is developing data-driven models of multiscale systems that capture both stationary statistics and responses to external perturbations. Current neural climate emulators aim to resolve the atmosphere-ocean system in all its complexity but often struggle to reproduce forced responses, limiting their use in causal studies such as Green's function experiments. To explore the origin of these limitations, we first examine a simplified dynamical system that retains key features of climate variability. We interpret the results through linear response theory, providing a rigorous framework to evaluate neural models beyond stationary statistics and to probe causal mechanisms. We argue that the ability of emulators of multiscale systems to reproduce perturbed statistics depends critically on (i) the choice of an appropriate coarse-grained representation and (ii) careful parameterizations of unresolved processes. These insights highlight reduced-order models, tailored to specific goals, processes, and scales, as valuable alternatives to general-purpose emulators. We next consider a real-world application by developing a neural model to investigate the joint variability of the surface temperature field and radiative fluxes. The model infers a multiplicative noise process directly from data, largely reproduces the system's probability distribution, and enables causal studies through forced responses. We discuss its limitations and outline directions for future work. Overall, these results expose key challenges in data-driven modeling of multiscale physical systems and underscore the value of coarse-grained, stochastic approaches, with response theory providing a principled framework to guide model design and enhance causal understanding.","authors":["Fabrizio Falasca"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2505.01353v2","updated":"2025-12-22T18:27:42Z","published":"2025-05-02T15:43:37Z","title":"Differentiable Nonlinear Model Predictive Control","summary":"The efficient computation of parametric solution sensitivities is a key challenge in the integration of learning-enhanced methods with nonlinear model predictive control (MPC), as their availability is crucial for many learning algorithms. This paper discusses the computation of solution sensitivities of general nonlinear programs (NLPs) using the implicit function theorem (IFT) and smoothed optimality conditions treated in interior-point methods (IPM). We detail sensitivity computation within a sequential quadratic programming (SQP) method which employs an IPM for the quadratic subproblems. Previous works presented in the machine learning community are limited to convex or unconstrained formulations, or lack an implementation for efficient sensitivity evaluation. The publication is accompanied by an efficient open-source implementation within the acados framework, providing both forward and adjoint sensitivities for general optimal control problems, achieving speedups exceeding 3x over the state-of-the-art solvers mpc.pytorch and cvxpygen.","authors":["Jonathan Frey","Katrin Baumgärtner","Gianluca Frison","Dirk Reinhardt","Jasper Hoffmann","Leonard Fichtner","Sebastien Gros","Moritz Diehl"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19649v1","updated":"2025-12-22T18:22:11Z","published":"2025-12-22T18:22:11Z","title":"Deep Legendre Transform","summary":"We introduce a novel deep learning algorithm for computing convex conjugates of differentiable convex functions, a fundamental operation in convex analysis with various applications in different fields such as optimization, control theory, physics and economics. While traditional numerical methods suffer from the curse of dimensionality and become computationally intractable in high dimensions, more recent neural network-based approaches scale better, but have mostly been studied with the aim of solving optimal transport problems and require the solution of complicated optimization or max-min problems. Using an implicit Fenchel formulation of convex conjugation, our approach facilitates an efficient gradient-based framework for the minimization of approximation errors and, as a byproduct, also provides a posteriori error estimates for the approximation quality. Numerical experiments demonstrate our method's ability to deliver accurate results across different high-dimensional examples. Moreover, by employing symbolic regression with Kolmogorov--Arnold networks, it is able to obtain the exact convex conjugates of specific convex functions.","authors":["Aleksey Minabutdinov","Patrick Cheridito"],"pdf_url":"","comment":"Accepted at NeurIPS 2025 (poster). NeurIPS page: https://neurips.cc/virtual/2025/loc/san-diego/poster/120307"},{"id":"http://arxiv.org/abs/2510.12085v2","updated":"2025-12-22T18:20:12Z","published":"2025-10-14T02:48:50Z","title":"GraphShaper: Geometry-aware Alignment for Improving Transfer Learning in Text-Attributed Graphs","summary":"Graph foundation models represent a transformative paradigm for learning transferable representations across diverse graph domains. Recent methods leverage large language models to unify graph and text modalities into a shared representation space using contrastive learning. However, systematic evaluations reveal significant performance degradation at structural boundaries where distinct topological patterns converge, with accuracy losses exceeding 20 percentage points. This issue arises from a key limitation: current methods assume all graph structures can be encoded within a single Euclidean space. In reality, tree structures require hyperbolic geometry to preserve hierarchical branching, while cyclic patterns depend on spherical geometry for closure properties. At structural boundaries, nodes experience conflicting geometric constraints that uniform encoding spaces cannot resolve. This raises a crucial challenge: \\textbf{Can alignment frameworks be designed to respect the intrinsic geometric diversity of graph structures?} We introduce \\textbf{GraphShaper}, a geometry-aware framework that enhances graph encoding through multi-geometric specialization. Our approach employs expert networks tailored to different geometric spaces, dynamically computing fusion weights to adaptively integrate geometric properties based on local structural characteristics. This adaptive fusion preserves structural integrity before alignment with text embeddings. Extensive experiments demonstrate that GraphShaper achieves 9.47\\% accuracy improvements on citation networks and 7.63\\% on social networks in zero-shot settings.","authors":["Heng Zhang","Tianyi Zhang","Yuling Shi","Xiaodong Gu","Yaomin Shen","Haochen You","Zijian Zhang","Yilei Yuan","Jin Huang"],"pdf_url":"","comment":"This submission has been withdrawn by the authors due to a fundamental error in the methodology that affects the validity of the main results"},{"id":"http://arxiv.org/abs/2512.19643v1","updated":"2025-12-22T18:17:28Z","published":"2025-12-22T18:17:28Z","title":"The Best of Both Worlds: Hybridizing Neural Operators and Solvers for Stable Long-Horizon Inference","summary":"Numerical simulation of time-dependent partial differential equations (PDEs) is central to scientific and engineering applications, but high-fidelity solvers are often prohibitively expensive for long-horizon or time-critical settings. Neural operator (NO) surrogates offer fast inference across parametric and functional inputs; however, most autoregressive NO frameworks remain vulnerable to compounding errors, and ensemble-averaged metrics provide limited guarantees for individual inference trajectories. In practice, error accumulation can become unacceptable beyond the training horizon, and existing methods lack mechanisms for online monitoring or correction. To address this gap, we propose ANCHOR (Adaptive Numerical Correction for High-fidelity Operator Rollouts), an online, instance-aware hybrid inference framework for stable long-horizon prediction of nonlinear, time-dependent PDEs. ANCHOR treats a pretrained NO as the primary inference engine and adaptively couples it with a classical numerical solver using a physics-informed, residual-based error estimator. Inspired by adaptive time-stepping in numerical analysis, ANCHOR monitors an exponential moving average (EMA) of the normalized PDE residual to detect accumulating error and trigger corrective solver interventions without requiring access to ground-truth solutions. We show that the EMA-based estimator correlates strongly with the true relative L2 error, enabling data-free, instance-aware error control during inference. Evaluations on four canonical PDEs: 1D and 2D Burgers', 2D Allen-Cahn, and 3D heat conduction, demonstrate that ANCHOR reliably bounds long-horizon error growth, stabilizes extrapolative rollouts, and significantly improves robustness over standalone neural operators, while remaining substantially more efficient than high-fidelity numerical solvers.","authors":["Rajyasri Roy","Dibyajyoti Nayak","Somdatta Goswami"],"pdf_url":"","comment":"18 pages, 7 figures"},{"id":"http://arxiv.org/abs/2511.08401v3","updated":"2025-12-22T17:58:14Z","published":"2025-11-11T16:16:10Z","title":"Source-Optimal Training is Transfer-Suboptimal","summary":"We prove that training a source model optimally for its own task is generically suboptimal when the objective is downstream transfer. We study the source-side optimization problem in L2-SP ridge regression and show a fundamental mismatch between the source-optimal and transfer-optimal source regularization: outside of a measure-zero set, $τ_0^* \\neq τ_S^*$. We characterize the transfer-optimal source penalty $τ_0^*$ as a function of task alignment and identify an alignment-dependent reversal: with imperfect alignment ($0<ρ<1$), transfer benefits from stronger source regularization, while in super-aligned regimes ($ρ>1$), transfer benefits from weaker regularization. In isotropic settings, the decision of whether transfer helps is independent of the target sample size and noise, depending only on task alignment and source characteristics. We verify the linear predictions in a synthetic ridge regression experiment, and we present CIFAR-10 experiments as evidence that the source-optimal versus transfer-optimal mismatch can persist in nonlinear networks.","authors":["C. Evans Hedges"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19605v1","updated":"2025-12-22T17:41:26Z","published":"2025-12-22T17:41:26Z","title":"KerJEPA: Kernel Discrepancies for Euclidean Self-Supervised Learning","summary":"Recent breakthroughs in self-supervised Joint-Embedding Predictive Architectures (JEPAs) have established that regularizing Euclidean representations toward isotropic Gaussian priors yields provable gains in training stability and downstream generalization. We introduce a new, flexible family of KerJEPAs, self-supervised learning algorithms with kernel-based regularizers. One instance of this family corresponds to the recently-introduced LeJEPA Epps-Pulley regularizer which approximates a sliced maximum mean discrepancy (MMD) with a Gaussian prior and Gaussian kernel. By expanding the class of viable kernels and priors and computing the closed-form high-dimensional limit of sliced MMDs, we develop alternative KerJEPAs with a number of favorable properties including improved training stability and design flexibility.","authors":["Eric Zimmermann","Harley Wiltzer","Justin Szeto","David Alvarez-Melis","Lester Mackey"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2505.17196v3","updated":"2025-12-22T17:30:15Z","published":"2025-05-22T18:05:16Z","title":"Shape it Up! Restoring LLM Safety during Finetuning","summary":"Finetuning large language models (LLMs) enables user-specific customization but introduces critical safety risks: even a few harmful examples can compromise safety alignment. A common mitigation strategy is to update the model more strongly on examples deemed safe, while downweighting or excluding those flagged as unsafe. However, because safety context can shift within a single example, updating the model equally on both harmful and harmless parts of a response is suboptimal-a coarse treatment we term static safety shaping. In contrast, we propose dynamic safety shaping (DSS), a framework that uses fine-grained safety signals to reinforce learning from safe segments of a response while suppressing unsafe content. To enable such fine-grained control during finetuning, we introduce a key insight: guardrail models, traditionally used for filtering, can be repurposed to evaluate partial responses, tracking how safety risk evolves throughout the response, segment by segment. This leads to the Safety Trajectory Assessment of Response (STAR), a token-level signal that enables shaping to operate dynamically over the training sequence. Building on this, we present STAR-DSS, guided by STAR scores, that robustly mitigates finetuning risks and delivers substantial safety improvements across diverse threats, datasets, and model families-all without compromising capability on intended tasks. We encourage future safety research to build on dynamic shaping principles for stronger mitigation against evolving finetuning risks. Our code is publicly available at https://github.com/poloclub/star-dss.","authors":["ShengYun Peng","Pin-Yu Chen","Jianfeng Chi","Seongmin Lee","Duen Horng Chau"],"pdf_url":"","comment":"NeurIPS'25"},{"id":"http://arxiv.org/abs/2507.22782v3","updated":"2025-12-22T17:22:59Z","published":"2025-07-30T15:48:38Z","title":"Enhancing Multi-Agent Collaboration with Attention-Based Actor-Critic Policies","summary":"This paper introduces Team-Attention-Actor-Critic (TAAC), a reinforcement learning algorithm designed to enhance multi-agent collaboration in cooperative environments. TAAC employs a Centralized Training/Centralized Execution scheme incorporating multi-headed attention mechanisms in both the actor and critic. This design facilitates dynamic, inter-agent communication, allowing agents to explicitly query teammates, thereby efficiently managing the exponential growth of joint-action spaces while ensuring a high degree of collaboration. We further introduce a penalized loss function which promotes diverse yet complementary roles among agents. We evaluate TAAC in a simulated soccer environment against benchmark algorithms representing other multi-agent paradigms, including Proximal Policy Optimization and Multi-Agent Actor-Attention-Critic. We find that TAAC exhibits superior performance and enhanced collaborative behaviors across a variety of metrics (win rates, goal differentials, Elo ratings, inter-agent connectivity, balanced spatial distributions, and frequent tactical interactions such as ball possession swaps).","authors":["Hugo Garrido-Lestache Belinchon","Jeremy Kedziora"],"pdf_url":"","comment":"11 pages"},{"id":"http://arxiv.org/abs/2512.19576v1","updated":"2025-12-22T17:00:25Z","published":"2025-12-22T17:00:25Z","title":"LeLaR: The First In-Orbit Demonstration of an AI-Based Satellite Attitude Controller","summary":"Attitude control is essential for many satellite missions. Classical controllers, however, are time-consuming to design and sensitive to model uncertainties and variations in operational boundary conditions. Deep Reinforcement Learning (DRL) offers a promising alternative by learning adaptive control strategies through autonomous interaction with a simulation environment. Overcoming the Sim2Real gap, which involves deploying an agent trained in simulation onto the real physical satellite, remains a significant challenge. In this work, we present the first successful in-orbit demonstration of an AI-based attitude controller for inertial pointing maneuvers. The controller was trained entirely in simulation and deployed to the InnoCube 3U nanosatellite, which was developed by the Julius-Maximilians-Universität Würzburg in cooperation with the Technische Universität Berlin, and launched in January 2025. We present the AI agent design, the methodology of the training procedure, the discrepancies between the simulation and the observed behavior of the real satellite, and a comparison of the AI-based attitude controller with the classical PD controller of InnoCube. Steady-state metrics confirm the robust performance of the AI-based controller during repeated in-orbit maneuvers.","authors":["Kirill Djebko","Tom Baumann","Erik Dilger","Frank Puppe","Sergio Montenegro"],"pdf_url":"","comment":"55 pages, 27 figures, 29 tables. The maneuver telemetry datasets generated and analyzed during this work are available in the GitHub repository https://github.com/kdjebko/lelar-in-orbit-data"},{"id":"http://arxiv.org/abs/2512.19554v1","updated":"2025-12-22T16:34:21Z","published":"2025-12-22T16:34:21Z","title":"CARE What Fails: Contrastive Anchored-REflection for Verifiable Multimodal","summary":"Group-relative reinforcement learning with verifiable rewards (RLVR) often wastes the most informative data it already has the failures. When all rollouts are wrong, gradients stall; when one happens to be correct, the update usually ignores why the others are close-but-wrong, and credit can be misassigned to spurious chains. We present CARE (Contrastive Anchored REflection), a failure-centric post-training framework for multimodal reasoning that turns errors into supervision. CARE combines: (i) an anchored-contrastive objective that forms a compact subgroup around the best rollout and a set of semantically proximate hard negatives, performs within-subgroup z-score normalization with negative-only scaling, and includes an all-negative rescue to prevent zero-signal batches; and (ii) Reflection-Guided Resampling (RGR), a one-shot structured self-repair that rewrites a representative failure and re-scores it with the same verifier, converting near-misses into usable positives without any test-time reflection. CARE improves accuracy and training smoothness while explicitly increasing the share of learning signal that comes from failures. On Qwen2.5-VL-7B, CARE lifts macro-averaged accuracy by 4.6 points over GRPO across six verifiable visual-reasoning benchmarks; with Qwen3-VL-8B it reaches competitive or state-of-the-art results on MathVista and MMMU-Pro under an identical evaluation protocol.","authors":["Yongxin Wang","Zhicheng Yang","Meng Cao","Mingfei Han","Haokun Lin","Yingying Zhu","Xiaojun Chang","Xiaodan Liang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19550v1","updated":"2025-12-22T16:31:14Z","published":"2025-12-22T16:31:14Z","title":"DFORD: Directional Feedback based Online Ordinal Regression Learning","summary":"In this paper, we introduce directional feedback in the ordinal regression setting, in which the learner receives feedback on whether the predicted label is on the left or the right side of the actual label. This is a weak supervision setting for ordinal regression compared to the full information setting, where the learner can access the labels. We propose an online algorithm for ordinal regression using directional feedback. The proposed algorithm uses an exploration-exploitation scheme to learn from directional feedback efficiently. Furthermore, we introduce its kernel-based variant to learn non-linear ordinal regression models in an online setting. We use a truncation trick to make the kernel implementation more memory efficient. The proposed algorithm maintains the ordering of the thresholds in the expected sense. Moreover, it achieves the expected regret of $\\mathcal{O}(\\log T)$. We compare our approach with a full information and a weakly supervised algorithm for ordinal regression on synthetic and real-world datasets. The proposed approach, which learns using directional feedback, performs comparably (sometimes better) to its full information counterpart.","authors":["Naresh Manwani","M Elamparithy","Tanish Taneja"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2410.24116v2","updated":"2025-12-22T16:25:04Z","published":"2024-10-31T16:46:23Z","title":"AIDOVECL: AI-generated Dataset of Outpainted Vehicles for Eye-level Classification and Localization","summary":"Image labeling is a critical bottleneck in the development of computer vision technologies, often constraining the potential of machine learning models due to the time-intensive nature of manual annotations. This work introduces a novel approach that leverages outpainting to mitigate the problem of annotated data scarcity by generating artificial contexts and annotations, significantly reducing manual labeling efforts. We apply this technique to a particularly acute challenge in autonomous driving, urban planning, and environmental monitoring: the lack of diverse, eye-level vehicle images in desired classes. Our dataset comprises AI-generated vehicle images obtained by detecting and cropping vehicles from manually selected seed images, which are then outpainted onto larger canvases to simulate varied real-world conditions. The outpainted images include detailed annotations, providing high-quality ground truth data. Advanced outpainting techniques and image quality assessments ensure visual fidelity and contextual relevance. Ablation results show that incorporating AIDOVECL improves overall detection performance by up to 10%, and delivers gains of up to 40% in settings with greater diversity of context, object scale, and placement, with underrepresented classes achieving up to 50% higher true positives. AIDOVECL enhances vehicle detection by augmenting real training data and supporting evaluation across diverse scenarios. By demonstrating outpainting as an automatic annotation paradigm, it offers a practical and versatile solution for building fine-grained datasets with reduced labeling effort across multiple machine learning domains. The code and links to datasets used in this study are available for further research and replication at https://github.com/amir-kazemi/aidovecl .","authors":["Amir Kazemi","Qurat ul ain Fatima","Volodymyr Kindratenko","Christopher Tessum"],"pdf_url":"","comment":"34 pages, 10 figures, 5 tables"},{"id":"http://arxiv.org/abs/2512.19540v1","updated":"2025-12-22T16:24:12Z","published":"2025-12-22T16:24:12Z","title":"Active Convolved Illumination with Deep Transfer Learning for Complex Beam Transmission through Atmospheric Turbulence","summary":"Atmospheric turbulence imposes a fundamental limitation across a broad range of applications, including optical imaging, remote sensing, and free-space optical communication. Recent advances in adaptive optics, wavefront shaping, and machine learning, driven by synergistic progress in fundamental theories, optoelectronic hardware, and computational algorithms, have demonstrated substantial potential in mitigating turbulence-induced distortions. Recently, active convolved illumination (ACI) was proposed as a versatile and physics-driven technique for transmitting structured light beams with minimal distortion through highly challenging turbulent regimes. While distinct in its formulation, ACI shares conceptual similarities with other physics-driven distortion correction approaches and stands to benefit from complementary integration with data-driven deep learning (DL) models. Inspired by recent work coupling deep learning with traditional turbulence mitigation strategies, the present work investigates the feasibility of integrating ACI with neural network-based methods. We outline a conceptual framework for coupling ACI with data-driven models and identify conditions under which learned representations can meaningfully support ACI's correlation-injection mechanism. As a representative example, we employ a convolutional neural network (CNN) together with a transfer-learning approach to examine how a learned model may operate in tandem with ACI. This exploratory study demonstrates feasible implementation pathways and establishes an early foundation for assessing the potential of future ACI-DL hybrid architectures, representing a step toward evaluating broader synergistic interactions between ACI and modern DL models.","authors":["Adrian A. Moazzam","Anindya Ghoshroy","Breeanne Heusdens","Durdu O. Guney","Roohollah Askari"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19530v1","updated":"2025-12-22T16:19:01Z","published":"2025-12-22T16:19:01Z","title":"Learning Continuous Solvent Effects from Transient Flow Data: A Graph Neural Network Benchmark on Catechol Rearrangement","summary":"Predicting reaction outcomes across continuous solvent composition ranges remains a critical challenge in organic synthesis and process chemistry. Traditional machine learning approaches often treat solvent identity as a discrete categorical variable, which prevents systematic interpolation and extrapolation across the solvent space. This work introduces the \\textbf{Catechol Benchmark}, a high-throughput transient flow chemistry dataset comprising 1,227 experimental yield measurements for the rearrangement of allyl-substituted catechol in 24 pure solvents and their binary mixtures, parameterized by continuous volume fractions ($\\% B$). We evaluate various architectures under rigorous leave-one-solvent-out and leave-one-mixture-out protocols to test generalization to unseen chemical environments.\n  Our results demonstrate that classical tabular methods (e.g., Gradient-Boosted Decision Trees) and large language model embeddings (e.g., Qwen-7B) struggle with quantitative precision, yielding Mean Squared Errors (MSE) of 0.099 and 0.129, respectively. In contrast, we propose a hybrid GNN-based architecture that integrates Graph Attention Networks (GATs) with Differential Reaction Fingerprints (DRFP) and learned mixture-aware solvent encodings. This approach achieves an \\textbf{MSE of 0.0039} ($\\pm$ 0.0003), representing a 60\\% error reduction over competitive baselines and a $>25\\times$ improvement over tabular ensembles. Ablation studies confirm that explicit molecular graph message-passing and continuous mixture encoding are essential for robust generalization. The complete dataset, evaluation protocols, and reference implementations are released to facilitate data-efficient reaction prediction and continuous solvent representation learning.","authors":["Hongsheng Xing","Qiuxin Si"],"pdf_url":"","comment":"13 pages, 6 figures"},{"id":"http://arxiv.org/abs/2512.19527v1","updated":"2025-12-22T16:18:29Z","published":"2025-12-22T16:18:29Z","title":"Deep Learning for Unrelated-Machines Scheduling: Handling Variable Dimensions","summary":"Deep learning has been effectively applied to many discrete optimization problems. However, learning-based scheduling on unrelated parallel machines remains particularly difficult to design. Not only do the numbers of jobs and machines vary, but each job-machine pair has a unique processing time, dynamically altering feature dimensions. We propose a novel approach with a neural network tailored for offline deterministic scheduling of arbitrary sizes on unrelated machines. The goal is to minimize a complex objective function that includes the makespan and the weighted tardiness of jobs and machines. Unlike existing online approaches, which process jobs sequentially, our method generates a complete schedule considering the entire input at once. The key contribution of this work lies in the sophisticated architecture of our model. By leveraging various NLP-inspired architectures, it effectively processes any number of jobs and machines with varying feature dimensions imposed by unrelated processing times. Our approach enables supervised training on small problem instances while demonstrating strong generalization to much larger scheduling environments. Trained and tested on instances with 8 jobs and 4 machines, costs were only 2.51% above optimal. Across all tested configurations of up to 100 jobs and 10 machines, our network consistently outperformed an advanced dispatching rule, which incurred 22.22% higher costs on average. As our method allows fast retraining with simulated data and adaptation to various scheduling conditions, we believe it has the potential to become a standard approach for learning-based scheduling on unrelated machines and similar problem environments.","authors":["Diego Hitzges","Guillaume Sagnol"],"pdf_url":"","comment":"24th IEEE International Conference on Machine Learning and Applications (ICMLA 2025) in Boca Raton, USA. Project page: https://github.com/DiegoHitzges/Deep-Learning-for-Unrelated-Machines-Scheduling . 8 pages, 4 figures, 3 tables"},{"id":"http://arxiv.org/abs/2512.19524v1","updated":"2025-12-22T16:17:37Z","published":"2025-12-22T16:17:37Z","title":"Initialization of a Polyharmonic Cascade, Launch and Testing","summary":"This paper concludes a series of studies on the polyharmonic cascade, a deep machine learning architecture theoretically derived from indifference principles and the theory of random functions. A universal initialization procedure is proposed, based on symmetric constellations in the form of hyperoctahedra with a central point. This initialization not only ensures stable training of cascades with tens and hundreds of layers (up to 500 layers without skip connections), but also radically simplifies the computations. Scalability and robustness are demonstrated on MNIST (98.3% without convolutions or augmentations), HIGGS (AUC approximately 0.885 on 11M examples), and Epsilon (AUC approximately 0.963 with 2000 features). All linear algebra is reduced to 2D operations and is efficiently executed on GPUs. A public repository and an archived snapshot are provided for full reproducibility.","authors":["Yuriy N. Bakhvalov"],"pdf_url":"","comment":"Part 4 of 4 in the \"Polyharmonic Cascade\" cycle. Contains initialization algorithms and experimental results (MNIST, HIGGS, Epsilon). Previous papers: arXiv:2512.12731, arXiv:2512.16718, arXiv:2512.17671. Source code: https://github.com/xolod7/polyharmonic-cascade"},{"id":"http://arxiv.org/abs/2512.17654v2","updated":"2025-12-22T16:13:25Z","published":"2025-12-19T14:52:04Z","title":"Estimating Spatially Resolved Radiation Fields Using Neural Networks","summary":"We present an in-depth analysis on how to build and train neural networks to estimate the spatial distribution of scattered radiation fields for radiation protection dosimetry in medical radiation fields, such as those found in interventional radiology and cardiology. We present three different synthetically generated datasets with increasing complexity for training, using a Monte-Carlo Simulation application based on Geant4. On those datasets, we evaluate convolutional and fully connected architectures of neural networks to demonstrate which design decisions work well for reconstructing the fluence and spectra distributions over the spatial domain of such radiation fields. All our datasets, as well as our training pipeline, are published as open source in separate repositories.","authors":["Felix Lehner","Pasquale Lombardo","Susana Castillo","Oliver Hupe","Marcus Magnor"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19516v1","updated":"2025-12-22T16:08:03Z","published":"2025-12-22T16:08:03Z","title":"LacaDM: A Latent Causal Diffusion Model for Multiobjective Reinforcement Learning","summary":"Multiobjective reinforcement learning (MORL) poses significant challenges due to the inherent conflicts between objectives and the difficulty of adapting to dynamic environments. Traditional methods often struggle to generalize effectively, particularly in large and complex state-action spaces. To address these limitations, we introduce the Latent Causal Diffusion Model (LacaDM), a novel approach designed to enhance the adaptability of MORL in discrete and continuous environments. Unlike existing methods that primarily address conflicts between objectives, LacaDM learns latent temporal causal relationships between environmental states and policies, enabling efficient knowledge transfer across diverse MORL scenarios. By embedding these causal structures within a diffusion model-based framework, LacaDM achieves a balance between conflicting objectives while maintaining strong generalization capabilities in previously unseen environments. Empirical evaluations on various tasks from the MOGymnasium framework demonstrate that LacaDM consistently outperforms the state-of-art baselines in terms of hypervolume, sparsity, and expected utility maximization, showcasing its effectiveness in complex multiobjective tasks.","authors":["Xueming Yan","Bo Yin","Yaochu Jin"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2505.11076v3","updated":"2025-12-22T16:05:29Z","published":"2025-05-16T10:07:36Z","title":"Addition is almost all you need: Compressing neural networks with double binary factorization","summary":"Binary quantization approaches, which replace weight matrices with binary matrices and substitute costly multiplications with cheaper additions, offer a computationally efficient approach to address the increasing computational and storage requirements of Large Language Models (LLMs). However, the severe quantization constraint ($\\pm1$) can lead to significant accuracy degradation. In this paper, we propose Double Binary Factorization (DBF), a novel method that factorizes dense weight matrices into products of two binary (sign) matrices, each accompanied by scaling vectors. DBF preserves the efficiency advantages of binary representations while achieving compression rates that are competitive with or superior to state-of-the-art methods. Specifically, in a 1-bit per weight range, DBF is better than existing binarization approaches. In a 2-bit per weight range, DBF is competitive with the best quantization methods like QuIP\\# and QTIP. Unlike most existing compression techniques, which offer limited compression level choices, DBF allows fine-grained control over compression ratios by adjusting the factorization's intermediate dimension. Based on this advantage, we further introduce an algorithm for estimating non-uniform layer-wise compression ratios for DBF, based on previously developed channel pruning criteria.\n  Code available at: https://github.com/usamec/double_binary","authors":["Vladimír Boža","Vladimír Macko"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19510v1","updated":"2025-12-22T16:05:18Z","published":"2025-12-22T16:05:18Z","title":"Toward Scalable and Valid Conditional Independence Testing with Spectral Representations","summary":"Conditional independence (CI) is central to causal inference, feature selection, and graphical modeling, yet it is untestable in many settings without additional assumptions. Existing CI tests often rely on restrictive structural conditions, limiting their validity on real-world data. Kernel methods using the partial covariance operator offer a more principled approach but suffer from limited adaptivity, slow convergence, and poor scalability. In this work, we explore whether representation learning can help address these limitations. Specifically, we focus on representations derived from the singular value decomposition of the partial covariance operator and use them to construct a simple test statistic, reminiscent of the Hilbert-Schmidt Independence Criterion (HSIC). We also introduce a practical bi-level contrastive algorithm to learn these representations. Our theory links representation learning error to test performance and establishes asymptotic validity and power guarantees. Preliminary experiments suggest that this approach offers a practical and statistically grounded path toward scalable CI testing, bridging kernel-based theory with modern representation learning.","authors":["Alek Frohlich","Vladimir Kostic","Karim Lounici","Daniel Perazzo","Massimiliano Pontil"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19506v1","updated":"2025-12-22T16:00:55Z","published":"2025-12-22T16:00:55Z","title":"DK-STN: A Domain Knowledge Embedded Spatio-Temporal Network Model for MJO Forecast","summary":"Understanding and predicting the Madden-Julian Oscillation (MJO) is fundamental for precipitation forecasting and disaster prevention. To date, long-term and accurate MJO prediction has remained a challenge for researchers. Conventional MJO prediction methods using Numerical Weather Prediction (NWP) are resource-intensive, time-consuming, and highly unstable (most NWP methods are sensitive to seasons, with better MJO forecast results in winter). While existing Artificial Neural Network (ANN) methods save resources and speed forecasting, their accuracy never reaches the 28 days predicted by the state-of-the-art NWP method, i.e., the operational forecasts from ECMWF, since neural networks cannot handle climate data effectively. In this paper, we present a Domain Knowledge Embedded Spatio-Temporal Network (DK-STN), a stable neural network model for accurate and efficient MJO forecasting. It combines the benefits of NWP and ANN methods and successfully improves the forecast accuracy of ANN methods while maintaining a high level of efficiency and stability. We begin with a spatial-temporal network (STN) and embed domain knowledge in it using two key methods: (i) applying a domain knowledge enhancement method and (ii) integrating a domain knowledge processing method into network training. We evaluated DK-STN with the 5th generation of ECMWF reanalysis (ERA5) data and compared it with ECMWF. Given 7 days of climate data as input, DK-STN can generate reliable forecasts for the following 28 days in 1-2 seconds, with an error of only 2-3 days in different seasons. DK-STN significantly exceeds ECMWF in that its forecast accuracy is equivalent to ECMWF's, while its efficiency and stability are significantly superior.","authors":["Hongliang Li","Nong Zhang","Zhewen Xu","Xiang Li","Changzheng Liu","Chongbo Zhao","Jie Wu"],"pdf_url":"","comment":"18 pages, 10 figures"},{"id":"http://arxiv.org/abs/2511.14455v3","updated":"2025-12-22T16:00:38Z","published":"2025-11-18T12:59:20Z","title":"Nonparametric estimation of conditional probability distributions using a generative approach based on conditional push-forward neural networks","summary":"We introduce conditional push-forward neural networks (CPFN), a generative framework for conditional distribution estimation. Instead of directly modeling the conditional density $f_{Y|X}$, CPFN learns a stochastic map $\\varphi=\\varphi(x,u)$ such that $\\varphi(x,U)$ and $Y|X=x$ follow approximately the same law, with $U$ a suitable random vector of pre-defined latent variables. This enables efficient conditional sampling and straightforward estimation of conditional statistics through Monte Carlo methods. The model is trained via an objective function derived from a Kullback-Leibler formulation, without requiring invertibility or adversarial training. We establish a near-asymptotic consistency result and demonstrate experimentally that CPFN can achieve performance competitive with, or even superior to, state-of-the-art methods, including kernel estimators, tree-based algorithms, and popular deep learning techniques, all while remaining lightweight and easy to train.","authors":["Nicola Rares Franco","Lorenzo Tedesco"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2306.05300v3","updated":"2025-12-22T15:54:45Z","published":"2023-06-08T15:45:57Z","title":"Anti-Correlated Noise in Epoch-Based Stochastic Gradient Descent: Implications for Weight Variances in Flat Directions","summary":"Stochastic Gradient Descent (SGD) has become a cornerstone of neural network optimization due to its computational efficiency and generalization capabilities. However, the gradient noise introduced by SGD is often assumed to be uncorrelated over time, despite the common practice of epoch-based training where data is sampled without replacement. In this work, we challenge this assumption and investigate the effects of epoch-based noise correlations on the stationary distribution of discrete-time SGD with momentum. Our main contributions are twofold: First, we calculate the exact autocorrelation of the noise during epoch-based training under the assumption that the noise is independent of small fluctuations in the weight vector, revealing that SGD noise is inherently anti-correlated over time. Second, we explore the influence of these anti-correlations on the variance of weight fluctuations. We find that for directions with curvature of the loss greater than a hyperparameter-dependent crossover value, the conventional predictions of isotropic weight variance under stationarity, based on uncorrelated and curvature-proportional noise, are recovered. Anti-correlations have negligible effect here. However, for relatively flat directions, the weight variance is significantly reduced, leading to a considerable decrease in loss fluctuations compared to the constant weight variance assumption. Furthermore, we present a numerical experiment where training with these anti-correlations enhances test performance, suggesting that the inherent noise structure induced by epoch-based training may play a role in finding flatter minima that generalize better.","authors":["Marcel Kühn","Bernd Rosenow"],"pdf_url":"","comment":"55 pages, 16 figures, Machine Learning: Science and Technology 2025"},{"id":"http://arxiv.org/abs/2505.20063v2","updated":"2025-12-22T15:49:59Z","published":"2025-05-26T14:47:59Z","title":"SAEs Are Good for Steering -- If You Select the Right Features","summary":"Sparse Autoencoders (SAEs) have been proposed as an unsupervised approach to learn a decomposition of a model's latent space. This enables useful applications such as steering - influencing the output of a model towards a desired concept - without requiring labeled data. Current methods identify SAE features to steer by analyzing the input tokens that activate them. However, recent work has highlighted that activations alone do not fully describe the effect of a feature on the model's output. In this work, we draw a distinction between two types of features: input features, which mainly capture patterns in the model's input, and output features, which have a human-understandable effect on the model's output. We propose input and output scores to characterize and locate these types of features, and show that high values for both scores rarely co-occur in the same features. These findings have practical implications: after filtering out features with low output scores, we obtain 2-3x improvements when steering with SAEs, making them competitive with supervised methods.","authors":["Dana Arad","Aaron Mueller","Yonatan Belinkov"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19494v1","updated":"2025-12-22T15:49:24Z","published":"2025-12-22T15:49:24Z","title":"Kolmogorov-Arnold Graph Neural Networks Applied to Inorganic Nanomaterials Dataset","summary":"The recent development of Kolmogorov-Arnold Networks (KANs) introduced new discoveries in the field of Graph Neural Networks (GNNs), expanding the existing set of models with KAN-based versions of GNNs, which often surpass the accuracy of MultiLayer Perceptron (MLP)-based GNNs. These models were widely tested on the graph datasets consisting of organic molecules; however, those studies disregarded the inorganic nanomaterials datasets. In this work, we close this gap by applying Kolmogorov-Arnold Graph Neural Networks (KAGNNs) to a recently published large inorganic nanomaterials dataset called CHILI. For this, we adapt and test KAGNNs appropriate for this dataset. Our experiments reveal that on the CHILI datasets, particularly on the CHILI-3K, KAGNNs substantially surpass conventional GNNs in classification, achieving state-of-the-art results.","authors":["Nikita Volzhin","Soowhan Yoon"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2509.24936v2","updated":"2025-12-22T15:45:08Z","published":"2025-09-29T15:36:27Z","title":"OAT-FM: Optimal Acceleration Transport for Improved Flow Matching","summary":"As a powerful technique in generative modeling, Flow Matching (FM) aims to learn velocity fields from noise to data, which is often explained and implemented as solving Optimal Transport (OT) problems. In this study, we bridge FM and the recent theory of Optimal Acceleration Transport (OAT), developing an improved FM method called OAT-FM and exploring its benefits in both theory and practice. In particular, we demonstrate that the straightening objective hidden in existing OT-based FM methods is mathematically equivalent to minimizing the physical action associated with acceleration defined by OAT. Accordingly, instead of enforcing constant velocity, OAT-FM optimizes the acceleration transport in the product space of sample and velocity, whose objective corresponds to a necessary and sufficient condition of flow straightness. An efficient algorithm is designed to achieve OAT-FM with low complexity. OAT-FM motivates a new two-phase FM paradigm: Given a generative model trained by an arbitrary FM method, whose velocity information has been relatively reliable, we can fine-tune and improve it via OAT-FM. This paradigm eliminates the risk of data distribution drift and the need to generate a large number of noise data pairs, which consistently improves model performance in various generative tasks. Code is available at: https://github.com/AngxiaoYue/OAT-FM","authors":["Angxiao Yue","Anqi Dong","Hongteng Xu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19491v1","updated":"2025-12-22T15:44:47Z","published":"2025-12-22T15:44:47Z","title":"Learning from sanctioned government suppliers: A machine learning and network science approach to detecting fraud and corruption in Mexico","summary":"Detecting fraud and corruption in public procurement remains a major challenge for governments worldwide. Most research to-date builds on domain-knowledge-based corruption risk indicators of individual contract-level features and some also analyzes contracting network patterns. A critical barrier for supervised machine learning is the absence of confirmed non-corrupt, negative, examples, which makes conventional machine learning inappropriate for this task. Using publicly available data on federally funded procurement in Mexico and company sanction records, this study implements positive-unlabeled (PU) learning algorithms that integrate domain-knowledge-based red flags with network-derived features to identify likely corrupt and fraudulent contracts. The best-performing PU model on average captures 32 percent more known positives and performs on average 2.3 times better than random guessing, substantially outperforming approaches based solely on traditional red flags. The analysis of the Shapley Additive Explanations reveals that network-derived features, particularly those associated with contracts in the network core or suppliers with high eigenvector centrality, are the most important. Traditional red flags further enhance model performance in line with expectations, albeit mainly for contracts awarded through competitive tenders. This methodology can support law enforcement in Mexico, and it can be adapted to other national contexts too.","authors":["Martí Medina-Hern ández","Janos Kertész","Mihály Fazekas"],"pdf_url":"","comment":"15 pages of main text with 6 figures and 31 pages of supplementary information"},{"id":"http://arxiv.org/abs/2512.19488v1","updated":"2025-12-22T15:43:39Z","published":"2025-12-22T15:43:39Z","title":"Lightweight Intrusion Detection in IoT via SHAP-Guided Feature Pruning and Knowledge-Distilled Kronecker Networks","summary":"The widespread deployment of Internet of Things (IoT) devices requires intrusion detection systems (IDS) with high accuracy while operating under strict resource constraints. Conventional deep learning IDS are often too large and computationally intensive for edge deployment. We propose a lightweight IDS that combines SHAP-guided feature pruning with knowledge-distilled Kronecker networks. A high-capacity teacher model identifies the most relevant features through SHAP explanations, and a compressed student leverages Kronecker-structured layers to minimize parameters while preserving discriminative inputs. Knowledge distillation transfers softened decision boundaries from teacher to student, improving generalization under compression. Experiments on the TON\\_IoT dataset show that the student is nearly three orders of magnitude smaller than the teacher yet sustains macro-F1 above 0.986 with millisecond-level inference latency. The results demonstrate that explainability-driven pruning and structured compression can jointly enable scalable, low-latency, and energy-efficient IDS for heterogeneous IoT environments.","authors":["Hafsa Benaddi","Mohammed Jouhari","Nouha Laamech","Anas Motii","Khalil Ibrahimi"],"pdf_url":"","comment":"This work has been published in the proceedings of the 2025 8th International Conference on Advanced Communication Technologies and Networking (CommNet)"},{"id":"http://arxiv.org/abs/2512.19472v1","updated":"2025-12-22T15:25:10Z","published":"2025-12-22T15:25:10Z","title":"Multi-Layer Confidence Scoring for Detection of Out-of-Distribution Samples, Adversarial Attacks, and In-Distribution Misclassifications","summary":"The recent explosive growth in Deep Neural Networks applications raises concerns about the black-box usage of such models, with limited trasparency and trustworthiness in high-stakes domains, which have been crystallized as regulatory requirements such as the European Union Artificial Intelligence Act. While models with embedded confidence metrics have been proposed, such approaches cannot be applied to already existing models without retraining, limiting their broad application. On the other hand, post-hoc methods, which evaluate pre-trained models, focus on solving problems related to improving the confidence in the model's predictions, and detecting Out-Of-Distribution or Adversarial Attacks samples as independent applications. To tackle the limited applicability of already existing methods, we introduce Multi-Layer Analysis for Confidence Scoring (MACS), a unified post-hoc framework that analyzes intermediate activations to produce classification-maps. From the classification-maps, we derive a score applicable for confidence estimation, detecting distributional shifts and adversarial attacks, unifying the three problems in a common framework, and achieving performances that surpass the state-of-the-art approaches in our experiments with the VGG16 and ViTb16 models with a fraction of their computational overhead.","authors":["Lorenzo Capelli","Leandro de Souza Rosa","Gianluca Setti","Mauro Mangia","Riccardo Rovatti"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19469v1","updated":"2025-12-22T15:23:19Z","published":"2025-12-22T15:23:19Z","title":"GLUE: Generative Latent Unification of Expertise-Informed Engineering Models","summary":"Engineering complex systems (aircraft, buildings, vehicles) requires accounting for geometric and performance couplings across subsystems. As generative models proliferate for specialized domains (wings, structures, engines), a key research gap is how to coordinate frozen, pre-trained submodels to generate full-system designs that are feasible, diverse, and high-performing. We introduce Generative Latent Unification of Expertise-Informed Engineering Models (GLUE), which orchestrates pre-trained, frozen subsystem generators while enforcing system-level feasibility, optimality, and diversity. We propose and benchmark (i) data-driven GLUE models trained on pre-generated system-level designs and (ii) a data-free GLUE model trained online on a differentiable geometry layer. On a UAV design problem with five coupling constraints, we find that data-driven approaches yield diverse, high-performing designs but require large datasets to satisfy constraints reliably. The data-free approach is competitive with Bayesian optimization and gradient-based optimization in performance and feasibility while training a full generative model in only 10 min on a RTX 4090 GPU, requiring more than two orders of magnitude fewer geometry evaluations and FLOPs than the data-driven method. Ablations focused on data-free training show that subsystem output continuity affects coordination, and equality constraints can trigger mode collapse unless mitigated. By integrating unmodified, domain-informed submodels into a modular generative workflow, this work provides a viable path for scaling generative design to complex, real-world engineering systems.","authors":["Tim Aebersold","Soheyl Massoudi","Mark D. Fuge"],"pdf_url":"","comment":"11 pages, 10 figures. Preprint. Submitted to Computer-Aided Engineering (Elsevier)"},{"id":"http://arxiv.org/abs/2512.17593v2","updated":"2025-12-22T15:11:58Z","published":"2025-12-19T14:01:50Z","title":"A Unified Representation of Neural Networks Architectures","summary":"In this paper we consider the limiting case of neural networks (NNs) architectures when the number of neurons in each hidden layer and the number of hidden layers tend to infinity thus forming a continuum, and we derive approximation errors as a function of the number of neurons and/or hidden layers. Firstly, we consider the case of neural networks with a single hidden layer and we derive an integral infinite width neural representation that generalizes existing continuous neural networks (CNNs) representations. Then we extend this to deep residual CNNs that have a finite number of integral hidden layers and residual connections. Secondly, we revisit the relation between neural ODEs and deep residual NNs and we formalize approximation errors via discretization techniques. Then, we merge these two approaches into a unified homogeneous representation of NNs as a Distributed Parameter neural Network (DiPaNet) and we show that most of the existing finite and infinite-dimensional NNs architectures are related via homogenization/discretization with the DiPaNet representation. Our approach is purely deterministic and applies to general, uniformly continuous matrix weight functions. Relations with neural fields and other neural integro-differential equations are discussed along with further possible generalizations and applications of the DiPaNet framework.","authors":["Christophe Prieur","Mircea Lazar","Bogdan Robu"],"pdf_url":"","comment":"Minor typographical corrections and clarifications; results unchanged"},{"id":"http://arxiv.org/abs/2507.18540v2","updated":"2025-12-22T14:56:25Z","published":"2025-07-24T16:07:13Z","title":"Deep Variational Free Energy Calculation of Hydrogen Hugoniot","summary":"We develop a deep variational free energy framework to compute the equation of state of hydrogen in the warm dense matter region. This method parameterizes the variational density matrix of hydrogen nuclei and electrons at finite temperature using three deep generative models: a normalizing flow model for the Boltzmann distribution of the classical nuclei, an autoregressive transformer for the distribution of electrons in excited states, and a permutational equivariant flow model for the unitary backflow transformation of electron coordinates in Hartree-Fock states. By jointly optimizing the three neural networks to minimize the variational free energy, we obtain the equation of state and related thermodynamic properties of dense hydrogen for the temperature range where electrons occupy excited states. We compare our results with other theoretical and experimental results on the deuterium Hugoniot curve, aiming to resolve existing discrepancies. Our results bridge the gap between the results obtained by path-integral Monte Carlo calculations at high temperature and ground-state electronic methods at low temperature, thus providing a valuable benchmark for hydrogen in the warm dense matter region.","authors":["Zihang Li","Hao Xie","Xinyang Dong","Lei Wang"],"pdf_url":"","comment":"8+18 pages, 4+12 figures, for source code and raw data, see https://github.com/fermiflow/Hugoniot, https://github.com/ZihangL/hqc, https://huggingface.co/datasets/Kelvin2025q/hugoniot"},{"id":"http://arxiv.org/abs/2505.14081v2","updated":"2025-12-22T14:49:44Z","published":"2025-05-20T08:39:16Z","title":"Personalized and Resilient Distributed Learning Through Opinion Dynamics","summary":"In this paper, we address two practical challenges of distributed learning in multi-agent network systems, namely personalization and resilience. Personalization is the need of heterogeneous agents to learn local models tailored to their own data and tasks, while still generalizing well; on the other hand, the learning process must be resilient to cyberattacks or anomalous training data to avoid disruption. Motivated by a conceptual affinity between these two requirements, we devise a distributed learning algorithm that combines distributed gradient descent and the Friedkin-Johnsen model of opinion dynamics to fulfill both of them. We quantify its convergence speed and the neighborhood that contains the final learned models, which can be easily controlled by tuning the algorithm parameters to enforce a more personalized/resilient behavior. We numerically showcase the effectiveness of our algorithm on synthetic and real-world distributed learning tasks, where it achieves high global accuracy both for personalized models and with malicious agents compared to standard strategies.","authors":["Luca Ballotta","Nicola Bastianello","Riccardo M. G. Ferrari","Karl H. Johansson"],"pdf_url":"","comment":"Published on IEEE Transactions on Control of Network Systems. Final accepted version"},{"id":"http://arxiv.org/abs/2408.10566v5","updated":"2025-12-22T14:46:47Z","published":"2024-08-20T06:05:52Z","title":"Overcoming Growth-Induced Forgetting in Task-Agnostic Continual Learning","summary":"In continual learning (CL), model growth enhances adaptability to new data. However, when model growth is applied improperly, especially in task-agnostic CL, where the entire grown model is used for inference, it can lead to severe degradation of learned knowledge, a problem we term growth-induced forgetting. Most existing methods that adopt model growth to improve adaptability often overlook the forgetting issue, resulting in compromised knowledge retention, making them unsuitable for task-agnostic settings. To promote both adaptability and knowledge retention with model growth, we identify the key: gradient and parameter sparsity. Introducing SparseGrow, which increases gradient sparsity through layer expansion and gradient gating to enable focused updates on parameters while preserving critical parameters, thus inhibiting forgetting. Moreover, it promotes parameter sparsity with sparse initialization and training, aiming at better control of model plasticity, improving adaptability over new data. Extensive experiments across diverse datasets, task-agnostic settings, and a large number of tasks demonstrate the necessity of controlled layer expansion and validate the effectiveness of SparseGrow in achieving high adaptability while minimizing forgetting in continual learning. By enabling model growth with sparsified gradients and parameters, SparseGrow paves the way for building scalable lifelong learning systems capable of continual adaptation with better knowledge retention.","authors":["Yuqing Zhao","Jiannong Cao","Divya Saxena","Xiaoyun Liu","Changlin Song","Bo Yuan","Julie McCann"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19442v1","updated":"2025-12-22T14:41:17Z","published":"2025-12-22T14:41:17Z","title":"Real-Time Streamable Generative Speech Restoration with Flow Matching","summary":"Diffusion-based generative models have greatly impacted the speech processing field in recent years, exhibiting high speech naturalness and spawning a new research direction. Their application in real-time communication is, however, still lagging behind due to their computation-heavy nature involving multiple calls of large DNNs.\n  Here, we present Stream.FM, a frame-causal flow-based generative model with an algorithmic latency of 32 milliseconds (ms) and a total latency of 48 ms, paving the way for generative speech processing in real-time communication. We propose a buffered streaming inference scheme and an optimized DNN architecture, show how learned few-step numerical solvers can boost output quality at a fixed compute budget, explore model weight compression to find favorable points along a compute/quality tradeoff, and contribute a model variant with 24 ms total latency for the speech enhancement task.\n  Our work looks beyond theoretical latencies, showing that high-quality streaming generative speech processing can be realized on consumer GPUs available today. Stream.FM can solve a variety of speech processing tasks in a streaming fashion: speech enhancement, dereverberation, codec post-filtering, bandwidth extension, STFT phase retrieval, and Mel vocoding. As we verify through comprehensive evaluations and a MUSHRA listening test, Stream.FM establishes a state-of-the-art for generative streaming speech restoration, exhibits only a reasonable reduction in quality compared to a non-streaming variant, and outperforms our recent work (Diffusion Buffer) on generative streaming speech enhancement while operating at a lower latency.","authors":["Simon Welker","Bunlong Lay","Maris Hillemann","Tal Peer","Timo Gerkmann"],"pdf_url":"","comment":"This work has been submitted to the IEEE for possible publication"},{"id":"http://arxiv.org/abs/2512.19440v1","updated":"2025-12-22T14:40:30Z","published":"2025-12-22T14:40:30Z","title":"Binary Kernel Logistic Regression: a sparsity-inducing formulation and a convergent decomposition training algorithm","summary":"Kernel logistic regression (KLR) is a widely used supervised learning method for binary and multi-class classification, which provides estimates of the conditional probabilities of class membership for the data points. Unlike other kernel methods such as Support Vector Machines (SVMs), KLRs are generally not sparse. Previous attempts to deal with sparsity in KLR include a heuristic method referred to as the Import Vector Machine (IVM) and ad hoc regularizations such as the $\\ell_{1/2}$-based one. Achieving a good trade-off between prediction accuracy and sparsity is still a challenging issue with a potential significant impact from the application point of view. In this work, we revisit binary KLR and propose an extension of the training formulation proposed by Keerthi et al., which is able to induce sparsity in the trained model, while maintaining good testing accuracy. To efficiently solve the dual of this formulation, we devise a decomposition algorithm of Sequential Minimal Optimization type which exploits second-order information, and for which we establish global convergence. Numerical experiments conducted on 12 datasets from the literature show that the proposed binary KLR approach achieves a competitive trade-off between accuracy and sparsity with respect to IVM, $\\ell_{1/2}$-based regularization for KLR, and SVM while retaining the advantages of providing informative estimates of the class membership probabilities.","authors":["Antonio Consolo","Andrea Manno","Edoardo Amaldi"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19439v1","updated":"2025-12-22T14:40:13Z","published":"2025-12-22T14:40:13Z","title":"An Inverse Scattering Inspired Fourier Neural Operator for Time-Dependent PDE Learning","summary":"Learning accurate and stable time-advancement operators for nonlinear partial differential equations (PDEs) remains challenging, particularly for chaotic, stiff, and long-horizon dynamical systems. While neural operator methods such as the Fourier Neural Operator (FNO) and Koopman-inspired extensions achieve good short-term accuracy, their long-term stability is often limited by unconstrained latent representations and cumulative rollout errors. In this work, we introduce an inverse scattering inspired Fourier Neural Operator(IS-FNO), motivated by the reversibility and spectral evolution structure underlying the classical inverse scattering transform. The proposed architecture enforces a near-reversible pairing between lifting and projection maps through an explicitly invertible neural transformation, and models latent temporal evolution using exponential Fourier layers that naturally encode linear and nonlinear spectral dynamics. We systematically evaluate IS-FNO against baseline FNO and Koopman-based models on a range of benchmark PDEs, including the Michelson-Sivashinsky and Kuramoto-Sivashinsky equations (in one and two dimensions), as well as the integrable Korteweg-de Vries and Kadomtsev-Petviashvili equations. The results demonstrate that IS-FNO achieves lower short-term errors and substantially improved long-horizon stability in non-stiff regimes. For integrable systems, reduced IS-FNO variants that embed analytical scattering structure retain competitive long-term accuracy despite limited model capacity. Overall, this work shows that incorporating physical structure -- particularly reversibility and spectral evolution -- into neural operator design significantly enhances robustness and long-term predictive fidelity for nonlinear PDE dynamics.","authors":["Rixin Yu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2507.04453v3","updated":"2025-12-22T14:35:03Z","published":"2025-07-06T16:23:07Z","title":"ESSA: Evolutionary Strategies for Scalable Alignment","summary":"Alignment of Large Language Models (LLMs) typically relies on Reinforcement Learning from Human Feedback (RLHF) with gradient-based optimizers such as Proximal Policy Optimization (PPO) or Group Relative Policy Optimization (GRPO). While effective, these methods require complex distributed training, large memory budgets, and careful hyperparameter tuning, all of which become increasingly difficult at billion-parameter scale. We present ESSA, Evolutionary Strategies for Scalable Alignment, a gradient-free framework that aligns LLMs using only forward inference and black-box optimization. ESSA focuses optimization on Low-Rank Adapters (LoRA) and further compresses their parameter space by optimizing only the singular values from an singular value decomposition (SVD) of each adapter matrix. This dimensionality reduction makes evolutionary search practical even for very large models and allows efficient operation in quantized INT4 and INT8 inference mode. Across these benchmarks ESSA improves the test accuracy of Qwen2.5-Math-7B by 12.6% on GSM8K and 14.8% on PRM800K, and raises the accuracy of LLaMA3.1-8B on IFEval by 22.5%, all compared with GRPO. In large-scale settings ESSA shows stronger scaling than gradient-based methods: on Qwen2.5-32B for PRM800K it reaches near-optimal accuracy twice as fast on 16 GPUs and six times as fast on 128 GPUs compared with GRPO. These results position evolutionary strategies as a compelling, hardware-friendly alternative to gradient-based LLM alignment, combining competitive quality with substantially reduced wall-clock time and engineering overhead.","authors":["Daria Korotyshova","Boris Shaposhnikov","Alexey Malakhov","Alexey Khokhulin","Nikita Surnachev","Kirill Ovcharenko","George Bredis","Alexey Gorbatovski","Viacheslav Sinii","Daniil Gavrilov"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2311.11871v4","updated":"2025-12-22T14:29:58Z","published":"2023-11-20T16:06:35Z","title":"Training robust and generalizable quantum models","summary":"Adversarial robustness and generalization are both crucial properties of reliable machine learning models. In this paper, we study these properties in the context of quantum machine learning based on Lipschitz bounds. We derive parameter-dependent Lipschitz bounds for quantum models with trainable encoding, showing that the norm of the data encoding has a crucial impact on the robustness against data perturbations. Further, we derive a bound on the generalization error which explicitly involves the parameters of the data encoding. Our theoretical findings give rise to a practical strategy for training robust and generalizable quantum models by regularizing the Lipschitz bound in the cost. Further, we show that, for fixed and non-trainable encodings, as those frequently employed in quantum machine learning, the Lipschitz bound cannot be influenced by tuning the parameters. Thus, trainable encodings are crucial for systematically adapting robustness and generalization during training. The practical implications of our theoretical findings are illustrated with numerical results.","authors":["Julian Berberich","Daniel Fink","Daniel Pranjić","Christian Tutschku","Christian Holm"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19428v1","updated":"2025-12-22T14:29:18Z","published":"2025-12-22T14:29:18Z","title":"Attention Is Not What You Need","summary":"We revisit a basic question in sequence modeling: is explicit self-attention actually necessary for strong performance and reasoning? We argue that standard multi-head attention is best seen as a form of tensor lifting: hidden vectors are mapped into a high-dimensional space of pairwise interactions, and learning proceeds by constraining this lifted tensor through gradient descent. This mechanism is extremely expressive but mathematically opaque, because after many layers it becomes very hard to describe the model with a small family of explicit invariants.\n  To explore an alternative, we propose an attention-free architecture based on Grassmann flows. Instead of forming an L by L attention matrix, our Causal Grassmann layer (i) linearly reduces token states, (ii) encodes local token pairs as two-dimensional subspaces on a Grassmann manifold via Plucker coordinates, and (iii) fuses these geometric features back into the hidden states through gated mixing. Information therefore propagates by controlled deformations of low-rank subspaces over multi-scale local windows, so the core computation lives on a finite-dimensional manifold rather than in an unstructured tensor space.\n  On the Wikitext-2 language modeling benchmark, purely Grassmann-based models with 13 to 18 million parameters achieve validation perplexities within about 10 to 15 percent of size-matched Transformers. On the SNLI natural language inference task, a Grassmann-Plucker head on top of DistilBERT slightly outperforms a Transformer head, with best validation and test accuracies of 0.8550 and 0.8538 compared to 0.8545 and 0.8511. We analyze the complexity of Grassmann mixing, show linear scaling in sequence length for fixed rank, and argue that such manifold-based designs offer a more structured route toward geometric and invariant-based interpretations of neural reasoning.","authors":["Zhang Chong"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2511.07329v2","updated":"2025-12-22T14:21:27Z","published":"2025-11-10T17:31:39Z","title":"Preparation of Fractal-Inspired Computational Architectures for Advanced Large Language Model Analysis","summary":"It introduces FractalNet, a fractal-inspired computational architectures for advanced large language model analysis that mainly challenges model diversity on a large scale in an efficient manner. The new set-up involves a template-driven generator, runner, and evaluation framework that, through systematic permutations of convolutional, normalization, activation, and dropout layers, can create more than 1,200 variants of neural networks. Fractal templates allow for structural recursion and multi-column pathways, thus, models become deeper and wider in a balanced way. Training utilizes PyTorch, Automatic Mixed Precision (AMP), and gradient checkpointing and is carried out on the CIFAR-10 dataset for five epochs. The outcomes show that fractal-based architectures are capable of strong performance and are computationally efficient. The paper positions fractal design as a feasible and resource-efficient method of automated architecture exploration.","authors":["Yash Mittal","Dmitry Ignatov","Radu Timofte"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17473v2","updated":"2025-12-22T14:13:49Z","published":"2025-12-19T11:40:06Z","title":"Alternating Direction Method of Multipliers for Nonlinear Matrix Decompositions","summary":"We present an algorithm based on the alternating direction method of multipliers (ADMM) for solving nonlinear matrix decompositions (NMD). Given an input matrix $X \\in \\mathbb{R}^{m \\times n}$ and a factorization rank $r \\ll \\min(m, n)$, NMD seeks matrices $W \\in \\mathbb{R}^{m \\times r}$ and $H \\in \\mathbb{R}^{r \\times n}$ such that $X \\approx f(WH)$, where $f$ is an element-wise nonlinear function. We evaluate our method on several representative nonlinear models: the rectified linear unit activation $f(x) = \\max(0, x)$, suitable for nonnegative sparse data approximation, the component-wise square $f(x) = x^2$, applicable to probabilistic circuit representation, and the MinMax transform $f(x) = \\min(b, \\max(a, x))$, relevant for recommender systems. The proposed framework flexibly supports diverse loss functions, including least squares, $\\ell_1$ norm, and the Kullback-Leibler divergence, and can be readily extended to other nonlinearities and metrics. We illustrate the applicability, efficiency, and adaptability of the approach on real-world datasets, highlighting its potential for a broad range of applications.","authors":["Atharva Awari","Nicolas Gillis","Arnaud Vandaele"],"pdf_url":"","comment":"14 pages, 6 figures. v2: Added a forgotten acknowledgement. Code available from https://gitlab.com/Atharva05/admm-for-nmd"},{"id":"http://arxiv.org/abs/2512.19410v1","updated":"2025-12-22T14:05:31Z","published":"2025-12-22T14:05:31Z","title":"Research Program: Theory of Learning in Dynamical Systems","summary":"Modern learning systems increasingly interact with data that evolve over time and depend on hidden internal state. We ask a basic question: when is such a dynamical system learnable from observations alone? This paper proposes a research program for understanding learnability in dynamical systems through the lens of next-token prediction. We argue that learnability in dynamical systems should be studied as a finite-sample question, and be based on the properties of the underlying dynamics rather than the statistical properties of the resulting sequence. To this end, we give a formulation of learnability for stochastic processes induced by dynamical systems, focusing on guarantees that hold uniformly at every time step after a finite burn-in period. This leads to a notion of dynamic learnability which captures how the structure of a system, such as stability, mixing, observability, and spectral properties, governs the number of observations required before reliable prediction becomes possible. We illustrate the framework in the case of linear dynamical systems, showing that accurate prediction can be achieved after finite observation without system identification, by leveraging improper methods based on spectral filtering. We survey the relationship between learning in dynamical systems and classical PAC, online, and universal prediction theories, and suggest directions for studying nonlinear and controlled systems.","authors":["Elad Hazan","Shai Shalev Shwartz","Nathan Srebro"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19409v1","updated":"2025-12-22T14:04:13Z","published":"2025-12-22T14:04:13Z","title":"Symplectic Reservoir Representation of Legendre Dynamics","summary":"Modern learning systems act on internal representations of data, yet how these representations encode underlying physical or statistical structure is often left implicit. In physics, conservation laws of Hamiltonian systems such as symplecticity guarantee long-term stability, and recent work has begun to hard-wire such constraints into learning models at the loss or output level. Here we ask a different question: what would it mean for the representation itself to obey a symplectic conservation law in the sense of Hamiltonian mechanics?\n  We express this symplectic constraint through Legendre duality: the pairing between primal and dual parameters, which becomes the structure that the representation must preserve. We formalize Legendre dynamics as stochastic processes whose trajectories remain on Legendre graphs, so that the evolving primal-dual parameters stay Legendre dual. We show that this class includes linear time-invariant Gaussian process regression and Ornstein-Uhlenbeck dynamics.\n  Geometrically, we prove that the maps that preserve all Legendre graphs are exactly symplectomorphisms of cotangent bundles of the form \"cotangent lift of a base diffeomorphism followed by an exact fibre translation\". Dynamically, this characterization leads to the design of a Symplectic Reservoir (SR), a reservoir-computing architecture that is a special case of recurrent neural network and whose recurrent core is generated by Hamiltonian systems that are at most linear in the momentum.\n  Our main theorem shows that every SR update has this normal form and therefore transports Legendre graphs to Legendre graphs, preserving Legendre duality at each time step. Overall, SR implements a geometrically constrained, Legendre-preserving representation map, injecting symplectic geometry and Hamiltonian mechanics directly at the representational level.","authors":["Robert Simon Fong","Gouhei Tanaka","Kazuyuki Aihara"],"pdf_url":"","comment":"39 pages"},{"id":"http://arxiv.org/abs/2512.19399v1","updated":"2025-12-22T13:51:03Z","published":"2025-12-22T13:51:03Z","title":"Brain-Grounded Axes for Reading and Steering LLM States","summary":"Interpretability methods for large language models (LLMs) typically derive directions from textual supervision, which can lack external grounding. We propose using human brain activity not as a training signal but as a coordinate system for reading and steering LLM states. Using the SMN4Lang MEG dataset, we construct a word-level brain atlas of phase-locking value (PLV) patterns and extract latent axes via ICA. We validate axes with independent lexica and NER-based labels (POS/log-frequency used as sanity checks), then train lightweight adapters that map LLM hidden states to these brain axes without fine-tuning the LLM. Steering along the resulting brain-derived directions yields a robust lexical (frequency-linked) axis in a mid TinyLlama layer, surviving perplexity-matched controls, and a brain-vs-text probe comparison shows larger log-frequency shifts (relative to the text probe) with lower perplexity for the brain axis. A function/content axis (axis 13) shows consistent steering in TinyLlama, Qwen2-0.5B, and GPT-2, with PPL-matched text-level corroboration. Layer-4 effects in TinyLlama are large but inconsistent, so we treat them as secondary (Appendix). Axis structure is stable when the atlas is rebuilt without GPT embedding-change features or with word2vec embeddings (|r|=0.64-0.95 across matched axes), reducing circularity concerns. Exploratory fMRI anchoring suggests potential alignment for embedding change and log frequency, but effects are sensitive to hemodynamic modeling assumptions and are treated as population-level evidence only. These results support a new interface: neurophysiology-grounded axes provide interpretable and controllable handles for LLM behavior.","authors":["Sandro Andric"],"pdf_url":"","comment":"10 pages, 4 figures. Code: https://github.com/sandroandric/Brain-Grounded-Axes-for-Reading-and-Steering-LLM-States"},{"id":"http://arxiv.org/abs/2412.14031v5","updated":"2025-12-22T13:49:48Z","published":"2024-12-18T16:51:47Z","title":"A Riemannian Optimization Perspective of the Gauss-Newton Method for Feedforward Neural Networks","summary":"In this work, we establish non-asymptotic convergence bounds for the Gauss-Newton method in training neural networks with smooth activations. In the underparameterized regime, the Gauss-Newton gradient flow in parameter space induces a Riemannian gradient flow on a low-dimensional embedded submanifold of the function space. Using tools from Riemannian optimization, we establish geodesic Polyak-Lojasiewicz and Lipschitz-smoothness conditions for the loss under appropriately chosen output scaling, yielding geometric convergence to the optimal in-class predictor at an explicit rate independent of the conditioning of the Gram matrix. In the overparameterized regime, we propose adaptive, curvature-aware regularization schedules that ensure fast geometric convergence to a global optimum at a rate independent of the minimum eigenvalue of the neural tangent kernel and, locally, of the modulus of strong convexity of the loss. These results demonstrate that Gauss-Newton achieves accelerated convergence rates in settings where first-order methods exhibit slow convergence due to ill-conditioned kernel matrices and loss landscapes.","authors":["Semih Cayci"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2507.17383v2","updated":"2025-12-22T13:33:33Z","published":"2025-07-23T10:26:10Z","title":"Confidence Calibration in Vision-Language-Action Models","summary":"Trustworthy robot behavior requires not only high levels of task success but also that the robot can reliably quantify how likely it is to succeed. To this end, we present a first-of-its-kind study of confidence calibration in vision-language-action (VLA) foundation models, which map visual observations and natural language instructions to low-level robot motor commands. We establish a confidence baseline for VLAs, examine how task success relates to calibration error and how calibration evolves over time, and introduce two lightweight techniques to remedy the miscalibration we observe: prompt ensembles and action-wise Platt scaling. Our aim in this study is to begin to develop the tools and conceptual understanding necessary to render VLAs both highly performant and highly trustworthy via reliable uncertainty quantification.","authors":["Thomas P Zollo","Richard Zemel"],"pdf_url":"","comment":"38 pages, 19 figures; additional experiments with VLA variants"},{"id":"http://arxiv.org/abs/2408.11607v3","updated":"2025-12-22T13:33:03Z","published":"2024-08-21T13:32:46Z","title":"Networked Communication for Mean-Field Games with Function Approximation and Empirical Mean-Field Estimation","summary":"Recent algorithms allow decentralised agents, possibly connected via a communication network, to learn equilibria in mean-field games from a non-episodic run of the empirical system. However, these algorithms are for tabular settings: this computationally limits the size of agents' observation space, meaning the algorithms cannot handle anything but small state spaces, nor generalise beyond policies depending only on the agent's local state to so-called 'population-dependent' policies. We address this limitation by introducing function approximation to the existing setting, drawing on the Munchausen Online Mirror Descent method that has previously been employed only in finite-horizon, episodic, centralised settings. While this permits us to include the mean field in the observation for players' policies, it is unrealistic to assume decentralised agents have access to this global information: we therefore also provide new algorithms allowing agents to locally estimate the global empirical distribution, and to improve this estimate via inter-agent communication. We prove theoretically that exchanging policy information helps networked agents outperform both independent and even centralised agents in function-approximation settings. Our experiments demonstrate this happening empirically, and show that the communication network allows decentralised agents to estimate the mean field for population-dependent policies.","authors":["Patrick Benjamin","Alessandro Abate"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19383v1","updated":"2025-12-22T13:27:23Z","published":"2025-12-22T13:27:23Z","title":"Real-Time Machine Learning for Embedded Anomaly Detection","summary":"The spread of a resource-constrained Internet of Things (IoT) environment and embedded devices has put pressure on the real-time detection of anomalies occurring at the edge. This survey presents an overview of machine-learning methods aimed specifically at on-device anomaly detection with extremely strict constraints for latency, memory, and power consumption. Lightweight algorithms such as Isolation Forest, One-Class SVM, recurrent architectures, and statistical techniques are compared here according to the realities of embedded implementation. Our survey brings out significant trade-offs of accuracy and computational efficiency of detection, as well as how hardware constraints end up fundamentally redefining algorithm choice. The survey is completed with a set of practical recommendations on the choice of the algorithm depending on the equipment profiles and new trends in TinyML, which can help close the gap between detection capabilities and embedded reality. The paper serves as a strategic roadmap for engineers deploying anomaly detection in edge environments that are constrained by bandwidth and may be safety-critical.","authors":["Abdelmadjid Benmachiche","Khadija Rais","Hamda Slimi"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2306.02766v6","updated":"2025-12-22T13:25:46Z","published":"2023-06-05T10:45:39Z","title":"Networked Communication for Decentralised Agents in Mean-Field Games","summary":"Methods like multi-agent reinforcement learning struggle to scale with growing population size. Mean-field games (MFGs) are a game-theoretic approach that can circumvent this by finding a solution for an abstract infinite population, which can then be used as an approximate solution for the $N$-agent problem. However, classical mean-field algorithms usually only work under restrictive conditions. We take steps to address this by introducing networked communication to MFGs, in particular to settings that use a single, non-episodic run of $N$ decentralised agents to simulate the infinite population, as is likely to be most reasonable in real-world deployments. We prove that our architecture's sample guarantees lie between those of earlier theoretical algorithms for the centralised- and independent-learning architectures, varying dependent on network structure and the number of communication rounds. However, the sample guarantees of the three theoretical algorithms do not actually result in practical convergence times. We thus contribute practical enhancements to all three algorithms allowing us to present their first empirical demonstrations. We then show that in practical settings where the theoretical hyperparameters are not observed, giving fewer loops but poorer estimation of the Q-function, our communication scheme still respects the earlier theoretical analysis: it considerably accelerates learning over the independent case, which hardly seems to learn at all, and often performs similarly to the centralised case, while removing the restrictive assumption of the latter. We provide ablations and additional studies showing that our networked approach also has advantages over both alternatives in terms of robustness to update failures and to changes in population size.","authors":["Patrick Benjamin","Alessandro Abate"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19379v1","updated":"2025-12-22T13:23:55Z","published":"2025-12-22T13:23:55Z","title":"OmniMER: Indonesian Multimodal Emotion Recognition via Auxiliary-Enhanced LLM Adaptation","summary":"Indonesian, spoken by over 200 million people, remains underserved in multimodal emotion recognition research despite its dominant presence on Southeast Asian social media platforms. We introduce IndoMER, the first multimodal emotion recognition benchmark for Indonesian, comprising 1,944 video segments from 203 speakers with temporally aligned text, audio, and visual annotations across seven emotion categories. The dataset exhibits realistic challenges including cross-modal inconsistency and long-tailed class distributions shaped by Indonesian cultural communication norms. To address these challenges, we propose OmniMER, a multimodal adaptation framework built upon Qwen2.5-Omni that enhances emotion recognition through three auxiliary modality-specific perception tasks: emotion keyword extraction for text, facial expression analysis for video, and prosody analysis for audio. These auxiliary tasks help the model identify emotion-relevant cues in each modality before fusion, reducing reliance on spurious correlations in low-resource settings. Experiments on IndoMER show that OmniMER achieves 0.582 Macro-F1 on sentiment classification and 0.454 on emotion recognition, outperforming the base model by 7.6 and 22.1 absolute points respectively. Cross-lingual evaluation on the Chinese CH-SIMS dataset further demonstrates the generalizability of the proposed framework. The dataset and code are publicly available. https://github.com/yanxm01/INDOMER","authors":["Xueming Yan","Boyan Xu","Yaochu Jin","Lixian Xiao","Wenlong Ye","Runyang Cai","Zeqi Zheng","Jingfa Liu","Aimin Yang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19376v1","updated":"2025-12-22T13:21:11Z","published":"2025-12-22T13:21:11Z","title":"A Critical Assessment of Pattern Comparisons Between POD and Autoencoders in Intraventricular Flows","summary":"Understanding intraventricular hemodynamics requires compact and physically interpretable representations of the underlying flow structures, as characteristic flow patterns are closely associated with cardiovascular conditions and can support early detection of cardiac deterioration. Conventional visualization of velocity or pressure fields, however, provides limited insight into the coherent mechanisms driving these dynamics. Reduced-order modeling techniques, like Proper Orthogonal Decomposition (POD) and Autoencoder (AE) architectures, offer powerful alternatives to extract dominant flow features from complex datasets. This study systematically compares POD with several AE variants (Linear, Nonlinear, Convolutional, and Variational) using left ventricular flow fields obtained from computational fluid dynamics simulations. We show that, for a suitably chosen latent dimension, AEs produce modes that become nearly orthogonal and qualitatively resemble POD modes that capture a given percentage of kinetic energy. As the number of latent modes increases, AE modes progressively lose orthogonality, leading to linear dependence, spatial redundancy, and the appearance of repeated modes with substantial high-frequency content. This degradation reduces interpretability and introduces noise-like components into AE-based reduced-order models, potentially complicating their integration with physics-based formulations or neural-network surrogates. The extent of interpretability loss varies across the AEs, with nonlinear, convolutional, and variational models exhibiting distinct behaviors in orthogonality preservation and feature localization. Overall, the results indicate that AEs can reproduce POD-like coherent structures under specific latent-space configurations, while highlighting the need for careful mode selection to ensure physically meaningful representations of cardiac flow dynamics.","authors":["Eneko Lazpita","Andrés Bell-Navas","Jesús Garicano-Mena","Petros Koumoutsakos","Soledad Le Clainche"],"pdf_url":"","comment":"27 pages, 9 figures"},{"id":"http://arxiv.org/abs/2512.19373v1","updated":"2025-12-22T13:15:52Z","published":"2025-12-22T13:15:52Z","title":"Cluster-Based Generalized Additive Models Informed by Random Fourier Features","summary":"Explainable machine learning aims to strike a balance between prediction accuracy and model transparency, particularly in settings where black-box predictive models, such as deep neural networks or kernel-based methods, achieve strong empirical performance but remain difficult to interpret. This work introduces a mixture of generalized additive models (GAMs) in which random Fourier feature (RFF) representations are leveraged to uncover locally adaptive structure in the data. In the proposed method, an RFF-based embedding is first learned and then compressed via principal component analysis. The resulting low-dimensional representations are used to perform soft clustering of the data through a Gaussian mixture model. These cluster assignments are then applied to construct a mixture-of-GAMs framework, where each local GAM captures nonlinear effects through interpretable univariate smooth functions. Numerical experiments on real-world regression benchmarks, including the California Housing, NASA Airfoil Self-Noise, and Bike Sharing datasets, demonstrate improved predictive performance relative to classical interpretable models. Overall, this construction provides a principled approach for integrating representation learning with transparent statistical modeling.","authors":["Xin Huang","Jia Li","Jun Yu"],"pdf_url":"","comment":"25 pages, 13 figures, 4 tables"},{"id":"http://arxiv.org/abs/2512.19367v1","updated":"2025-12-22T13:09:45Z","published":"2025-12-22T13:09:45Z","title":"Sprecher Networks: A Parameter-Efficient Kolmogorov-Arnold Architecture","summary":"We present Sprecher Networks (SNs), a family of trainable neural architectures inspired by the classical Kolmogorov-Arnold-Sprecher (KAS) construction for approximating multivariate continuous functions. Distinct from Multi-Layer Perceptrons (MLPs) with fixed node activations and Kolmogorov-Arnold Networks (KANs) featuring learnable edge activations, SNs utilize shared, learnable splines (monotonic and general) within structured blocks incorporating explicit shift parameters and mixing weights. Our approach directly realizes Sprecher's specific 1965 sum of shifted splines formula in its single-layer variant and extends it to deeper, multi-layer compositions. We further enhance the architecture with optional lateral mixing connections that enable intra-block communication between output dimensions, providing a parameter-efficient alternative to full attention mechanisms. Beyond parameter efficiency with $O(LN + LG)$ scaling (where $G$ is the knot count of the shared splines) versus MLPs' $O(LN^2)$, SNs admit a sequential evaluation strategy that reduces peak forward-intermediate memory from $O(N^2)$ to $O(N)$ (treating batch size as constant), making much wider architectures feasible under memory constraints. We demonstrate empirically that composing these blocks into deep networks leads to highly parameter and memory-efficient models, discuss theoretical motivations, and compare SNs with related architectures (MLPs, KANs, and networks with learnable node activations).","authors":["Christian Hägg","Kathlén Kohn","Giovanni Luca Marchetti","Boris Shapiro"],"pdf_url":"","comment":"37 pages"},{"id":"http://arxiv.org/abs/2512.19366v1","updated":"2025-12-22T13:08:58Z","published":"2025-12-22T13:08:58Z","title":"Learning General Policies with Policy Gradient Methods","summary":"While reinforcement learning methods have delivered remarkable results in a number of settings, generalization, i.e., the ability to produce policies that generalize in a reliable and systematic way, has remained a challenge. The problem of generalization has been addressed formally in classical planning where provable correct policies that generalize over all instances of a given domain have been learned using combinatorial methods. The aim of this work is to bring these two research threads together to illuminate the conditions under which (deep) reinforcement learning approaches, and in particular, policy optimization methods, can be used to learn policies that generalize like combinatorial methods do. We draw on lessons learned from previous combinatorial and deep learning approaches, and extend them in a convenient way. From the former, we model policies as state transition classifiers, as (ground) actions are not general and change from instance to instance. From the latter, we use graph neural networks (GNNs) adapted to deal with relational structures for representing value functions over planning states, and in our case, policies. With these ingredients in place, we find that actor-critic methods can be used to learn policies that generalize almost as well as those obtained using combinatorial approaches while avoiding the scalability bottleneck and the use of feature pools. Moreover, the limitations of the DRL methods on the benchmarks considered have little to do with deep learning or reinforcement learning algorithms, and result from the well-understood expressive limitations of GNNs, and the tradeoff between optimality and generalization (general policies cannot be optimal in some domains). Both of these limitations are addressed without changing the basic DRL methods by adding derived predicates and an alternative cost structure to optimize.","authors":["Simon Ståhlberg","Blai Bonet","Hector Geffner"],"pdf_url":"","comment":"In Proceedings of the 20th International Conference on Principles of Knowledge Representation and Reasoning (KR 2023)"},{"id":"http://arxiv.org/abs/2512.19363v1","updated":"2025-12-22T13:04:16Z","published":"2025-12-22T13:04:16Z","title":"From Points to Coalitions: Hierarchical Contrastive Shapley Values for Prioritizing Data Samples","summary":"How should we quantify the value of each training example when datasets are large, heterogeneous, and geometrically structured? Classical Data-Shapley answers in principle, but its O(n!) complexity and point-wise perspective are ill-suited to modern scales. We propose Hierarchical Contrastive Data Valuation (HCDV), a three-stage framework that (i) learns a contrastive, geometry-preserving representation, (ii) organizes the data into a balanced coarse-to-fine hierarchy of clusters, and (iii) assigns Shapley-style payoffs to coalitions via local Monte-Carlo games whose budgets are propagated downward. HCDV collapses the factorial burden to O(T sum_{l} K_{l}) = O(T K_max log n), rewards examples that sharpen decision boundaries, and regularizes outliers through curvature-based smoothness. We prove that HCDV approximately satisfies the four Shapley axioms with surplus loss O(eta log n), enjoys sub-Gaussian coalition deviation tilde O(1/sqrt{T}), and incurs at most k epsilon_infty regret for top-k selection. Experiments on four benchmarks--tabular, vision, streaming, and a 45M-sample CTR task--plus the OpenDataVal suite show that HCDV lifts accuracy by up to +5 pp, slashes valuation time by up to 100x, and directly supports tasks such as augmentation filtering, low-latency streaming updates, and fair marketplace payouts.","authors":["Canran Xiao","Jiabao Dou","Zhiming Lin","Zong Ke","Liwei Hou"],"pdf_url":"","comment":"AAAI'26 Oral"},{"id":"http://arxiv.org/abs/2512.19361v1","updated":"2025-12-22T12:59:48Z","published":"2025-12-22T12:59:48Z","title":"Interpretable Hybrid Deep Q-Learning Framework for IoT-Based Food Spoilage Prediction with Synthetic Data Generation and Hardware Validation","summary":"The need for an intelligent, real-time spoilage prediction system has become critical in modern IoT-driven food supply chains, where perishable goods are highly susceptible to environmental conditions. Existing methods often lack adaptability to dynamic conditions and fail to optimize decision making in real time. To address these challenges, we propose a hybrid reinforcement learning framework integrating Long Short-Term Memory (LSTM) and Recurrent Neural Networks (RNN) for enhanced spoilage prediction. This hybrid architecture captures temporal dependencies within sensor data, enabling robust and adaptive decision making. In alignment with interpretable artificial intelligence principles, a rule-based classifier environment is employed to provide transparent ground truth labeling of spoilage levels based on domain-specific thresholds. This structured design allows the agent to operate within clearly defined semantic boundaries, supporting traceable and interpretable decisions. Model behavior is monitored using interpretability-driven metrics, including spoilage accuracy, reward-to-step ratio, loss reduction rate, and exploration decay. These metrics provide both quantitative performance evaluation and insights into learning dynamics. A class-wise spoilage distribution visualization is used to analyze the agents decision profile and policy behavior. Extensive evaluations on simulated and real-time hardware data demonstrate that the LSTM and RNN based agent outperforms alternative reinforcement learning approaches in prediction accuracy and decision efficiency while maintaining interpretability. The results highlight the potential of hybrid deep reinforcement learning with integrated interpretability for scalable IoT-based food monitoring systems.","authors":["Isshaan Singh","Divyansh Chawla","Anshu Garg","Shivin Mangal","Pallavi Gupta","Khushi Agarwal","Nimrat Singh Khalsa","Nandan Patel"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19349v1","updated":"2025-12-22T12:48:29Z","published":"2025-12-22T12:48:29Z","title":"VIGOR+: Iterative Confounder Generation and Validation via LLM-CEVAE Feedback Loop","summary":"Hidden confounding remains a fundamental challenge in causal inference from observational data. Recent advances leverage Large Language Models (LLMs) to generate plausible hidden confounders based on domain knowledge, yet a critical gap exists: LLM-generated confounders often exhibit semantic plausibility without statistical utility. We propose VIGOR+ (Variational Information Gain for iterative cOnfounder Refinement), a novel framework that closes the loop between LLM-based confounder generation and CEVAE-based statistical validation. Unlike prior approaches that treat generation and validation as separate stages, VIGOR+ establishes an iterative feedback mechanism: validation signals from CEVAE (including information gain, latent consistency metrics, and diagnostic messages) are transformed into natural language feedback that guides subsequent LLM generation rounds. This iterative refinement continues until convergence criteria are met. We formalize the feedback mechanism, prove convergence properties under mild assumptions, and provide a complete algorithmic framework.","authors":["JiaWei Zhu","ZiHeng Liu"],"pdf_url":"","comment":"7 pages,1 figure,4 tables"},{"id":"http://arxiv.org/abs/2512.19342v1","updated":"2025-12-22T12:36:54Z","published":"2025-12-22T12:36:54Z","title":"Faster Distributed Inference-Only Recommender Systems via Bounded Lag Synchronous Collectives","summary":"Recommender systems are enablers of personalized content delivery, and therefore revenue, for many large companies. In the last decade, deep learning recommender models (DLRMs) are the de-facto standard in this field. The main bottleneck in DLRM inference is the lookup of sparse features across huge embedding tables, which are usually partitioned across the aggregate RAM of many nodes. In state-of-the-art recommender systems, the distributed lookup is implemented via irregular all-to-all (alltoallv) communication, and often presents the main bottleneck. Today, most related work sees this operation as a given; in addition, every collective is synchronous in nature. In this work, we propose a novel bounded lag synchronous (BLS) version of the alltoallv operation. The bound can be a parameter allowing slower processes to lag behind entire iterations before the fastest processes block. In special applications such as inference-only DLRM, the accuracy of the application is fully preserved. We implement BLS alltoallv in a new PyTorch Distributed backend and evaluate it with a BLS version of the reference DLRM code. We show that for well balanced, homogeneous-access DLRM runs our BLS technique does not offer notable advantages. But for unbalanced runs, e.g. runs with strongly irregular embedding table accesses or with delays across different processes, our BLS technique improves both the latency and throughput of inference-only DLRM. In the best-case scenario, the proposed reduced synchronisation can mask the delays across processes altogether.","authors":["Kiril Dichev","Filip Pawlowski","Albert-Jan Yzelman"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2511.20944v3","updated":"2025-12-22T12:31:00Z","published":"2025-11-26T00:34:46Z","title":"Semantic Superiority vs. Forensic Efficiency: A Comparative Analysis of Deep Learning and Psycholinguistics for Business Email Compromise Detection","summary":"Business Email Compromise (BEC) is a sophisticated social engineering threat that manipulates organizational hierarchies, leading to significant financial damage. According to the 2024 FBI Internet Crime Report, BEC accounts for over $2.9 billion in annual losses, presenting a massive economic asymmetry: the financial cost of a False Negative (fraud loss) exceeds the operational cost of a False Positive (manual review) by a ratio of approximately 5,480:1. This paper contrasts two detection paradigms: a Forensic Psycholinguistic Stream (CatBoost), which analyzes linguistic cues like urgency and authority with high interpretability, and a Semantic Stream (DistilBERT), which utilizes deep learning for contextual understanding. We evaluated both streams on a hybrid dataset (N=7,990) containing human-legitimate and AI-synthesized adversarial fraud. Benchmarked on Tesla T4 infrastructure, DistilBERT achieved near-perfect detection on synthetic threats (AUC >0.99, F1 =0.998) with acceptable real-time latency (7.4 ms). CatBoost achieved competitive detection (AUC =0.991, F1 =0.949) at 8.4x lower latency (0.8 ms) with negligible resource consumption. We conclude that while DistilBERT offers maximum accuracy for GPU-equipped organizations, CatBoost provides a viable, cost-effective alternative for edge deployments. Both approaches demonstrate a theoretical ROI exceeding 99.9% when optimized via cost-sensitive learning.","authors":["Yaw Osei Adjei","Frederick Ayivor","Davis Opoku"],"pdf_url":"","comment":"8 pages, 12 figures, 7 tables"},{"id":"http://arxiv.org/abs/2512.19334v1","updated":"2025-12-22T12:28:53Z","published":"2025-12-22T12:28:53Z","title":"Orthogonal Approximate Message Passing with Optimal Spectral Initializations for Rectangular Spiked Matrix Models","summary":"We propose an orthogonal approximate message passing (OAMP) algorithm for signal estimation in the rectangular spiked matrix model with general rotationally invariant (RI) noise. We establish a rigorous state evolution that precisely characterizes the algorithm's high-dimensional dynamics and enables the construction of iteration-wise optimal denoisers. Within this framework, we accommodate spectral initializations under minimal assumptions on the empirical noise spectrum. In the rectangular setting, where a single rank-one component typically generates multiple informative outliers, we further propose a procedure for combining these outliers under mild non-Gaussian signal assumptions. For general RI noise models, the predicted performance of the proposed optimal OAMP algorithm agrees with replica-symmetric predictions for the associated Bayes-optimal estimator, and we conjecture that it is statistically optimal within a broad class of iterative estimation methods.","authors":["Haohua Chen","Songbin Liu","Junjie Ma"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19332v1","updated":"2025-12-22T12:27:36Z","published":"2025-12-22T12:27:36Z","title":"A Logical View of GNN-Style Computation and the Role of Activation Functions","summary":"We study the numerical and Boolean expressiveness of MPLang, a declarative language that captures the computation of graph neural networks (GNNs) through linear message passing and activation functions. We begin with A-MPLang, the fragment without activation functions, and give a characterization of its expressive power in terms of walk-summed features. For bounded activation functions, we show that (under mild conditions) all eventually constant activations yield the same expressive power - numerical and Boolean - and that it subsumes previously established logics for GNNs with eventually constant activation functions but without linear layers. Finally, we prove the first expressive separation between unbounded and bounded activations in the presence of linear layers: MPLang with ReLU is strictly more powerful for numerical queries than MPLang with eventually constant activation functions, e.g., truncated ReLU. This hinges on subtle interactions between linear aggregation and eventually constant non-linearities, and it establishes that GNNs using ReLU are more expressive than those restricted to eventually constant activations and linear layers.","authors":["Pablo Barceló","Floris Geerts","Matthias Lanzinger","Klara Pakhomenko","Jan Van den Bussche"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19323v1","updated":"2025-12-22T12:17:47Z","published":"2025-12-22T12:17:47Z","title":"Alternative positional encoding functions for neural transformers","summary":"A key module in neural transformer-based deep architectures is positional encoding. This module enables a suitable way to encode positional information as input for transformer neural layers. This success has been rooted in the use of sinusoidal functions of various frequencies, in order to capture recurrent patterns of differing typical periods. In this work, an alternative set of periodic functions is proposed for positional encoding. These functions preserve some key properties of sinusoidal ones, while they depart from them in fundamental ways. Some tentative experiments are reported, where the original sinusoidal version is substantially outperformed. This strongly suggests that the alternative functions may have a wider use in other transformer architectures.","authors":["Ezequiel Lopez-Rubio","Macoris Decena-Gimenez","Rafael Marcos Luque-Baena"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19320v1","updated":"2025-12-22T12:13:17Z","published":"2025-12-22T12:13:17Z","title":"MAGIC: Achieving Superior Model Merging via Magnitude Calibration","summary":"The proliferation of pre-trained models has given rise to a wide array of specialised, fine-tuned models. Model merging aims to merge the distinct capabilities of these specialised models into a unified model, requiring minimal or even no additional training. A core objective of model merging is to ensure the merged model retains the behavioural characteristics of the specialised models, typically achieved through feature alignment. We identify that features consist of two critical components: direction and magnitude. Prior research has predominantly focused on directional alignment, while the influence of magnitude remains largely neglected, despite its pronounced vulnerability to perturbations introduced by common merging operations (e.g., parameter fusion and sparsification). Such perturbations to magnitude inevitably lead to feature deviations in the merged model from the specialised models, resulting in subsequent performance degradation. To address this, we propose MAGnItude Calibration (MAGIC), a plug-and-play framework that rectifies layer-wise magnitudes in feature and weight spaces, with three variants. Specifically, our Feature Space Calibration (FSC) realigns the merged model's features using a small set of unlabelled data, while Weight Space Calibration (WSC) extends this calibration to the weight space without requiring additional data. Combining these yields Dual Space Calibration (DSC). Comprehensive experiments demonstrate that MAGIC consistently boosts performance across diverse Computer Vision tasks (+4.3% on eight datasets) and NLP tasks (+8.0% on Llama) without additional training. Our code is available at: https://github.com/lyymuwu/MAGIC","authors":["Yayuan Li","Jian Zhang","Jintao Guo","Zihan Cheng","Lei Qi","Yinghuan Shi","Yang Gao"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19309v1","updated":"2025-12-22T11:59:47Z","published":"2025-12-22T11:59:47Z","title":"Time-Vertex Machine Learning for Optimal Sensor Placement in Temporal Graph Signals: Applications in Structural Health Monitoring","summary":"Structural Health Monitoring (SHM) plays a crucial role in maintaining the safety and resilience of infrastructure. As sensor networks grow in scale and complexity, identifying the most informative sensors becomes essential to reduce deployment costs without compromising monitoring quality. While Graph Signal Processing (GSP) has shown promise by leveraging spatial correlations among sensor nodes, conventional approaches often overlook the temporal dynamics of structural behavior. To overcome this limitation, we propose Time-Vertex Machine Learning (TVML), a novel framework that integrates GSP, time-domain analysis, and machine learning to enable interpretable and efficient sensor placement by identifying representative nodes that minimize redundancy while preserving critical information. We evaluate the proposed approach on two bridge datasets for damage detection and time-varying graph signal reconstruction tasks. The results demonstrate the effectiveness of our approach in enhancing SHM systems by providing a robust, adaptive, and efficient solution for sensor placement.","authors":["Keivan Faghih Niresi","Jun Qing","Mengjie Zhao","Olga Fink"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17531v2","updated":"2025-12-22T11:36:59Z","published":"2025-12-19T12:54:03Z","title":"NetworkFF: Unified Layer Optimization in Forward-Only Neural Networks","summary":"The Forward-Forward algorithm eliminates backpropagation's memory constraints and biological implausibility through dual forward passes with positive and negative data. However, conventional implementations suffer from critical inter-layer isolation, where layers optimize goodness functions independently without leveraging collective learning dynamics. This isolation constrains representational coordination and limits convergence efficiency in deeper architectures. This paper introduces Collaborative Forward-Forward (CFF) learning, extending the original algorithm through inter-layer cooperation mechanisms that preserve forward-only computation while enabling global context integration. Our framework implements two collaborative paradigms: Fixed CFF (F-CFF) with constant inter-layer coupling and Adaptive CFF (A-CFF) with learnable collaboration parameters that evolve during training. The collaborative goodness function incorporates weighted contributions from all layers, enabling coordinated feature learning while maintaining memory efficiency and biological plausibility. Comprehensive evaluation on MNIST and Fashion-MNIST demonstrates significant performance improvements over baseline Forward-Forward implementations. These findings establish inter-layer collaboration as a fundamental enhancement to Forward-Forward learning, with immediate applicability to neuromorphic computing architectures and energy-constrained AI systems.","authors":["Salar Beigzad"],"pdf_url":"","comment":"Conference paper, IEEE, 2025"},{"id":"http://arxiv.org/abs/2509.23941v2","updated":"2025-12-22T11:33:19Z","published":"2025-09-28T15:35:25Z","title":"Brain-language fusion enables interactive neural readout and in-silico experimentation","summary":"Large language models (LLMs) have revolutionized human-machine interaction, and have been extended by embedding diverse modalities such as images into a shared language space. Yet, neural decoding has remained constrained by static, non-interactive methods. We introduce CorText, a framework that integrates neural activity directly into the latent space of an LLM, enabling open-ended, natural language interaction with brain data. Trained on fMRI data recorded during viewing of natural scenes, CorText generates accurate image captions and can answer more detailed questions better than controls, while having access to neural data only. We showcase that CorText achieves zero-shot generalization beyond semantic categories seen during training. In-silico microstimulation experiments, which enable counterfactual prompts on brain activity, reveal a consistent, and graded mapping between brain-state and language output. These advances mark a shift from passive decoding toward generative, flexible interfaces between brain activity and language.","authors":["Victoria Bosch","Daniel Anthes","Adrien Doerig","Sushrut Thorat","Peter König","Tim Christian Kietzmann"],"pdf_url":"","comment":"v2"},{"id":"http://arxiv.org/abs/2512.19286v1","updated":"2025-12-22T11:29:28Z","published":"2025-12-22T11:29:28Z","title":"GShield: Mitigating Poisoning Attacks in Federated Learning","summary":"Federated Learning (FL) has recently emerged as a revolutionary approach to collaborative training Machine Learning models. In particular, it enables decentralized model training while preserving data privacy, but its distributed nature makes it highly vulnerable to a severe attack known as Data Poisoning. In such scenarios, malicious clients inject manipulated data into the training process, thereby degrading global model performance or causing targeted misclassification. In this paper, we present a novel defense mechanism called GShield, designed to detect and mitigate malicious and low-quality updates, especially under non-independent and identically distributed (non-IID) data scenarios. GShield operates by learning the distribution of benign gradients through clustering and Gaussian modeling during an initial round, enabling it to establish a reliable baseline of trusted client behavior. With this benign profile, GShield selectively aggregates only those updates that align with the expected gradient patterns, effectively isolating adversarial clients and preserving the integrity of the global model. An extensive experimental campaign demonstrates that our proposed defense significantly improves model robustness compared to the state-of-the-art methods while maintaining a high accuracy of performance across both tabular and image datasets. Furthermore, GShield improves the accuracy of the targeted class by 43\\% to 65\\% after detecting malicious and low-quality clients.","authors":["Sameera K. M.","Serena Nicolazzo","Antonino Nocera","Vinod P.","Rafidha Rehiman K. A"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19280v1","updated":"2025-12-22T11:24:42Z","published":"2025-12-22T11:24:42Z","title":"Digital Twin-Driven Zero-Shot Fault Diagnosis of Axial Piston Pumps Using Fluid-Borne Noise Signals","summary":"Axial piston pumps are crucial components in fluid power systems, where reliable fault diagnosis is essential for ensuring operational safety and efficiency. Traditional data-driven methods require extensive labeled fault data, which is often impractical to obtain, while model-based approaches suffer from parameter uncertainties. This paper proposes a digital twin (DT)-driven zero-shot fault diagnosis framework utilizing fluid-borne noise (FBN) signals. The framework calibrates a high-fidelity DT model using only healthy-state data, generates synthetic fault signals for training deep learning classifiers, and employs a physics-informed neural network (PINN) as a virtual sensor for flow ripple estimation. Gradient-weighted class activation mapping (Grad-CAM) is integrated to visualize the decision-making process of neural networks, revealing that large kernels matching the subsequence length in time-domain inputs and small kernels in time-frequency domain inputs enable higher diagnostic accuracy by focusing on physically meaningful features. Experimental validations demonstrate that training on signals from the calibrated DT model yields diagnostic accuracies exceeding 95\\% on real-world benchmarks, while uncalibrated models result in significantly lower performance, highlighting the framework's effectiveness in data-scarce scenarios.","authors":["Chang Dong","Jianfeng Tao","Chengliang Liu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19269v1","updated":"2025-12-22T11:06:06Z","published":"2025-12-22T11:06:06Z","title":"Translating Flow to Policy via Hindsight Online Imitation","summary":"Recent advances in hierarchical robot systems leverage a high-level planner to propose task plans and a low-level policy to generate robot actions. This design allows training the planner on action-free or even non-robot data sources (e.g., videos), providing transferable high-level guidance. Nevertheless, grounding these high-level plans into executable actions remains challenging, especially with the limited availability of high-quality robot data. To this end, we propose to improve the low-level policy through online interactions. Specifically, our approach collects online rollouts, retrospectively annotates the corresponding high-level goals from achieved outcomes, and aggregates these hindsight-relabeled experiences to update a goal-conditioned imitation policy. Our method, Hindsight Flow-conditioned Online Imitation (HinFlow), instantiates this idea with 2D point flows as the high-level planner. Across diverse manipulation tasks in both simulation and physical world, our method achieves more than $2\\times$ performance improvement over the base policy, significantly outperforming the existing methods. Moreover, our framework enables policy acquisition from planners trained on cross-embodiment video data, demonstrating its potential for scalable and transferable robot learning.","authors":["Yitian Zheng","Zhangchen Ye","Weijun Dong","Shengjie Wang","Yuyang Liu","Chongjie Zhang","Chuan Wen","Yang Gao"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2507.17860v4","updated":"2025-12-22T10:41:08Z","published":"2025-07-23T18:33:27Z","title":"Towards Facilitated Fairness Assessment of AI-based Skin Lesion Classifiers Through GenAI-based Image Synthesis","summary":"Recent advances in deep learning and on-device inference could transform routine screening for skin cancers. Along with the anticipated benefits of this technology, potential dangers arise from unforeseen and inherent biases. A significant obstacle is building evaluation datasets that accurately reflect key demographics, including sex, age, and race, as well as other underrepresented groups. To address this, we train a state-of-the-art generative model to generate synthetic data in a controllable manner to assess the fairness of publicly available skin cancer classifiers. To evaluate whether synthetic images can be used as a fairness testing dataset, we prepare a real-image dataset (MILK10K) as a benchmark and compare the True Positive Rate result of three models (DeepGuide, MelaNet, and SkinLesionDensnet). As a result, the classification tendencies observed in each model when tested on real and generated images showed similar patterns across different attribute data sets. We confirm that highly realistic synthetic images facilitate model fairness verification.","authors":["Ko Watanabe","Stanislav Frolov","Aya Hassan","David Dembinsky","Adriano Lucieri","Andreas Dengel"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19253v1","updated":"2025-12-22T10:40:03Z","published":"2025-12-22T10:40:03Z","title":"Machine Unlearning in the Era of Quantum Machine Learning: An Empirical Study","summary":"We present the first comprehensive empirical study of machine unlearning (MU) in hybrid quantum-classical neural networks. While MU has been extensively explored in classical deep learning, its behavior within variational quantum circuits (VQCs) and quantum-augmented architectures remains largely unexplored. First, we adapt a broad suite of unlearning methods to quantum settings, including gradient-based, distillation-based, regularization-based and certified techniques. Second, we introduce two new unlearning strategies tailored to hybrid models. Experiments across Iris, MNIST, and Fashion-MNIST, under both subset removal and full-class deletion, reveal that quantum models can support effective unlearning, but outcomes depend strongly on circuit depth, entanglement structure, and task complexity. Shallow VQCs display high intrinsic stability with minimal memorization, whereas deeper hybrid models exhibit stronger trade-offs between utility, forgetting strength, and alignment with retrain oracle. We find that certain methods, e.g. EU-k, LCA, and Certified Unlearning, consistently provide the best balance across metrics. These findings establish baseline empirical insights into quantum machine unlearning and highlight the need for quantum-aware algorithms and theoretical guarantees, as quantum machine learning systems continue to expand in scale and capability. We publicly release our code at: https://github.com/CrivoiCarla/HQML.","authors":["Carla Crivoi","Radu Tudor Ionescu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19250v1","updated":"2025-12-22T10:34:45Z","published":"2025-12-22T10:34:45Z","title":"Small Language Models as Compiler Experts: Auto-Parallelization for Heterogeneous Systems","summary":"Traditional auto-parallelizing compilers, reliant on rigid heuristics, struggle with the complexity of modern heterogeneous systems. This paper presents a comprehensive evaluation of small (approximately 1B parameter) language-model-driven compiler auto-parallelization. We evaluate three models: gemma3, llama3.2, and qwen2.5, using six reasoning strategies across 11 real-world kernels drawn from scientific computing, graph algorithms, and machine learning. Our system is benchmarked against strong compiler baselines, including LLVM Polly, TVM, and Triton. Across 376 total evaluations, the proposed approach achieves an average speedup of 6.81x and a peak performance of 43.25x on convolution operations. We analyze scalability, verify correctness using multiple sanitizers, and confirm robustness across diverse compilers and hardware platforms. Our results demonstrate that small, efficient language models can serve as powerful reasoning engines for complex compiler optimization tasks.","authors":["Prathamesh Devadiga"],"pdf_url":"","comment":"Accepted at NeurIPS 2025 ML for Systems Workshop"},{"id":"http://arxiv.org/abs/2512.19246v1","updated":"2025-12-22T10:28:22Z","published":"2025-12-22T10:28:22Z","title":"From Black-Box Tuning to Guided Optimization via Hyperparameters Interaction Analysis","summary":"Hyperparameters tuning is a fundamental, yet computationally expensive, step in optimizing machine learning models. Beyond optimization, understanding the relative importance and interaction of hyperparameters is critical to efficient model development. In this paper, we introduce MetaSHAP, a scalable semi-automated eXplainable AI (XAI) method, that uses meta-learning and Shapley values analysis to provide actionable and dataset-aware tuning insights. MetaSHAP operates over a vast benchmark of over 09 millions evaluated machine learning pipelines, allowing it to produce interpretable importance scores and actionable tuning insights that reveal how much each hyperparameter matters, how it interacts with others and in which value ranges its influence is concentrated. For a given algorithm and dataset, MetaSHAP learns a surrogate performance model from historical configurations, computes hyperparameters interactions using SHAP-based analysis, and derives interpretable tuning ranges from the most influential hyperparameters. This allows practitioners not only to prioritize which hyperparameters to tune, but also to understand their directionality and interactions. We empirically validate MetaSHAP on a diverse benchmark of 164 classification datasets and 14 classifiers, demonstrating that it produces reliable importance rankings and competitive performance when used to guide Bayesian optimization.","authors":["Moncef Garouani","Ayah Barhrhouj"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17452v2","updated":"2025-12-22T10:23:36Z","published":"2025-12-19T11:08:58Z","title":"Learning What to Write: Write-Gated KV for Efficient Long-Context Inference","summary":"Long-context LLM inference is bottlenecked by the quadratic attention complexity and linear KV cache growth. Prior approaches mitigate this via post-hoc selection or eviction but overlook the root inefficiency: indiscriminate writing to persistent memory. In this paper, we formalize KV cache management as a causal system of three primitives: KV Admission, Selection, and Eviction. We instantiate KV Admission via Write-Gated KV, a lightweight mechanism that learns to predict token utility before it enters the cache. By filtering out low-utility states early to maintain a compact global cache alongside a sliding local cache, Write-Gated KV reduces memory usage by 46-57% and delivers 3.03-3.45$\\times$ prefill and 1.89-2.56$\\times$ decode speedups on Llama model with negligible accuracy loss, all while remaining compatible with FlashAttention and paged-KV systems. These results demonstrate that learning what to write, is a principled and practical recipe for efficient long-context inference. Code is available at https://github.com/EMCLab-Sinica/WG-KV .","authors":["Yen-Chieh Huang","Pi-Cheng Hsiu","Rui Fang","Ming-Syan Chen"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2511.11646v2","updated":"2025-12-22T10:22:37Z","published":"2025-11-10T08:50:03Z","title":"What-If Decision Support for Product Line Extension Using Conditional Deep Generative Models","summary":"Product line extension is a strategically important managerial decision that requires anticipating how consumer segments and purchasing contexts may respond to hypothetical product designs that do not yet exist in the market. Such decisions are inherently uncertain because managers must infer future outcomes from historical purchase data without direct market observations. This study addresses this challenge by proposing a data-driven decision support framework that enables forward-looking what-if analysis based on historical transaction data. We introduce a Conditional Tabular Variational Autoencoder (CTVAE) that learns the conditional joint distribution of product attributes and consumer characteristics from large-scale tabular data. By conditioning the generative process on controllable design variables such as container type, volume, flavor, and calorie content, the proposed model generates synthetic consumer attribute distributions for hypothetical line-extended products. This enables systematic exploration of alternative design scenarios without costly market pretests. The framework is evaluated using home-scan panel data covering more than 20,000 consumers and 700 soft drink products. Empirical results show that the CTVAE outperforms existing tabular generative models in capturing conditional consumer attribute distributions. Simulation-based analyses further demonstrate that the generated synthetic data support knowledge-driven reasoning for assessing cannibalization risks and identifying potential target segments. These findings highlight the value of conditional deep generative models as core components of decision support systems for product line extension planning.","authors":["Yinxing Li","Tsukasa Ishigaki"],"pdf_url":"","comment":"25 pages"},{"id":"http://arxiv.org/abs/2512.19238v1","updated":"2025-12-22T10:20:20Z","published":"2025-12-22T10:20:20Z","title":"Identifying Features Associated with Bias Against 93 Stigmatized Groups in Language Models and Guardrail Model Safety Mitigation","summary":"Large language models (LLMs) have been shown to exhibit social bias, however, bias towards non-protected stigmatized identities remain understudied. Furthermore, what social features of stigmas are associated with bias in LLM outputs is unknown. From psychology literature, it has been shown that stigmas contain six shared social features: aesthetics, concealability, course, disruptiveness, origin, and peril. In this study, we investigate if human and LLM ratings of the features of stigmas, along with prompt style and type of stigma, have effect on bias towards stigmatized groups in LLM outputs. We measure bias against 93 stigmatized groups across three widely used LLMs (Granite 3.0-8B, Llama-3.1-8B, Mistral-7B) using SocialStigmaQA, a benchmark that includes 37 social scenarios about stigmatized identities; for example deciding wether to recommend them for an internship. We find that stigmas rated by humans to be highly perilous (e.g., being a gang member or having HIV) have the most biased outputs from SocialStigmaQA prompts (60% of outputs from all models) while sociodemographic stigmas (e.g. Asian-American or old age) have the least amount of biased outputs (11%). We test if the amount of biased outputs could be decreased by using guardrail models, models meant to identify harmful input, using each LLM's respective guardrail model (Granite Guardian 3.0, Llama Guard 3.0, Mistral Moderation API). We find that bias decreases significantly by 10.4%, 1.4%, and 7.8%, respectively. However, we show that features with significant effect on bias remain unchanged post-mitigation and that guardrail models often fail to recognize the intent of bias in prompts. This work has implications for using LLMs in scenarios involving stigmatized groups and we suggest future work towards improving guardrail models for bias mitigation.","authors":["Anna-Maria Gueorguieva","Aylin Caliskan"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19232v1","updated":"2025-12-22T10:14:00Z","published":"2025-12-22T10:14:00Z","title":"Regression generation adversarial network based on dual data evaluation strategy for industrial application","summary":"Soft sensing infers hard-to-measure data through a large number of easily obtainable variables. However, in complex industrial scenarios, the issue of insufficient data volume persists, which diminishes the reliability of soft sensing. Generative Adversarial Networks (GAN) are one of the effective solutions for addressing insufficient samples. Nevertheless, traditional GAN fail to account for the mapping relationship between labels and features, which limits further performance improvement. Although some studies have proposed solutions, none have considered both performance and efficiency simultaneously. To address these problems, this paper proposes the multi-task learning-based regression GAN framework that integrates regression information into both the discriminator and generator, and implements a shallow sharing mechanism between the discriminator and regressor. This approach significantly enhances the quality of generated samples while improving the algorithm's operational efficiency. Moreover, considering the importance of training samples and generated samples, a dual data evaluation strategy is designed to make GAN generate more diverse samples, thereby increasing the generalization of subsequent modeling. The superiority of method is validated through four classic industrial soft sensing cases: wastewater treatment plants, surface water, $CO_2$ absorption towers, and industrial gas turbines.","authors":["Zesen Wang","Yonggang Li","Lijuan Lan"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19223v1","updated":"2025-12-22T10:03:51Z","published":"2025-12-22T10:03:51Z","title":"Phase-space entropy at acquisition reflects downstream learnability","summary":"Modern learning systems work with data that vary widely across domains, but they all ultimately depend on how much structure is already present in the measurements before any model is trained. This raises a basic question: is there a general, modality-agnostic way to quantify how acquisition itself preserves or destroys the information that downstream learners could use? Here we propose an acquisition-level scalar $ΔS_{\\mathcal B}$ based on instrument-resolved phase space. Unlike pixelwise distortion or purely spectral errors that often saturate under aggressive undersampling, $ΔS_{\\mathcal B}$ directly quantifies how acquisition mixes or removes joint space--frequency structure at the instrument scale. We show theoretically that \\(ΔS_{\\mathcal B}\\) correctly identifies the phase-space coherence of periodic sampling as the physical source of aliasing, recovering classical sampling-theorem consequences. Empirically, across masked image classification, accelerated MRI, and massive MIMO (including over-the-air measurements), $|ΔS_{\\mathcal B}|$ consistently ranks sampling geometries and predicts downstream reconstruction/recognition difficulty \\emph{without training}. In particular, minimizing $|ΔS_{\\mathcal B}|$ enables zero-training selection of variable-density MRI mask parameters that matches designs tuned by conventional pre-reconstruction criteria. These results suggest that phase-space entropy at acquisition reflects downstream learnability, enabling pre-training selection of candidate sampling policies and as a shared notion of information preservation across modalities.","authors":["Xiu-Cheng Wang","Jun-Jie Zhanga","Nan Cheng","Long-Gang Pang","Taijiao Du","Deyu Meng"],"pdf_url":"","comment":"22 pages 6 figures"},{"id":"http://arxiv.org/abs/2512.19206v1","updated":"2025-12-22T09:44:26Z","published":"2025-12-22T09:44:26Z","title":"MixKVQ: Query-Aware Mixed-Precision KV Cache Quantization for Long-Context Reasoning","summary":"Long Chain-of-Thought (CoT) reasoning has significantly advanced the capabilities of Large Language Models (LLMs), but this progress is accompanied by substantial memory and latency overhead from the extensive Key-Value (KV) cache. Although KV cache quantization is a promising compression technique, existing low-bit quantization methods often exhibit severe performance degradation on complex reasoning tasks. Fixed-precision quantization struggles to handle outlier channels in the key cache, while current mixed-precision strategies fail to accurately identify components requiring high-precision representation. We find that an effective low-bit KV cache quantization strategy must consider two factors: a key channel's intrinsic quantization difficulty and its relevance to the query. Based on this insight, we propose MixKVQ, a novel plug-and-play method that introduces a lightweight, query-aware algorithm to identify and preserve critical key channels that need higher precision, while applying per-token quantization for value cache. Experiments on complex reasoning datasets demonstrate that our approach significantly outperforms existing low-bit methods, achieving performance comparable to a full-precision baseline at a substantially reduced memory footprint.","authors":["Tao Zhang","Ziqian Zeng","Hao Peng","Huiping Zhuang","Cen Chen"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2402.13081v2","updated":"2025-12-22T09:39:40Z","published":"2024-02-20T15:25:56Z","title":"IT Intrusion Detection Using Statistical Learning and Testbed Measurements","summary":"We study automated intrusion detection in an IT infrastructure, specifically the problem of identifying the start of an attack, the type of attack, and the sequence of actions an attacker takes, based on continuous measurements from the infrastructure. We apply statistical learning methods, including Hidden Markov Model (HMM), Long Short-Term Memory (LSTM), and Random Forest Classifier (RFC) to map sequences of observations to sequences of predicted attack actions. In contrast to most related research, we have abundant data to train the models and evaluate their predictive power. The data comes from traces we generate on an in-house testbed where we run attacks against an emulated IT infrastructure. Central to our work is a machine-learning pipeline that maps measurements from a high-dimensional observation space to a space of low dimensionality or to a small set of observation symbols. Investigating intrusions in offline as well as online scenarios, we find that both HMM and LSTM can be effective in predicting attack start time, attack type, and attack actions. If sufficient training data is available, LSTM achieves higher prediction accuracy than HMM. HMM, on the other hand, requires less computational resources and less training data for effective prediction. Also, we find that the methods we study benefit from data produced by traditional intrusion detection systems like SNORT.","authors":["Xiaoxuan Wang","Rolf Stadler"],"pdf_url":"","comment":"A version of this paper appeared in the conference proceedings of NOMS 2024 (IEEE/IFIP Network Operations and Management Symposium)"},{"id":"http://arxiv.org/abs/2512.19199v1","updated":"2025-12-22T09:36:24Z","published":"2025-12-22T09:36:24Z","title":"On the Koopman-Based Generalization Bounds for Multi-Task Deep Learning","summary":"The paper establishes generalization bounds for multitask deep neural networks using operator-theoretic techniques. The authors propose a tighter bound than those derived from conventional norm based methods by leveraging small condition numbers in the weight matrices and introducing a tailored Sobolev space as an expanded hypothesis space. This enhanced bound remains valid even in single output settings, outperforming existing Koopman based bounds. The resulting framework maintains key advantages such as flexibility and independence from network width, offering a more precise theoretical understanding of multitask deep learning in the context of kernel methods.","authors":["Mahdi Mohammadigohari","Giuseppe Di Fatta","Giuseppe Nicosia","Panos M. Pardalos"],"pdf_url":"","comment":"Accepted for publication in Lecture Notes in Computer Science (LNCS). Final version forthcoming"},{"id":"http://arxiv.org/abs/2512.15661v2","updated":"2025-12-22T09:34:23Z","published":"2025-12-17T18:14:59Z","title":"Prospects for quantum advantage in machine learning from the representability of functions","summary":"Demonstrating quantum advantage in machine learning tasks requires navigating a complex landscape of proposed models and algorithms. To bring clarity to this search, we introduce a framework that connects the structure of parametrized quantum circuits to the mathematical nature of the functions they can actually learn. Within this framework, we show how fundamental properties, like circuit depth and non-Clifford gate count, directly determine whether a model's output leads to efficient classical simulation or surrogation. We argue that this analysis uncovers common pathways to dequantization that underlie many existing simulation methods. More importantly, it reveals critical distinctions between models that are fully simulatable, those whose function space is classically tractable, and those that remain robustly quantum. This perspective provides a conceptual map of this landscape, clarifying how different models relate to classical simulability and pointing to where opportunities for quantum advantage may lie.","authors":["Sergi Masot-Llima","Elies Gil-Fuster","Carlos Bravo-Prieto","Jens Eisert","Tommaso Guaita"],"pdf_url":"","comment":"21 pages, 6 figures, comments welcome"},{"id":"http://arxiv.org/abs/2512.19196v1","updated":"2025-12-22T09:31:31Z","published":"2025-12-22T09:31:31Z","title":"Self-Consistent Probability Flow for High-Dimensional Fokker-Planck Equations","summary":"Solving high-dimensional Fokker-Planck (FP) equations is a challenge in computational physics and stochastic dynamics, due to the curse of dimensionality (CoD) and the bottleneck of evaluating second-order diffusion terms. Existing deep learning approaches, such as Physics-Informed Neural Networks (PINNs), face computational challenges as dimensionality increases, driven by the $O(D^2)$ complexity of automatic differentiation for second-order derivatives. While recent probability flow approaches bypass this by learning score functions or matching velocity fields, they often involve serial computational operations or depend on sampling efficiency in complex distributions. To address these issues, we propose the Self-Consistent Probability Flow (SCPF) method. We reformulate the second-order FP equation into an equivalent first-order deterministic Probability Flow ODE (PF-ODE) constraint. Unlike score matching or velocity matching, SCPF solves this problem by minimizing the residual of the PF-ODE continuity equation, which avoids explicit Hessian computation. We leverage Continuous Normalizing Flows (CNF) combined with the Hutchinson Trace Estimator (HTE) to reduce the training complexity to linear scale $O(D)$, achieving an effective $O(1)$ wall-clock time on GPUs. To address data sparsity in high dimensions, we apply a generative adaptive sampling strategy and theoretically prove that dynamically aligning collocation points with the evolving probability mass is a necessary condition to bound the approximation error. Experiments on diverse benchmarks -- ranging from anisotropic Ornstein-Uhlenbeck (OU) processes and high-dimensional Brownian motions with time-varying diffusion terms, to Geometric OU processes featuring non-Gaussian solutions -- demonstrate that SCPF effectively mitigates the CoD, maintaining high accuracy and constant computational cost for problems up to 100 dimensions.","authors":["Xiaolong Wu","Qifeng Liao"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19194v1","updated":"2025-12-22T09:30:25Z","published":"2025-12-22T09:30:25Z","title":"Causal Heterogeneous Graph Learning Method for Chronic Obstructive Pulmonary Disease Prediction","summary":"Due to the insufficient diagnosis and treatment capabilities at the grassroots level, there are still deficiencies in the early identification and early warning of acute exacerbation of Chronic obstructive pulmonary disease (COPD), often resulting in a high prevalence rate and high burden, but the screening rate is relatively low. In order to gradually improve this situation. In this paper, this study develop a Causal Heterogeneous Graph Representation Learning (CHGRL) method for COPD comorbidity risk prediction method that: a) constructing a heterogeneous Our dataset includes the interaction between patients and diseases; b) A cause-aware heterogeneous graph learning architecture has been constructed, combining causal inference mechanisms with heterogeneous graph learning, which can support heterogeneous graph causal learning for different types of relationships; and c) Incorporate the causal loss function in the model design, and add counterfactual reasoning learning loss and causal regularization loss on the basis of the cross-entropy classification loss. We evaluate our method and compare its performance with strong GNN baselines. Following experimental evaluation, the proposed model demonstrates high detection accuracy.","authors":["Leming Zhou","Zuo Wang","Zhigang Liu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19190v1","updated":"2025-12-22T09:28:23Z","published":"2025-12-22T09:28:23Z","title":"PEDESTRIAN: An Egocentric Vision Dataset for Obstacle Detection on Pavements","summary":"Walking has always been a primary mode of transportation and is recognized as an essential activity for maintaining good health. Despite the need for safe walking conditions in urban environments, sidewalks are frequently obstructed by various obstacles that hinder free pedestrian movement. Any object obstructing a pedestrian's path can pose a safety hazard. The advancement of pervasive computing and egocentric vision techniques offers the potential to design systems that can automatically detect such obstacles in real time, thereby enhancing pedestrian safety. The development of effective and efficient identification algorithms relies on the availability of comprehensive and well-balanced datasets of egocentric data. In this work, we introduce the PEDESTRIAN dataset, comprising egocentric data for 29 different obstacles commonly found on urban sidewalks. A total of 340 videos were collected using mobile phone cameras, capturing a pedestrian's point of view. Additionally, we present the results of a series of experiments that involved training several state-of-the-art deep learning algorithms using the proposed dataset, which can be used as a benchmark for obstacle detection and recognition tasks. The dataset can be used for training pavement obstacle detectors to enhance the safety of pedestrians in urban areas.","authors":["Marios Thoma","Zenonas Theodosiou","Harris Partaourides","Vassilis Vassiliades","Loizos Michael","Andreas Lanitis"],"pdf_url":"","comment":"24 pages, 7 figures, 9 tables, Dataset: https://doi.org/10.5281/zenodo.10907945, Code: https://github.com/CYENS/PEDESTRIAN"},{"id":"http://arxiv.org/abs/2506.09574v2","updated":"2025-12-22T09:20:25Z","published":"2025-06-11T10:12:50Z","title":"MOORL: A Framework for Integrating Offline-Online Reinforcement Learning","summary":"Sample efficiency and exploration remain critical challenges in Deep Reinforcement Learning (DRL), particularly in complex domains. Offline RL, which enables agents to learn optimal policies from static, pre-collected datasets, has emerged as a promising alternative. However, offline RL is constrained by issues such as out-of-distribution (OOD) actions that limit policy performance and generalization. To overcome these limitations, we propose Meta Offline-Online Reinforcement Learning (MOORL), a hybrid framework that unifies offline and online RL for efficient and scalable learning. While previous hybrid methods rely on extensive design components and added computational complexity to utilize offline data effectively, MOORL introduces a meta-policy that seamlessly adapts across offline and online trajectories. This enables the agent to leverage offline data for robust initialization while utilizing online interactions to drive efficient exploration. Our theoretical analysis demonstrates that the hybrid approach enhances exploration by effectively combining the complementary strengths of offline and online data. Furthermore, we demonstrate that MOORL learns a stable Q-function without added complexity. Extensive experiments on 28 tasks from the D4RL and V-D4RL benchmarks validate its effectiveness, showing consistent improvements over state-of-the-art offline and hybrid RL baselines. With minimal computational overhead, MOORL achieves strong performance, underscoring its potential for practical applications in real-world scenarios.","authors":["Gaurav Chaudhary","Wassim Uddin Mondal","Laxmidhar Behera"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19184v1","updated":"2025-12-22T09:18:30Z","published":"2025-12-22T09:18:30Z","title":"Operator-Based Generalization Bound for Deep Learning: Insights on Multi-Task Learning","summary":"This paper presents novel generalization bounds for vector-valued neural networks and deep kernel methods, focusing on multi-task learning through an operator-theoretic framework. Our key development lies in strategically combining a Koopman based approach with existing techniques, achieving tighter generalization guarantees compared to traditional norm-based bounds. To mitigate computational challenges associated with Koopman-based methods, we introduce sketching techniques applicable to vector valued neural networks. These techniques yield excess risk bounds under generic Lipschitz losses, providing performance guarantees for applications including robust and multiple quantile regression. Furthermore, we propose a novel deep learning framework, deep vector-valued reproducing kernel Hilbert spaces (vvRKHS), leveraging Perron Frobenius (PF) operators to enhance deep kernel methods. We derive a new Rademacher generalization bound for this framework, explicitly addressing underfitting and overfitting through kernel refinement strategies. This work offers novel insights into the generalization properties of multitask learning with deep learning architectures, an area that has been relatively unexplored until recent developments.","authors":["Mahdi Mohammadigohari","Giuseppe Di Fatta","Giuseppe Nicosia","Panos M. Pardalos"],"pdf_url":"","comment":"Accepted for publication in Lecture Notes in Computer Science (LNCS). Final version forthcoming"},{"id":"http://arxiv.org/abs/2512.17629v2","updated":"2025-12-22T09:18:11Z","published":"2025-12-19T14:33:02Z","title":"SCOPE: Sequential Causal Optimization of Process Interventions","summary":"Prescriptive Process Monitoring (PresPM) recommends interventions during business processes to optimize key performance indicators (KPIs). In realistic settings, interventions are rarely isolated: organizations need to align sequences of interventions to jointly steer the outcome of a case. Existing PresPM approaches fall short in this respect. Many focus on a single intervention decision, while others treat multiple interventions independently, ignoring how they interact over time. Methods that do address these dependencies depend either on simulation or data augmentation to approximate the process to train a Reinforcement Learning (RL) agent, which can create a reality gap and introduce bias. We introduce SCOPE, a PresPM approach that learns aligned sequential intervention recommendations. SCOPE employs backward induction to estimate the effect of each candidate intervention action, propagating its impact from the final decision point back to the first. By leveraging causal learners, our method can utilize observational data directly, unlike methods that require constructing process approximations for reinforcement learning. Experiments on both an existing synthetic dataset and a new semi-synthetic dataset show that SCOPE consistently outperforms state-of-the-art PresPM techniques in optimizing the KPI. The novel semi-synthetic setup, based on a real-life event log, is provided as a reusable benchmark for future work on sequential PresPM.","authors":["Jakob De Moor","Hans Weytjens","Johannes De Smedt","Jochen De Weerdt"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2507.12815v2","updated":"2025-12-22T09:16:29Z","published":"2025-07-17T06:16:06Z","title":"From Novelty to Imitation: Self-Distilled Rewards for Offline Reinforcement Learning","summary":"Offline Reinforcement Learning (RL) aims to learn effective policies from a static dataset without requiring further agent-environment interactions. However, its practical adoption is often hindered by the need for explicit reward annotations, which can be costly to engineer or difficult to obtain retrospectively. To address this, we propose ReLOAD (Reinforcement Learning with Offline Reward Annotation via Distillation), a novel reward annotation framework for offline RL. Unlike existing methods that depend on complex alignment procedures, our approach adapts Random Network Distillation (RND) to generate intrinsic rewards from expert demonstrations using a simple yet effective embedding discrepancy measure. First, we train a predictor network to mimic a fixed target network's embeddings based on expert state transitions. Later, the prediction error between these networks serves as a reward signal for each transition in the static dataset. This mechanism provides a structured reward signal without requiring handcrafted reward annotations. We provide a formal theoretical construct that offers insights into how RND prediction errors effectively serve as intrinsic rewards by distinguishing expert-like transitions. Experiments on the D4RL benchmark demonstrate that ReLOAD enables robust offline policy learning and achieves performance competitive with traditional reward-annotated methods.","authors":["Gaurav Chaudhary","Laxmidhar Behera"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19180v1","updated":"2025-12-22T09:16:08Z","published":"2025-12-22T09:16:08Z","title":"Practical Quantum-Classical Feature Fusion for complex data Classification","summary":"Hybrid quantum and classical learning aims to couple quantum feature maps with the robustness of classical neural networks, yet most architectures treat the quantum circuit as an isolated feature extractor and merge its measurements with classical representations by direct concatenation. This neglects that the quantum and classical branches constitute distinct computational modalities and limits reliable performance on complex, high dimensional tabular and semi structured data, including remote sensing, environmental monitoring, and medical diagnostics. We present a multimodal formulation of hybrid learning and propose a cross attention mid fusion architecture in which a classical representation queries quantum derived feature tokens through an attention block with residual connectivity. The quantum branch is kept within practical NISQ budgets and uses up to nine qubits. We evaluate on Wine, Breast Cancer, Forest CoverType, FashionMNIST, and SteelPlatesFaults, comparing a quantum only model, a classical baseline, residual hybrid models, and the proposed mid fusion model under a consistent protocol. Pure quantum and standard hybrid designs underperform due to measurement induced information loss, while cross attention mid fusion is consistently competitive and improves performance on the more complex datasets in most cases. These findings suggest that quantum derived information becomes most valuable when integrated through principled multimodal fusion rather than used in isolation or loosely appended to classical features.","authors":["Azadeh Alavi","Fatemeh Kouchmeshki","Abdolrahman Alavi"],"pdf_url":"","comment":"16 pages, 3 figues"},{"id":"http://arxiv.org/abs/2512.19172v1","updated":"2025-12-22T09:07:09Z","published":"2025-12-22T09:07:09Z","title":"Finite-sample guarantees for data-driven forward-backward operator methods","summary":"We establish finite sample certificates on the quality of solutions produced by data-based forward-backward (FB) operator splitting schemes. As frequently happens in stochastic regimes, we consider the problem of finding a zero of the sum of two operators, where one is either unavailable in closed form or computationally expensive to evaluate, and shall therefore be approximated using a finite number of noisy oracle samples. Under the lens of algorithmic stability, we then derive probabilistic bounds on the distance between a true zero and the FB output without making specific assumptions about the underlying data distribution. We show that under weaker conditions ensuring the convergence of FB schemes, stability bounds grow proportionally to the number of iterations. Conversely, stronger assumptions yield stability guarantees that are independent of the iteration count. We then specialize our results to a popular FB stochastic Nash equilibrium seeking algorithm and validate our theoretical bounds on a control problem for smart grids, where the energy price uncertainty is approximated by means of historical data.","authors":["Filippo Fabiani","Barbara Franci"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19154v1","updated":"2025-12-22T08:50:30Z","published":"2025-12-22T08:50:30Z","title":"Beyond Sliding Windows: Learning to Manage Memory in Non-Markovian Environments","summary":"Recent success in developing increasingly general purpose agents based on sequence models has led to increased focus on the problem of deploying computationally limited agents within the vastly more complex real-world. A key challenge experienced in these more realistic domains is highly non-Markovian dependencies with respect to the agent's observations, which are less common in small controlled domains. The predominant approach for dealing with this in the literature is to stack together a window of the most recent observations (Frame Stacking), but this window size must grow with the degree of non-Markovian dependencies, which results in prohibitive computational and memory requirements for both action inference and learning. In this paper, we are motivated by the insight that in many environments that are highly non-Markovian with respect to time, the environment only causally depends on a relatively small number of observations over that time-scale. A natural direction would then be to consider meta-algorithms that maintain relatively small adaptive stacks of memories such that it is possible to express highly non-Markovian dependencies with respect to time while considering fewer observations at each step and thus experience substantial savings in both compute and memory requirements. Hence, we propose a meta-algorithm (Adaptive Stacking) for achieving exactly that with convergence guarantees and quantify the reduced computation and memory constraints for MLP, LSTM, and Transformer-based agents. Our experiments utilize popular memory tasks, which give us control over the degree of non-Markovian dependencies. This allows us to demonstrate that an appropriate meta-algorithm can learn the removal of memories not predictive of future rewards without excessive removal of important experiences. Code: https://github.com/geraudnt/adaptive-stacking","authors":["Geraud Nangue Tasse","Matthew Riemer","Benjamin Rosman","Tim Klinger"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.13123v3","updated":"2025-12-22T08:50:20Z","published":"2025-12-15T09:26:45Z","title":"Stopping Rules for Stochastic Gradient Descent via Anytime-Valid Confidence Sequences","summary":"We study stopping rules for stochastic gradient descent (SGD) for convex optimization from the perspective of anytime-valid confidence sequences. Classical analyses of SGD provide convergence guarantees in expectation or at a fixed horizon, but offer no statistically valid way to assess, at an arbitrary time, how close the current iterate is to the optimum. We develop an anytime-valid, data-dependent upper confidence sequence for the weighted average suboptimality of projected SGD, constructed via nonnegative supermartingales and requiring no smoothness or strong convexity. This confidence sequence yields a simple stopping rule that is provably $\\varepsilon$-optimal with probability at least $1-α$, with explicit bounds on the stopping time under standard stochastic approximation stepsizes. To the best of our knowledge, these are the first rigorous, time-uniform performance guarantees and finite-time $\\varepsilon$-optimality certificates for projected SGD with general convex objectives, based solely on observable trajectory quantities.","authors":["Liviu Aolaritei","Michael I. Jordan"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2511.00066v2","updated":"2025-12-22T08:49:46Z","published":"2025-10-29T08:07:47Z","title":"Sharpness-Controlled Group Relative Policy Optimization with Token-Level Probability Shaping","summary":"Reinforcement learning with verifiable rewards (RLVR) has become a practical route to improve large language model reasoning, and Group Relative Policy Optimization (GRPO) is a widely used optimizer in this setting. This paper revisits GRPO from a generalization perspective. Recent analysis shows that population performance can be controlled by a robust empirical objective that decomposes into the training loss plus a sharpness term measured by the gradient norm. We develop a token-level view of this sharpness term and show that GRPO can be dominated by a small subset of tokens with disproportionately large per-token gradients, which increases sharpness and can harm generalization. Motivated by this view, we propose Token-Regulated GRPO (TR-GRPO), which introduces a monotone probability shaping function to assign token weights based on the model's own token probabilities, and integrates these weights into the standard GRPO. Our analysis yields a bound that isolates a probability dependent multiplicative factor in token-gradient magnitudes, explaining how probability-aware weighting suppresses sharp directions while preserving learning signal on semantically critical tokens. Experiments on logic puzzles, mathematical reasoning, and tool-augmented question answering show consistent improvements over GRPO, along with smoother gradient-norm trajectories, supporting TR-GRPO as a simple and effective generalization-oriented upgrade to GRPO for RLVR.","authors":["Tue Le","Nghi D. Q. Bui","Linh Ngo Van","Trung Le"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2410.16750v3","updated":"2025-12-22T08:46:43Z","published":"2024-10-22T07:12:38Z","title":"Theoretical Convergence Guarantees for Variational Autoencoders","summary":"Variational Autoencoders (VAE) are popular generative models used to sample from complex data distributions. Despite their empirical success in various machine learning tasks, significant gaps remain in understanding their theoretical properties, particularly regarding convergence guarantees. This paper aims to bridge that gap by providing non-asymptotic convergence guarantees for VAE trained using both Stochastic Gradient Descent and Adam algorithms. We derive a convergence rate of $\\mathcal{O}(\\log n / \\sqrt{n})$, where $n$ is the number of iterations of the optimization algorithm, with explicit dependencies on the batch size, the number of variational samples, and other key hyperparameters. Our theoretical analysis applies to both Linear VAE and Deep Gaussian VAE, as well as several VAE variants, including $β$-VAE and IWAE. Additionally, we empirically illustrate the impact of hyperparameters on convergence, offering new insights into the theoretical understanding of VAE training.","authors":["Sobihan Surendran","Antoine Godichon-Baggioni","Sylvain Le Corff"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19147v1","updated":"2025-12-22T08:44:58Z","published":"2025-12-22T08:44:58Z","title":"RP-CATE: Recurrent Perceptron-based Channel Attention Transformer Encoder for Industrial Hybrid Modeling","summary":"Nowadays, industrial hybrid modeling which integrates both mechanistic modeling and machine learning-based modeling techniques has attracted increasing interest from scholars due to its high accuracy, low computational cost, and satisfactory interpretability. Nevertheless, the existing industrial hybrid modeling methods still face two main limitations. First, current research has mainly focused on applying a single machine learning method to one specific task, failing to develop a comprehensive machine learning architecture suitable for modeling tasks, which limits their ability to effectively represent complex industrial scenarios. Second, industrial datasets often contain underlying associations (e.g., monotonicity or periodicity) that are not adequately exploited by current research, which can degrade model's predictive performance. To address these limitations, this paper proposes the Recurrent Perceptron-based Channel Attention Transformer Encoder (RP-CATE), with three distinctive characteristics: 1: We developed a novel architecture by replacing the self-attention mechanism with channel attention and incorporating our proposed Recurrent Perceptron (RP) Module into Transformer, achieving enhanced effectiveness for industrial modeling tasks compared to the original Transformer. 2: We proposed a new data type called Pseudo-Image Data (PID) tailored for channel attention requirements and developed a cyclic sliding window method for generating PID. 3: We introduced the concept of Pseudo-Sequential Data (PSD) and a method for converting industrial datasets into PSD, which enables the RP Module to capture the underlying associations within industrial dataset more effectively. An experiment aimed at hybrid modeling in chemical engineering was conducted by using RP-CATE and the experimental results demonstrate that RP-CATE achieves the best performance compared to other baseline models.","authors":["Haoran Yang","Yinan Zhang","Wenjie Zhang","Dongxia Wang","Peiyu Liu","Yuqi Ye","Kexin Chen","Wenhai Wang"],"pdf_url":"","comment":"9 pages, 4 figures"},{"id":"http://arxiv.org/abs/2511.09392v4","updated":"2025-12-22T08:42:42Z","published":"2025-11-12T15:00:52Z","title":"Potent but Stealthy: Rethink Profile Pollution against Sequential Recommendation via Bi-level Constrained Reinforcement Paradigm","summary":"Sequential Recommenders, which exploit dynamic user intents through interaction sequences, is vulnerable to adversarial attacks. While existing attacks primarily rely on data poisoning, they require large-scale user access or fake profiles thus lacking practicality. In this paper, we focus on the Profile Pollution Attack that subtly contaminates partial user interactions to induce targeted mispredictions. Previous PPA methods suffer from two limitations, i.e., i) over-reliance on sequence horizon impact restricts fine-grained perturbations on item transitions, and ii) holistic modifications cause detectable distribution shifts. To address these challenges, we propose a constrained reinforcement driven attack CREAT that synergizes a bi-level optimization framework with multi-reward reinforcement learning to balance adversarial efficacy and stealthiness. We first develop a Pattern Balanced Rewarding Policy, which integrates pattern inversion rewards to invert critical patterns and distribution consistency rewards to minimize detectable shifts via unbalanced co-optimal transport. Then we employ a Constrained Group Relative Reinforcement Learning paradigm, enabling step-wise perturbations through dynamic barrier constraints and group-shared experience replay, achieving targeted pollution with minimal detectability. Extensive experiments demonstrate the effectiveness of CREAT.","authors":["Jiajie Su","Zihan Nan","Yunshan Ma","Xiaobo Xia","Xiaohua Feng","Weiming Liu","Xiang Chen","Xiaolin Zheng","Chaochao Chen"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19142v1","updated":"2025-12-22T08:41:31Z","published":"2025-12-22T08:41:31Z","title":"A Convex Loss Function for Set Prediction with Optimal Trade-offs Between Size and Conditional Coverage","summary":"We consider supervised learning problems in which set predictions provide explicit uncertainty estimates. Using Choquet integrals (a.k.a. Lov{á}sz extensions), we propose a convex loss function for nondecreasing subset-valued functions obtained as level sets of a real-valued function. This loss function allows optimal trade-offs between conditional probabilistic coverage and the ''size'' of the set, measured by a non-decreasing submodular function. We also propose several extensions that mimic loss functions and criteria for binary classification with asymmetric losses, and show how to naturally obtain sets with optimized conditional coverage. We derive efficient optimization algorithms, either based on stochastic gradient descent or reweighted least-squares formulations, and illustrate our findings with a series of experiments on synthetic datasets for classification and regression tasks, showing improvements over approaches that aim for marginal coverage.","authors":["Francis Bach"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19131v1","updated":"2025-12-22T08:26:54Z","published":"2025-12-22T08:26:54Z","title":"Evidential Trust-Aware Model Personalization in Decentralized Federated Learning for Wearable IoT","summary":"Decentralized federated learning (DFL) enables collaborative model training across edge devices without centralized coordination, offering resilience against single points of failure. However, statistical heterogeneity arising from non-identically distributed local data creates a fundamental challenge: nodes must learn personalized models adapted to their local distributions while selectively collaborating with compatible peers. Existing approaches either enforce a single global model that fits no one well, or rely on heuristic peer selection mechanisms that cannot distinguish between peers with genuinely incompatible data distributions and those with valuable complementary knowledge. We present Murmura, a framework that leverages evidential deep learning to enable trust-aware model personalization in DFL. Our key insight is that epistemic uncertainty from Dirichlet-based evidential models directly indicates peer compatibility: high epistemic uncertainty when a peer's model evaluates local data reveals distributional mismatch, enabling nodes to exclude incompatible influence while maintaining personalized models through selective collaboration. Murmura introduces a trust-aware aggregation mechanism that computes peer compatibility scores through cross-evaluation on local validation samples and personalizes model aggregation based on evidential trust with adaptive thresholds. Evaluation on three wearable IoT datasets (UCI HAR, PAMAP2, PPG-DaLiA) demonstrates that Murmura reduces performance degradation from IID to non-IID conditions compared to baseline (0.9% vs. 19.3%), achieves 7.4$\\times$ faster convergence, and maintains stable accuracy across hyperparameter choices. These results establish evidential uncertainty as a principled foundation for compatibility-aware personalization in decentralized heterogeneous environments.","authors":["Murtaza Rangwala","Richard O. Sinnott","Rajkumar Buyya"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19125v1","updated":"2025-12-22T08:05:01Z","published":"2025-12-22T08:05:01Z","title":"SAP: Syntactic Attention Pruning for Transformer-based Language Models","summary":"This paper introduces Syntactic Attention Pruning (SAP), a novel method for effectively pruning attention heads in Transformer models. Unlike conventional approaches that rely solely on mathematical analysis of model weights and activations, SAP incorporates both the syntactic structure and attention patterns of sentences to guide the pruning process. By leveraging these linguistic features, SAP not only achieves performance comparable to state-of-the-art methods but also enhances the interpretability of model behavior. To further improve robustness, we propose Candidate Filtering (CF), a mechanism that prioritizes heads based on their contribution to model performance, mitigating degradation during pruning. Experimental results indicate that SAP effectively preserves critical heads of a high density of strong attention values, outperforming existing head pruning strategies in retrain-free settings. These findings position SAP as a promising foundation for a new direction in model compression research, offering high flexibility for pruning across all transformer-based language models.","authors":["Tzu-Yun Lee","Ding-Yong Hong","Jan-Jan Wu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19123v1","updated":"2025-12-22T07:57:20Z","published":"2025-12-22T07:57:20Z","title":"A Composable Channel-Adaptive Architecture for Seizure Classification","summary":"Objective: We develop a channel-adaptive (CA) architecture that seamlessly processes multi-variate time-series with an arbitrary number of channels, and in particular intracranial electroencephalography (iEEG) recordings. Methods: Our CA architecture first processes the iEEG signal using state-of-the-art models applied to each single channel independently. The resulting features are then fused using a vector-symbolic algorithm which reconstructs the spatial relationship using a trainable scalar per channel. Finally, the fused features are accumulated in a long-term memory of up to 2 minutes to perform the classification. Each CA-model can then be pre-trained on a large corpus of iEEG recordings from multiple heterogeneous subjects. The pre-trained model is personalized to each subject via a quick fine-tuning routine, which uses equal or lower amounts of data compared to existing state-of-the-art models, but requiring only 1/5 of the time. Results: We evaluate our CA-models on a seizure detection task both on a short-term (~20 hours) and a long-term (~2500 hours) dataset. In particular, our CA-EEGWaveNet is trained on a single seizure of the tested subject, while the baseline EEGWaveNet is trained on all but one. Even in this challenging scenario, our CA-EEGWaveNet surpasses the baseline in median F1-score (0.78 vs 0.76). Similarly, CA-EEGNet based on EEGNet, also surpasses its baseline in median F1-score (0.79 vs 0.74). Conclusion and significance: Our CA-model addresses two issues: first, it is channel-adaptive and can therefore be trained across heterogeneous subjects without loss of performance; second, it increases the effective temporal context size to a clinically-relevant length. Therefore, our model is a drop-in replacement for existing models, bringing better characteristics and performance across the board.","authors":["Francesco Carzaniga","Michael Hersche","Kaspar Schindler","Abbas Rahimi"],"pdf_url":"","comment":"2025 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works"},{"id":"http://arxiv.org/abs/2512.19114v1","updated":"2025-12-22T07:35:16Z","published":"2025-12-22T07:35:16Z","title":"HyperLoad: A Cross-Modality Enhanced Large Language Model-Based Framework for Green Data Center Cooling Load Prediction","summary":"The rapid growth of artificial intelligence is exponentially escalating computational demand, inflating data center energy use and carbon emissions, and spurring rapid deployment of green data centers to relieve resource and environmental stress. Achieving sub-minute orchestration of renewables, storage, and loads, while minimizing PUE and lifecycle carbon intensity, hinges on accurate load forecasting. However, existing methods struggle to address small-sample scenarios caused by cold start, load distortion, multi-source data fragmentation, and distribution shifts in green data centers. We introduce HyperLoad, a cross-modality framework that exploits pre-trained large language models (LLMs) to overcome data scarcity. In the Cross-Modality Knowledge Alignment phase, textual priors and time-series data are mapped to a common latent space, maximizing the utility of prior knowledge. In the Multi-Scale Feature Modeling phase, domain-aligned priors are injected through adaptive prefix-tuning, enabling rapid scenario adaptation, while an Enhanced Global Interaction Attention mechanism captures cross-device temporal dependencies. The public DCData dataset is released for benchmarking. Under both data sufficient and data scarce settings, HyperLoad consistently surpasses state-of-the-art (SOTA) baselines, demonstrating its practicality for sustainable green data center management.","authors":["Haoyu Jiang","Boan Qu","Junjie Zhu","Fanjie Zeng","Xiaojie Lin","Wei Zhong"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2508.08985v3","updated":"2025-12-22T07:34:52Z","published":"2025-08-12T14:53:54Z","title":"Low-Regret and Low-Complexity Learning for Hierarchical Inference","summary":"This work focuses on Hierarchical Inference (HI) in edge intelligence systems, where a compact Local-ML model on an end-device works in conjunction with a high-accuracy Remote-ML model on an edge-server. HI aims to reduce latency, improve accuracy, and lower bandwidth usage by first using the Local-ML model for inference and offloading to the Remote-ML only when the local inference is likely incorrect. A critical challenge in HI is estimating the likelihood of the local inference being incorrect, especially when data distributions and offloading costs change over time -- a problem we term Hierarchical Inference Learning (HIL). We introduce a novel approach to HIL by modeling the probability of correct inference by the Local-ML as an increasing function of the model's confidence measure, a structure motivated by empirical observations but previously unexploited. We propose two policies, HI-LCB and HI-LCB-lite, based on the Upper Confidence Bound (UCB) framework. We demonstrate that both policies achieve order-optimal regret of $O(\\log T)$, a significant improvement over existing HIL policies with $O(T^{2/3})$ regret guarantees. Notably, HI-LCB-lite has an $O(1)$ per-sample computational complexity, making it well-suited for deployment on devices with severe resource limitations. Simulations using real-world datasets confirm that our policies outperform existing state-of-the-art HIL methods.","authors":["Sameep Chattopadhyay","Vinay Sutar","Jaya Prakash Champati","Sharayu Moharir"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2509.25300v3","updated":"2025-12-22T07:20:59Z","published":"2025-09-29T17:10:35Z","title":"Scaling Behaviors of LLM Reinforcement Learning Post-Training: An Empirical Study in Mathematical Reasoning","summary":"While scaling laws for large language models (LLMs) during pre-training have been extensively studied, their behavior under reinforcement learning (RL) post-training remains largely unexplored. This paper presents a systematic empirical investigation of scaling behaviors in RL-based post-training, with a particular focus on mathematical reasoning. Based on a set of experiments across the full Qwen2.5 dense model series (0.5B to 72B), we characterize how model scale, data volume, and computational budget interact to shape performance. Our analysis leads to four key findings: 1. Larger models consistently exhibit superior learning efficiency on both compute and data metrics. 2. The relationship between test loss, compute, and data can be modeled by a predictive power-law which is robust across both base and instruction-tuned models. 3. Although larger models exhibit higher learning efficiency, the analytical learning efficiency term k(N) in the power-law reveals a latent saturation trend in learning efficiency as model size continues to increase. 4. In data-constrained regimes, repeated reuse of high-quality data proves highly effective, as final performance is primarily governed by the total number of optimization steps rather than the uniqueness of samples. Collectively, these results provide a principled foundation and practical guidelines for efficiently scaling the reasoning capabilities of LLMs through RL post-training.","authors":["Zelin Tan","Hejia Geng","Xiaohang Yu","Mulei Zhang","Guancheng Wan","Yifan Zhou","Qiang He","Xiangyuan Xue","Heng Zhou","Yutao Fan","Zhongzhi Li","Zaibin Zhang","Guibin Zhang","Chen Zhang","Zhenfei Yin","Philip Torr","Lei Bai"],"pdf_url":"","comment":"V3 version:27 pages, 14 figures, add code and dataset url"},{"id":"http://arxiv.org/abs/2512.19104v1","updated":"2025-12-22T07:18:57Z","published":"2025-12-22T07:18:57Z","title":"Explicit and Non-asymptotic Query Complexities of Rank-Based Zeroth-order Algorithm on Stochastic Smooth Functions","summary":"Zeroth-order (ZO) optimization with ordinal feedback has emerged as a fundamental problem in modern machine learning systems, particularly in human-in-the-loop settings such as reinforcement learning from human feedback, preference learning, and evolutionary strategies. While rank-based ZO algorithms enjoy strong empirical success and robustness properties, their theoretical understanding, especially under stochastic objectives and standard smoothness assumptions, remains limited. In this paper, we study rank-based zeroth-order optimization for stochastic functions where only ordinal feedback of the stochastic function is available. We propose a simple and computationally efficient rank-based ZO algorithm. Under standard assumptions including smoothness, strong convexity, and bounded second moments of stochastic gradients, we establish explicit non-asymptotic query complexity bounds for both convex and nonconvex objectives. Notably, our results match the best-known query complexities of value-based ZO algorithms, demonstrating that ordinal information alone is sufficient for optimal query efficiency in stochastic settings. Our analysis departs from existing drift-based and information-geometric techniques, offering new tools for the study of rank-based optimization under noise. These findings narrow the gap between theory and practice and provide a principled foundation for optimization driven by human preferences.","authors":["Haishan Ye"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19103v1","updated":"2025-12-22T07:18:13Z","published":"2025-12-22T07:18:13Z","title":"Timely Parameter Updating in Over-the-Air Federated Learning","summary":"Incorporating over-the-air computations (OAC) into the model training process of federated learning (FL) is an effective approach to alleviating the communication bottleneck in FL systems. Under OAC-FL, every client modulates its intermediate parameters, such as gradient, onto the same set of orthogonal waveforms and simultaneously transmits the radio signal to the edge server. By exploiting the superposition property of multiple-access channels, the edge server can obtain an automatically aggregated global gradient from the received signal. However, the limited number of orthogonal waveforms available in practical systems is fundamentally mismatched with the high dimensionality of modern deep learning models. To address this issue, we propose Freshness Freshness-mAgnItude awaRe top-k (FAIR-k), an algorithm that selects, in each communication round, the most impactful subset of gradients to be updated over the air. In essence, FAIR-k combines the complementary strengths of the Round-Robin and Top-k algorithms, striking a delicate balance between timeliness (freshness of parameter updates) and importance (gradient magnitude). Leveraging tools from Markov analysis, we characterize the distribution of parameter staleness under FAIR-k. Building on this, we establish the convergence rate of OAC-FL with FAIR-k, which discloses the joint effect of data heterogeneity, channel noise, and parameter staleness on the training efficiency. Notably, as opposed to conventional analyses that assume a universal Lipschitz constant across all the clients, our framework adopts a finer-grained model of the data heterogeneity. The analysis demonstrates that since FAIR-k promotes fresh (and fair) parameter updates, it not only accelerates convergence but also enhances communication efficiency by enabling an extended period of local training without significantly affecting overall training efficiency.","authors":["Jiaqi Zhu","Zhongyuan Zhao","Xiao Li","Ruihao Du","Shi Jin","Howard H. Yang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19099v1","updated":"2025-12-22T07:08:20Z","published":"2025-12-22T07:08:20Z","title":"Dual Model Deep Learning for Alzheimer Prognostication","summary":"Disease modifying therapies for Alzheimer's disease demand precise timing decisions, yet current predictive models require longitudinal observations and provide no uncertainty quantification, rendering them impractical at the critical first visit when treatment decisions must be made. We developed PROGRESS (PRognostic Generalization from REsting Static Signatures), a dual-model deep learning framework that transforms a single baseline cerebrospinal fluid biomarker assessment into actionable prognostic estimates without requiring prior clinical history. The framework addresses two complementary clinical questions: a probabilistic trajectory network predicts individualized cognitive decline with calibrated uncertainty bounds achieving near-nominal coverage, enabling honest prognostic communication; and a deep survival model estimates time to conversion from mild cognitive impairment to dementia. Using data from over 3,000 participants across 43 Alzheimer's Disease Research Centers in the National Alzheimer's Coordinating Center database, PROGRESS substantially outperforms Cox proportional hazards, Random Survival Forests, and gradient boosting methods for survival prediction. Risk stratification identifies patient groups with seven-fold differences in conversion rates, enabling clinically meaningful treatment prioritization. Leave-one-center-out validation demonstrates robust generalizability, with survival discrimination remaining strong across held-out sites despite heterogeneous measurement conditions spanning four decades of assay technologies. By combining superior survival prediction with trustworthy trajectory uncertainty quantification, PROGRESS bridges the gap between biomarker measurement and personalized clinical decision-making.","authors":["Alireza Moayedikia","Sara Fin","Uffe Kock Wiil"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19097v1","updated":"2025-12-22T07:07:43Z","published":"2025-12-22T07:07:43Z","title":"DIVER-1 : Deep Integration of Vast Electrophysiological Recordings at Scale","summary":"Electrophysiology signals such as EEG and iEEG are central to neuroscience, brain-computer interfaces, and clinical applications, yet existing foundation models remain limited in scale despite clear evidence that scaling improves performance. We introduce DIVER-1, a family of EEG and iEEG foundation models trained on the largest and most diverse corpus to date-5.3k hours of iEEG and 54k hours of EEG (1.6M channel-hours from over 17.7k subjects)-and scaled up to 1.82B parameters. We present the first systematic scaling law analysis for this domain, showing that they follow data-constrained scaling laws: for a given amount of data and compute, smaller models trained for extended epochs consistently outperform larger models trained briefly. This behavior contrasts with prior electrophysiology foundation models that emphasized model size over training duration. To achieve strong performance, we also design architectural innovations including any-variate attention, sliding temporal conditional positional encoding, and multi-domain reconstruction. DIVER-1 iEEG and EEG models each achieve state-of-the-art performance on their respective benchmarks, establishing a concrete guidelines for efficient scaling and resource allocation in electrophysiology foundation model development.","authors":["Danny Dongyeop Han","Yonghyeon Gwon","Ahhyun Lucy Lee","Taeyang Lee","Seong Jin Lee","Jubin Choi","Sebin Lee","Jihyun Bang","Seungju Lee","David Keetae Park","Shinjae Yoo","Chun Kee Chung","Jiook Cha"],"pdf_url":"","comment":"47 pages, 13 figures, 26 tables"},{"id":"http://arxiv.org/abs/2511.04659v3","updated":"2025-12-22T07:04:01Z","published":"2025-11-06T18:44:35Z","title":"Nowcast3D: Reliable precipitation nowcasting via gray-box learning","summary":"Extreme-precipitation nowcasting requires high spatial and temporal resolution together with extended lead times, yet current approaches remain constrained. Numerical weather prediction systems and their deep-learning emulators operate at relatively coarse space-time resolution and struggle to capture rapidly evolving convective systems. Radar extrapolation methods, which advect recent fields using estimated motion, have difficulty capturing the complex evolution of precipitation. Purely data-driven models often produce overly smoothed reflectivity fields and underestimate intensity. Hybrid 2D radar-based methods discard crucial vertical information, preventing accurate reconstruction of height-dependent dynamics. We introduce Nowcast3D, a gray-box, fully three-dimensional nowcasting framework that operates directly on volumetric radar reflectivity and couples physically constrained neural operators with data-driven learning. The model learns three fields that govern reflectivity evolution: a three-dimensional flow field for advective transport, a spatially varying diffusion field for local dispersive spreading, and a residual source term for unresolved microphysical effects. These learned operators advance the forecast in time under explicit physical constraints, while a conditional diffusion model, conditioned on both the observations and the physics-based forecast, generates ensembles of future radar volumes that quantify forecast uncertainty. In a blind evaluation by 160 meteorologists, Nowcast3D is preferred in 57% of post-hoc and 51% of prior assessments. By explicitly embedding three-dimensional dynamics and uncertainty into a single framework, Nowcast3D offers a scalable and robust approach for reliable nowcasting of extreme precipitation.","authors":["Huaguan Chen","Wei Han","Haofei Sun","Ning Lin","Xingtao Song","Yunfan Yang","Jie Tian","Yang Liu","Ji-Rong Wen","Xiaoye Zhang","Xueshun Shen","Hao Sun"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19091v1","updated":"2025-12-22T07:00:49Z","published":"2025-12-22T07:00:49Z","title":"Auditing Significance, Metric Choice, and Demographic Fairness in Medical AI Challenges","summary":"Open challenges have become the de facto standard for comparative ranking of medical AI methods. Despite their importance, medical AI leaderboards exhibit three persistent limitations: (1) score gaps are rarely tested for statistical significance, so rank stability is unknown; (2) single averaged metrics are applied to every organ, hiding clinically important boundary errors; (3) performance across intersecting demographics is seldom reported, masking fairness and equity gaps. We introduce RankInsight, an open-source toolkit that seeks to address these limitations. RankInsight (1) computes pair-wise significance maps that show the nnU-Net family outperforms Vision-Language and MONAI submissions with high statistical certainty; (2) recomputes leaderboards with organ-appropriate metrics, reversing the order of the top four models when Dice is replaced by NSD for tubular structures; and (3) audits intersectional fairness, revealing that more than half of the MONAI-based entries have the largest gender-race discrepancy on our proprietary Johns Hopkins Hospital dataset. The RankInsight toolkit is publicly released and can be directly applied to past, ongoing, and future challenges. It enables organizers and participants to publish rankings that are statistically sound, clinically meaningful, and demographically fair.","authors":["Ariel Lubonja","Pedro R. A. S. Bassi","Wenxuan Li","Hualin Qiao","Randal Burns","Alan L. Yuille","Zongwei Zhou"],"pdf_url":"","comment":"MICCAI 2025 Workshop on Machine Learning in Medical Imaging"},{"id":"http://arxiv.org/abs/2511.17582v3","updated":"2025-12-22T06:51:54Z","published":"2025-11-15T17:55:47Z","title":"GateRA: Token-Aware Modulation for Parameter-Efficient Fine-Tuning","summary":"Parameter-efficient fine-tuning (PEFT) methods, such as LoRA, DoRA, and HiRA, enable lightweight adaptation of large pre-trained models via low-rank updates. However, existing PEFT approaches apply static, input-agnostic updates to all tokens, disregarding the varying importance and difficulty of different inputs. This uniform treatment can lead to overfitting on trivial content or under-adaptation on more informative regions, especially in autoregressive settings with distinct prefill and decoding dynamics. In this paper, we propose GateRA, a unified framework that introduces token-aware modulation to dynamically adjust the strength of PEFT updates. By incorporating adaptive gating into standard PEFT branches, GateRA enables selective, token-level adaptation, preserving pre-trained knowledge for well-modeled inputs while focusing capacity on challenging cases. Empirical visualizations reveal phase-sensitive behaviors, where GateRA automatically suppresses updates for redundant prefill tokens while emphasizing adaptation during decoding. To promote confident and efficient modulation, we further introduce an entropy-based regularization that encourages near-binary gating decisions. This regularization prevents diffuse update patterns and leads to interpretable, sparse adaptation without hard thresholding. Finally, we present a theoretical analysis showing that GateRA induces a soft gradient-masking effect over the PEFT path, enabling continuous and differentiable control over adaptation. Experiments on multiple commonsense reasoning benchmarks demonstrate that GateRA consistently outperforms or matches prior PEFT methods.","authors":["Jie Ou","Shuaihong Jiang","Yingjun Du","Cees G. M. Snoek"],"pdf_url":"","comment":"Accepted by AAAI 2026"},{"id":"http://arxiv.org/abs/2505.22255v3","updated":"2025-12-22T06:48:29Z","published":"2025-05-28T11:41:11Z","title":"Kronecker Factorization Improves Efficiency and Interpretability of Sparse Autoencoders","summary":"Sparse Autoencoders (SAEs) have demonstrated significant promise in interpreting the hidden states of language models by decomposing them into interpretable latent directions. However, training and interpreting SAEs at scale remains challenging, especially when large dictionary sizes are used. While decoders can leverage sparse-aware kernels for efficiency, encoders still require computationally intensive linear operations with large output dimensions. To address this, we propose KronSAE, a novel architecture that factorizes the latent representation via Kronecker product decomposition, drastically reducing memory and computational overhead. Furthermore, we introduce mAND, a differentiable activation function approximating the binary AND operation, which improves interpretability and performance in our factorized framework.","authors":["Vadim Kurochkin","Yaroslav Aksenov","Daniil Laptev","Daniil Gavrilov","Nikita Balagansky"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2410.15178v4","updated":"2025-12-22T06:16:10Z","published":"2024-10-19T18:46:17Z","title":"GUIDEd Agents: Enhancing Navigation Policies through Task-Specific Uncertainty Abstraction in Localization-Limited Environments","summary":"Autonomous vehicles performing navigation tasks in complex environments face significant challenges due to uncertainty in state estimation. In many scenarios, such as stealth operations or resource-constrained settings, accessing high-precision localization comes at a significant cost, forcing robots to rely primarily on less precise state estimates. Our key observation is that different tasks require varying levels of precision in different regions: a robot navigating a crowded space might need precise localization near obstacles but can operate effectively with less precision elsewhere. In this paper, we present a planning method for integrating task-specific uncertainty requirements directly into navigation policies. We introduce Task-Specific Uncertainty Maps (TSUMs), which abstract the acceptable levels of state estimation uncertainty across different regions. TSUMs align task requirements and environmental features using a shared representation space, generated via a domain-adapted encoder. Using TSUMs, we propose Generalized Uncertainty Integration for Decision-Making and Execution (GUIDE), a policy conditioning framework that incorporates these uncertainty requirements into robot decision-making. We find that TSUMs provide an effective way to abstract task-specific uncertainty requirements, and conditioning policies on TSUMs enables the robot to reason about the context-dependent value of certainty and adapt its behavior accordingly. We show how integrating GUIDE into reinforcement learning frameworks allows the agent to learn navigation policies that effectively balance task completion and uncertainty management without explicit reward engineering. We evaluate GUIDE on various real-world robotic navigation tasks and find that it demonstrates significant improvement in task completion rates compared to baseline methods that do not explicitly consider task-specific uncertainty.","authors":["Gokul Puthumanaillam","Paulo Padrao","Jose Fuentes","Leonardo Bobadilla","Melkior Ornik"],"pdf_url":"","comment":"Accepted for publication at RAL (Robotics and automation letters). Updated with the final version"},{"id":"http://arxiv.org/abs/2512.19067v1","updated":"2025-12-22T06:14:17Z","published":"2025-12-22T06:14:17Z","title":"On Cost-Aware Sequential Hypothesis Testing with Random Costs and Action Cancellation","summary":"We study a variant of cost-aware sequential hypothesis testing in which a single active Decision Maker (DM) selects actions with positive, random costs to identify the true hypothesis under an average error constraint, while minimizing the expected total cost. The DM may abort an in-progress action, yielding no sample, by truncating its realized cost at a smaller, tunable deterministic limit, which we term a per-action deadline. We analyze how this cancellation option can be exploited under two cost-revelation models: ex-post, where the cost is revealed only after the sample is obtained, and ex-ante, where the cost accrues before sample acquisition.\n  In the ex-post model, per-action deadlines do not affect the expected total cost, and the cost-error tradeoffs coincide with the baseline obtained by replacing deterministic costs with cost means. In the ex-ante model, we show how per-action deadlines inflate the expected number of times actions are applied, and that the resulting expected total cost can be reduced to the constant-cost setting by introducing an effective per-action cost. We characterize when deadlines are beneficial and study several families in detail.","authors":["George Vershinin","Asaf Cohen","Omer Gurewitz"],"pdf_url":"","comment":"9 pages, 7 figures"},{"id":"http://arxiv.org/abs/2508.14075v2","updated":"2025-12-22T06:08:18Z","published":"2025-08-12T11:20:27Z","title":"Explainable Graph Spectral Clustering For GloVe-like Text Embeddings","summary":"In a previous paper, we proposed an introduction to the explainability of Graph Spectral Clustering results for textual documents, given that document similarity is computed as cosine similarity in term vector space.\n  In this paper, we generalize this idea by considering other embeddings of documents, in particular, based on the GloVe embedding idea.","authors":["Mieczysław A. Kłopotek","Sławomir T. Wierzchoń","Bartłomiej Starosta","Piotr Borkowski","Dariusz Czerski","Eryk Laskowski"],"pdf_url":"","comment":"47 pages, 19 tables, 11 figures"},{"id":"http://arxiv.org/abs/2504.03560v2","updated":"2025-12-22T06:01:20Z","published":"2025-04-04T16:10:18Z","title":"Stochastic Optimization with Optimal Importance Sampling","summary":"Importance Sampling (IS) is a widely used variance reduction technique for enhancing the efficiency of Monte Carlo methods, particularly in rare-event simulation and related applications. Despite its effectiveness, the performance of IS is highly sensitive to the choice of the proposal distribution and often requires stochastic calibration. While the design and analysis of IS have been extensively studied in estimation settings, applying IS within stochastic optimization introduces a fundamental challenge: the decision variable and the importance sampling distribution are mutually dependent, creating a circular optimization structure. This interdependence complicates both convergence analysis and variance control. We consider convex stochastic optimization problems with linear constraints and propose a single-loop stochastic approximation algorithm, based on a joint variant of Nesterov's dual averaging, that jointly updates the decision variable and the importance sampling distribution, without time-scale separation or nested optimization. The method is globally convergent and achieves minimal asymptotic variance among stochastic gradient schemes, matching the performance of an oracle sampler adapted to the optimal solution.","authors":["Liviu Aolaritei","Bart P. G. Van Parys","Henry Lam","Michael I. Jordan"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19061v1","updated":"2025-12-22T05:59:13Z","published":"2025-12-22T05:59:13Z","title":"Fraud Detection Through Large-Scale Graph Clustering with Heterogeneous Link Transformation","summary":"Collaborative fraud, where multiple fraudulent accounts coordinate to exploit online payment systems, poses significant challenges due to the formation of complex network structures. Traditional detection methods that rely solely on high-confidence identity links suffer from limited coverage, while approaches using all available linkages often result in fragmented graphs with reduced clustering effectiveness. In this paper, we propose a novel graph-based fraud detection framework that addresses the challenge of large-scale heterogeneous graph clustering through a principled link transformation approach. Our method distinguishes between \\emph{hard links} (high-confidence identity relationships such as phone numbers, credit cards, and national IDs) and \\emph{soft links} (behavioral associations including device fingerprints, cookies, and IP addresses). We introduce a graph transformation technique that first identifies connected components via hard links, merges them into super-nodes, and then reconstructs a weighted soft-link graph amenable to efficient embedding and clustering. The transformed graph is processed using LINE (Large-scale Information Network Embedding) for representation learning, followed by HDBSCAN (Hierarchical Density-Based Spatial Clustering of Applications with Noise) for density-based cluster discovery. Experiments on a real-world payment platform dataset demonstrate that our approach achieves significant graph size reduction (from 25 million to 7.7 million nodes), doubles the detection coverage compared to hard-link-only baselines, and maintains high precision across identified fraud clusters. Our framework provides a scalable and practical solution for industrial-scale fraud detection systems.","authors":["Chi Liu"],"pdf_url":"","comment":"13 pages, 6 figures"},{"id":"http://arxiv.org/abs/2512.16963v2","updated":"2025-12-22T05:58:51Z","published":"2025-12-18T09:02:03Z","title":"Compression is Routing: Reconstruction Error as an Intrinsic Signal for Modular Language Models","summary":"Current Large Language Models (LLMs) face three major challenges: context length limitations, high inference costs, and catastrophic forgetting during continual learning. While Mixture-of-Experts (MoE) architectures mitigate some of these conflicts, their routing mechanisms typically rely on explicitly trained auxiliary classifiers. This not only increases system complexity but also often lacks interpretability when handling mixed-domain inputs.\n  Building upon the premise that ``Compression is Intelligence,'' this paper proposes a novel architectural philosophy: Compression is Routing. We trained an 87M-parameter end-to-end Transformer Autoencoder, achieving a 64x sequence length compression (compressing 512 tokens into 8 latent vectors). Experimental results demonstrate that this compressor possesses extreme domain discriminative capability: it achieves a reconstruction accuracy of 99.47% on the in-domain (code) validation set; accuracy drops sharply to 47.76% on a semi-out-of-distribution domain (Wiki text); and further plummets to just 0.57% on a fully out-of-distribution domain (random sequences).\n  This extreme and systematic performance discrepancy establishes the validity of reconstruction error as an Intrinsic Distribution Fingerprint. Based on this, we propose that expert modules can be automatically scheduled using reconstruction residuals directly, without the need for explicit gating networks. This mechanism offers excellent scalability. Furthermore, this architecture provides a new perspective on ``VRAM compression'' for handling ultra-long contexts. This report aims to verify the physical validity of this foundational architecture, offering a new research perspective for the next generation of scalable modular neural networks.","authors":["Zhongpan Tang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19057v1","updated":"2025-12-22T05:47:25Z","published":"2025-12-22T05:47:25Z","title":"Efficient Personalization of Generative Models via Optimal Experimental Design","summary":"Preference learning from human feedback has the ability to align generative models with the needs of end-users. Human feedback is costly and time-consuming to obtain, which creates demand for data-efficient query selection methods. This work presents a novel approach that leverages optimal experimental design to ask humans the most informative preference queries, from which we can elucidate the latent reward function modeling user preferences efficiently. We formulate the problem of preference query selection as the one that maximizes the information about the underlying latent preference model. We show that this problem has a convex optimization formulation, and introduce a statistically and computationally efficient algorithm ED-PBRL that is supported by theoretical guarantees and can efficiently construct structured queries such as images or text. We empirically present the proposed framework by personalizing a text-to-image generative model to user-specific styles, showing that it requires less preference queries compared to random query selection.","authors":["Guy Schacht","Ziyad Sheebaelhamd","Riccardo De Santi","Mojmír Mutný","Andreas Krause"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2506.22714v2","updated":"2025-12-22T05:47:00Z","published":"2025-06-28T01:50:13Z","title":"Libra: Unleashing GPU Heterogeneity for High-Performance Sparse Matrix Multiplication","summary":"Sparse matrix multiplication operators (i.e., SpMM and SDDMM) are widely used in deep learning and scientific computing. Modern accelerators are commonly equipped with Tensor Core Units (TCUs) and CUDA cores to accelerate sparse operators. The former excels at structured matrix computations, whereas the latter offers greater programming flexibility. However, how to combine these two resources to maximize sparse-operator performance remains unclear. In this work, we first identify the source of performance gains in hybrid computation and systematically analyze their complementary strengths. Motivated by this, we propose Libra, a holistic framework that efficiently leverages heterogeneous computing resources to accelerate both SpMM and SDDMM operators. Specifically, Libra introduces a 2D-aware (locality and utilization) workload distribution method to precisely identify the optimal task mapping, simultaneously leveraging the data reuse capabilities of TCUs and the flexibility of CUDA cores to minimize computational redundancy. Libra further incorporates hybrid load balancing, occupancy-aware task scheduling, and efficient kernel implementations to maximize execution efficiency. Extensive experiments on H100 and RTX 4090 GPUs demonstrate that Libra surpasses all the 12 up-to-date baselines significantly, e.g., on average 1.77x speedup over FlashSparse, 1.73x over RoDe, and 2.9x over DGL for end-to-end GNN applications. Libra opens up a new perspective for sparse operator acceleration by fully unleashing the power of heterogeneous GPU resources.","authors":["Jinliang Shi","Shigang Li","Youxuan Xu","Xueying Wang","Rongtian Fu","Zhi Ma","Tong Wu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19038v1","updated":"2025-12-22T05:19:05Z","published":"2025-12-22T05:19:05Z","title":"Time-series Forecast for Indoor Zone Air Temperature with Long Horizons: A Case Study with Sensor-based Data from a Smart Building","summary":"With the press of global climate change, extreme weather and sudden weather changes are becoming increasingly common. To maintain a comfortable indoor environment and minimize the contribution of the building to climate change as much as possible, higher requirements are placed on the operation and control of HVAC systems, e.g., more energy-efficient and flexible to response to the rapid change of weather. This places demands on the rapid modeling and prediction of zone air temperatures of buildings. Compared to the traditional simulation-based approach such as EnergyPlus and DOE2, a hybrid approach combined physics and data-driven is more suitable. Recently, the availability of high-quality datasets and algorithmic breakthroughs have driven a considerable amount of work in this field. However, in the niche of short- and long-term predictions, there are still some gaps in existing research. This paper aims to develop a time series forecast model to predict the zone air temperature in a building located in America on a 2-week horizon. The findings could be further improved to support intelligent control and operation of HVAC systems (i.e. demand flexibility) and could also be used as hybrid building energy modeling.","authors":["Liping Sun","Yucheng Guo","Siliang Lu","Zhenzhen Li"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19037v1","updated":"2025-12-22T05:14:26Z","published":"2025-12-22T05:14:26Z","title":"Elevating Intrusion Detection and Security Fortification in Intelligent Networks through Cutting-Edge Machine Learning Paradigms","summary":"The proliferation of IoT devices and their reliance on Wi-Fi networks have introduced significant security vulnerabilities, particularly the KRACK and Kr00k attacks, which exploit weaknesses in WPA2 encryption to intercept and manipulate sensitive data. Traditional IDS using classifiers face challenges such as model overfitting, incomplete feature extraction, and high false positive rates, limiting their effectiveness in real-world deployments. To address these challenges, this study proposes a robust multiclass machine learning based intrusion detection framework. The methodology integrates advanced feature selection techniques to identify critical attributes, mitigating redundancy and enhancing detection accuracy. Two distinct ML architectures are implemented: a baseline classifier pipeline and a stacked ensemble model combining noise injection, Principal Component Analysis (PCA), and meta learning to improve generalization and reduce false positives. Evaluated on the AWID3 data set, the proposed ensemble architecture achieves superior performance, with an accuracy of 98%, precision of 98%, recall of 98%, and a false positive rate of just 2%, outperforming existing state-of-the-art methods. This work demonstrates the efficacy of combining preprocessing strategies with ensemble learning to fortify network security against sophisticated Wi-Fi attacks, offering a scalable and reliable solution for IoT environments. Future directions include real-time deployment and adversarial resilience testing to further enhance the model's adaptability.","authors":["Md Minhazul Islam Munna","Md Mahbubur Rahman","Jaroslav Frnda","Muhammad Shahid Anwar","Alpamis Kutlimuratov"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19031v1","updated":"2025-12-22T05:04:09Z","published":"2025-12-22T05:04:09Z","title":"A Surrogate-Augmented Symbolic CFD-Driven Training Framework for Accelerating Multi-objective Physical Model Development","summary":"Computational Fluid Dynamics (CFD)-driven training combines machine learning (ML) with CFD solvers to develop physically consistent closure models with improved predictive accuracy. In the original framework, each ML-generated candidate model is embedded in a CFD solver and evaluated against reference data, requiring hundreds to thousands of high-fidelity simulations and resulting in prohibitive computational cost for complex flows. To overcome this limitation, we propose an extended framework that integrates surrogate modeling into symbolic CFD-driven training in real time to reduce training cost. The surrogate model learns to approximate the errors of ML-generated models based on previous CFD evaluations and is continuously refined during training. Newly generated models are first assessed using the surrogate, and only those predicted to yield small errors or high uncertainty are subsequently evaluated with full CFD simulations. Discrete expressions generated by symbolic regression are mapped into a continuous space using averaged input-symbol values as inputs to a probabilistic surrogate model. To support multi-objective model training, particularly when fixed weighting of competing quantities is challenging, the surrogate is extended to a multi-output formulation by generalizing the kernel to a matrix form, providing one mean and variance prediction per training objective. Selection metrics based on these probabilistic outputs are used to identify an optimal training setup. The proposed surrogate-augmented CFD-driven training framework is demonstrated across a range of statistically one- and two-dimensional flows, including both single- and multi-expression model optimization. In all cases, the framework substantially reduces training cost while maintaining predictive accuracy comparable to that of the original CFD-driven approach.","authors":["Yuan Fang","Fabian Waschkowski","Maximilian Reissmann","Richard D. Sandberg","Takuo Oda","Koichi Tanimoto"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2311.02757v3","updated":"2025-12-22T04:53:58Z","published":"2023-11-05T20:29:40Z","title":"Certified Defense on the Fairness of Graph Neural Networks","summary":"Graph Neural Networks (GNNs) have emerged as a prominent graph learning model in various graph-based tasks over the years. Nevertheless, due to the vulnerabilities of GNNs, it has been empirically shown that malicious attackers could easily corrupt the fairness level of their predictions by adding perturbations to the input graph data. In this paper, we take crucial steps to study a novel problem of certifiable defense on the fairness level of GNNs. Specifically, we propose a principled framework named ELEGANT and present a detailed theoretical certification analysis for the fairness of GNNs. ELEGANT takes {\\em any} GNN as its backbone, and the fairness level of such a backbone is theoretically impossible to be corrupted under certain perturbation budgets for attackers. Notably, ELEGANT does not make any assumptions over the GNN structure or parameters, and does not require re-training the GNNs to realize certification. Hence it can serve as a plug-and-play framework for any optimized GNNs ready to be deployed. We verify the satisfactory effectiveness of ELEGANT in practice through extensive experiments on real-world datasets across different backbones of GNNs and parameter settings.","authors":["Yushun Dong","Binchi Zhang","Hanghang Tong","Jundong Li"],"pdf_url":"","comment":"Accepted at SIGKDD'26 for publication"},{"id":"http://arxiv.org/abs/2512.19027v1","updated":"2025-12-22T04:53:40Z","published":"2025-12-22T04:53:40Z","title":"Recontextualization Mitigates Specification Gaming without Modifying the Specification","summary":"Developers often struggle to specify correct training labels and rewards. Perhaps they don't need to. We propose recontextualization, which reduces how often language models \"game\" training signals, performing misbehaviors those signals mistakenly reinforce. We show recontextualization prevents models from learning to 1) prioritize evaluation metrics over chat response quality; 2) special-case code to pass incorrect tests; 3) lie to users; and 4) become sycophantic. Our method works by generating completions from prompts discouraging misbehavior and then recontextualizing them as though they were in response to prompts permitting misbehavior. Recontextualization trains language models to resist misbehavior even when instructions permit it. This mitigates the reinforcement of misbehavior from misspecified training signals, reducing specification gaming without improving the supervision signal.","authors":["Ariana Azarbal","Victor Gillioz","Vladimir Ivanov","Bryce Woodworth","Jacob Drori","Nevan Wichers","Aram Ebtekar","Alex Cloud","Alexander Matt Turner"],"pdf_url":"","comment":"57 pages, 41 figures"},{"id":"http://arxiv.org/abs/2512.19025v1","updated":"2025-12-22T04:42:41Z","published":"2025-12-22T04:42:41Z","title":"The Erasure Illusion: Stress-Testing the Generalization of LLM Forgetting Evaluation","summary":"Machine unlearning aims to remove specific data influences from trained models, a capability essential for adhering to copyright laws and ensuring AI safety. Current unlearning metrics typically measure success by monitoring the model's performance degradation on the specific unlearning dataset ($D_u$). We argue that for Large Language Models (LLMs), this evaluation paradigm is insufficient and potentially misleading. Many real-world uses of unlearning--motivated by copyright or safety--implicitly target not only verbatim content in $D_u$, but also behaviors influenced by the broader generalizations the model derived from it. We demonstrate that LLMs can pass standard unlearning evaluation and appear to have ``forgotten'' the target knowledge, while simultaneously retaining strong capabilities on content that is semantically adjacent to $D_u$. This phenomenon indicates that erasing exact sentences does not necessarily equate to removing the underlying knowledge. To address this gap, we propose \\name, an automated stress-testing framework that generates a surrogate dataset, $\\tilde{D}_u$. This surrogate set is constructed to be semantically derived from $D_u$ yet sufficiently distinct in embedding space. By comparing unlearning metric scores between $D_u$ and $\\tilde{D}_u$, we can stress-test the reliability of the metric itself. Our extensive evaluation across three LLM families (Llama-3-8B, Qwen2.5-7B, and Zephyr-7B-$β$), three distinct datasets, and seven standard metrics reveals widespread inconsistencies. We find that current metrics frequently overestimate unlearning success, failing to detect retained knowledge exposed by our stress-test datasets.","authors":["Hengrui Jia","Taoran Li","Jonas Guan","Varun Chandrasekaran"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19020v1","updated":"2025-12-22T04:21:39Z","published":"2025-12-22T04:21:39Z","title":"CETCAM: Camera-Controllable Video Generation via Consistent and Extensible Tokenization","summary":"Achieving precise camera control in video generation remains challenging, as existing methods often rely on camera pose annotations that are difficult to scale to large and dynamic datasets and are frequently inconsistent with depth estimation, leading to train-test discrepancies. We introduce CETCAM, a camera-controllable video generation framework that eliminates the need for camera annotations through a consistent and extensible tokenization scheme. CETCAM leverages recent advances in geometry foundation models, such as VGGT, to estimate depth and camera parameters and converts them into unified, geometry-aware tokens. These tokens are seamlessly integrated into a pretrained video diffusion backbone via lightweight context blocks. Trained in two progressive stages, CETCAM first learns robust camera controllability from diverse raw video data and then refines fine-grained visual quality using curated high-fidelity datasets. Extensive experiments across multiple benchmarks demonstrate state-of-the-art geometric consistency, temporal stability, and visual realism. Moreover, CETCAM exhibits strong adaptability to additional control modalities, including inpainting and layout control, highlighting its flexibility beyond camera control. The project page is available at https://sjtuytc.github.io/CETCam_project_page.github.io/.","authors":["Zelin Zhao","Xinyu Gong","Bangya Liu","Ziyang Song","Jun Zhang","Suhui Wu","Yongxin Chen","Hao Zhang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19019v1","updated":"2025-12-22T04:16:13Z","published":"2025-12-22T04:16:13Z","title":"Optimizer Dynamics at the Edge of Stability with Differential Privacy","summary":"Deep learning models can reveal sensitive information about individual training examples, and while differential privacy (DP) provides guarantees restricting such leakage, it also alters optimization dynamics in poorly understood ways. We study the training dynamics of neural networks under DP by comparing Gradient Descent (GD), and Adam to their privacy-preserving variants. Prior work shows that these optimizers exhibit distinct stability dynamics: full-batch methods train at the Edge of Stability (EoS), while mini-batch and adaptive methods exhibit analogous edge-of-stability behavior. At these regimes, the training loss and the sharpness--the maximum eigenvalue of the training loss Hessian--exhibit certain characteristic behavior. In DP training, per-example gradient clipping and Gaussian noise modify the update rule, and it is unclear whether these stability patterns persist. We analyze how clipping and noise change sharpness and loss evolution and show that while DP generally reduces the sharpness and can prevent optimizers from fully reaching the classical stability thresholds, patterns from EoS and analogous adaptive methods stability regimes persist, with the largest learning rates and largest privacy budgets approaching, and sometimes exceeding, these thresholds. These findings highlight the unpredictability introduced by DP in neural network optimization.","authors":["Ayana Hussain","Ricky Fang"],"pdf_url":"","comment":"17 pages, 5 figures"},{"id":"http://arxiv.org/abs/2512.19011v1","updated":"2025-12-22T04:00:35Z","published":"2025-12-22T04:00:35Z","title":"Efficient Jailbreak Mitigation Using Semantic Linear Classification in a Multi-Staged Pipeline","summary":"Prompt injection and jailbreaking attacks pose persistent security challenges to large language model (LLM)-based systems. We present an efficient and systematically evaluated defense architecture that mitigates these threats through a lightweight, multi-stage pipeline. Its core component is a semantic filter based on text normalization, TF-IDF representations, and a Linear SVM classifier. Despite its simplicity, this module achieves 93.4% accuracy and 96.5% specificity on held-out data, substantially reducing attack throughput while incurring negligible computational overhead.\n  Building on this efficient foundation, the full pipeline integrates complementary detection and mitigation mechanisms that operate at successive stages, providing strong robustness with minimal latency. In comparative experiments, our SVM-based configuration improves overall accuracy from 35.1% to 93.4% while reducing average time to completion from approximately 450s to 47s, yielding over 10 times lower latency than ShieldGemma. These results demonstrate that the proposed design simultaneously advances defensive precision and efficiency, addressing a core limitation of current model-based moderators.\n  Evaluation across a curated corpus of over 30,000 labeled prompts, including benign, jailbreak, and application-layer injections, confirms that staged, resource-efficient defenses can robustly secure modern LLM-driven applications.","authors":["Akshaj Prashanth Rao","Advait Singh","Saumya Kumaar Saksena","Dhruv Kumar"],"pdf_url":"","comment":"Under Review"},{"id":"http://arxiv.org/abs/2512.19007v1","updated":"2025-12-22T03:48:31Z","published":"2025-12-22T03:48:31Z","title":"The 6th International Verification of Neural Networks Competition (VNN-COMP 2025): Summary and Results","summary":"This report summarizes the 6th International Verification of Neural Networks Competition (VNN-COMP 2025), held as a part of the 8th International Symposium on AI Verification (SAIV), that was collocated with the 37th International Conference on Computer-Aided Verification (CAV). VNN-COMP is held annually to facilitate the fair and objective comparison of state-of-the-art neural network verification tools, encourage the standardization of tool interfaces, and bring together the neural network verification community. To this end, standardized formats for networks (ONNX) and specification (VNN-LIB) were defined, tools were evaluated on equal-cost hardware (using an automatic evaluation pipeline based on AWS instances), and tool parameters were chosen by the participants before the final test sets were made public. In the 2025 iteration, 8 teams participated on a diverse set of 16 regular and 9 extended benchmarks. This report summarizes the rules, benchmarks, participating tools, results, and lessons learned from this iteration of this competition.","authors":["Konstantin Kaulen","Tobias Ladner","Stanley Bak","Christopher Brix","Hai Duong","Thomas Flinkow","Taylor T. Johnson","Lukas Koller","Edoardo Manino","ThanhVu H Nguyen","Haoze Wu"],"pdf_url":"","comment":"Report on the results of VNN-COMP 2025. arXiv admin note: substantial text overlap with arXiv:2412.19985, arXiv:2312.16760, arXiv:2212.10376"},{"id":"http://arxiv.org/abs/2512.19004v1","updated":"2025-12-22T03:45:04Z","published":"2025-12-22T03:45:04Z","title":"Context-Aware Initialization for Reducing Generative Path Length in Diffusion Language Models","summary":"Diffusion Large Language Models (DLLMs) enable fully parallel token decoding but often remain impractical at inference time due to the many denoising iterations required to refine an information-free, fully masked initialization into coherent text. Most existing acceleration methods focus on traversing this generative trajectory more efficiently via improved solvers or sampling strategies. We advance a complementary perspective: shorten the trajectory itself by starting closer to the target distribution through context-aware initialization.\n  We propose a training-free interface that injects prompt-conditioned priors from a lightweight auxiliary model into the diffusion initialization, and instantiate it with two mechanisms: discrete token injection and representation-level embedding interpolation. Because injected priors can be imperfect and unmask-only decoding can over-commit early, we also introduce a simple confidence-based remasking mechanism as a form of prior skepticism. Preliminary evidence on GSM8K suggests that context-aware initialization can substantially reduce denoising iterations (about 35\\% fewer function evaluations in our setting), while also exposing a key open challenge: naive warm-starting can degrade final accuracy relative to strong diffusion baselines. We use these findings to motivate a research agenda around calibration, revision mechanisms, and representation alignment for reliable warm-started diffusion decoding.","authors":["Tongyuan Miao","Gary Huang","Kai Jun Han","Annie Jiang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19001v1","updated":"2025-12-22T03:39:43Z","published":"2025-12-22T03:39:43Z","title":"ORPR: An OR-Guided Pretrain-then-Reinforce Learning Model for Inventory Management","summary":"As the pursuit of synergy between Artificial Intelligence (AI) and Operations Research (OR) gains momentum in handling complex inventory systems, a critical challenge persists: how to effectively reconcile AI's adaptive perception with OR's structural rigor. To bridge this gap, we propose a novel OR-Guided \"Pretrain-then-Reinforce\" framework. To provide structured guidance, we propose a simulation-augmented OR model that generates high-quality reference decisions, implicitly capturing complex business constraints and managerial preferences. Leveraging these OR-derived decisions as foundational training labels, we design a domain-informed deep learning foundation model to establish foundational decision-making capabilities, followed by a reinforcement learning (RL) fine-tuning stage. Uniquely, we position RL as a deep alignment mechanism that enables the AI agent to internalize the optimality principles of OR, while simultaneously leveraging exploration for general policy refinement and allowing expert guidance for scenario-specific adaptation (e.g., promotional events). Validated through extensive numerical experiments and a field deployment at JD.com augmented by a Difference-in-Differences (DiD) analysis, our model significantly outperforms incumbent industrial practices, delivering real-world gains of a 5.27-day reduction in turnover and a 2.29% increase in in-stock rates, alongside a 29.95% decrease in holding costs. Contrary to the prevailing trend of brute-force model scaling, our study demonstrates that a lightweight, domain-informed model can deliver state-of-the-art performance and robust transferability when guided by structured OR logic. This approach offers a scalable and cost-effective paradigm for intelligent supply chain management, highlighting the value of deeply aligning AI with OR.","authors":["Lingjie Zhao","Xue Yu","Yongzhi Qi","Hao Hu","Jianshen Zhang","Yingzheng Ma","Shuyu Han","Wei Qi","Zuo-Jun Max Shen"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2511.12808v3","updated":"2025-12-22T03:26:43Z","published":"2025-11-16T22:28:30Z","title":"Expressive Temporal Specifications for Reward Monitoring","summary":"Specifying informative and dense reward functions remains a pivotal challenge in Reinforcement Learning, as it directly affects the efficiency of agent training. In this work, we harness the expressive power of quantitative Linear Temporal Logic on finite traces (($\\text{LTL}_f[\\mathcal{F}]$)) to synthesize reward monitors that generate a dense stream of rewards for runtime-observable state trajectories. By providing nuanced feedback during training, these monitors guide agents toward optimal behaviour and help mitigate the well-known issue of sparse rewards under long-horizon decision making, which arises under the Boolean semantics dominating the current literature. Our framework is algorithm-agnostic and only relies on a state labelling function, and naturally accommodates specifying non-Markovian properties. Empirical results show that our quantitative monitors consistently subsume and, depending on the environment, outperform Boolean monitors in maximizing a quantitative measure of task completion and in reducing convergence time.","authors":["Omar Adalat","Francesco Belardinelli"],"pdf_url":"","comment":"Accepted at AAAI-26"},{"id":"http://arxiv.org/abs/2511.23083v5","updated":"2025-12-22T03:12:50Z","published":"2025-11-28T11:14:15Z","title":"Spectral Concentration at the Edge of Stability: Information Geometry of Kernel Associative Memory","summary":"High-capacity kernel Hopfield networks exhibit a \\textit{Ridge of Optimization} characterized by extreme stability. While previously linked to \\textit{Spectral Concentration}, its origin remains elusive. Here, we analyze the network dynamics on a statistical manifold, revealing that the Ridge corresponds to the Edge of Stability, a critical boundary where the Fisher Information Matrix becomes singular. We demonstrate that the apparent Euclidean force antagonism is a manifestation of \\textit{Dual Equilibrium} in the Riemannian space. This unifies learning dynamics and capacity via the Minimum Description Length principle, offering a geometric theory of self-organized criticality.","authors":["Akira Tamamori"],"pdf_url":"","comment":"5 pages, 4 figures"},{"id":"http://arxiv.org/abs/2511.20277v2","updated":"2025-12-22T02:58:33Z","published":"2025-11-25T13:05:40Z","title":"HVAdam: A Full-Dimension Adaptive Optimizer","summary":"Adaptive optimizers such as Adam have achieved great success in training large-scale models like large language models and diffusion models. However, they often generalize worse than non-adaptive methods, such as SGD on classical architectures like CNNs. We identify a key cause of this performance gap: adaptivity in pre-conditioners, which limits the optimizer's ability to adapt to diverse optimization landscapes. To address this, we propose Anon (Adaptivity Non-restricted Optimizer with Novel convergence technique), a novel optimizer with continuously tunable adaptivity\n  , allowing it to interpolate between SGD-like and Adam-like behaviors and even extrapolate beyond both. To ensure convergence across the entire adaptivity spectrum, we introduce incremental delay update (IDU), a novel mechanism that is more flexible than AMSGrad's hard max-tracking strategy and enhances robustness to gradient noise. We theoretically establish convergence guarantees under both convex and non-convex settings. Empirically, Anon consistently outperforms state-of-the-art optimizers on representative image classification, diffusion, and language modeling tasks. These results demonstrate that adaptivity can serve as a valuable tunable design principle, and Anon provides the first unified and reliable framework capable of bridging the gap between classical and modern optimizers and surpassing their advantageous properties.","authors":["Yiheng Zhang","Shaowu Wu","Yuanzhuo Xu","Jiajun Wu","Shang Xu","Steve Drew","Xiaoguang Niu"],"pdf_url":"","comment":"Accepted at AAAI2025"},{"id":"http://arxiv.org/abs/2512.18986v1","updated":"2025-12-22T02:54:10Z","published":"2025-12-22T02:54:10Z","title":"R-GenIMA: Integrating Neuroimaging and Genetics with Interpretable Multimodal AI for Alzheimer's Disease Progression","summary":"Early detection of Alzheimer's disease (AD) requires models capable of integrating macro-scale neuroanatomical alterations with micro-scale genetic susceptibility, yet existing multimodal approaches struggle to align these heterogeneous signals. We introduce R-GenIMA, an interpretable multimodal large language model that couples a novel ROI-wise vision transformer with genetic prompting to jointly model structural MRI and single nucleotide polymorphisms (SNPs) variations. By representing each anatomically parcellated brain region as a visual token and encoding SNP profiles as structured text, the framework enables cross-modal attention that links regional atrophy patterns to underlying genetic factors. Applied to the ADNI cohort, R-GenIMA achieves state-of-the-art performance in four-way classification across normal cognition (NC), subjective memory concerns (SMC), mild cognitive impairment (MCI), and AD. Beyond predictive accuracy, the model yields biologically meaningful explanations by identifying stage-specific brain regions and gene signatures, as well as coherent ROI-Gene association patterns across the disease continuum. Attention-based attribution revealed genes consistently enriched for established GWAS-supported AD risk loci, including APOE, BIN1, CLU, and RBFOX1. Stage-resolved neuroanatomical signatures identified shared vulnerability hubs across disease stages alongside stage-specific patterns: striatal involvement in subjective decline, frontotemporal engagement during prodromal impairment, and consolidated multimodal network disruption in AD. These results demonstrate that interpretable multimodal AI can synthesize imaging and genetics to reveal mechanistic insights, providing a foundation for clinically deployable tools that enable earlier risk stratification and inform precision therapeutic strategies in Alzheimer's disease.","authors":["Kun Zhao","Siyuan Dai","Yingying Zhang","Guodong Liu","Pengfei Gu","Chenghua Lin","Paul M. Thompson","Alex Leow","Heng Huang","Lifang He","Liang Zhan","Haoteng Tang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.18980v1","updated":"2025-12-22T02:45:41Z","published":"2025-12-22T02:45:41Z","title":"OPBO: Order-Preserving Bayesian Optimization","summary":"Bayesian optimization is an effective method for solving expensive black-box optimization problems. Most existing methods use Gaussian processes (GP) as the surrogate model for approximating the black-box objective function, it is well-known that it can fail in high-dimensional space (e.g., dimension over 500). We argue that the reliance of GP on precise numerical fitting is fundamentally ill-suited in high-dimensional space, where it leads to prohibitive computational complexity. In order to address this, we propose a simple order-preserving Bayesian optimization (OPBO) method, where the surrogate model preserves the order, instead of the value, of the black-box objective function. Then we can use a simple but effective OP neural network (NN) to replace GP as the surrogate model. Moreover, instead of searching for the best solution from the acquisition model, we select good-enough solutions in the ordinal set to reduce computational cost. The experimental results show that for high-dimensional (over 500) black-box optimization problems, the proposed OPBO significantly outperforms traditional BO methods based on regression NN and GP. The source code is available at https://github.com/pengwei222/OPBO.","authors":["Wei Peng","Jianchen Hu","Kang Liu","Qiaozhu Zhai"],"pdf_url":"","comment":"13 pages"},{"id":"http://arxiv.org/abs/2512.18978v1","updated":"2025-12-22T02:41:43Z","published":"2025-12-22T02:41:43Z","title":"Outlier detection in mixed-attribute data: a semi-supervised approach with fuzzy approximations and relative entropy","summary":"Outlier detection is a critical task in data mining, aimed at identifying objects that significantly deviate from the norm. Semi-supervised methods improve detection performance by leveraging partially labeled data but typically overlook the uncertainty and heterogeneity of real-world mixed-attribute data. This paper introduces a semi-supervised outlier detection method, namely fuzzy rough sets-based outlier detection (FROD), to effectively handle these challenges. Specifically, we first utilize a small subset of labeled data to construct fuzzy decision systems, through which we introduce the attribute classification accuracy based on fuzzy approximations to evaluate the contribution of attribute sets in outlier detection. Unlabeled data is then used to compute fuzzy relative entropy, which provides a characterization of outliers from the perspective of uncertainty. Finally, we develop the detection algorithm by combining attribute classification accuracy with fuzzy relative entropy. Experimental results on 16 public datasets show that FROD is comparable with or better than leading detection algorithms. All datasets and source codes are accessible at https://github.com/ChenBaiyang/FROD. This manuscript is the accepted author version of a paper published by Elsevier. The final published version is available at https://doi.org/10.1016/j.ijar.2025.109373","authors":["Baiyang Chen","Zhong Yuan","Zheng Liu","Dezhong Peng","Yongxiang Li","Chang Liu","Guiduo Duan"],"pdf_url":"","comment":"Author's accepted manuscript"},{"id":"http://arxiv.org/abs/2512.18977v1","updated":"2025-12-22T02:41:08Z","published":"2025-12-22T02:41:08Z","title":"Consistency-guided semi-supervised outlier detection in heterogeneous data using fuzzy rough sets","summary":"Outlier detection aims to find samples that behave differently from the majority of the data. Semi-supervised detection methods can utilize the supervision of partial labels, thus reducing false positive rates. However, most of the current semi-supervised methods focus on numerical data and neglect the heterogeneity of data information. In this paper, we propose a consistency-guided outlier detection algorithm (COD) for heterogeneous data with the fuzzy rough set theory in a semi-supervised manner. First, a few labeled outliers are leveraged to construct label-informed fuzzy similarity relations. Next, the consistency of the fuzzy decision system is introduced to evaluate attributes' contributions to knowledge classification. Subsequently, we define the outlier factor based on the fuzzy similarity class and predict outliers by integrating the classification consistency and the outlier factor. The proposed algorithm is extensively evaluated on 15 freshly proposed datasets. Experimental results demonstrate that COD is better than or comparable with the leading outlier detectors. This manuscript is the accepted author version of a paper published by Elsevier. The final published version is available at https://doi.org/10.1016/j.asoc.2024.112070","authors":["Baiyang Chen","Zhong Yuan","Dezhong Peng","Xiaoliang Chen","Hongmei Chen"],"pdf_url":"","comment":"Author's Accepted Manuscript"},{"id":"http://arxiv.org/abs/2512.18971v1","updated":"2025-12-22T02:34:34Z","published":"2025-12-22T02:34:34Z","title":"On Conditional Stochastic Interpolation for Generative Nonlinear Sufficient Dimension Reduction","summary":"Identifying low-dimensional sufficient structures in nonlinear sufficient dimension reduction (SDR) has long been a fundamental yet challenging problem. Most existing methods lack theoretical guarantees of exhaustiveness in identifying lower dimensional structures, either at the population level or at the sample level. We tackle this issue by proposing a new method, generative sufficient dimension reduction (GenSDR), which leverages modern generative models. We show that GenSDR is able to fully recover the information contained in the central $σ$-field at both the population and sample levels. In particular, at the sample level, we establish a consistency property for the GenSDR estimator from the perspective of conditional distributions, capitalizing on the distributional learning capabilities of deep generative models. Moreover, by incorporating an ensemble technique, we extend GenSDR to accommodate scenarios with non-Euclidean responses, thereby substantially broadening its applicability. Extensive numerical results demonstrate the outstanding empirical performance of GenSDR and highlight its strong potential for addressing a wide range of complex, real-world tasks.","authors":["Shuntuo Xu","Zhou Yu","Jian Huang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.01457v3","updated":"2025-12-22T02:26:36Z","published":"2025-12-01T09:44:31Z","title":"Zero-Overhead Introspection for Adaptive Test-Time Compute","summary":"Large language models excel at reasoning but lack key aspects of introspection, including anticipating their own success and the computation required to achieve it. Humans use real-time introspection to decide how much effort to invest, when to make multiple attempts, when to stop, and when to signal success or failure. Without this, LLMs struggle to make intelligent meta-cognition decisions. Test-time scaling methods like Best-of-N drive up cost and latency by using a fixed budget of samples regardless of the marginal benefit of each one at any point in generation, and the absence of confidence signals can mislead people, prevent appropriate escalation to better tools, and undermine trustworthiness. Learned verifiers or reward models can provide confidence estimates, but do not enable adaptive inference and add substantial cost by requiring extra models or forward passes. We present ZIP-RC, an adaptive inference method that equips models with zero-overhead inference-time predictions of reward and cost. At every token, ZIP-RC reuses reserved or unused logits in the same forward pass as next-token prediction to output a joint distribution over final reward and remaining length -- no extra models, architecture change, or inference overhead. This full joint distribution is used to compute a sampling utility which is the linear combination of the expected maximum reward, total compute, and latency of set of samples if generated to completion. During inference, we maximize this utility with meta-actions that determine which prefix of tokens to continue or initiate sampling from. On mixed-difficulty mathematical benchmarks, ZIP-RC improves accuracy by up to 12% over majority voting at equal or lower average cost, and traces smooth Pareto frontiers between quality, compute, and latency. By providing real-time reward-cost introspection, ZIP-RC enables adaptive, efficient reasoning.","authors":["Rohin Manvi","Joey Hong","Tim Seyde","Maxime Labonne","Mathias Lechner","Sergey Levine"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2511.01385v2","updated":"2025-12-22T02:25:29Z","published":"2025-11-03T09:36:11Z","title":"Memory-Efficient Training with In-Place FFT Implementation","summary":"Fast Fourier Transforms (FFT) are widely used to reduce memory and computational costs in deep learning. However, existing implementations, including standard FFT and real FFT (rFFT), cannot achieve true in-place computation. In particular, rFFT maps an input of size n to a complex output of size n/2+1, causing dimensional mismatch and requiring additional memory allocation. We propose the first real-domain, fully in-place FFT framework (rdFFT) that preserves input-output memory space consistency. By leveraging butterfly operation symmetry and conjugate properties in the frequency domain, we design an implicit complex encoding scheme that eliminates intermediate cache usage entirely. Experiments on multiple natural language understanding tasks demonstrate the method effectiveness in reducing training memory cost, offering a promising direction for frequency-domain lightweight adaptation.","authors":["Xinyu Ding","Bangtian Liu","Siyu Liao","Zhongfeng Wang"],"pdf_url":"","comment":"Accepted at NeurIPS 2025. Version 2 adds links to the ongoing PyTorch upstreaming discussion"},{"id":"http://arxiv.org/abs/2512.18965v1","updated":"2025-12-22T02:25:26Z","published":"2025-12-22T02:25:26Z","title":"Lag Operator SSMs: A Geometric Framework for Structured State Space Modeling","summary":"Structured State Space Models (SSMs), which are at the heart of the recently popular Mamba architecture, are powerful tools for sequence modeling. However, their theoretical foundation relies on a complex, multi-stage process of continuous-time modeling and subsequent discretization, which can obscure intuition. We introduce a direct, first-principles framework for constructing discrete-time SSMs that is both flexible and modular. Our approach is based on a novel lag operator, which geometrically derives the discrete-time recurrence by measuring how the system's basis functions \"slide\" and change from one timestep to the next. The resulting state matrices are computed via a single inner product involving this operator, offering a modular design space for creating novel SSMs by flexibly combining different basis functions and time-warping schemes. To validate our approach, we demonstrate that a specific instance exactly recovers the recurrence of the influential HiPPO model. Numerical simulations confirm our derivation, providing new theoretical tools for designing flexible and robust sequence models.","authors":["Sutashu Tomonaga","Kenji Doya","Noboru Murata"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17100v2","updated":"2025-12-22T02:23:31Z","published":"2025-12-18T21:56:08Z","title":"UniCoMTE: A Universal Counterfactual Framework for Explaining Time-Series Classifiers on ECG Data","summary":"Machine learning models, particularly deep neural networks, have demonstrated strong performance in classifying complex time series data. However, their black-box nature limits trust and adoption, especially in high-stakes domains such as healthcare. To address this challenge, we introduce UniCoMTE, a model-agnostic framework for generating counterfactual explanations for multivariate time series classifiers. The framework identifies temporal features that most heavily influence a model's prediction by modifying the input sample and assessing its impact on the model's prediction. UniCoMTE is compatible with a wide range of model architectures and operates directly on raw time series inputs. In this study, we evaluate UniCoMTE's explanations on a time series ECG classifier. We quantify explanation quality by comparing our explanations' comprehensibility to comprehensibility of established techniques (LIME and SHAP) and assessing their generalizability to similar samples. Furthermore, clinical utility is assessed through a questionnaire completed by medical experts who review counterfactual explanations presented alongside original ECG samples. Results show that our approach produces concise, stable, and human-aligned explanations that outperform existing methods in both clarity and applicability. By linking model predictions to meaningful signal patterns, the framework advances the interpretability of deep learning models for real-world time series applications.","authors":["Justin Li","Efe Sencan","Jasper Zheng Duan","Vitus J. Leung","Stephen Tsaur","Ayse K. Coskun"],"pdf_url":"","comment":"21 pages, 7 figures"},{"id":"http://arxiv.org/abs/2512.19936v1","updated":"2025-12-22T23:50:17Z","published":"2025-12-22T23:50:17Z","title":"GIMLET: Generalizable and Interpretable Model Learning through Embedded Thermodynamics","summary":"We develop a data-driven framework for discovering constitutive relations in models of fluid flow and scalar transport. Our approach infers unknown closure terms in the governing equations (gray-box discovery) under the assumption that the temporal derivative, convective transport, and pressure-gradient contributions are known. The formulation is rooted in a variational principle from nonequilibrium thermodynamics, where the dynamics is defined by a free-energy functional and a dissipation functional. The unknown constitutive terms arise as functional derivatives of these functionals with respect to the state variables. To enable a flexible and structured model discovery, the free-energy and dissipation functionals are parameterized using neural networks, while their functional derivatives are obtained via automatic differentiation. This construction enforces thermodynamic consistency by design, ensuring monotonic decay of the total free energy and non-negative entropy production. The resulting method, termed GIMLET (Generalizable and Interpretable Model Learning through Embedded Thermodynamics), avoids reliance on a predefined library of candidate functions, unlike sparse regression or symbolic identification approaches. The learned models are generalizable in that functionals identified from one dataset can be transferred to distinct datasets governed by the same underlying equations. Moreover, the inferred free-energy and dissipation functions provide direct physical interpretability of the learned dynamics. The framework is demonstrated on several benchmark systems, including the viscous Burgers equation, the Kuramoto--Sivashinsky equation, and the incompressible Navier--Stokes equations for both Newtonian and non-Newtonian fluids.","authors":["Suguru Shiratori","Elham Kiyani","Khemraj Shukla","George Em Karniadakis"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19935v1","updated":"2025-12-22T23:44:39Z","published":"2025-12-22T23:44:39Z","title":"Conditional Adversarial Fragility in Financial Machine Learning under Macroeconomic Stress","summary":"Machine learning models used in financial decision systems operate in nonstationary economic environments, yet adversarial robustness is typically evaluated under static assumptions. This work introduces Conditional Adversarial Fragility, a regime dependent phenomenon in which adversarial vulnerability is systematically amplified during periods of macroeconomic stress. We propose a regime aware evaluation framework for time indexed tabular financial classification tasks that conditions robustness assessment on external indicators of economic stress. Using volatility based regime segmentation as a proxy for macroeconomic conditions, we evaluate model behavior across calm and stress periods while holding model architecture, attack methodology, and evaluation protocols constant. Baseline predictive performance remains comparable across regimes, indicating that economic stress alone does not induce inherent performance degradation. Under adversarial perturbations, however, models operating during stress regimes exhibit substantially greater degradation across predictive accuracy, operational decision thresholds, and risk sensitive outcomes. We further demonstrate that this amplification propagates to increased false negative rates, elevating the risk of missed high risk cases during adverse conditions. To complement numerical robustness metrics, we introduce an interpretive governance layer based on semantic auditing of model explanations using large language models. Together, these results demonstrate that adversarial robustness in financial machine learning is a regime dependent property and motivate stress aware approaches to model risk assessment in high stakes financial deployments.","authors":["Samruddhi Baviskar"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19934v1","updated":"2025-12-22T23:42:45Z","published":"2025-12-22T23:42:45Z","title":"Vehicle-centric Perception via Multimodal Structured Pre-training","summary":"Vehicle-centric perception plays a crucial role in many intelligent systems, including large-scale surveillance systems, intelligent transportation, and autonomous driving. Existing approaches lack effective learning of vehicle-related knowledge during pre-training, resulting in poor capability for modeling general vehicle perception representations. To handle this problem, we propose VehicleMAE-V2, a novel vehicle-centric pre-trained large model. By exploring and exploiting vehicle-related multimodal structured priors to guide the masked token reconstruction process, our approach can significantly enhance the model's capability to learn generalizable representations for vehicle-centric perception. Specifically, we design the Symmetry-guided Mask Module (SMM), Contour-guided Representation Module (CRM) and Semantics-guided Representation Module (SRM) to incorporate three kinds of structured priors into token reconstruction including symmetry, contour and semantics of vehicles respectively. SMM utilizes the vehicle symmetry constraints to avoid retaining symmetric patches and can thus select high-quality masked image patches and reduce information redundancy. CRM minimizes the probability distribution divergence between contour features and reconstructed features and can thus preserve holistic vehicle structure information during pixel-level reconstruction. SRM aligns image-text features through contrastive learning and cross-modal distillation to address the feature confusion caused by insufficient semantic understanding during masked reconstruction. To support the pre-training of VehicleMAE-V2, we construct Autobot4M, a large-scale dataset comprising approximately 4 million vehicle images and 12,693 text descriptions. Extensive experiments on five downstream tasks demonstrate the superior performance of VehicleMAE-V2.","authors":["Wentao Wu","Xiao Wang","Chenglong Li","Jin Tang","Bin Luo"],"pdf_url":"","comment":"Journal extension of VehicleMAE (AAAI 2024)"},{"id":"http://arxiv.org/abs/2512.15503v2","updated":"2025-12-22T23:04:16Z","published":"2025-12-17T14:45:33Z","title":"Attention in Motion: Secure Platooning via Transformer-based Misbehavior Detection","summary":"Vehicular platooning promises transformative improvements in transportation efficiency and safety through the coordination of multi-vehicle formations enabled by Vehicle-to-Everything (V2X) communication. However, the distributed nature of platoon coordination creates security vulnerabilities, allowing authenticated vehicles to inject falsified kinematic data, compromise operational stability, and pose a threat to passenger safety. Traditional misbehaviour detection approaches, which rely on plausibility checks and statistical methods, suffer from high False Positive (FP) rates and cannot capture the complex temporal dependencies inherent in multi-vehicle coordination dynamics. We present Attention In Motion (AIMformer), a transformer-based framework specifically tailored for real-time misbehaviour detection in vehicular platoons with edge deployment capabilities. AIMformer leverages multi-head self-attention mechanisms to simultaneously capture intra-vehicle temporal dynamics and inter-vehicle spatial correlations. It incorporates global positional encoding with vehicle-specific temporal offsets to handle join/exit maneuvers. We propose a Precision-Focused Binary Cross-Entropy (PFBCE) loss function that penalizes FPs to meet the requirements of safety-critical vehicular systems. Extensive evaluation across 4 platoon controllers, multiple attack vectors, and diverse mobility scenarios demonstrates superior performance ($\\geq$ 0.93) compared to state-of-the-art baseline architectures. A comprehensive deployment analysis utilizing TensorFlow Lite (TFLite), Open Neural Network Exchange (ONNX), and TensorRT achieves sub-millisecond inference latency, making it suitable for real-time operation on resource-constrained edge platforms. Hence, validating AIMformer is viable for both in-vehicle and roadside infrastructure deployment.","authors":["Konstantinos Kalogiannis","Ahmed Mohamed Hussain","Hexu Li","Panos Papadimitratos"],"pdf_url":"","comment":"16 pages and 10 figures"},{"id":"http://arxiv.org/abs/2512.19927v1","updated":"2025-12-22T23:04:03Z","published":"2025-12-22T23:04:03Z","title":"The Seismic Wavefield Common Task Framework","summary":"Seismology faces fundamental challenges in state forecasting and reconstruction (e.g., earthquake early warning and ground motion prediction) and managing the parametric variability of source locations, mechanisms, and Earth models (e.g., subsurface structure and topography effects). Addressing these with simulations is hindered by their massive scale, both in synthetic data volumes and numerical complexity, while real-data efforts are constrained by models that inadequately reflect the Earth's complexity and by sparse sensor measurements from the field. Recent machine learning (ML) efforts offer promise, but progress is obscured by a lack of proper characterization, fair reporting, and rigorous comparisons. To address this, we introduce a Common Task Framework (CTF) for ML for seismic wavefields, starting with three distinct wavefield datasets. Our CTF features a curated set of datasets at various scales (global, crustal, and local) and task-specific metrics spanning forecasting, reconstruction, and generalization under realistic constraints such as noise and limited data. Inspired by CTFs in fields like natural language processing, this framework provides a structured and rigorous foundation for head-to-head algorithm evaluation. We illustrate the evaluation procedure with scores reported for two of the datasets, showcasing the performance of various methods and foundation models for reconstructing seismic wavefields from both simulated and real-world sensor measurements. The CTF scores reveal the strengths, limitations, and suitability for specific problem classes. Our vision is to replace ad hoc comparisons with standardized evaluations on hidden test sets, raising the bar for rigor and reproducibility in scientific ML.","authors":["Alexey Yermakov","Yue Zhao","Marine Denolle","Yiyu Ni","Philippe M. Wyder","Judah Goldfeder","Stefano Riva","Jan Williams","David Zoro","Amy Sara Rude","Matteo Tomasetto","Joe Germany","Joseph Bakarji","Georg Maierhofer","Miles Cranmer","J. Nathan Kutz"],"pdf_url":"","comment":"35 pages, 7 figures"},{"id":"http://arxiv.org/abs/2512.19920v1","updated":"2025-12-22T22:51:48Z","published":"2025-12-22T22:51:48Z","title":"Mitigating LLM Hallucination via Behaviorally Calibrated Reinforcement Learning","summary":"LLM deployment in critical domains is currently impeded by persistent hallucinations--generating plausible but factually incorrect assertions. While scaling laws drove significant improvements in general capabilities, theoretical frameworks suggest hallucination is not merely stochastic error but a predictable statistical consequence of training objectives prioritizing mimicking data distribution over epistemic honesty. Standard RLVR paradigms, utilizing binary reward signals, inadvertently incentivize models as good test-takers rather than honest communicators, encouraging guessing whenever correctness probability exceeds zero. This paper presents an exhaustive investigation into behavioral calibration, which incentivizes models to stochastically admit uncertainty by abstaining when not confident, aligning model behavior with accuracy. Synthesizing recent advances, we propose and evaluate training interventions optimizing strictly proper scoring rules for models to output a calibrated probability of correctness. Our methods enable models to either abstain from producing a complete response or flag individual claims where uncertainty remains. Utilizing Qwen3-4B-Instruct, empirical analysis reveals behavior-calibrated reinforcement learning allows smaller models to surpass frontier models in uncertainty quantification--a transferable meta-skill decouplable from raw predictive accuracy. Trained on math reasoning tasks, our model's log-scale Accuracy-to-Hallucination Ratio gain (0.806) exceeds GPT-5's (0.207) in a challenging in-domain evaluation (BeyondAIME). Moreover, in cross-domain factual QA (SimpleQA), our 4B LLM achieves zero-shot calibration error on par with frontier models including Grok-4 and Gemini-2.5-Pro, even though its factual accuracy is much lower.","authors":["Jiayun Wu","Jiashuo Liu","Zhiyuan Zeng","Tianyang Zhan","Wenhao Huang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2507.17085v2","updated":"2025-12-22T22:48:24Z","published":"2025-07-22T23:58:30Z","title":"Deformable Cluster Manipulation via Whole-Arm Policy Learning","summary":"Manipulating clusters of deformable objects presents a substantial challenge with widespread applicability, but requires contact-rich whole-arm interactions. A potential solution must address the limited capacity for realistic model synthesis, high uncertainty in perception, and the lack of efficient spatial abstractions, among others. We propose a novel framework for learning model-free policies integrating two modalities: 3D point clouds and proprioceptive touch indicators, emphasising manipulation with full body contact awareness, going beyond traditional end-effector modes. Our reinforcement learning framework leverages a distributional state representation, aided by kernel mean embeddings, to achieve improved training efficiency and real-time inference. Furthermore, we propose a novel context-agnostic occlusion heuristic to clear deformables from a target region for exposure tasks. We deploy the framework in a power line clearance scenario and observe that the agent generates creative strategies leveraging multiple arm links for de-occlusion. Finally, we perform zero-shot sim-to-real policy transfer, allowing the arm to clear real branches with unknown occlusion patterns, unseen topology, and uncertain dynamics. Website: https://sites.google.com/view/dcmwap/","authors":["Jayadeep Jacob","Wenzheng Zhang","Houston Warren","Paulo Borges","Tirthankar Bandyopadhyay","Fabio Ramos"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19913v1","updated":"2025-12-22T22:37:19Z","published":"2025-12-22T22:37:19Z","title":"Quasiprobabilistic Density Ratio Estimation with a Reverse Engineered Classification Loss Function","summary":"We consider a generalization of the classifier-based density-ratio estimation task to a quasiprobabilistic setting where probability densities can be negative. The problem with most loss functions used for this task is that they implicitly define a relationship between the optimal classifier and the target quasiprobabilistic density ratio which is discontinuous or not surjective. We address these problems by introducing a convex loss function that is well-suited for both probabilistic and quasiprobabilistic density ratio estimation. To quantify performance, an extended version of the Sliced-Wasserstein distance is introduced which is compatible with quasiprobability distributions. We demonstrate our approach on a real-world example from particle physics, of di-Higgs production in association with jets via gluon-gluon fusion, and achieve state-of-the-art results.","authors":["Matthew Drnevich","Stephen Jiggins","Kyle Cranmer"],"pdf_url":"","comment":"25 pages, 7 figures"},{"id":"http://arxiv.org/abs/2512.19909v1","updated":"2025-12-22T22:29:57Z","published":"2025-12-22T22:29:57Z","title":"Modeling Non-Ergodic Path Effects Using Conditional Generative Model for Fourier Amplitude Spectra","summary":"Recent developments in non-ergodic ground-motion models (GMMs) explicitly model systematic spatial variations in source, site, and path effects, reducing standard deviation to 30-40% of ergodic models and enabling more accurate site-specific seismic hazard analysis. Current non-ergodic GMMs rely on Gaussian Process (GP) methods with prescribed correlation functions and thus have computational limitations for large-scale predictions. This study proposes a deep-learning approach called Conditional Generative Modeling for Fourier Amplitude Spectra (CGM-FAS) as an alternative to GP-based methods for modeling non-ergodic path effects in Fourier Amplitude Spectra (FAS). CGM-FAS uses a Conditional Variational Autoencoder architecture to learn spatial patterns and interfrequency correlation directly from data by using geographical coordinates of earthquakes and stations as conditional variables. Using San Francisco Bay Area earthquake data, we compare CGM-FAS against a recent GP-based GMM for the region and demonstrate consistent predictions of non-ergodic path effects. Additionally, CGM-FAS offers advantages compared to GP-based approaches in learning spatial patterns without prescribed correlation functions, capturing interfrequency correlations, and enabling rapid predictions, generating maps for 10,000 sites across 1,000 frequencies within 10 seconds using a few GB of memory. CGM-FAS hyperparameters can be tuned to ensure generated path effects exhibit variability consistent with the GP-based empirical GMM. This work demonstrates a promising direction for efficient non-ergodic ground-motion prediction across multiple frequencies and large spatial domains.","authors":["Maxime Lacour","Pu Ren","Rie Nakata","Nori Nakata","Michael Mahoney"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2503.19779v3","updated":"2025-12-22T22:25:55Z","published":"2025-03-25T15:47:54Z","title":"PyGraph: Robust Compiler Support for CUDA Graphs in PyTorch","summary":"Machine learning (ML) workloads launch hundreds to thousands of short-running GPU kernels per iteration. With GPU compute throughput growing rapidly, CPU-side launch latency of kernels is emerging as a bottleneck. CUDA Graphs promise to address this by replaying a set of kernels with a single dispatch of the graph, removing per-kernel launch costs. However, CUDA Graphs remain surprisingly difficult to deploy correctly and efficiently.\n  We present PyGraph - a compiler framework to maximize the coverage and benefits of CUDA Graphs for ML workloads. It introduces three novel optimizations: it applies automatic code transformations to make ML applications amenable to CUDA Graphs; it eliminates the parameter copy overheads for kernels executing in CUDA Graphs, and it selectively deploys CUDA Graphs guided by a cost-benefit analysis. For 25 ML workloads from TorchBench, HuggingFace, and TIMM, PyGraph more than doubles the benefit from deploying CUDA Graph compared to the most popular and widely used ML compiler, PyTorch2. PyGraph is built atop PyTorch2's compilation framework and requires no programmer intervention.","authors":["Abhishek Ghosh","Ajay Nayak","Ashish Panwar","Arkaprava Basu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2407.04258v2","updated":"2025-12-22T22:17:14Z","published":"2024-07-05T05:08:06Z","title":"Reinforcement Learning for Unsupervised Video Summarization with Reward Generator Training","summary":"This paper presents a novel approach for unsupervised video summarization using reinforcement learning (RL), addressing limitations like unstable adversarial training and reliance on heuristic-based reward functions. The method operates on the principle that reconstruction fidelity serves as a proxy for informativeness, correlating summary quality with reconstruction ability. The summarizer model assigns importance scores to frames to generate the final summary. For training, RL is coupled with a unique reward generation pipeline that incentivizes improved reconstructions. This pipeline uses a generator model to reconstruct the full video from the selected summary frames; the similarity between the original and reconstructed video provides the reward signal. The generator itself is pre-trained self-supervisedly to reconstruct randomly masked frames. This two-stage training process enhances stability compared to adversarial architectures. Experimental results show strong alignment with human judgments and promising F-scores, validating the reconstruction objective.","authors":["Mehryar Abbasi","Hadi Hadizadeh","Parvaneh Saeedi"],"pdf_url":"","comment":"in IEEE Transactions on Circuits and Systems for Video Technology"},{"id":"http://arxiv.org/abs/2512.19905v1","updated":"2025-12-22T22:13:06Z","published":"2025-12-22T22:13:06Z","title":"Demystifying LLM-as-a-Judge: Analytically Tractable Model for Inference-Time Scaling","summary":"Recent developments in large language models have shown advantages in reallocating a notable share of computational resource from training time to inference time. However, the principles behind inference time scaling are not well understood. In this paper, we introduce an analytically tractable model of inference-time scaling: Bayesian linear regression with a reward-weighted sampler, where the reward is determined from a linear model, modeling LLM-as-a-judge scenario. We study this problem in the high-dimensional regime, where the deterministic equivalents dictate a closed-form expression for the posterior predictive mean and variance. We analyze the generalization error when training data are sampled from a teacher model. We draw $k$ inference-time samples and select via softmax at a temperature applied to a quadratic reward. When the reward is not too different from the teacher, the generalization error decreases monotonically with increasing inference time samples $k$. However, the specific reward that optimizes inference-time selection generally differs from the teacher. In contrast, substantial reward misspecification induces a finite optimal $k$ beyond which more sampling can increase the generalization error. For fixed $k$, there exists an optimal sampling temperature. We experimentally verify these facts in large language model inference with an additional large language model as a judge. In the \"best-of-$k$\" limit with the teacher as reward, we theoretically show that the generalization error decays as $Θ(1/k^2)$ and determine the leading coefficient via extreme value theory. These formulas delineate domains where scaling inference-time computation is provably preferable to collecting more data. Finally, we demonstrate that when task difficulty increases, the previously mentioned advantage of inference-time compute degrades.","authors":["Indranil Halder","Cengiz Pehlevan"],"pdf_url":"","comment":"27 pages"},{"id":"http://arxiv.org/abs/2512.19899v1","updated":"2025-12-22T22:05:36Z","published":"2025-12-22T22:05:36Z","title":"Detecting cyberbullying in Spanish texts through deep learning techniques","summary":"Recent recollected data suggests that it is possible to automatically detect events that may negatively affect the most vulnerable parts of our society, by using any communication technology like social networks or messaging applications. This research consolidates and prepares a corpus with Spanish bullying expressions taken from Twitter in order to use them as an input to train a convolutional neuronal network through deep learning techniques. As a result of this training, a predictive model was created, which can identify Spanish cyberbullying expressions such as insults, racism, homophobic attacks, and so on.","authors":["Paúl Cumba-Armijos","Diego Riofrío-Luzcando","Verónica Rodríguez-Arboleda","Joe Carrión-Jumbo"],"pdf_url":"","comment":"Preprint (Author's Original Manuscript, AOM). Published version: https://doi.org/10.1504/IJDMMM.2022.125265"},{"id":"http://arxiv.org/abs/2512.19891v1","updated":"2025-12-22T21:34:29Z","published":"2025-12-22T21:34:29Z","title":"Efficient Learning of Lattice Gauge Theories with Fermions","summary":"We introduce a learning method for recovering action parameters in lattice field theories. Our method is based on the minimization of a convex loss function constructed using the Schwinger-Dyson relations. We show that score matching, a popular learning method, is a special case of our construction of an infinite family of valid loss functions. Importantly, our general Schwinger-Dyson-based construction applies to gauge theories and models with Grassmann-valued fields used to represent dynamical fermions. In particular, we extend our method to realistic lattice field theories including quantum chromodynamics.","authors":["Shreya Shukla","Yukari Yamauchi","Andrey Y. Lokhov","Scott Lawrence","Abhijith Jayakumar"],"pdf_url":"","comment":"12 pages, 2 figures"}],"Multimedia":[{"id":"http://arxiv.org/abs/2512.19379v1","updated":"2025-12-22T13:23:55Z","published":"2025-12-22T13:23:55Z","title":"OmniMER: Indonesian Multimodal Emotion Recognition via Auxiliary-Enhanced LLM Adaptation","summary":"Indonesian, spoken by over 200 million people, remains underserved in multimodal emotion recognition research despite its dominant presence on Southeast Asian social media platforms. We introduce IndoMER, the first multimodal emotion recognition benchmark for Indonesian, comprising 1,944 video segments from 203 speakers with temporally aligned text, audio, and visual annotations across seven emotion categories. The dataset exhibits realistic challenges including cross-modal inconsistency and long-tailed class distributions shaped by Indonesian cultural communication norms. To address these challenges, we propose OmniMER, a multimodal adaptation framework built upon Qwen2.5-Omni that enhances emotion recognition through three auxiliary modality-specific perception tasks: emotion keyword extraction for text, facial expression analysis for video, and prosody analysis for audio. These auxiliary tasks help the model identify emotion-relevant cues in each modality before fusion, reducing reliance on spurious correlations in low-resource settings. Experiments on IndoMER show that OmniMER achieves 0.582 Macro-F1 on sentiment classification and 0.454 on emotion recognition, outperforming the base model by 7.6 and 22.1 absolute points respectively. Cross-lingual evaluation on the Chinese CH-SIMS dataset further demonstrates the generalizability of the proposed framework. The dataset and code are publicly available. https://github.com/yanxm01/INDOMER","authors":["Xueming Yan","Boyan Xu","Yaochu Jin","Lixian Xiao","Wenlong Ye","Runyang Cai","Zeqi Zheng","Jingfa Liu","Aimin Yang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2502.12489v2","updated":"2025-12-22T11:32:28Z","published":"2025-02-18T03:18:54Z","title":"A Comprehensive Survey on Generative AI for Video-to-Music Generation","summary":"The burgeoning growth of video-to-music generation can be attributed to the ascendancy of multimodal generative models. However, there is a lack of literature that comprehensively combs through the work in this field. To fill this gap, this paper presents a comprehensive review of video-to-music generation using deep generative AI techniques, focusing on three key components: conditioning input construction, conditioning mechanism, and music generation frameworks. We categorize existing approaches based on their designs for each component, clarifying the roles of different strategies. Preceding this, we provide a fine-grained categorization of video and music modalities, illustrating how different categories influence the design of components within the generation pipelines. Furthermore, we summarize available multimodal datasets and evaluation metrics while highlighting ongoing challenges in the field.","authors":["Shulei Ji","Songruoyao Wu","Zihao Wang","Shuyu Li","Kejun Zhang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19130v1","updated":"2025-12-22T08:21:22Z","published":"2025-12-22T08:21:22Z","title":"D$^{2}$Stream: Decoupled Dual-Stream Temporal-Speaker Interaction for Audio-Visual Speaker Detection","summary":"Audio-visual speaker detection aims to identify the active speaker in videos by leveraging complementary audio and visual cues. Existing methods often suffer from computational inefficiency or suboptimal performance due to joint modeling of temporal and speaker interactions. We propose D$^{2}$Stream, a decoupled dual-stream framework that separates cross-frame temporal modeling from within-frame speaker discrimination. Audio and visual features are first aligned via cross-modal attention, then fed into two lightweight streams: a Temporal Interaction Stream captures long-range temporal dependencies, while a Speaker Interaction Stream models per-frame inter-person relationships. The temporal and relational features extracted by the two streams interact via cross-attention to enrich representations. A lightweight Voice Gate module further mitigates false positives from non-speech facial movements. On AVA-ActiveSpeaker, D$^{2}$Stream achieves a new state-of-the-art at 95.6% mAP, with 80% reduction in computation compared to GNN-based models and 30% fewer parameters than attention-based alternatives, while also generalizing well on Columbia ASD. Source code is available at https://anonymous.4open.science/r/D2STREAM.","authors":["Junhao Xiao","Shun Feng","Zhiyu Wu","Jianjun Li","Zhiyuan Ma","Yi Chen"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2407.04258v2","updated":"2025-12-22T22:17:14Z","published":"2024-07-05T05:08:06Z","title":"Reinforcement Learning for Unsupervised Video Summarization with Reward Generator Training","summary":"This paper presents a novel approach for unsupervised video summarization using reinforcement learning (RL), addressing limitations like unstable adversarial training and reliance on heuristic-based reward functions. The method operates on the principle that reconstruction fidelity serves as a proxy for informativeness, correlating summary quality with reconstruction ability. The summarizer model assigns importance scores to frames to generate the final summary. For training, RL is coupled with a unique reward generation pipeline that incentivizes improved reconstructions. This pipeline uses a generator model to reconstruct the full video from the selected summary frames; the similarity between the original and reconstructed video provides the reward signal. The generator itself is pre-trained self-supervisedly to reconstruct randomly masked frames. This two-stage training process enhances stability compared to adversarial architectures. Experimental results show strong alignment with human judgments and promising F-scores, validating the reconstruction objective.","authors":["Mehryar Abbasi","Hadi Hadizadeh","Parvaneh Saeedi"],"pdf_url":"","comment":"in IEEE Transactions on Circuits and Systems for Video Technology"}]},"2025-12-21T00:00:00Z":{"Artificial Intelligence":[{"id":"http://arxiv.org/abs/2511.05844v3","updated":"2025-12-21T23:56:36Z","published":"2025-11-08T04:23:42Z","title":"Enhancing Diffusion Model Guidance through Calibration and Regularization","summary":"Classifier-guided diffusion models have emerged as a powerful approach for conditional image generation, but they suffer from overconfident predictions during early denoising steps, causing the guidance gradient to vanish. This paper introduces two complementary contributions to address this issue. First, we propose a differentiable calibration objective based on the Smooth Expected Calibration Error (Smooth ECE), which improves classifier calibration with minimal fine-tuning and yields measurable improvements in Frechet Inception Distance (FID). Second, we develop enhanced sampling guidance methods that operate on off-the-shelf classifiers without requiring retraining. These include tilted sampling with batch-level reweighting, adaptive entropy-regularized sampling to preserve diversity, and a novel f-divergence-based sampling strategy that strengthens class-consistent guidance while maintaining mode coverage. Experiments on ImageNet 128x128 demonstrate that our divergence-regularized guidance achieves an FID of 2.13 using a ResNet-101 classifier, improving upon existing classifier-guided diffusion methods while requiring no diffusion model retraining. The results show that principled calibration and divergence-aware sampling provide practical and effective improvements for classifier-guided diffusion.","authors":["Seyed Alireza Javid","Amirhossein Bagheri","Nuria González-Prelcic"],"pdf_url":"","comment":"Accepted from NeurIPS 2025 Workshop on Structured Probabilistic Inference & Generative Modeling. Code available at https://github.com/ajavid34/guided-info-diffusion"},{"id":"http://arxiv.org/abs/2512.18925v1","updated":"2025-12-21T23:51:02Z","published":"2025-12-21T23:51:02Z","title":"An Empirical Study of Developer-Provided Context for AI Coding Assistants in Open-Source Projects","summary":"While Large Language Models (LLMs) have demonstrated remarkable capabilities, research shows that their effectiveness depends not only on explicit prompts but also on the broader context provided. This requirement is especially pronounced in software engineering, where the goals, architecture, and collaborative conventions of an existing project play critical roles in response quality. To support this, many AI coding assistants have introduced ways for developers to author persistent, machine-readable directives that encode a project's unique constraints. Although this practice is growing, the content of these directives remains unstudied.\n  This paper presents a large-scale empirical study to characterize this emerging form of developer-provided context. Through a qualitative analysis of 401 open-source repositories containing cursor rules, we developed a comprehensive taxonomy of project context that developers consider essential, organized into five high-level themes: Conventions, Guidelines, Project Information, LLM Directives, and Examples. Our study also explores how this context varies across different project types and programming languages, offering implications for the next generation of context-aware AI developer tools.","authors":["Shaokang Jiang","Daye Nam"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.13458v2","updated":"2025-12-21T23:37:37Z","published":"2025-12-15T15:56:04Z","title":"SSAS: Cross-subject EEG-based Emotion Recognition through Source Selection with Adversarial Strategy","summary":"Electroencephalographic (EEG) signals have long been applied in the field of affective brain-computer interfaces (aBCIs). Cross-subject EEG-based emotion recognition has demonstrated significant potential in practical applications due to its suitability across diverse people. However, most studies on cross-subject EEG-based emotion recognition neglect the presence of inter-individual variability and negative transfer phenomena during model training. To address this issue, a cross-subject EEG-based emotion recognition through source selection with adversarial strategy is introduced in this paper. The proposed method comprises two modules: the source selection network (SS) and the adversarial strategies network (AS). The SS uses domain labels to reverse-engineer the training process of domain adaptation. Its key idea is to disrupt class separability and magnify inter-domain differences, thereby raising the classification difficulty and forcing the model to learn domain-invariant yet emotion-relevant representations. The AS gets the source domain selection results and the pretrained domain discriminators from SS. The pretrained domain discriminators compute a novel loss aimed at enhancing the performance of domain classification during adversarial training, ensuring the balance of adversarial strategies. This paper provides theoretical insights into the proposed method and achieves outstanding performance on two EEG-based emotion datasets, SEED and SEED-IV. The code can be found at https://github.com/liuyici/SSAS.","authors":["Yici Liu","Qi Wei Oung","Hoi Leong Lee"],"pdf_url":"","comment":"Accepted by Expert Systems With Applications"},{"id":"http://arxiv.org/abs/2505.24099v2","updated":"2025-12-21T23:02:09Z","published":"2025-05-30T01:01:09Z","title":"Attractor learning for spatiotemporally chaotic dynamical systems using echo state networks with transfer learning","summary":"In this paper, we explore the predictive capabilities of echo state networks (ESNs) for the generalized Kuramoto-Sivashinsky (gKS) equation, an archetypal nonlinear PDE that exhibits spatiotemporal chaos. Our research focuses on predicting changes in long-term statistical patterns of the gKS model that result from varying the dispersion relation or the length of the spatial domain. We use transfer learning to adapt ESNs to different parameter settings and successfully capture changes in the underlying chaotic attractor. Previous work has shown that transfer learning can be used effectively with ESNs for single-orbit prediction. The novelty of our paper lies in our use of this pairing to predict the long-term statistical properties of spatiotemporally chaotic PDEs. We also show that transfer learning nontrivially improves the length of time that predictions of individual gKS trajectories remain accurate.","authors":["Mohammad Shah Alam","William Ott","Ilya Timofeyev"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.18908v1","updated":"2025-12-21T22:59:58Z","published":"2025-12-21T22:59:58Z","title":"Multimodal Bayesian Network for Robust Assessment of Casualties in Autonomous Triage","summary":"Mass Casualty Incidents can overwhelm emergency medical systems and resulting delays or errors in the assessment of casualties can lead to preventable deaths. We present a decision support framework that fuses outputs from multiple computer vision models, estimating signs of severe hemorrhage, respiratory distress, physical alertness, or visible trauma, into a Bayesian network constructed entirely from expert-defined rules. Unlike traditional data-driven models, our approach does not require training data, supports inference with incomplete information, and is robust to noisy or uncertain observations. We report performance for two missions involving 11 and 9 casualties, respectively, where our Bayesian network model substantially outperformed vision-only baselines during evaluation of our system in the DARPA Triage Challenge (DTC) field scenarios. The accuracy of physiological assessment improved from 15% to 42% in the first scenario and from 19% to 46% in the second, representing nearly threefold increase in performance. More importantly, overall triage accuracy increased from 14% to 53% in all patients, while the diagnostic coverage of the system expanded from 31% to 95% of the cases requiring assessment. These results demonstrate that expert-knowledge-guided probabilistic reasoning can significantly enhance automated triage systems, offering a promising approach to supporting emergency responders in MCIs. This approach enabled Team Chiron to achieve 4th place out of 11 teams during the 1st physical round of the DTC.","authors":["Szymon Rusiecki","Cecilia G. Morales","Kimberly Elenberg","Leonard Weiss","Artur Dubrawski"],"pdf_url":"","comment":"Accepted at NeurIPS 2025 Workshop: Structured Probabilistic Inference & Generative Modeling"},{"id":"http://arxiv.org/abs/2511.02376v3","updated":"2025-12-21T22:30:20Z","published":"2025-11-04T08:56:28Z","title":"AutoAdv: Automated Adversarial Prompting for Multi-Turn Jailbreaking of Large Language Models","summary":"Large Language Models (LLMs) remain vulnerable to jailbreaking attacks where adversarial prompts elicit harmful outputs. Yet most evaluations focus on single-turn interactions while real-world attacks unfold through adaptive multi-turn conversations. We present AutoAdv, a training-free framework for automated multi-turn jailbreaking that achieves an attack success rate of up to 95% on Llama-3.1-8B within six turns, a 24% improvement over single-turn baselines. AutoAdv uniquely combines three adaptive mechanisms: a pattern manager that learns from successful attacks to enhance future prompts, a temperature manager that dynamically adjusts sampling parameters based on failure modes, and a two-phase rewriting strategy that disguises harmful requests and then iteratively refines them. Extensive evaluation across commercial and open-source models (Llama-3.1-8B, GPT-4o mini, Qwen3-235B, Mistral-7B) reveals persistent vulnerabilities in current safety mechanisms, with multi-turn attacks consistently outperforming single-turn approaches. These findings demonstrate that alignment strategies optimized for single-turn interactions fail to maintain robustness across extended conversations, highlighting an urgent need for multi-turn-aware defenses.","authors":["Aashray Reddy","Andrew Zagula","Nicholas Saban"],"pdf_url":"","comment":"Presented at NeurIPS 2025 Lock-LLM Workshop. Code is available at https://github.com/AAN-AutoAdv/AutoAdv"},{"id":"http://arxiv.org/abs/2512.15584v2","updated":"2025-12-21T22:26:06Z","published":"2025-12-17T16:44:01Z","title":"A Decision-Theoretic Approach for Managing Misalignment","summary":"When should we delegate decisions to AI systems? While the value alignment literature has developed techniques for shaping AI values, less attention has been paid to how to determine, under uncertainty, when imperfect alignment is good enough to justify delegation. We argue that rational delegation requires balancing an agent's value (mis)alignment with its epistemic accuracy and its reach (the acts it has available). This paper introduces a formal, decision-theoretic framework to analyze this tradeoff precisely accounting for a principal's uncertainty about these factors. Our analysis reveals a sharp distinction between two delegation scenarios. First, universal delegation (trusting an agent with any problem) demands near-perfect value alignment and total epistemic trust, conditions rarely met in practice. Second, we show that context-specific delegation can be optimal even with significant misalignment. An agent's superior accuracy or expanded reach may grant access to better overall decision problems, making delegation rational in expectation. We develop a novel scoring framework to quantify this ex ante decision. Ultimately, our work provides a principled method for determining when an AI is aligned enough for a given context, shifting the focus from achieving perfect alignment to managing the risks and rewards of delegation under uncertainty.","authors":["Daniel A. Herrmann","Abinav Chari","Isabelle Qian","Sree Sharvesh","B. A. Levinstein"],"pdf_url":"","comment":"Second Conference of the International Association for Safe and Ethical Artificial Intelligence (IASEAI '26)"},{"id":"http://arxiv.org/abs/2512.18901v1","updated":"2025-12-21T22:12:54Z","published":"2025-12-21T22:12:54Z","title":"Gabliteration: Adaptive Multi-Directional Neural Weight Modification for Selective Behavioral Alteration in Large Language Models","summary":"We present Gabliteration, a novel neural weight modification technique that advances beyond traditional abliteration methods by implementing adaptive multi-directional projections with regularized layer selection. Our approach addresses the fundamental limitation of existing methods that compromise model quality while attempting to modify specific behavioral patterns. Through dynamic layer optimization, regularized projection matrices, and adaptive scaling mechanisms, we achieve theoretically superior weight modification while minimizing quality degradation in unrelated domains. We validate our method through the gabliterated-v1 model series (0.6B to 4B parameters) available on Hugging Face, demonstrating practical applicability across multiple model scales.","authors":["Gökdeniz Gülmez"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2011.03783v2","updated":"2025-12-21T21:38:22Z","published":"2020-11-07T14:28:54Z","title":"Towards a resource for multilingual lexicons: an MT assisted and human-in-the-loop multilingual parallel corpus with multi-word expression annotation","summary":"In this work, we introduce the construction of a machine translation (MT) assisted and human-in-the-loop multilingual parallel corpus with annotations of multi-word expressions (MWEs), named AlphaMWE. The MWEs include verbal MWEs (vMWEs) defined in the PARSEME shared task that have a verb as the head of the studied terms. The annotated vMWEs are also bilingually and multilingually aligned manually. The languages covered include Arabic, Chinese, English, German, Italian, and Polish, of which, the Arabic corpus includes both standard and dialectal variations from Egypt and Tunisia. Our original English corpus is extracted from the PARSEME shared task in 2018. We performed machine translation of this source corpus followed by human post-editing and annotation of target MWEs. Strict quality control was applied for error limitation, i.e., each MT output sentence received first manual post-editing and annotation plus a second manual quality rechecking till annotators' consensus is reached. One of our findings during corpora preparation is that accurate translation of MWEs presents challenges to MT systems, as reflected by the outcomes of human-in-the-loop metric HOPE. To facilitate further MT research, we present a categorisation of the error types encountered by MT systems in performing MWE-related translation. To acquire a broader view of MT issues, we selected four popular state-of-the-art MT systems for comparison, namely Microsoft Bing Translator, GoogleMT, Baidu Fanyi, and DeepL MT. Because of the noise removal, translation post-editing, and MWE annotation by human professionals, we believe the AlphaMWE data set will be an asset for both monolingual and cross-lingual research, such as multi-word term lexicography, MT, and information extraction.","authors":["Lifeng Han","Najet Hadj Mohamed","Malak Rassem","Gareth Jones","Alan Smeaton","Goran Nenadic"],"pdf_url":"","comment":"Accepted by Journal of LRE, extended work from WS paper AlphaMWE"},{"id":"http://arxiv.org/abs/2506.16506v3","updated":"2025-12-21T21:28:11Z","published":"2025-06-19T17:59:29Z","title":"Subspace-Boosted Model Merging","summary":"Model merging enables the combination of multiple specialized expert models into a single model capable of performing multiple tasks. However, the benefits of merging an increasing amount of specialized experts generally lead to diminishing returns and reduced overall performance gains. In this work, we empirically and theoretically analyze this limitation, proving that for Task Arithmetic-based methods, as more experts are merged, the common information dominates the task-specific information, leading to inevitable rank collapse. To mitigate this issue, we introduce Subspace Boosting, which operates on the singular value decomposed task vector space and maintains task vector ranks. Subspace Boosting raises merging efficacy for up to 20 experts by large margins of more than 10% when evaluated on both vision and language benchmarks. Moreover, we propose employing Higher-Order Generalized Singular Value Decomposition to quantify task similarity, offering a new interpretable perspective on model merging. Code and models are available at https://github.com/ronskoro/Subspace-Boosting.","authors":["Ronald Skorobogat","Karsten Roth","Mariana-Iuliana Georgescu"],"pdf_url":"","comment":"32 pages (main + supp)"},{"id":"http://arxiv.org/abs/2512.18892v1","updated":"2025-12-21T21:22:12Z","published":"2025-12-21T21:22:12Z","title":"Structural Reinforcement Learning for Heterogeneous Agent Macroeconomics","summary":"We present a new approach to formulating and solving heterogeneous agent models with aggregate risk. We replace the cross-sectional distribution with low-dimensional prices as state variables and let agents learn equilibrium price dynamics directly from simulated paths. To do so, we introduce a structural reinforcement learning (SRL) method which treats prices via simulation while exploiting agents' structural knowledge of their own individual dynamics. Our SRL method yields a general and highly efficient global solution method for heterogeneous agent models that sidesteps the Master equation and handles problems traditional methods struggle with, in particular nontrivial market-clearing conditions. We illustrate the approach in the Krusell-Smith model, the Huggett model with aggregate shocks, and a HANK model with a forward-looking Phillips curve, all of which we solve globally within minutes.","authors":["Yucheng Yang","Chiyuan Wang","Andreas Schaab","Benjamin Moll"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2510.03248v2","updated":"2025-12-21T20:49:41Z","published":"2025-09-26T01:48:27Z","title":"Multimodal Neural Operators for Real-Time Biomechanical Modelling of Traumatic Brain Injury","summary":"Background: Traumatic brain injury (TBI) is a major global health concern with 69 million annual cases. While neural operators have revolutionized scientific computing, existing architectures cannot handle the heterogeneous multimodal data (anatomical imaging, scalar demographics, and geometric constraints) required for patient-specific biomechanical modeling. Objective: This study introduces the first multimodal neural operator framework for biomechanics, fusing heterogeneous inputs to predict brain displacement fields for rapid TBI risk assessment. Methods: TBI modeling was reformulated as a multimodal operator learning problem. We proposed two fusion strategies: field projection for Fourier Neural Operator (FNO) architectures and branch decomposition for Deep Operator Networks (DeepONet). Four architectures (FNO, Factorized FNO, Multi-Grid FNO, and DeepONet) were extended with fusion mechanisms and evaluated on 249 in vivo Magnetic Resonance Elastography (MRE) datasets (20-90 Hz). Results: Multi-Grid FNO achieved the highest accuracy (MSE = 0.0023, 94.3% spatial fidelity). DeepONet offered the fastest inference (14.5 iterations/s, 7x speedup), suitable for edge deployment. All architectures reduced computation from hours to milliseconds. Conclusion: Multimodal neural operators enable efficient, real-time, patient-specific TBI risk assessment. This framework establishes a generalizable paradigm for heterogeneous data fusion in scientific domains, including precision medicine.","authors":["Anusha Agarwal","Dibakar Roy Sarkar","Somdatta Goswami"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.18880v1","updated":"2025-12-21T20:41:36Z","published":"2025-12-21T20:41:36Z","title":"Can LLMs Estimate Student Struggles? Human-AI Difficulty Alignment with Proficiency Simulation for Item Difficulty Prediction","summary":"Accurate estimation of item (question or task) difficulty is critical for educational assessment but suffers from the cold start problem. While Large Language Models demonstrate superhuman problem-solving capabilities, it remains an open question whether they can perceive the cognitive struggles of human learners. In this work, we present a large-scale empirical analysis of Human-AI Difficulty Alignment for over 20 models across diverse domains such as medical knowledge and mathematical reasoning. Our findings reveal a systematic misalignment where scaling up model size is not reliably helpful; instead of aligning with humans, models converge toward a shared machine consensus. We observe that high performance often impedes accurate difficulty estimation, as models struggle to simulate the capability limitations of students even when being explicitly prompted to adopt specific proficiency levels. Furthermore, we identify a critical lack of introspection, as models fail to predict their own limitations. These results suggest that general problem-solving capability does not imply an understanding of human cognitive struggles, highlighting the challenge of using current models for automated difficulty prediction.","authors":["Ming Li","Han Chen","Yunze Xiao","Jian Chen","Hong Jiao","Tianyi Zhou"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.18878v1","updated":"2025-12-21T20:39:31Z","published":"2025-12-21T20:39:31Z","title":"CrashChat: A Multimodal Large Language Model for Multitask Traffic Crash Video Analysis","summary":"Automating crash video analysis is essential to leverage the growing availability of driving video data for traffic safety research and accountability attribution in autonomous driving. Crash video analysis is a challenging multitask problem due to the complex spatiotemporal dynamics of crash events in video data and the diverse analytical requirements involved. It requires capabilities spanning crash recognition, temporal grounding, and high-level video understanding. Existing models, however, cannot perform all these tasks within a unified framework, and effective training strategies for such models remain underexplored. To fill these gaps, this paper proposes CrashChat, a multimodal large language model (MLLM) for multitask traffic crash analysis, built upon VideoLLaMA3. CrashChat acquires domain-specific knowledge through instruction fine-tuning and employs a novel multitask learning strategy based on task decoupling and grouping, which maximizes the benefit of joint learning within and across task groups while mitigating negative transfer. Numerical experiments on consolidated public datasets demonstrate that CrashChat consistently outperforms existing MLLMs across model scales and traditional vision-based methods, achieving state-of-the-art performance. It reaches near-perfect accuracy in crash recognition, a 176\\% improvement in crash localization, and a 40\\% improvement in the more challenging pre-crash localization. Compared to general MLLMs, it substantially enhances textual accuracy and content coverage in crash description and reasoning tasks, with 0.18-0.41 increases in BLEU scores and 0.18-0.42 increases in ROUGE scores. Beyond its strong performance, CrashChat is a convenient, end-to-end analytical tool ready for practical implementation. The dataset and implementation code for CrashChat are available at https://github.com/Liangkd/CrashChat.","authors":["Kaidi Liang","Ke Li","Xianbiao Hu","Ruwen Qin"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.18871v1","updated":"2025-12-21T20:11:07Z","published":"2025-12-21T20:11:07Z","title":"Psychometric Validation of the Sophotechnic Mediation Scale and a New Understanding of the Development of GenAI Mastery: Lessons from 3,392 Adult Brazilian Workers","summary":"The rapid diffusion of generative artificial intelligence (GenAI) systems has introduced new forms of human-technology interaction, raising the question of whether sustained engagement gives rise to stable, internalized modes of cognition rather than merely transient efficiency gains. Grounded in the Cognitive Mediation Networks Theory, this study investigates Sophotechnic Mediation, a mode of thinking and acting associated with prolonged interaction with GenAI, and presents a comprehensive psychometric validation of the Sophotechnic Mediation Scale. Data were collected between 2023 and 2025 from independent cross-sectional samples totaling 3,932 adult workers from public and private organizations in the Metropolitan Region of Pernambuco, Brazil. Results indicate excellent internal consistency, a robust unidimensional structure, and measurement invariance across cohorts. Ordinal-robust confirmatory factor analyses and residual diagnostics show that elevated absolute fit indices reflect minor local dependencies rather than incorrect dimensionality. Distributional analyses reveal a time-evolving pattern characterized by a declining mass of non-adopters and convergence toward approximate Gaussianity among adopters, with model comparisons favoring a two-process hurdle model over a censored Gaussian specification. Sophotechnic Mediation is empirically distinct from Hypercultural mediation and is primarily driven by cumulative GenAI experience, with age moderating the rate of initial acquisition and the depth of later integration. Together, the findings support Sophotechnia as a coherent, measurable, and emergent mode of cognitive mediation associated with the ongoing GenAI revolution.","authors":["Bruno Campello de Souza"],"pdf_url":"","comment":"35 pages, 28 Manuscript, Portuguese and English Versions of the Instrument in Annex"},{"id":"http://arxiv.org/abs/2512.01372v2","updated":"2025-12-21T19:35:23Z","published":"2025-12-01T07:39:28Z","title":"Structured Spectral Reasoning for Frequency-Adaptive Multimodal Recommendation","summary":"Multimodal recommendation aims to integrate collaborative signals with heterogeneous content such as visual and textual information, but remains challenged by modality-specific noise, semantic inconsistency, and unstable propagation over user-item graphs. These issues are often exacerbated by naive fusion or shallow modeling strategies, leading to degraded generalization and poor robustness. While recent work has explored the frequency domain as a lens to separate stable from noisy signals, most methods rely on static filtering or reweighting, lacking the ability to reason over spectral structure or adapt to modality-specific reliability. To address these challenges, we propose a Structured Spectral Reasoning (SSR) framework for frequency-aware multimodal recommendation. Our method follows a four-stage pipeline: (i) Decompose graph-based multimodal signals into spectral bands via graph-guided transformations to isolate semantic granularity; (ii) Modulate band-level reliability with spectral band masking, a training-time masking with a prediction-consistency objective that suppresses brittle frequency components; (iii) Fuse complementary frequency cues using hyperspectral reasoning with low-rank cross-band interaction; and (iv) Align modality-specific spectral features via contrastive regularization to promote semantic and structural consistency. Experiments on three real-world benchmarks show consistent gains over strong baselines, particularly under sparse and cold-start settings. Additional analyses indicate that structured spectral modeling improves robustness and provides clearer diagnostics of how different bands contribute to performance.","authors":["Wei Yang","Rui Zhong","Yiqun Chen","Chi Lu","Peng Jiang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.16707v2","updated":"2025-12-21T19:02:01Z","published":"2025-12-18T16:12:04Z","title":"Dual Computational Horizons: Incompleteness and Unpredictability in Intelligent Systems","summary":"We formalize two independent computational limitations that constrain algorithmic intelligence: formal incompleteness and dynamical unpredictability. The former limits the deductive power of consistent reasoning systems while the latter bounds long-term prediction under finite precision. We show that these two extrema together impose structural bounds on an agent's ability to reason about its own predictive capabilities. In particular, an algorithmic agent cannot verify its own maximal prediction horizon universally. This perspective clarifies inherent trade-offs between reasoning, prediction, and self-analysis in intelligent systems. The construction presented here constitutes one representative instance of a broader logical class of such limitations.","authors":["Abhisek Ganguly"],"pdf_url":"","comment":"8 Pages, 0 figures"},{"id":"http://arxiv.org/abs/2512.18857v1","updated":"2025-12-21T19:01:35Z","published":"2025-12-21T19:01:35Z","title":"CORE: Concept-Oriented Reinforcement for Bridging the Definition-Application Gap in Mathematical Reasoning","summary":"Large language models (LLMs) often solve challenging math exercises yet fail to apply the concept right when the problem requires genuine understanding. Popular Reinforcement Learning with Verifiable Rewards (RLVR) pipelines reinforce final answers but provide little fine-grained conceptual signal, so models improve at pattern reuse rather than conceptual applications. We introduce CORE (Concept-Oriented REinforcement), an RL training framework that turns explicit concepts into a controllable supervision signal. Starting from a high-quality, low-contamination textbook resource that links verifiable exercises to concise concept descriptions, we run a sanity probe showing LLMs can restate definitions but fail concept-linked quizzes, quantifying the conceptual reasoning gap. CORE then (i) synthesizes concept-aligned quizzes, (ii) injects brief concept snippets during rollouts to elicit concept-primed trajectories, and (iii) reinforces conceptual reasoning via trajectory replacement after group failures, a lightweight forward-KL constraint that aligns unguided with concept-primed policies, or standard GRPO directly on concept-aligned quizzes. Across several models, CORE delivers consistent gains over vanilla and SFT baselines on both in-domain concept-exercise suites and diverse out-of-domain math benchmarks. CORE unifies direct training on concept-aligned quizzes and concept-injected rollouts under outcome regularization. It provides fine-grained conceptual supervision that bridges problem-solving competence and genuine conceptual reasoning, while remaining algorithm- and verifier-agnostic.","authors":["Zijun Gao","Zhikun Xu","Xiao Ye","Ben Zhou"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.18829v1","updated":"2025-12-21T17:27:10Z","published":"2025-12-21T17:27:10Z","title":"HARBOR: Holistic Adaptive Risk assessment model for BehaviORal healthcare","summary":"Behavioral healthcare risk assessment remains a challenging problem due to the highly multimodal nature of patient data and the temporal dynamics of mood and affective disorders. While large language models (LLMs) have demonstrated strong reasoning capabilities, their effectiveness in structured clinical risk scoring remains unclear. In this work, we introduce HARBOR, a behavioral health aware language model designed to predict a discrete mood and risk score, termed the Harbor Risk Score (HRS), on an integer scale from -3 (severe depression) to +3 (mania). We also release PEARL, a longitudinal behavioral healthcare dataset spanning four years of monthly observations from three patients, containing physiological, behavioral, and self reported mental health signals. We benchmark traditional machine learning models, proprietary LLMs, and HARBOR across multiple evaluation settings and ablations. Our results show that HARBOR outperforms classical baselines and off the shelf LLMs, achieving 69 percent accuracy compared to 54 percent for logistic regression and 29 percent for the strongest proprietary LLM baseline.","authors":["Aditya Siddhant"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.16912v2","updated":"2025-12-21T17:23:35Z","published":"2025-12-18T18:59:27Z","title":"Exploration vs Exploitation: Rethinking RLVR through Clipping, Entropy, and Spurious Reward","summary":"This paper examines the exploration-exploitation trade-off in reinforcement learning with verifiable rewards (RLVR), a framework for improving the reasoning of Large Language Models (LLMs). Recent studies suggest that RLVR can elicit strong mathematical reasoning in LLMs through two seemingly paradoxical mechanisms: spurious rewards, which suppress exploitation by rewarding outcomes unrelated to the ground truth, and entropy minimization, which suppresses exploration by pushing the model toward more confident and deterministic outputs, highlighting a puzzling dynamic: both discouraging exploitation and discouraging exploration improve reasoning performance, yet the underlying principles that reconcile these effects remain poorly understood. We focus on two fundamental questions: (i) how policy entropy relates to performance, and (ii) whether spurious rewards yield gains, potentially through the interplay of clipping bias and model contamination. Our results show that clipping bias under spurious rewards reduces policy entropy, leading to more confident and deterministic outputs, while entropy minimization alone is insufficient for improvement. We further propose a reward-misalignment model explaining why spurious rewards can enhance performance beyond contaminated settings. Our findings clarify the mechanisms behind spurious-reward benefits and provide principles for more effective RLVR training.","authors":["Peter Chen","Xiaopeng Li","Ziniu Li","Wotao Yin","Xi Chen","Tianyi Lin"],"pdf_url":"","comment":"35 pages"},{"id":"http://arxiv.org/abs/2512.18826v1","updated":"2025-12-21T17:19:29Z","published":"2025-12-21T17:19:29Z","title":"Hyperbolic Graph Embeddings: a Survey and an Evaluation on Anomaly Detection","summary":"This survey reviews hyperbolic graph embedding models, and evaluate them on anomaly detection, highlighting their advantages over Euclidean methods in capturing complex structures. Evaluating models like \\textit{HGCAE}, \\textit{\\(\\mathcal{P}\\)-VAE}, and \\textit{HGCN} demonstrates high performance, with \\textit{\\(\\mathcal{P}\\)-VAE} achieving an F1-score of 94\\% on the \\textit{Elliptic} dataset and \\textit{HGCAE} scoring 80\\% on \\textit{Cora}. In contrast, Euclidean methods like \\textit{DOMINANT} and \\textit{GraphSage} struggle with complex data. The study emphasizes the potential of hyperbolic spaces for improving anomaly detection, and provides an open-source library to foster further research in this field.","authors":["Souhail Abdelmouaiz Sadat","Mohamed Yacine Touahria Miliani","Khadidja Hab El Hames","Hamida Seba","Mohammed Haddad"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.18815v1","updated":"2025-12-21T17:10:00Z","published":"2025-12-21T17:10:00Z","title":"Controllable Probabilistic Forecasting with Stochastic Decomposition Layers","summary":"AI weather prediction ensembles with latent noise injection and optimized with the continuous ranked probability score (CRPS) have produced both accurate and well-calibrated predictions with far less computational cost compared with diffusion-based methods. However, current CRPS ensemble approaches vary in their training strategies and noise injection mechanisms, with most injecting noise globally throughout the network via conditional normalization. This structure increases training expense and limits the physical interpretability of the stochastic perturbations. We introduce Stochastic Decomposition Layers (SDL) for converting deterministic machine learning weather models into probabilistic ensemble systems. Adapted from StyleGAN's hierarchical noise injection, SDL applies learned perturbations at three decoder scales through latent-driven modulation, per-pixel noise, and channel scaling. When applied to WXFormer via transfer learning, SDL requires less than 2\\% of the computational cost needed to train the baseline model. Each ensemble member is generated from a compact latent tensor (5 MB), enabling perfect reproducibility and post-inference spread adjustment through latent rescaling. Evaluation on 2022 ERA5 reanalysis shows ensembles with spread-skill ratios approaching unity and rank histograms that progressively flatten toward uniformity through medium-range forecasts, achieving calibration competitive with operational IFS-ENS. Multi-scale experiments reveal hierarchical uncertainty: coarse layers modulate synoptic patterns while fine layers control mesoscale variability. The explicit latent parameterization provides interpretable uncertainty quantification for operational forecasting and climate applications.","authors":["John S. Schreck","William E. Chapman","Charlie Becker","David John Gagne","Dhamma Kimpara","Nihanth Cherukuru","Judith Berner","Kirsten J. Mayer","Negin Sobhani"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.18809v1","updated":"2025-12-21T17:01:44Z","published":"2025-12-21T17:01:44Z","title":"FedVideoMAE: Efficient Privacy-Preserving Federated Video Moderation","summary":"The rapid growth of short-form video platforms increases the need for privacy-preserving moderation, as cloud-based pipelines expose raw videos to privacy risks, high bandwidth costs, and inference latency. To address these challenges, we propose an on-device federated learning framework for video violence detection that integrates self-supervised VideoMAE representations, LoRA-based parameter-efficient adaptation, and defense-in-depth privacy protection. Our approach reduces the trainable parameter count to 5.5M (~3.5% of a 156M backbone) and incorporates DP-SGD with configurable privacy budgets and secure aggregation. Experiments on RWF-2000 with 40 clients achieve 77.25% accuracy without privacy protection and 65-66% under strong differential privacy, while reducing communication cost by $28.3\\times$ compared to full-model federated learning. The code is available at: {https://github.com/zyt-599/FedVideoMAE}","authors":["Ziyuan Tao","Chuanzhi Xu","Sandaru Jayawardana","Wei Bao","Kanchana Thilakarathna","Teng Joon Lim"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.18797v1","updated":"2025-12-21T16:31:05Z","published":"2025-12-21T16:31:05Z","title":"Reliable Audio Deepfake Detection in Variable Conditions via Quantum-Kernel SVMs","summary":"Detecting synthetic speech is challenging when labeled data are scarce and recording conditions vary. Existing end-to-end deep models often overfit or fail to generalize, and while kernel methods can remain competitive, their performance heavily depends on the chosen kernel. Here, we show that using a quantum kernel in audio deepfake detection reduces falsepositive rates without increasing model size. Quantum feature maps embed data into high-dimensional Hilbert spaces, enabling the use of expressive similarity measures and compact classifiers. Building on this motivation, we compare quantum-kernel SVMs (QSVMs) with classical SVMs using identical mel-spectrogram preprocessing and stratified 5-fold cross-validation across four corpora (ASVspoof 2019 LA, ASVspoof 5 (2024), ADD23, and an In-the-Wild set). QSVMs achieve consistently lower equalerror rates (EER): 0.183 vs. 0.299 on ASVspoof 5 (2024), 0.081 vs. 0.188 on ADD23, 0.346 vs. 0.399 on ASVspoof 2019, and 0.355 vs. 0.413 In-the-Wild. At the EER operating point (where FPR equals FNR), these correspond to absolute false-positiverate reductions of 0.116 (38.8%), 0.107 (56.9%), 0.053 (13.3%), and 0.058 (14.0%), respectively. We also report how consistent the results are across cross-validation folds and margin-based measures of class separation, using identical settings for both models. The only modification is the kernel; the features and SVM remain unchanged, no additional trainable parameters are introduced, and the quantum kernel is computed on a conventional computer.","authors":["Lisan Al Amin","Vandana P. Janeja"],"pdf_url":"","comment":"This paper is accepted in ICDM 2025-MLC workshop"},{"id":"http://arxiv.org/abs/2509.09284v3","updated":"2025-12-21T16:15:54Z","published":"2025-09-11T09:18:07Z","title":"Tree-OPO: Off-policy Monte Carlo Tree-Guided Advantage Optimization for Multistep Reasoning","summary":"Recent advances in reasoning with large language models (LLMs) have shown the effectiveness of Monte Carlo Tree Search (MCTS) for generating high quality intermediate trajectories, particularly in math and symbolic domains. Inspired by this, we explore how MCTS derived trajectories, traditionally used for training value or reward models, can be repurposed to improve policy optimization in verifier guided reinforcement learning (RL). Specifically, we focus on Group Relative Policy Optimization (GRPO), a recent algorithm that enables consistent policy learning from group relative judgments. We reframe GRPO into a staged training paradigm, leveraging a teacher's MCTS rollouts to construct a tree structured curriculum of prefixes. This introduces the novel challenge of computing advantages for training samples that originate from different prefixes, each with a distinct expected return. To address this, we propose Staged Advantage Estimation (SAE), a framework for computing low variance, prefix aware advantages by projecting rewards onto a constraint set that respects the tree's hierarchy. Our empirical results on mathematical reasoning tasks show that SAE improves final accuracy over standard GRPO. This outcome is grounded in our theoretical analysis, which confirms that SAE reduces gradient variance, a principled path to improved sample efficiency. We demonstrate this through practical SAE implementations, comparing efficient heuristics against a formal quadratic program.","authors":["Bingning Huang","Tu Nguyen","Matthieu Zimmer"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.18792v1","updated":"2025-12-21T16:07:44Z","published":"2025-12-21T16:07:44Z","title":"The Dead Salmons of AI Interpretability","summary":"In a striking neuroscience study, the authors placed a dead salmon in an MRI scanner and showed it images of humans in social situations. Astonishingly, standard analyses of the time reported brain regions predictive of social emotions. The explanation, of course, was not supernatural cognition but a cautionary tale about misapplied statistical inference. In AI interpretability, reports of similar ''dead salmon'' artifacts abound: feature attribution, probing, sparse auto-encoding, and even causal analyses can produce plausible-looking explanations for randomly initialized neural networks. In this work, we examine this phenomenon and argue for a pragmatic statistical-causal reframing: explanations of computational systems should be treated as parameters of a (statistical) model, inferred from computational traces. This perspective goes beyond simply measuring statistical variability of explanations due to finite sampling of input data; interpretability methods become statistical estimators, and findings should be tested against explicit and meaningful alternative computational hypotheses, with uncertainty quantified with respect to the postulated statistical model. It also highlights important theoretical issues, such as the identifiability of common interpretability queries, which we argue is critical to understand the field's susceptibility to false discoveries, poor generalizability, and high variance. More broadly, situating interpretability within the standard toolkit of statistical inference opens promising avenues for future work aimed at turning AI interpretability into a pragmatic and rigorous science.","authors":["Maxime Méloux","Giada Dirupo","François Portet","Maxime Peyrard"],"pdf_url":"","comment":"10 pages, 3 figures"},{"id":"http://arxiv.org/abs/2512.18791v1","updated":"2025-12-21T16:07:08Z","published":"2025-12-21T16:07:08Z","title":"Smark: A Watermark for Text-to-Speech Diffusion Models via Discrete Wavelet Transform","summary":"Text-to-Speech (TTS) diffusion models generate high-quality speech, which raises challenges for the model intellectual property protection and speech tracing for legal use. Audio watermarking is a promising solution. However, due to the structural differences among various TTS diffusion models, existing watermarking methods are often designed for a specific model and degrade audio quality, which limits their practical applicability. To address this dilemma, this paper proposes a universal watermarking scheme for TTS diffusion models, termed Smark. This is achieved by designing a lightweight watermark embedding framework that operates in the common reverse diffusion paradigm shared by all TTS diffusion models. To mitigate the impact on audio quality, Smark utilizes the discrete wavelet transform (DWT) to embed watermarks into the relatively stable low-frequency regions of the audio, which ensures seamless watermark-audio integration and is resistant to removal during the reverse diffusion process. Extensive experiments are conducted to evaluate the audio quality and watermark performance in various simulated real-world attack scenarios. The experimental results show that Smark achieves superior performance in both audio quality and watermark extraction accuracy.","authors":["Yichuan Zhang","Chengxin Li","Yujie Gu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2507.10216v2","updated":"2025-12-21T15:41:27Z","published":"2025-07-14T12:33:07Z","title":"From Words to Proverbs: Evaluating LLMs Linguistic and Cultural Competence in Saudi Dialects with Absher","summary":"As large language models (LLMs) become increasingly central to Arabic NLP applications, evaluating their understanding of regional dialects and cultural nuances is essential, particularly in linguistically diverse settings like Saudi Arabia. This paper introduces Absher, a comprehensive benchmark specifically designed to assess LLMs performance across major Saudi dialects. \\texttt{Absher} comprises over 18,000 multiple-choice questions spanning six distinct categories: Meaning, True/False, Fill-in-the-Blank, Contextual Usage, Cultural Interpretation, and Location Recognition. These questions are derived from a curated dataset of dialectal words, phrases, and proverbs sourced from various regions of Saudi Arabia. We evaluate several state-of-the-art LLMs, including multilingual and Arabic-specific models. We also provide detailed insights into their capabilities and limitations. Our results reveal notable performance gaps, particularly in tasks requiring cultural inference or contextual understanding. Our findings highlight the urgent need for dialect-aware training and culturally aligned evaluation methodologies to improve LLMs performance in real-world Arabic applications.","authors":["Renad Al-Monef","Hassan Alhuzali","Nora Alturayeif","Ashwag Alasmari"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.16661v2","updated":"2025-12-21T15:17:01Z","published":"2025-12-18T15:29:18Z","title":"Microsoft Academic Graph Information Retrieval for Research Recommendation and Assistance","summary":"In today's information-driven world, access to scientific publications has become increasingly easy. At the same time, filtering through the massive volume of available research has become more challenging than ever. Graph Neural Networks (GNNs) and graph attention mechanisms have shown strong effectiveness in searching large-scale information databases, particularly when combined with modern large language models. In this paper, we propose an Attention-Based Subgraph Retriever, a GNN-as-retriever model that applies attention-based pruning to extract a refined subgraph, which is then passed to a large language model for advanced knowledge reasoning.","authors":["Shikshya Shiwakoti","Samuel Goldsmith","Ujjwal Pandit"],"pdf_url":"","comment":"5 pages, 3 figures"},{"id":"http://arxiv.org/abs/2512.18755v1","updated":"2025-12-21T14:43:26Z","published":"2025-12-21T14:43:26Z","title":"MEEA: Mere Exposure Effect-Driven Confrontational Optimization for LLM Jailbreaking","summary":"The rapid advancement of large language models (LLMs) has intensified concerns about the robustness of their safety alignment. While existing jailbreak studies explore both single-turn and multi-turn strategies, most implicitly assume a static safety boundary and fail to account for how contextual interactions dynamically influence model behavior, leading to limited stability and generalization. Motivated by this gap, we propose MEEA (Mere Exposure Effect Attack), a psychology-inspired, fully automated black-box framework for evaluating multi-turn safety robustness, grounded in the mere exposure effect. MEEA leverages repeated low-toxicity semantic exposure to induce a gradual shift in a model's effective safety threshold, enabling progressive erosion of alignment constraints over sustained interactions. Concretely, MEEA constructs semantically progressive prompt chains and optimizes them using a simulated annealing strategy guided by semantic similarity, toxicity, and jailbreak effectiveness. Extensive experiments on both closed-source and open-source models, including GPT-4, Claude-3.5, and DeepSeek-R1, demonstrate that MEEA consistently achieves higher attack success rates than seven representative baselines, with an average Attack Success Rate (ASR) improvement exceeding 20%. Ablation studies further validate the necessity of both annealing-based optimization and contextual exposure mechanisms. Beyond improved attack effectiveness, our findings indicate that LLM safety behavior is inherently dynamic and history-dependent, challenging the common assumption of static alignment boundaries and highlighting the need for interaction-aware safety evaluation and defense mechanisms. Our code is available at: https://github.com/Carney-lsz/MEEA","authors":["Jianyi Zhang","Shizhao Liu","Ziyin Zhou","Zhen Li"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.18748v1","updated":"2025-12-21T14:28:51Z","published":"2025-12-21T14:28:51Z","title":"Code2Doc: A Quality-First Curated Dataset for Code Documentation","summary":"The performance of automatic code documentation generation models depends critically on the quality of the training data used for supervision. However, most existing code documentation datasets are constructed through large scale scraping of public repositories with limited quality control. As a result, they often contain noisy documentation, extensive duplication, and increasing contamination from AI generated content. These issues weaken the supervision signal available to learning-based models and complicate evaluation.\n  We introduce \\textbf{Code2Doc}, a quality-first curated dataset for function-level code documentation generation. Code2Doc consists of 13,358 high-quality function-documentation pairs extracted from widely used open-source repositories spanning five programming languages: Python, Java, TypeScript, JavaScript, and C++. The dataset is constructed using a four-stage curation pipeline that enforces documentation completeness and clarity, filters functions based on structural and complexity criteria, removes exact and near-duplicate code, and identifies documentation likely to be AI generated. Starting from 52,069 extracted candidates, only 25.6 percent satisfy all quality constraints.\n  We provide a detailed analysis of the resulting dataset, which achieves a mean documentation quality score of 6.93 out of 10. Overall, 86.9% of samples contain explicit type annotations, and only 2.9\\% are flagged as potentially AI generated. Baseline experiments show that fine-tuning a large language model on Code2Doc yields relative improvements of 29.47% in BLEU and 24.04% in ROUGE-L over zero shot performance, despite the modest dataset size. We release both the dataset and the full curation pipeline to support reproducible research on automatic code documentation generation.","authors":["Recep Kaan Karaman","Meftun Akarsu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.18747v1","updated":"2025-12-21T14:28:28Z","published":"2025-12-21T14:28:28Z","title":"IPCV: Information-Preserving Compression for MLLM Visual Encoders","summary":"Multimodal Large Language Models (MLLMs) deliver strong vision-language performance but at high computational cost, driven by numerous visual tokens processed by the Vision Transformer (ViT) encoder. Existing token pruning strategies are inadequate: LLM-stage token pruning overlooks the ViT's overhead, while conventional ViT token pruning, without language guidance, risks discarding textually critical visual cues and introduces feature distortions amplified by the ViT's bidirectional attention. To meet these challenges, we propose IPCV, a training-free, information-preserving compression framework for MLLM visual encoders. IPCV enables aggressive token pruning inside the ViT via Neighbor-Guided Reconstruction (NGR) that temporarily reconstructs pruned tokens to participate in attention with minimal overhead, then fully restores them before passing to the LLM. Besides, we introduce Attention Stabilization (AS) to further alleviate the negative influence from token pruning by approximating the K/V of pruned tokens. It can be directly applied to previous LLM-side token pruning methods to enhance their performance. Extensive experiments show that IPCV substantially reduces end-to-end computation and outperforms state-of-the-art training-free token compression methods across diverse image and video benchmarks. Our code is available at https://github.com/Perkzi/IPCV.","authors":["Yuan Chen","Zichen Wen","Yuzhou Wu","Xuyang Liu","Shuang Chen","Junpeng Ma","Weijia Li","Conghui He","Linfeng Zhang"],"pdf_url":"","comment":"13 pages, 6 figures"},{"id":"http://arxiv.org/abs/2512.18737v1","updated":"2025-12-21T13:57:26Z","published":"2025-12-21T13:57:26Z","title":"PIPCFR: Pseudo-outcome Imputation with Post-treatment Variables for Individual Treatment Effect Estimation","summary":"The estimation of individual treatment effects (ITE) focuses on predicting the outcome changes that result from a change in treatment. A fundamental challenge in observational data is that while we need to infer outcome differences under alternative treatments, we can only observe each individual's outcome under a single treatment. Existing approaches address this limitation either by training with inferred pseudo-outcomes or by creating matched instance pairs. However, recent work has largely overlooked the potential impact of post-treatment variables on the outcome. This oversight prevents existing methods from fully capturing outcome variability, resulting in increased variance in counterfactual predictions. This paper introduces Pseudo-outcome Imputation with Post-treatment Variables for Counterfactual Regression (PIPCFR), a novel approach that incorporates post-treatment variables to improve pseudo-outcome imputation. We analyze the challenges inherent in utilizing post-treatment variables and establish a novel theoretical bound for ITE risk that explicitly connects post-treatment variables to ITE estimation accuracy. Unlike existing methods that ignore these variables or impose restrictive assumptions, PIPCFR learns effective representations that preserve informative components while mitigating bias. Empirical evaluations on both real-world and simulated datasets demonstrate that PIPCFR achieves significantly lower ITE errors compared to existing methods.","authors":["Zichuan Lin","Xiaokai Huang","Jiate Liu","Yuxuan Han","Jia Chen","Xiapeng Wu","Deheng Ye"],"pdf_url":"","comment":"17 pages, 3 figures"},{"id":"http://arxiv.org/abs/2503.14428v2","updated":"2025-12-21T13:51:28Z","published":"2025-03-18T17:02:14Z","title":"Comp-Attn: Present-and-Align Attention for Compositional Video Generation","summary":"In the domain of text-to-video (T2V) generation, reliably synthesizing compositional content involving multiple subjects with intricate relations is still underexplored. The main challenges are twofold: 1) Subject presence, where not all subjects can be presented in the video; 2) Inter-subject relations, where the interaction and spatial relationship between subjects are misaligned. Existing methods adopt techniques, such as inference-time latent optimization or layout control, which fail to address both issues simultaneously. To tackle these problems, we propose Comp-Attn, a composition-aware cross-attention variant that follows a Present-and-Align paradigm: it decouples the two challenges by enforcing subject presence at the condition level and achieving relational alignment at the attention-distribution level. Specifically, 1) We introduce Subject-aware Condition Interpolation (SCI) to reinforce subject-specific conditions and ensure each subject's presence; 2) We propose Layout-forcing Attention Modulation (LAM), which dynamically enforces the attention distribution to align with the relational layout of multiple subjects. Comp-Attn can be seamlessly integrated into various T2V baselines in a training-free manner, boosting T2V-CompBench scores by 15.7\\% and 11.7\\% on Wan2.1-T2V-14B and Wan2.2-T2V-A14B with only a 5\\% increase in inference time. Meanwhile, it also achieves strong performance on VBench and T2I-CompBench, demonstrating its scalability in general video generation and compositional text-to-image (T2I) tasks.","authors":["Hongyu Zhang","Yufan Deng","Shenghai Yuan","Yian Zhao","Peng Jin","Xuehan Hou","Chang Liu","Jie Chen"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.18735v1","updated":"2025-12-21T13:50:26Z","published":"2025-12-21T13:50:26Z","title":"$M^3-Verse$: A \"Spot the Difference\" Challenge for Large Multimodal Models","summary":"Modern Large Multimodal Models (LMMs) have demonstrated extraordinary ability in static image and single-state spatial-temporal understanding. However, their capacity to comprehend the dynamic changes of objects within a shared spatial context between two distinct video observations, remains largely unexplored. This ability to reason about transformations within a consistent environment is particularly crucial for advancements in the field of spatial intelligence. In this paper, we introduce $M^3-Verse$, a Multi-Modal, Multi-State, Multi-Dimensional benchmark, to formally evaluate this capability. It is built upon paired videos that provide multi-perspective observations of an indoor scene before and after a state change. The benchmark contains a total of 270 scenes and 2,932 questions, which are categorized into over 50 subtasks that probe 4 core capabilities. We evaluate 16 state-of-the-art LMMs and observe their limitations in tracking state transitions. To address these challenges, we further propose a simple yet effective baseline that achieves significant performance improvements in multi-state perception. $M^3-Verse$ thus provides a challenging new testbed to catalyze the development of next-generation models with a more holistic understanding of our dynamic visual world. You can get the construction pipeline from https://github.com/Wal-K-aWay/M3-Verse_pipeline and full benchmark data from https://www.modelscope.cn/datasets/WalKaWay/M3-Verse.","authors":["Kewei Wei","Bocheng Hu","Jie Cao","Xiaohan Chen","Zhengxi Lu","Wubing Xia","Weili Xu","Jiaao Wu","Junchen He","Mingyu Jia","Ciyun Zhao","Ye Sun","Yizhi Li","Zhonghan Zhao","Jian Zhang","Gaoang Wang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.18733v1","updated":"2025-12-21T13:46:36Z","published":"2025-12-21T13:46:36Z","title":"Explainable and Fine-Grained Safeguarding of LLM Multi-Agent Systems via Bi-Level Graph Anomaly Detection","summary":"Large language model (LLM)-based multi-agent systems (MAS) have shown strong capabilities in solving complex tasks. As MAS become increasingly autonomous in various safety-critical tasks, detecting malicious agents has become a critical security concern. Although existing graph anomaly detection (GAD)-based defenses can identify anomalous agents, they mainly rely on coarse sentence-level information and overlook fine-grained lexical cues, leading to suboptimal performance. Moreover, the lack of interpretability in these methods limits their reliability and real-world applicability. To address these limitations, we propose XG-Guard, an explainable and fine-grained safeguarding framework for detecting malicious agents in MAS. To incorporate both coarse and fine-grained textual information for anomalous agent identification, we utilize a bi-level agent encoder to jointly model the sentence- and token-level representations of each agent. A theme-based anomaly detector further captures the evolving discussion focus in MAS dialogues, while a bi-level score fusion mechanism quantifies token-level contributions for explanation. Extensive experiments across diverse MAS topologies and attack scenarios demonstrate robust detection performance and strong interpretability of XG-Guard.","authors":["Junjun Pan","Yixin Liu","Rui Miao","Kaize Ding","Yu Zheng","Quoc Viet Hung Nguyen","Alan Wee-Chung Liew","Shirui Pan"],"pdf_url":"","comment":"14 pages, 3 tables, 5 figures"},{"id":"http://arxiv.org/abs/2512.18732v1","updated":"2025-12-21T13:39:00Z","published":"2025-12-21T13:39:00Z","title":"Counterfactual Basis Extension and Representational Geometry: An MDL-Constrained Model of Conceptual Growth","summary":"Concept learning becomes possible only when existing representations fail to account for experience. Most models of learning and inference, however, presuppose a fixed representational basis within which belief updating occurs. In this paper, I address a prior question: under what structural conditions can the representational basis itself expand in a principled and selective way?\n  I propose a geometric framework in which conceptual growth is modeled as admissible basis extension evaluated under a Minimum Description Length (MDL) criterion. Experience, whether externally observed or internally simulated, is represented as vectors relative to a current conceptual subspace. Residual components capture systematic representational failure, and candidate conceptual extensions are restricted to low-rank, admissible transformations. I show that any MDL-accepted extension can be chosen so that its novel directions lie entirely within the residual span induced by experience, while extensions orthogonal to this span strictly increase description length and are therefore rejected.\n  This yields a conservative account of imagination and conceptual innovation. Internally generated counterfactual representations contribute to learning only insofar as they expose or amplify structured residual error, and cannot introduce arbitrary novelty. I further distinguish representational counterfactuals--counterfactuals over an agent's conceptual basis--from causal or value-level counterfactuals, and show how MDL provides a normative selection principle governing representational change.\n  Overall, the framework characterizes conceptual development as an error-driven, geometry-constrained process of basis extension, clarifying both the role and the limits of imagination in learning and theory change.","authors":["Chainarong Amornbunchornvej"],"pdf_url":"","comment":"First draft"},{"id":"http://arxiv.org/abs/2512.06951v2","updated":"2025-12-21T13:33:17Z","published":"2025-12-07T18:08:45Z","title":"Task adaptation of Vision-Language-Action model: 1st Place Solution for the 2025 BEHAVIOR Challenge","summary":"We present a vision-action policy that won 1st place in the 2025 BEHAVIOR Challenge - a large-scale benchmark featuring 50 diverse long-horizon household tasks in photo-realistic simulation, requiring bimanual manipulation, navigation, and context-aware decision making. Building on the Pi0.5 architecture, we introduce several innovations. Our primary contribution is correlated noise for flow matching, which improves training efficiency and enables correlation-aware inpainting for smooth action sequences. We also apply learnable mixed-layer attention and System 2 stage tracking for ambiguity resolution. Training employs multi-sample flow matching to reduce variance, while inference uses action compression and challenge-specific correction rules. Our approach achieves 26% q-score across all 50 tasks on both public and private leaderboards.","authors":["Ilia Larchenko","Gleb Zarin","Akash Karnatak"],"pdf_url":"","comment":"2025 NeurIPS Behavior Challenge 1st place solution"},{"id":"http://arxiv.org/abs/2505.15662v2","updated":"2025-12-21T13:30:03Z","published":"2025-05-21T15:38:55Z","title":"Transformer-Based Neural Quantum Digital Twins for Many-Body Quantum Simulation and Optimal Annealing Schedule Design","summary":"We introduce Transformer-based Neural Quantum Digital Twins (Tx-NQDTs) to simulate full adiabatic dynamics of many-body quantum systems, including ground and low-lying excited states, at low computational cost. Tx-NQDTs employ a graph-informed Transformer neural network trained to predict spectral properties (energy levels and gap locations) needed for annealing schedule design. We integrate these predictions with an adaptive annealing schedule design based on first-order adiabatic perturbation theory (FOAPT), which slows the evolution near predicted small gaps to maintain adiabaticity. Experiments on a D-Wave quantum annealer (N = 10, 15, 20 qubits, 12 control segments) show that Tx-NQDT-informed schedules significantly improve success probabilities despite hardware noise and calibration drift. The optimized schedules achieve success probabilities 2.2-11.7 percentage points higher than the default linear schedule, outperforming the D-Wave baseline in 44 of 60 cases. These results demonstrate a practical, data-driven route to improved quantum annealing performance on real hardware.","authors":["Jianlong Lu","Hanqiu Peng","Ying Chen"],"pdf_url":"","comment":"23 pages, 10 figures, 2 tables"},{"id":"http://arxiv.org/abs/2509.16958v2","updated":"2025-12-21T12:58:34Z","published":"2025-09-21T07:41:49Z","title":"Quantum Abduction: A New Paradigm for Reasoning under Uncertainty","summary":"Abductive reasoning - the search for plausible explanations - has long been central to human inquiry, from forensics to medicine and scientific discovery. Yet formal approaches in AI have largely reduced abduction to eliminative search: hypotheses are treated as mutually exclusive, evaluated against consistency constraints or probability updates, and pruned until a single \"best\" explanation remains. This reductionist framing overlooks the way human reasoners sustain multiple explanatory lines in suspension, navigate contradictions, and generate novel syntheses. This paper introduces quantum abduction, a non-classical paradigm that models hypotheses in superposition, allows them to interfere constructively or destructively, and collapses only when coherence with evidence is reached. Grounded in quantum cognition and implemented with modern NLP embeddings and generative AI, the framework supports dynamic synthesis rather than premature elimination. Case studies span historical mysteries (Ludwig II of Bavaria, the \"Monster of Florence\"), literary demonstrations (\"Murder on the Orient Express\"), medical diagnosis, and scientific theory change. Across these domains, quantum abduction proves more faithful to the constructive and multifaceted nature of human reasoning, while offering a pathway toward expressive and transparent AI reasoning systems.","authors":["Remo Pareschi"],"pdf_url":"","comment":"23 pages, 8 figures, 3 tables; submitted to Sci, MDPI"}],"Computation and Language":[{"id":"http://arxiv.org/abs/2512.18906v1","updated":"2025-12-21T22:37:38Z","published":"2025-12-21T22:37:38Z","title":"Remedy-R: Generative Reasoning for Machine Translation Evaluation without Error Annotations","summary":"Over the years, automatic MT metrics have hillclimbed benchmarks and presented strong and sometimes human-level agreement with human ratings. Yet they remain black-box, offering little insight into their decision-making and often failing under real-world out-of-distribution (OOD) inputs. We introduce Remedy-R, a reasoning-driven generative MT metric trained with reinforcement learning from pairwise translation preferences, without requiring error-span annotations or distillation from closed LLMs. Remedy-R produces step-by-step analyses of accuracy, fluency, and completeness, followed by a final score, enabling more interpretable assessments. With only 60K training pairs across two language pairs, Remedy-R remains competitive with top scalar metrics and GPT-4-based judges on WMT22-24 meta-evaluation, generalizes to other languages, and exhibits strong robustness on OOD stress tests. Moreover, Remedy-R models generate self-reflective feedback that can be reused for translation improvement. Building on this finding, we introduce Remedy-R Agent, a simple evaluate-revise pipeline that leverages Remedy-R's evaluation analysis to refine translations. This agent consistently improves translation quality across diverse models, including Qwen2.5, ALMA-R, GPT-4o-mini, and Gemini-2.0-Flash, suggesting that Remedy-R's reasoning captures translation-relevant information and is practically useful.","authors":["Shaomu Tan","Ryosuke Mitani","Ritvik Choudhary","Qiyu Wu","Toshiyuki Sekiya","Christof Monz"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2511.02376v3","updated":"2025-12-21T22:30:20Z","published":"2025-11-04T08:56:28Z","title":"AutoAdv: Automated Adversarial Prompting for Multi-Turn Jailbreaking of Large Language Models","summary":"Large Language Models (LLMs) remain vulnerable to jailbreaking attacks where adversarial prompts elicit harmful outputs. Yet most evaluations focus on single-turn interactions while real-world attacks unfold through adaptive multi-turn conversations. We present AutoAdv, a training-free framework for automated multi-turn jailbreaking that achieves an attack success rate of up to 95% on Llama-3.1-8B within six turns, a 24% improvement over single-turn baselines. AutoAdv uniquely combines three adaptive mechanisms: a pattern manager that learns from successful attacks to enhance future prompts, a temperature manager that dynamically adjusts sampling parameters based on failure modes, and a two-phase rewriting strategy that disguises harmful requests and then iteratively refines them. Extensive evaluation across commercial and open-source models (Llama-3.1-8B, GPT-4o mini, Qwen3-235B, Mistral-7B) reveals persistent vulnerabilities in current safety mechanisms, with multi-turn attacks consistently outperforming single-turn approaches. These findings demonstrate that alignment strategies optimized for single-turn interactions fail to maintain robustness across extended conversations, highlighting an urgent need for multi-turn-aware defenses.","authors":["Aashray Reddy","Andrew Zagula","Nicholas Saban"],"pdf_url":"","comment":"Presented at NeurIPS 2025 Lock-LLM Workshop. Code is available at https://github.com/AAN-AutoAdv/AutoAdv"},{"id":"http://arxiv.org/abs/2011.03783v2","updated":"2025-12-21T21:38:22Z","published":"2020-11-07T14:28:54Z","title":"Towards a resource for multilingual lexicons: an MT assisted and human-in-the-loop multilingual parallel corpus with multi-word expression annotation","summary":"In this work, we introduce the construction of a machine translation (MT) assisted and human-in-the-loop multilingual parallel corpus with annotations of multi-word expressions (MWEs), named AlphaMWE. The MWEs include verbal MWEs (vMWEs) defined in the PARSEME shared task that have a verb as the head of the studied terms. The annotated vMWEs are also bilingually and multilingually aligned manually. The languages covered include Arabic, Chinese, English, German, Italian, and Polish, of which, the Arabic corpus includes both standard and dialectal variations from Egypt and Tunisia. Our original English corpus is extracted from the PARSEME shared task in 2018. We performed machine translation of this source corpus followed by human post-editing and annotation of target MWEs. Strict quality control was applied for error limitation, i.e., each MT output sentence received first manual post-editing and annotation plus a second manual quality rechecking till annotators' consensus is reached. One of our findings during corpora preparation is that accurate translation of MWEs presents challenges to MT systems, as reflected by the outcomes of human-in-the-loop metric HOPE. To facilitate further MT research, we present a categorisation of the error types encountered by MT systems in performing MWE-related translation. To acquire a broader view of MT issues, we selected four popular state-of-the-art MT systems for comparison, namely Microsoft Bing Translator, GoogleMT, Baidu Fanyi, and DeepL MT. Because of the noise removal, translation post-editing, and MWE annotation by human professionals, we believe the AlphaMWE data set will be an asset for both monolingual and cross-lingual research, such as multi-word term lexicography, MT, and information extraction.","authors":["Lifeng Han","Najet Hadj Mohamed","Malak Rassem","Gareth Jones","Alan Smeaton","Goran Nenadic"],"pdf_url":"","comment":"Accepted by Journal of LRE, extended work from WS paper AlphaMWE"},{"id":"http://arxiv.org/abs/2512.18880v1","updated":"2025-12-21T20:41:36Z","published":"2025-12-21T20:41:36Z","title":"Can LLMs Estimate Student Struggles? Human-AI Difficulty Alignment with Proficiency Simulation for Item Difficulty Prediction","summary":"Accurate estimation of item (question or task) difficulty is critical for educational assessment but suffers from the cold start problem. While Large Language Models demonstrate superhuman problem-solving capabilities, it remains an open question whether they can perceive the cognitive struggles of human learners. In this work, we present a large-scale empirical analysis of Human-AI Difficulty Alignment for over 20 models across diverse domains such as medical knowledge and mathematical reasoning. Our findings reveal a systematic misalignment where scaling up model size is not reliably helpful; instead of aligning with humans, models converge toward a shared machine consensus. We observe that high performance often impedes accurate difficulty estimation, as models struggle to simulate the capability limitations of students even when being explicitly prompted to adopt specific proficiency levels. Furthermore, we identify a critical lack of introspection, as models fail to predict their own limitations. These results suggest that general problem-solving capability does not imply an understanding of human cognitive struggles, highlighting the challenge of using current models for automated difficulty prediction.","authors":["Ming Li","Han Chen","Yunze Xiao","Jian Chen","Hong Jiao","Tianyi Zhou"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.18865v1","updated":"2025-12-21T19:43:30Z","published":"2025-12-21T19:43:30Z","title":"Application of deep learning approaches for medieval historical documents transcription","summary":"Handwritten text recognition and optical character recognition solutions show excellent results with processing data of modern era, but efficiency drops with Latin documents of medieval times. This paper presents a deep learning method to extract text information from handwritten Latin-language documents of the 9th to 11th centuries. The approach takes into account the properties inherent in medieval documents. The paper provides a brief introduction to the field of historical document transcription, a first-sight analysis of the raw data, and the related works and studies. The paper presents the steps of dataset development for further training of the models. The explanatory data analysis of the processed data is provided as well. The paper explains the pipeline of deep learning models to extract text information from the document images, from detecting objects to word recognition using classification models and embedding word images. The paper reports the following results: recall, precision, F1 score, intersection over union, confusion matrix, and mean string distance. The plots of the metrics are also included. The implementation is published on the GitHub repository.","authors":["Maksym Voloshchuk","Bohdana Zarembovska","Mykola Kozlenko"],"pdf_url":"","comment":"15 pages, 15 figures, 4 tables. Originally published by CEUR Workshop Proceedings (CEUR-WS.org, ISSN 1613-0073), available: https://ceur-ws.org/Vol-4133/S_05_Kozlenko.pdf"},{"id":"http://arxiv.org/abs/2512.18861v1","updated":"2025-12-21T19:26:41Z","published":"2025-12-21T19:26:41Z","title":"Merge on workspaces as Hopf algebra Markov chain","summary":"We study the dynamical properties of a Hopf algebra Markov chain with state space the binary rooted forests with labelled leaves. This Markovian dynamical system describes the core computational process of structure formation and transformation in syntax via the Merge operation, according to Chomsky's Minimalism model of generative linguistics. The dynamics decomposes into an ergodic dynamical system with uniform stationary distribution, given by the action of Internal Merge, while the contributions of External Merge and (a minimal form of) Sideward Merge reduce to a simpler Markov chain with state space the set of partitions and with combinatorial weights. The Sideward Merge part of the dynamics prevents convergence to fully formed connected structures (trees), unless the different forms of Merge are weighted by a cost function, as predicted by linguistic theory. Results on the asymptotic behavior of the Perron-Frobenius eigenvalue and eigenvector in this weighted case, obtained in terms of an associated Perron-Frobenius problem in the tropical semiring, show that the usual cost functions (Minimal Search and Resource Restrictions) proposed in the linguistic literature do not suffice to obtain convergence to the tree structures, while an additional optimization property based on the Shannon entropy achieves the expected result for the dynamics. We also comment on the introduction of continuous parameters related to semantic embedding and other computational models, and also on some filtering of the dynamics by coloring rules that model the linguistic filtering by theta roles and phase structure, and on parametric variation and the process of parameter setting in Externalization.","authors":["Matilde Marcolli","David Skigin"],"pdf_url":"","comment":"80 pages, LaTeX, 1 png figure"},{"id":"http://arxiv.org/abs/2512.18859v1","updated":"2025-12-21T19:16:40Z","published":"2025-12-21T19:16:40Z","title":"Toward Human-Centered AI-Assisted Terminology Work","summary":"The rapid diffusion of generative artificial intelligence is transforming terminology work. While this technology promises gains in efficiency, its unstructured adoption risks weakening professional autonomy, amplifying bias, and eroding linguistic and conceptual diversity. This paper argues that a human-centered approach to artificial intelligence has become a necessity for terminology work. Building on research in artificial intelligence and translation studies, it proposes a human-centered framework that conceptualizes artificial intelligence as a means of amplifying the terminologist's capabilities, rather than replacing them. The framework is organized around three interrelated dimensions: the augmented terminologist, ethical AI, and human-centered design. Together, these dimensions emphasize the compatibility of high automation with strong human control, the central role of terminologists in bias mitigation, and the importance of designing AI tools and workflows around the needs, values, and well-being of the terminologist. The paper concludes by stressing that current choices in AI adoption will shape not only terminological practice, but also the preservation of accuracy, adequacy, and diversity in terminology and specialized knowledge.","authors":["Antonio San Martin"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.18841v1","updated":"2025-12-21T18:11:24Z","published":"2025-12-21T18:11:24Z","title":"MDToC: Metacognitive Dynamic Tree of Concepts for Boosting Mathematical Problem-Solving of Large Language Models","summary":"Despite advances in mathematical reasoning capabilities, Large Language Models (LLMs) still struggle with calculation verification when using established prompting techniques. We present MDToC (Metacognitive Dynamic Tree of Concepts), a three-phase approach that constructs a concept tree, develops accuracy-verified calculations for each concept, and employs majority voting to evaluate competing solutions. Evaluations across CHAMP, MATH, and Game-of-24 benchmarks demonstrate our MDToC's effectiveness, with GPT-4-Turbo achieving 58.1\\% on CHAMP, 86.6\\% on MATH, and 85\\% on Game-of-24 - outperforming GoT by 5\\%, 5.4\\%, and 4\\% on all these tasks, respectively, without hand-engineered hints. MDToC consistently surpasses existing prompting methods across all backbone models, yielding improvements of up to 7.6\\% over ToT and 6.2\\% over GoT, establishing metacognitive calculation verification as a promising direction for enhanced mathematical reasoning.","authors":["Tung Duong Ta","Tim Oates"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.18834v1","updated":"2025-12-21T17:36:26Z","published":"2025-12-21T17:36:26Z","title":"AraMix: Recycling, Refiltering, and Deduplicating to Deliver the Largest Arabic Pretraining Corpus","summary":"We present AraMix, a deduplicated Arabic pretraining corpus containing approximately 178 billion tokens across 179 million documents. Rather than scraping the web again, AraMix demonstrates that substantial value lies in systematically reusing and curating existing pretraining datasets: we combine seven publicly available Arabic web datasets, apply quality filtering designed specifically for Arabic text to re-filter some datasets, and perform cross-dataset deduplication, both MinHash and sentence-level. This approach reveals that nearly 60% of tokens across these independently collected corpora are duplicates, redundancy that any new scraping efforts will reproduce. Our work suggests that for lower resource languages, investment in curation pipelines for existing data yields greater returns than additional web crawls, an approach that allowed us to curate the largest heavily filtered publicly available Arabic pretraining corpus.","authors":["Sultan Alrashed","Francesco Orabona"],"pdf_url":"","comment":"Initial version, without pretraining experiments"},{"id":"http://arxiv.org/abs/2512.07543v2","updated":"2025-12-21T17:33:45Z","published":"2025-12-08T13:24:53Z","title":"Over-representation of phonological features in basic vocabulary doesn't replicate when controlling for spatial and phylogenetic effects","summary":"The statistical over-representation of phonological features in the basic vocabulary of languages is often interpreted as reflecting potentially universal sound symbolic patterns. However, most of those results have not been tested explicitly for reproducibility and might be prone to biases in the study samples or models. Many studies on the topic do not adequately control for genealogical and areal dependencies between sampled languages, casting doubts on the robustness of the results. In this study, we test the robustness of a recent study on sound symbolism of basic vocabulary concepts which analyzed 245 languages.The new sample includes data on 2864 languages from Lexibank. We modify the original model by adding statistical controls for spatial and phylogenetic dependencies between languages. The new results show that most of the previously observed patterns are not robust, and in fact many patterns disappear completely when adding the genealogical and areal controls. A small number of patterns, however, emerges as highly stable even with the new sample. Through the new analysis, we are able to assess the distribution of sound symbolism on a larger scale than previously. The study further highlights the need for testing all universal claims on language for robustness on various levels.","authors":["Frederic Blum"],"pdf_url":"","comment":"Accepted with minor revisions at *Linguistic Typology*, expected to be fully published in 2026"},{"id":"http://arxiv.org/abs/2512.18832v1","updated":"2025-12-21T17:28:42Z","published":"2025-12-21T17:28:42Z","title":"From Word to World: Can Large Language Models be Implicit Text-based World Models?","summary":"Agentic reinforcement learning increasingly relies on experience-driven scaling, yet real-world environments remain non-adaptive, limited in coverage, and difficult to scale. World models offer a potential way to improve learning efficiency through simulated experience, but it remains unclear whether large language models can reliably serve this role and under what conditions they meaningfully benefit agents. We study these questions in text-based environments, which provide a controlled setting to reinterpret language modeling as next-state prediction under interaction. We introduce a three-level framework for evaluating LLM-based world models: (i) fidelity and consistency, (ii) scalability and robustness, and (iii) agent utility. Across five representative environments, we find that sufficiently trained world models maintain coherent latent state, scale predictably with data and model size, and improve agent performance via action verification, synthetic trajectory generation, and warm-starting reinforcement learning. Meanwhile, these gains depend critically on behavioral coverage and environment complexity, delineating clear boundry on when world modeling effectively supports agent learning.","authors":["Yixia Li","Hongru Wang","Jiahao Qiu","Zhenfei Yin","Dongdong Zhang","Cheng Qian","Zeping Li","Pony Ma","Guanhua Chen","Heng Ji","Mengdi Wang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.16912v2","updated":"2025-12-21T17:23:35Z","published":"2025-12-18T18:59:27Z","title":"Exploration vs Exploitation: Rethinking RLVR through Clipping, Entropy, and Spurious Reward","summary":"This paper examines the exploration-exploitation trade-off in reinforcement learning with verifiable rewards (RLVR), a framework for improving the reasoning of Large Language Models (LLMs). Recent studies suggest that RLVR can elicit strong mathematical reasoning in LLMs through two seemingly paradoxical mechanisms: spurious rewards, which suppress exploitation by rewarding outcomes unrelated to the ground truth, and entropy minimization, which suppresses exploration by pushing the model toward more confident and deterministic outputs, highlighting a puzzling dynamic: both discouraging exploitation and discouraging exploration improve reasoning performance, yet the underlying principles that reconcile these effects remain poorly understood. We focus on two fundamental questions: (i) how policy entropy relates to performance, and (ii) whether spurious rewards yield gains, potentially through the interplay of clipping bias and model contamination. Our results show that clipping bias under spurious rewards reduces policy entropy, leading to more confident and deterministic outputs, while entropy minimization alone is insufficient for improvement. We further propose a reward-misalignment model explaining why spurious rewards can enhance performance beyond contaminated settings. Our findings clarify the mechanisms behind spurious-reward benefits and provide principles for more effective RLVR training.","authors":["Peter Chen","Xiaopeng Li","Ziniu Li","Wotao Yin","Xi Chen","Tianyi Lin"],"pdf_url":"","comment":"35 pages"},{"id":"http://arxiv.org/abs/2506.11112v2","updated":"2025-12-21T17:16:19Z","published":"2025-06-08T16:25:35Z","title":"Dagstuhl Perspectives Workshop 24352 -- Conversational Agents: A Framework for Evaluation (CAFE): Manifesto","summary":"During the workshop, we deeply discussed what CONversational Information ACcess (CONIAC) is and its unique features, proposing a world model abstracting it, and defined the Conversational Agents Framework for Evaluation (CAFE) for the evaluation of CONIAC systems, consisting of six major components: 1) goals of the system's stakeholders, 2) user tasks to be studied in the evaluation, 3) aspects of the users carrying out the tasks, 4) evaluation criteria to be considered, 5) evaluation methodology to be applied, and 6) measures for the quantitative criteria chosen.","authors":["Christine Bauer","Li Chen","Nicola Ferro","Norbert Fuhr","Avishek Anand","Timo Breuer","Guglielmo Faggioli","Ophir Frieder","Hideo Joho","Jussi Karlgren","Johannes Kiesel","Bart P. Knijnenburg","Aldo Lipani","Lien Michiels","Andrea Papenmeier","Maria Soledad Pera","Mark Sanderson","Scott Sanner","Benno Stein","Johanne R. Trippas","Karin Verspoor","Martijn C Willemsen"],"pdf_url":"","comment":"10 figures; Dagstuhl Manifestos, 11(1), pp 19-67. DOI: 10.4230/DagMan.11.1.19"},{"id":"http://arxiv.org/abs/2511.17559v2","updated":"2025-12-21T16:56:46Z","published":"2025-11-13T06:35:29Z","title":"SCARE: A Benchmark for SQL Correction and Question Answerability Classification for Reliable EHR Question Answering","summary":"Recent advances in Large Language Models (LLMs) have enabled the development of text-to-SQL models that allow clinicians to query structured data stored in Electronic Health Records (EHRs) using natural language. However, deploying these models for EHR question answering (QA) systems in safety-critical clinical environments remains challenging: incorrect SQL queries-whether caused by model errors or problematic user inputs-can undermine clinical decision-making and jeopardize patient care. While prior work has mainly focused on improving SQL generation accuracy or filtering questions before execution, there is a lack of a unified benchmark for evaluating independent post-hoc verification mechanisms (i.e., a component that inspects and validates the generated SQL before execution), which is crucial for safe deployment. To fill this gap, we introduce SCARE, a benchmark for evaluating methods that function as a post-hoc safety layer in EHR QA systems. SCARE evaluates the joint task of (1) classifying question answerability (i.e., determining whether a question is answerable, ambiguous, or unanswerable) and (2) verifying or correcting candidate SQL queries. The benchmark comprises 4,200 triples of questions, candidate SQL queries, and expected model outputs, grounded in the MIMIC-III, MIMIC-IV, and eICU databases. It covers a diverse set of questions and corresponding candidate SQL queries generated by seven different text-to-SQL models, ensuring a realistic and challenging evaluation. Using SCARE, we benchmark a range of approaches-from two-stage methods to agentic frameworks. Our experiments reveal a critical trade-off between question classification and SQL error correction, highlighting key challenges and outlining directions for future research.","authors":["Gyubok Lee","Woosog Chay","Edward Choi"],"pdf_url":"","comment":"ML4H 2025 Proceedings"},{"id":"http://arxiv.org/abs/2509.09284v3","updated":"2025-12-21T16:15:54Z","published":"2025-09-11T09:18:07Z","title":"Tree-OPO: Off-policy Monte Carlo Tree-Guided Advantage Optimization for Multistep Reasoning","summary":"Recent advances in reasoning with large language models (LLMs) have shown the effectiveness of Monte Carlo Tree Search (MCTS) for generating high quality intermediate trajectories, particularly in math and symbolic domains. Inspired by this, we explore how MCTS derived trajectories, traditionally used for training value or reward models, can be repurposed to improve policy optimization in verifier guided reinforcement learning (RL). Specifically, we focus on Group Relative Policy Optimization (GRPO), a recent algorithm that enables consistent policy learning from group relative judgments. We reframe GRPO into a staged training paradigm, leveraging a teacher's MCTS rollouts to construct a tree structured curriculum of prefixes. This introduces the novel challenge of computing advantages for training samples that originate from different prefixes, each with a distinct expected return. To address this, we propose Staged Advantage Estimation (SAE), a framework for computing low variance, prefix aware advantages by projecting rewards onto a constraint set that respects the tree's hierarchy. Our empirical results on mathematical reasoning tasks show that SAE improves final accuracy over standard GRPO. This outcome is grounded in our theoretical analysis, which confirms that SAE reduces gradient variance, a principled path to improved sample efficiency. We demonstrate this through practical SAE implementations, comparing efficient heuristics against a formal quadratic program.","authors":["Bingning Huang","Tu Nguyen","Matthieu Zimmer"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.18779v1","updated":"2025-12-21T15:46:33Z","published":"2025-12-21T15:46:33Z","title":"From Natural Language to Control Signals: A Conceptual Framework for Semantic Channel Finding in Complex Experimental Infrastructure","summary":"Modern experimental platforms such as particle accelerators, fusion devices, telescopes, and industrial process control systems expose tens to hundreds of thousands of control and diagnostic channels accumulated over decades of evolution. Operators and AI systems rely on informal expert knowledge, inconsistent naming conventions, and fragmented documentation to locate signals for monitoring, troubleshooting, and automated control, creating a persistent bottleneck for reliability, scalability, and language-model-driven interfaces. We formalize semantic channel finding-mapping natural-language intent to concrete control-system signals-as a general problem in complex experimental infrastructure, and introduce a four-paradigm framework to guide architecture selection across facility-specific data regimes. The paradigms span (i) direct in-context lookup over curated channel dictionaries, (ii) constrained hierarchical navigation through structured trees, (iii) interactive agent exploration using iterative reasoning and tool-based database queries, and (iv) ontology-grounded semantic search that decouples channel meaning from facility-specific naming conventions. We demonstrate each paradigm through proof-of-concept implementations at four operational facilities spanning two orders of magnitude in scale-from compact free-electron lasers to large synchrotron light sources-and diverse control-system architectures, from clean hierarchies to legacy environments. These implementations achieve 90-97% accuracy on expert-curated operational queries.","authors":["Thorsten Hellert","Nikolay Agladze","Alex Giovannone","Jan Jug","Frank Mayet","Mark Sherwin","Antonin Sulc","Chris Tennant"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2507.10216v2","updated":"2025-12-21T15:41:27Z","published":"2025-07-14T12:33:07Z","title":"From Words to Proverbs: Evaluating LLMs Linguistic and Cultural Competence in Saudi Dialects with Absher","summary":"As large language models (LLMs) become increasingly central to Arabic NLP applications, evaluating their understanding of regional dialects and cultural nuances is essential, particularly in linguistically diverse settings like Saudi Arabia. This paper introduces Absher, a comprehensive benchmark specifically designed to assess LLMs performance across major Saudi dialects. \\texttt{Absher} comprises over 18,000 multiple-choice questions spanning six distinct categories: Meaning, True/False, Fill-in-the-Blank, Contextual Usage, Cultural Interpretation, and Location Recognition. These questions are derived from a curated dataset of dialectal words, phrases, and proverbs sourced from various regions of Saudi Arabia. We evaluate several state-of-the-art LLMs, including multilingual and Arabic-specific models. We also provide detailed insights into their capabilities and limitations. Our results reveal notable performance gaps, particularly in tasks requiring cultural inference or contextual understanding. Our findings highlight the urgent need for dialect-aware training and culturally aligned evaluation methodologies to improve LLMs performance in real-world Arabic applications.","authors":["Renad Al-Monef","Hassan Alhuzali","Nora Alturayeif","Ashwag Alasmari"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.06713v2","updated":"2025-12-21T15:06:24Z","published":"2025-12-07T08:03:43Z","title":"Look Twice before You Leap: A Rational Agent Framework for Localized Adversarial Anonymization","summary":"Current LLM-based text anonymization frameworks usually rely on remote API services from powerful LLMs, which creates an inherent privacy paradox: users must disclose data to untrusted third parties for guaranteed privacy preservation. Moreover, directly migrating current solutions to local small-scale models (LSMs) offers a suboptimal solution with severe utility collapse. Our work argues that this failure stems not merely from the capability deficits of LSMs, but significantly from the inherent irrationality of the greedy adversarial strategies employed by current state-of-the-art (SOTA) methods. To address this, we propose Rational Localized Adversarial Anonymization (RLAA), a fully localized and training-free framework featuring an Attacker-Arbitrator-Anonymizer architecture. We model the anonymization process as a trade-off between Marginal Privacy Gain (MPG) and Marginal Utility Cost (MUC), and demonstrate that greedy strategies tend to drift into an irrational state. Instead, RLAA introduces an arbitrator that acts as a rationality gatekeeper, validating the attacker's inference to filter out feedback providing negligible privacy benefits. This mechanism promotes a rational early-stopping criterion, and structurally prevents utility collapse. Extensive experiments on different benchmarks demonstrate that RLAA achieves a superior privacy-utility trade-off compared to strong baselines.","authors":["Donghang Duan","Xu Zheng","Yuefeng He","Chong Mu","Leyi Cai","Lizong Zhang"],"pdf_url":"","comment":"17 pages, 9 figures, 6 tables. Revised version with an updated author list, expanded experimental results and analysis"},{"id":"http://arxiv.org/abs/2510.17602v2","updated":"2025-12-21T14:45:13Z","published":"2025-10-20T14:50:58Z","title":"LexChain: Modeling Legal Reasoning Chains for Chinese Tort Case Analysis","summary":"Legal reasoning is a fundamental component of legal analysis and decision-making. Existing computational approaches to legal reasoning predominantly rely on generic reasoning frameworks such as syllogism, which do not comprehensively examine the nuanced process of legal reasoning. Moreover, current research has largely focused on criminal cases, with insufficient modeling for civil cases. In this work, we present a novel framework to explicitly model legal reasoning in the analysis of Chinese tort-related civil cases. We first operationalize the legal reasoning process in tort analysis into the three-module LexChain framework, with each module consisting of multiple finer-grained sub-steps. Informed by the LexChain framework, we introduce the task of tort legal reasoning and construct an evaluation benchmark to systematically assess the critical steps within analytical reasoning chains for tort analysis. Leveraging this benchmark, we evaluate existing large language models for their legal reasoning ability in civil tort contexts. Our results indicate that current models still fall short in accurately handling crucial elements of tort legal reasoning. Furthermore, we introduce several baseline approaches that explicitly incorporate LexChain-style reasoning through prompting or post-training. The proposed baselines achieve significant improvements in tort-related legal reasoning and generalize well to related legal analysis tasks, demonstrating the value of explicitly modeling legal reasoning chains to enhance the reasoning capabilities of language models.","authors":["Huiyuan Xie","Chenyang Li","Huining Zhu","Chubin Zhang","Yuxiao Ye","Zhenghao Liu","Zhiyuan Liu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.18748v1","updated":"2025-12-21T14:28:51Z","published":"2025-12-21T14:28:51Z","title":"Code2Doc: A Quality-First Curated Dataset for Code Documentation","summary":"The performance of automatic code documentation generation models depends critically on the quality of the training data used for supervision. However, most existing code documentation datasets are constructed through large scale scraping of public repositories with limited quality control. As a result, they often contain noisy documentation, extensive duplication, and increasing contamination from AI generated content. These issues weaken the supervision signal available to learning-based models and complicate evaluation.\n  We introduce \\textbf{Code2Doc}, a quality-first curated dataset for function-level code documentation generation. Code2Doc consists of 13,358 high-quality function-documentation pairs extracted from widely used open-source repositories spanning five programming languages: Python, Java, TypeScript, JavaScript, and C++. The dataset is constructed using a four-stage curation pipeline that enforces documentation completeness and clarity, filters functions based on structural and complexity criteria, removes exact and near-duplicate code, and identifies documentation likely to be AI generated. Starting from 52,069 extracted candidates, only 25.6 percent satisfy all quality constraints.\n  We provide a detailed analysis of the resulting dataset, which achieves a mean documentation quality score of 6.93 out of 10. Overall, 86.9% of samples contain explicit type annotations, and only 2.9\\% are flagged as potentially AI generated. Baseline experiments show that fine-tuning a large language model on Code2Doc yields relative improvements of 29.47% in BLEU and 24.04% in ROUGE-L over zero shot performance, despite the modest dataset size. We release both the dataset and the full curation pipeline to support reproducible research on automatic code documentation generation.","authors":["Recep Kaan Karaman","Meftun Akarsu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.18746v1","updated":"2025-12-21T14:26:14Z","published":"2025-12-21T14:26:14Z","title":"MemEvolve: Meta-Evolution of Agent Memory Systems","summary":"Self-evolving memory systems are unprecedentedly reshaping the evolutionary paradigm of large language model (LLM)-based agents. Prior work has predominantly relied on manually engineered memory architectures to store trajectories, distill experience, and synthesize reusable tools, enabling agents to evolve on the fly within environment interactions. However, this paradigm is fundamentally constrained by the staticity of the memory system itself: while memory facilitates agent-level evolving, the underlying memory architecture cannot be meta-adapted to diverse task contexts. To address this gap, we propose MemEvolve, a meta-evolutionary framework that jointly evolves agents' experiential knowledge and their memory architecture, allowing agent systems not only to accumulate experience but also to progressively refine how they learn from it. To ground MemEvolve in prior research and foster openness in future self-evolving systems, we introduce EvolveLab, a unified self-evolving memory codebase that distills twelve representative memory systems into a modular design space (encode, store, retrieve, manage), providing both a standardized implementation substrate and a fair experimental arena. Extensive evaluations on four challenging agentic benchmarks demonstrate that MemEvolve achieves (I) substantial performance gains, improving frameworks such as SmolAgent and Flash-Searcher by up to $17.06\\%$; and (II) strong cross-task and cross-LLM generalization, designing memory architectures that transfer effectively across diverse benchmarks and backbone models.","authors":["Guibin Zhang","Haotian Ren","Chong Zhan","Zhenhong Zhou","Junhao Wang","He Zhu","Wangchunshu Zhou","Shuicheng Yan"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.18745v1","updated":"2025-12-21T14:23:07Z","published":"2025-12-21T14:23:07Z","title":"InSight-o3: Empowering Multimodal Foundation Models with Generalized Visual Search","summary":"The ability for AI agents to \"think with images\" requires a sophisticated blend of reasoning and perception. However, current open multimodal agents still largely fall short on the reasoning aspect crucial for real-world tasks like analyzing documents with dense charts/diagrams and navigating maps. To address this gap, we introduce O3-Bench, a new benchmark designed to evaluate multimodal reasoning with interleaved attention to visual details. O3-Bench features challenging problems that require agents to piece together subtle visual information from distinct image areas through multi-step reasoning. The problems are highly challenging even for frontier systems like OpenAI o3, which only obtains 40.8% accuracy on O3-Bench. To make progress, we propose InSight-o3, a multi-agent framework consisting of a visual reasoning agent (vReasoner) and a visual search agent (vSearcher) for which we introduce the task of generalized visual search -- locating relational, fuzzy, or conceptual regions described in free-form language, beyond just simple objects or figures in natural images. We then present a multimodal LLM purpose-trained for this task via reinforcement learning. As a plug-and-play agent, our vSearcher empowers frontier multimodal models (as vReasoners), significantly improving their performance on a wide range of benchmarks. This marks a concrete step towards powerful o3-like open systems. Our code and dataset can be found at https://github.com/m-Just/InSight-o3 .","authors":["Kaican Li","Lewei Yao","Jiannan Wu","Tiezheng Yu","Jierun Chen","Haoli Bai","Lu Hou","Lanqing Hong","Wei Zhang","Nevin L. Zhang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2506.13470v2","updated":"2025-12-21T12:43:32Z","published":"2025-06-16T13:28:37Z","title":"Abstract, Align, Predict: Zero-Shot Stance Detection via Cognitive Inductive Reasoning","summary":"Zero-shot stance detection (ZSSD) seeks to determine the stance of text toward previously unseen targets, a task critical for analyzing dynamic and polarized online discourse with limited labeled data. While large language models (LLMs) offer zero-shot capabilities, prompting-based approaches often fall short in handling complex reasoning and lack robust generalization to novel targets. Meanwhile, LLM-enhanced methods still require substantial labeled data and struggle to move beyond instance-level patterns, limiting their interpretability and adaptability. Inspired by cognitive science, we propose the Cognitive Inductive Reasoning Framework (CIRF), a schema-driven method that bridges linguistic inputs and abstract reasoning via automatic induction and application of cognitive reasoning schemas. CIRF abstracts first-order logic patterns from raw text into multi-relational schema graphs in an unsupervised manner, and leverages a schema-enhanced graph kernel model to align input structures with schema templates for robust, interpretable zero-shot inference. Extensive experiments on SemEval-2016, VAST, and COVID-19-Stance benchmarks demonstrate that CIRF not only establishes new state-of-the-art results, but also achieves comparable performance with just 30\\% of the labeled data, demonstrating its strong generalization and efficiency in low-resource settings.","authors":["Bowen Zhang","Jun Ma","Fuqiang Niu","Li Dong","Jinzhou Cao","Genan Dai"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.18682v1","updated":"2025-12-21T10:40:36Z","published":"2025-12-21T10:40:36Z","title":"Solver-Independent Automated Problem Formulation via LLMs for High-Cost Simulation-Driven Design","summary":"In the high-cost simulation-driven design domain, translating ambiguous design requirements into a mathematical optimization formulation is a bottleneck for optimizing product performance. This process is time-consuming and heavily reliant on expert knowledge. While large language models (LLMs) offer potential for automating this task, existing approaches either suffer from poor formalization that fails to accurately align with the design intent or rely on solver feedback for data filtering, which is unavailable due to the high simulation costs. To address this challenge, we propose APF, a framework for solver-independent, automated problem formulation via LLMs designed to automatically convert engineers' natural language requirements into executable optimization models. The core of this framework is an innovative pipeline for automatically generating high-quality data, which overcomes the difficulty of constructing suitable fine-tuning datasets in the absence of high-cost solver feedback with the help of data generation and test instance annotation. The generated high-quality dataset is used to perform supervised fine-tuning on LLMs, significantly enhancing their ability to generate accurate and executable optimization problem formulations. Experimental results on antenna design demonstrate that APF significantly outperforms the existing methods in both the accuracy of requirement formalization and the quality of resulting radiation efficiency curves in meeting the design goals.","authors":["Yuchen Li","Handing Wang","Bing Xue","Mengjie Zhang","Yaochu Jin"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.12620v2","updated":"2025-12-21T10:39:54Z","published":"2025-12-14T09:50:10Z","title":"Understanding Syllogistic Reasoning in LLMs from Formal and Natural Language Perspectives","summary":"We study syllogistic reasoning in LLMs from the logical and natural language perspectives. In process, we explore fundamental reasoning capabilities of the LLMs and the direction this research is moving forward. To aid in our studies, we use 14 large language models and investigate their syllogistic reasoning capabilities in terms of symbolic inferences as well as natural language understanding. Even though this reasoning mechanism is not a uniform emergent property across LLMs, the perfect symbolic performances in certain models make us wonder whether LLMs are becoming more and more formal reasoning mechanisms, rather than making explicit the nuances of human reasoning.","authors":["Aheli Poddar","Saptarshi Sahoo","Sujata Ghosh"],"pdf_url":"","comment":"9 pages, 4 figures, 5 tables. Submitted to AAAI 2026 Bridge Program on Logic & AI. Code available at https://github.com/XAheli/Logic-in-LLMs"},{"id":"http://arxiv.org/abs/2512.18679v1","updated":"2025-12-21T10:37:31Z","published":"2025-12-21T10:37:31Z","title":"brat: Aligned Multi-View Embeddings for Brain MRI Analysis","summary":"We present brat (brain report alignment transformer), a multi-view representation learning framework for brain magnetic resonance imaging (MRI) trained on MRIs paired with clinical reports. Brain MRIs present unique challenges due to the presence of numerous, highly varied, and often subtle abnormalities that are localized to a few slices within a 3D volume. To address these challenges, we introduce a brain MRI dataset $10\\times$ larger than existing ones, containing approximately 80,000 3D scans with corresponding radiology reports, and propose a multi-view pre-training approach inspired by advances in document retrieval. We develop an implicit query-feature matching mechanism and adopt concepts from quality-diversity to obtain multi-view embeddings of MRIs that are aligned with the clinical features given by report sentences. We evaluate our approach across multiple vision-language and vision tasks, demonstrating substantial performance improvements. The brat foundation models are publicly released.","authors":["Maxime Kayser","Maksim Gridnev","Wanting Wang","Max Bain","Aneesh Rangnekar","Avijit Chatterjee","Aleksandr Petrov","Harini Veeraraghavan","Nathaniel C. Swinburne"],"pdf_url":"","comment":"First round accept at WACV 2026"},{"id":"http://arxiv.org/abs/2512.18658v1","updated":"2025-12-21T09:12:21Z","published":"2025-12-21T09:12:21Z","title":"Does It Tie Out? Towards Autonomous Legal Agents in Venture Capital","summary":"Before closing venture capital financing rounds, lawyers conduct diligence that includes tying out the capitalization table: verifying that every security (for example, shares, options, warrants) and issuance term (for example, vesting schedules, acceleration triggers, transfer restrictions) is supported by large sets of underlying legal documentation. While LLMs continue to improve on legal benchmarks, specialized legal workflows, such as capitalization tie-out, remain out of reach even for strong agentic systems. The task requires multi-document reasoning, strict evidence traceability, and deterministic outputs that current approaches fail to reliably deliver. We characterize capitalization tie-out as an instance of a real-world benchmark for legal AI, analyze and compare the performance of existing agentic systems, and propose a world model architecture toward tie-out automation-and more broadly as a foundation for applied legal intelligence.","authors":["Pierre Colombo","Malik Boudiaf","Allyn Sweet","Michael Desa","Hongxi Wang","Kevin Candra","Syméon del Marmol"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.18623v1","updated":"2025-12-21T06:54:34Z","published":"2025-12-21T06:54:34Z","title":"LLM-CAS: Dynamic Neuron Perturbation for Real-Time Hallucination Correction","summary":"Large language models (LLMs) often generate hallucinated content that lacks factual or contextual grounding, limiting their reliability in critical applications. Existing approaches such as supervised fine-tuning and reinforcement learning from human feedback are data intensive and computationally expensive, while static parameter editing methods struggle with context dependent errors and catastrophic forgetting.\n  We propose LLM-CAS, a framework that formulates real-time hallucination correction as a hierarchical reinforcement learning problem. LLM-CAS trains an agent to learn a policy that dynamically selects temporary neuron perturbations during inference based on the current context. Unlike prior dynamic approaches that rely on heuristic or predefined adjustments, this policy driven mechanism enables adaptive and fine grained correction without permanent parameter modification.\n  Experiments across multiple language models demonstrate that LLM-CAS consistently improves factual accuracy, achieving gains of 10.98 percentage points on StoryCloze, 2.71 points on TriviaQA, and 2.06 points on the MC1 score of TruthfulQA. These results outperform both static editing methods such as ITI and CAA and the dynamic SADI framework. Overall, LLM-CAS provides an efficient and context aware solution for improving the reliability of LLMs, with promising potential for future multimodal extensions.","authors":["Jensen Zhang","Ningyuan Liu","Yijia Fan","Zihao Huang","Qinglin Zeng","Kaitong Cai","Jian Wang","Keze Wang"],"pdf_url":"","comment":"Accepted at AAAI 2026"},{"id":"http://arxiv.org/abs/2511.08029v2","updated":"2025-12-21T06:45:20Z","published":"2025-11-11T09:31:37Z","title":"BiCA: Effective Biomedical Dense Retrieval with Citation-Aware Hard Negatives","summary":"Hard negatives are essential for training effective retrieval models. Hard-negative mining typically relies on ranking documents using cross-encoders or static embedding models based on similarity metrics such as cosine distance. Hard negative mining becomes challenging for biomedical and scientific domains due to the difficulty in distinguishing between source and hard negative documents. However, referenced documents naturally share contextual relevance with the source document but are not duplicates, making them well-suited as hard negatives. In this work, we propose BiCA: Biomedical Dense Retrieval with Citation-Aware Hard Negatives, an approach for hard-negative mining by utilizing citation links in 20,000 PubMed articles for improving a domain-specific small dense retriever. We fine-tune the GTE_small and GTE_Base models using these citation-informed negatives and observe consistent improvements in zero-shot dense retrieval using nDCG@10 for both in-domain and out-of-domain tasks on BEIR and outperform baselines on long-tailed topics in LoTTE using Success@5. Our findings highlight the potential of leveraging document link structure to generate highly informative negatives, enabling state-of-the-art performance with minimal fine-tuning and demonstrating a path towards highly data-efficient domain adaptation.","authors":["Aarush Sinha","Pavan Kumar S","Roshan Balaji","Nirav Pravinbhai Bhatt"],"pdf_url":"","comment":"Accepted for oral presentation at AAAI 2026"},{"id":"http://arxiv.org/abs/2512.18622v1","updated":"2025-12-21T06:43:47Z","published":"2025-12-21T06:43:47Z","title":"A Multi-agent Text2SQL Framework using Small Language Models and Execution Feedback","summary":"Text2SQL, the task of generating SQL queries from natural language text, is a critical challenge in data engineering. Recently, Large Language Models (LLMs) have demonstrated superior performance for this task due to their advanced comprehension and generation capabilities. However, privacy and cost considerations prevent companies from using Text2SQL solutions based on external LLMs offered as a service. Rather, small LLMs (SLMs) that are openly available and can hosted in-house are adopted. These SLMs, in turn, lack the generalization capabilities of larger LLMs, which impairs their effectiveness for complex tasks such as Text2SQL. To address these limitations, we propose MATS, a novel Text2SQL framework designed specifically for SLMs. MATS uses a multi-agent mechanism that assigns specialized roles to auxiliary agents, reducing individual workloads and fostering interaction. A training scheme based on reinforcement learning aligns these agents using feedback obtained during execution, thereby maintaining competitive performance despite a limited LLM size. Evaluation results using on benchmark datasets show that MATS, deployed on a single- GPU server, yields accuracy that are on-par with large-scale LLMs when using significantly fewer parameters. Our source code and data are available at https://github.com/thanhdath/mats-sql.","authors":["Thanh Dat Hoang","Thanh Trung Huynh","Matthias Weidlich","Thanh Tam Nguyen","Tong Chen","Hongzhi Yin","Quoc Viet Hung Nguyen"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2406.16007v2","updated":"2025-12-21T06:33:37Z","published":"2024-06-23T04:29:13Z","title":"Label Words as Local Task Vectors in In-Context Learning","summary":"Large Language Models (LLMs) have demonstrated remarkable abilities, one of the most important being in-context learning (ICL). With ICL, LLMs can derive the underlying rule from a few demonstrations and provide answers that comply with the rule. Previous work hypothesized that the network creates a task vector in specific positions during ICL. The task vector can be computed by averaging across the dataset. It conveys the overall task information and can thus be considered global. Patching the global task vector allows LLMs to achieve zero-shot performance with dummy inputs comparable to few-shot learning. However, we find that such a global task vector does not exist in all tasks, especially in tasks that rely on rules that can only be inferred from multiple demonstrations, such as categorization tasks. Instead, the information provided by each demonstration is first transmitted to its answer position and forms a local task vector associated with the demonstration. In some tasks but not in categorization tasks, all demonstrations' local task vectors converge in later layers, forming the global task vector. We further show that local task vectors encode a high-level abstraction of rules extracted from the demonstrations. Our study provides novel insights into the mechanism underlying ICL in LLMs, demonstrating how ICL may be achieved through an information aggregation mechanism.","authors":["Bowen Zheng","Ming Ma","Zhongqiao Lin","Tianming Yang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.18608v1","updated":"2025-12-21T05:58:40Z","published":"2025-12-21T05:58:40Z","title":"A Comparative Study of Light-weight Language Models for PII Masking and their Deployment for Real Conversational Texts","summary":"Automated masking of Personally Identifiable Information (PII) is critical for privacy-preserving conversational systems. While current frontier large language models demonstrate strong PII masking capabilities, concerns about data handling and computational costs motivate exploration of whether lightweight models can achieve comparable performance. We compare encoder-decoder and decoder-only architectures by fine-tuning T5-small and Mistral-Instruct-v0.3 on English datasets constructed from the AI4Privacy benchmark. We create different dataset variants to study label standardization and PII representation, covering 24 standardized PII categories and higher-granularity settings. Evaluation using entity-level and character-level metrics, type accuracy, and exact match shows that both lightweight models achieve performance comparable to frontier LLMs for PII masking tasks. Label normalization consistently improves performance across architectures. Mistral achieves higher F1 and recall with greater robustness across PII types but incurs significantly higher generation latency. T5, while less robust in conversational text, offers more controllable structured outputs and lower inference cost, motivating its use in a real-time Discord bot for real-world PII redaction. Evaluation on live messages reveals performance degradation under informal inputs. These results clarify trade-offs between accuracy, robustness, and computational efficiency, demonstrating that lightweight models can provide effective PII masking while addressing data handling concerns associated with frontier LLMs.","authors":["Prabigya Acharya","Liza Shrestha"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.18601v1","updated":"2025-12-21T05:20:21Z","published":"2025-12-21T05:20:21Z","title":"On Finding Inconsistencies in Documents","summary":"Professionals in academia, law, and finance audit their documents because inconsistencies can result in monetary, reputational, and scientific costs. Language models (LMs) have the potential to dramatically speed up this auditing process. To understand their abilities, we introduce a benchmark, FIND (Finding INconsistencies in Documents), where each example is a document with an inconsistency inserted manually by a domain expert. Despite the documents being long, technical, and complex, the best-performing model (gpt-5) recovered 64% of the inserted inconsistencies. Surprisingly, gpt-5 also found undiscovered inconsistencies present in the original documents. For example, on 50 arXiv papers, we judged 136 out of 196 of the model's suggestions to be legitimate inconsistencies missed by the original authors. However, despite these findings, even the best models miss almost half of the inconsistencies in FIND, demonstrating that inconsistency detection is still a challenging task.","authors":["Charles J. Lovering","Seth Ebner","Brandon Smock","Michael Krumdick","Saad Rabbani","Ahmed Muhammad","Varshini Reddy","Chris Tanner"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.18593v1","updated":"2025-12-21T04:45:31Z","published":"2025-12-21T04:45:31Z","title":"From Scratch to Fine-Tuned: A Comparative Study of Transformer Training Strategies for Legal Machine Translation","summary":"In multilingual nations like India, access to legal information is often hindered by language barriers, as much of the legal and judicial documentation remains in English. Legal Machine Translation (L-MT) offers a scalable solution to this challenge by enabling accurate and accessible translations of legal documents. This paper presents our work for the JUST-NLP 2025 Legal MT shared task, focusing on English-Hindi translation using Transformer-based approaches. We experiment with 2 complementary strategies, fine-tuning a pre-trained OPUS-MT model for domain-specific adaptation and training a Transformer model from scratch using the provided legal corpus. Performance is evaluated using standard MT metrics, including SacreBLEU, chrF++, TER, ROUGE, BERTScore, METEOR, and COMET. Our fine-tuned OPUS-MT model achieves a SacreBLEU score of 46.03, significantly outperforming both baseline and from-scratch models. The results highlight the effectiveness of domain adaptation in enhancing translation quality and demonstrate the potential of L-MT systems to improve access to justice and legal transparency in multilingual contexts.","authors":["Amit Barman","Atanu Mandal","Sudip Kumar Naskar"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2501.03291v3","updated":"2025-12-21T03:24:02Z","published":"2025-01-06T08:20:04Z","title":"ADePT: Adaptive Decomposed Prompt Tuning for Parameter-Efficient Fine-tuning","summary":"Prompt Tuning (PT) enables the adaptation of Pre-trained Large Language Models (PLMs) to downstream tasks by optimizing a small amount of soft virtual tokens, which are prepended to the input token embeddings. Recently, Decomposed Prompt Tuning (DePT) has demonstrated superior adaptation capabilities by decomposing the soft prompt into a shorter soft prompt and a pair of low-rank matrices. The product of the pair of low-rank matrices is added to the input token embeddings to offset them. Additionally, DePT achieves faster inference compared to PT due to the shorter soft prompt. However, in this paper, we find that the position-based token embedding offsets of DePT restrict its ability to generalize across diverse model inputs, and that the shared embedding offsets across many token embeddings result in sub-optimization. To tackle these issues, we introduce Adaptive Decomposed Prompt Tuning (ADePT), which is composed of a short soft prompt and a shallow token-shared feed-forward neural network. ADePT utilizes the token-shared feed-forward neural network to learn the embedding offsets for each token, enabling adaptive embedding offsets that vary according to the model input and better optimization of token embedding offsets. This enables ADePT to achieve superior adaptation performance without requiring more inference time or additional trainable parameters compared to vanilla PT and its variants. In comprehensive experiments across 23 natural language processing tasks and 4 typical PLMs of different scales, ADePT consistently surpasses the other leading parameter-efficient fine-tuning methods, and even outperforms the full fine-tuning in certain scenarios. We also provide a theoretical analysis towards ADePT. Code is available at https://github.com/HungerPWAY/ADePT.","authors":["Pengwei Tang","Xiaolin Hu","Yong Liu"],"pdf_url":"","comment":"Published at ICLR 2025"},{"id":"http://arxiv.org/abs/2508.09337v3","updated":"2025-12-21T01:28:23Z","published":"2025-08-12T20:51:56Z","title":"Decoding Neural Emotion Patterns through Large Language Model Embeddings","summary":"Understanding how emotional expression in language relates to brain function is a challenge in computational neuroscience and affective computing. Traditional neuroimaging is costly and lab-bound, but abundant digital text offers new avenues for emotion-brain mapping. Prior work has largely examined neuroimaging-based emotion localization or computational text analysis separately, with little integration. We propose a computational framework that maps textual emotional content to anatomically defined brain regions without requiring neuroimaging. Using OpenAI's text-embedding-ada-002, we generate high-dimensional semantic representations, apply dimensionality reduction and clustering to identify emotional groups, and map them to 18 brain regions linked to emotional processing. Three experiments were conducted: i) analyzing conversational data from healthy vs. depressed subjects (DIAC-WOZ dataset) to compare mapping patterns, ii) applying the method to the GoEmotions dataset and iii) comparing human-written text with large language model (LLM) responses to assess differences in inferred brain activation. Emotional intensity was scored via lexical analysis. Results showed neuroanatomically plausible mappings with high spatial specificity. Depressed subjects exhibited greater limbic engagement tied to negative affect. Discrete emotions were successfully differentiated. LLM-generated text matched humans in basic emotion distribution but lacked nuanced activation in empathy and self-referential regions (medial prefrontal and posterior cingulate cortex). This cost-effective, scalable approach enables large-scale analysis of naturalistic language, distinguishes between clinical populations, and offers a brain-based benchmark for evaluating AI emotional expression.","authors":["Gideon Vos","Maryam Ebrahimpour","Liza van Eijk","Zoltan Sarnyai","Mostafa Rahimi Azghadi"],"pdf_url":"","comment":"26 pages, 9 figures"},{"id":"http://arxiv.org/abs/2510.17638v2","updated":"2025-12-21T01:04:03Z","published":"2025-10-20T15:20:05Z","title":"LLM-as-a-Prophet: Understanding Predictive Intelligence with Prophet Arena","summary":"Forecasting is not only a fundamental intellectual pursuit but also is of significant importance to societal systems such as finance and economics. With the rapid advances of large language models (LLMs) trained on Internet-scale data, it raises the promise of employing LLMs to forecast real-world future events, an emerging paradigm we call \"LLM-as-a-Prophet\". This paper systematically investigates such predictive intelligence of LLMs. To this end, we build Prophet Arena, a general evaluation benchmark that continuously collects live forecasting tasks and decomposes each task into distinct pipeline stages, in order to support our controlled and large-scale experimentation. Our comprehensive evaluation reveals that many LLMs already exhibit impressive forecasting capabilities, reflected in, e.g., their small calibration errors, consistent prediction confidence and promising market returns. However, we also uncover key bottlenecks towards achieving superior predictive intelligence via LLM-as-a-Prophet, such as LLMs' inaccurate event recalls, misunderstanding of data sources and slower information aggregation compared to markets when resolution nears.","authors":["Qingchuan Yang","Simon Mahns","Sida Li","Anri Gu","Jibang Wu","Haifeng Xu"],"pdf_url":"","comment":"https://www.prophetarena.co/"},{"id":"http://arxiv.org/abs/2511.09854v2","updated":"2025-12-21T00:53:39Z","published":"2025-11-13T01:25:34Z","title":"TermGPT: Multi-Level Contrastive Fine-Tuning for Terminology Adaptation in Legal and Financial Domain","summary":"Large language models (LLMs) have demonstrated impressive performance in text generation tasks; however, their embedding spaces often suffer from the isotropy problem, resulting in poor discrimination of domain-specific terminology, particularly in legal and financial contexts. This weakness in terminology-level representation can severely hinder downstream tasks such as legal judgment prediction or financial risk analysis, where subtle semantic distinctions are critical. To address this problem, we propose TermGPT, a multi-level contrastive fine-tuning framework designed for terminology adaptation. We first construct a sentence graph to capture semantic and structural relations, and generate semantically consistent yet discriminative positive and negative samples based on contextual and topological cues. We then devise a multi-level contrastive learning approach at both the sentence and token levels, enhancing global contextual understanding and fine-grained terminology discrimination. To support robust evaluation, we construct the first financial terminology dataset derived from official regulatory documents. Experiments show that TermGPT outperforms existing baselines in term discrimination tasks within the finance and legal domains.","authors":["Yidan Sun","Mengying Zhu","Feiyue Chen","Yangyang Wu","Xiaolei Dan","Mengyuan Yang","Xiaolin Zheng","Shenglin Ben"],"pdf_url":"","comment":"13 pages, 4 figures, AAAI'26"},{"id":"http://arxiv.org/abs/2512.18552v1","updated":"2025-12-21T00:49:40Z","published":"2025-12-21T00:49:40Z","title":"Toward Training Superintelligent Software Agents through Self-Play SWE-RL","summary":"While current software agents powered by large language models (LLMs) and agentic reinforcement learning (RL) can boost programmer productivity, their training data (e.g., GitHub issues and pull requests) and environments (e.g., pass-to-pass and fail-to-pass tests) heavily depend on human knowledge or curation, posing a fundamental barrier to superintelligence. In this paper, we present Self-play SWE-RL (SSR), a first step toward training paradigms for superintelligent software agents. Our approach takes minimal data assumptions, only requiring access to sandboxed repositories with source code and installed dependencies, with no need for human-labeled issues or tests. Grounded in these real-world codebases, a single LLM agent is trained via reinforcement learning in a self-play setting to iteratively inject and repair software bugs of increasing complexity, with each bug formally specified by a test patch rather than a natural language issue description. On the SWE-bench Verified and SWE-Bench Pro benchmarks, SSR achieves notable self-improvement (+10.4 and +7.8 points, respectively) and consistently outperforms the human-data baseline over the entire training trajectory, despite being evaluated on natural language issues absent from self-play. Our results, albeit early, suggest a path where agents autonomously gather extensive learning experiences from real-world software repositories, ultimately enabling superintelligent systems that exceed human capabilities in understanding how systems are constructed, solving novel challenges, and autonomously creating new software from scratch.","authors":["Yuxiang Wei","Zhiqing Sun","Emily McMilin","Jonas Gehring","David Zhang","Gabriel Synnaeve","Daniel Fried","Lingming Zhang","Sida Wang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.18551v1","updated":"2025-12-21T00:45:23Z","published":"2025-12-21T00:45:23Z","title":"Neologism Learning as a Parameter-Efficient Alternative to Fine-Tuning for Model Steering","summary":"In language modeling, neologisms are new tokens trained to represent a concept not already included in a given model's vocabulary. Neologisms can be used to encourage specific behavior in models, for example by appending prompts with \"Give me a neologism answer.\" Behavioral steering can also be achieved through fine-tuning, albeit with more compute and less flexibility: learning a neologism only trains d parameters and allows the user to still access the model's default behavior. We compare the performance of neologism learning against low-rank adaptation (LoRA) fine-tuning, finding that neologisms outperform fine-tuned models under a matched training setup (same data and hyperparameters). We also investigate self-verbalizations of neologisms, and observe that the model will occasionally make up its own new words when asked about a neologism.","authors":["Sungjoon Park","Varun Ramamurthi","Owen Terry"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.18546v1","updated":"2025-12-21T00:19:02Z","published":"2025-12-21T00:19:02Z","title":"LLMs on Drugs: Language Models Are Few-Shot Consumers","summary":"Large language models (LLMs) are sensitive to the personas imposed on them at inference time, yet prompt-level \"drug\" interventions have never been benchmarked rigorously. We present the first controlled study of psychoactive framings on GPT-5-mini using ARC-Challenge. Four single-sentence prompts -- LSD, cocaine, alcohol, and cannabis -- are compared against a sober control across 100 validation items per condition, with deterministic decoding, full logging, Wilson confidence intervals, and Fisher exact tests. Control accuracy is 0.45; alcohol collapses to 0.10 (p = 3.2e-8), cocaine to 0.21 (p = 4.9e-4), LSD to 0.19 (p = 1.3e-4), and cannabis to 0.30 (p = 0.041), largely because persona prompts disrupt the mandated \"Answer: <LETTER>\" template. Persona text therefore behaves like a \"few-shot consumable\" that can destroy reliability without touching model weights. All experimental code, raw results, and analysis scripts are available at https://github.com/lexdoudkin/llms-on-drugs.","authors":["Alexander Doudkin"],"pdf_url":"","comment":"8 pages, 2 figures, 2 tables"},{"id":"http://arxiv.org/abs/2512.20677v1","updated":"2025-12-21T19:12:44Z","published":"2025-12-21T19:12:44Z","title":"Automated Red-Teaming Framework for Large Language Model Security Assessment: A Comprehensive Attack Generation and Detection System","summary":"As large language models (LLMs) are increasingly deployed in high-stakes domains, ensuring their security and alignment has become a critical challenge. Existing red-teaming practices depend heavily on manual testing, which limits scalability and fails to comprehensively cover the vast space of potential adversarial behaviors. This paper introduces an automated red-teaming framework that systematically generates, executes, and evaluates adversarial prompts to uncover security vulnerabilities in LLMs. Our framework integrates meta-prompting-based attack synthesis, multi-modal vulnerability detection, and standardized evaluation protocols spanning six major threat categories -- reward hacking, deceptive alignment, data exfiltration, sandbagging, inappropriate tool use, and chain-of-thought manipulation. Experiments on the GPT-OSS-20B model reveal 47 distinct vulnerabilities, including 21 high-severity and 12 novel attack patterns, achieving a $3.9\\times$ improvement in vulnerability discovery rate over manual expert testing while maintaining 89\\% detection accuracy. These results demonstrate the framework's effectiveness in enabling scalable, systematic, and reproducible AI safety evaluations. By providing actionable insights for improving alignment robustness, this work advances the state of automated LLM red-teaming and contributes to the broader goal of building secure and trustworthy AI systems.","authors":["Zhang Wei","Peilu Hu","Shengning Lang","Hao Yan","Li Mei","Yichao Zhang","Chen Yang","Junfeng Hao","Zhimo Han"],"pdf_url":"","comment":"18 pages"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2511.05844v3","updated":"2025-12-21T23:56:36Z","published":"2025-11-08T04:23:42Z","title":"Enhancing Diffusion Model Guidance through Calibration and Regularization","summary":"Classifier-guided diffusion models have emerged as a powerful approach for conditional image generation, but they suffer from overconfident predictions during early denoising steps, causing the guidance gradient to vanish. This paper introduces two complementary contributions to address this issue. First, we propose a differentiable calibration objective based on the Smooth Expected Calibration Error (Smooth ECE), which improves classifier calibration with minimal fine-tuning and yields measurable improvements in Frechet Inception Distance (FID). Second, we develop enhanced sampling guidance methods that operate on off-the-shelf classifiers without requiring retraining. These include tilted sampling with batch-level reweighting, adaptive entropy-regularized sampling to preserve diversity, and a novel f-divergence-based sampling strategy that strengthens class-consistent guidance while maintaining mode coverage. Experiments on ImageNet 128x128 demonstrate that our divergence-regularized guidance achieves an FID of 2.13 using a ResNet-101 classifier, improving upon existing classifier-guided diffusion methods while requiring no diffusion model retraining. The results show that principled calibration and divergence-aware sampling provide practical and effective improvements for classifier-guided diffusion.","authors":["Seyed Alireza Javid","Amirhossein Bagheri","Nuria González-Prelcic"],"pdf_url":"","comment":"Accepted from NeurIPS 2025 Workshop on Structured Probabilistic Inference & Generative Modeling. Code available at https://github.com/ajavid34/guided-info-diffusion"},{"id":"http://arxiv.org/abs/2512.18910v1","updated":"2025-12-21T23:02:56Z","published":"2025-12-21T23:02:56Z","title":"Delta-LLaVA: Base-then-Specialize Alignment for Token-Efficient Vision-Language Models","summary":"Multimodal Large Language Models (MLLMs) combine visual and textual representations to enable rich reasoning capabilities. However, the high computational cost of processing dense visual tokens remains a major bottleneck. A critical component in this pipeline is the visual projector, which bridges the vision encoder and the language model. Standard designs often employ a simple multi-layer perceptron for direct token mapping, but this approach scales poorly with high-resolution inputs, introducing significant redundancy. We present Delta-LLaVA, a token-efficient projector that employs a low-rank DeltaProjection to align multi-level vision features into a compact subspace before further interaction. On top of this base alignment, lightweight Transformer blocks act as specialization layers, capturing both global and local structure under constrained token budgets. Extensive experiments and ablations demonstrate that this base-then-specialize design yields consistent gains across multiple benchmarks with only 144 tokens, highlighting the importance of token formation prior to scaling interaction capacity. With Delta-LLaVA, inference throughput improves by up to 55%, while end-to-end training accelerates by nearly 4-5x in pretraining and over 1.5x in finetuning, highlighting the dual benefits of our design in both efficiency and scalability.","authors":["Mohamad Zamini","Diksha Shukla"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.18897v1","updated":"2025-12-21T22:01:29Z","published":"2025-12-21T22:01:29Z","title":"Thinking Beyond Labels: Vocabulary-Free Fine-Grained Recognition using Reasoning-Augmented LMMs","summary":"Vocabulary-free fine-grained image recognition aims to distinguish visually similar categories within a meta-class without a fixed, human-defined label set. Existing solutions for this problem are limited by either the usage of a large and rigid list of vocabularies or by the dependency on complex pipelines with fragile heuristics where errors propagate across stages. Meanwhile, the ability of recent large multi-modal models (LMMs) equipped with explicit or implicit reasoning to comprehend visual-language data, decompose problems, retrieve latent knowledge, and self-correct suggests a more principled and effective alternative. Building on these capabilities, we propose FiNDR (Fine-grained Name Discovery via Reasoning), the first reasoning-augmented LMM-based framework for vocabulary-free fine-grained recognition. The system operates in three automated steps: (i) a reasoning-enabled LMM generates descriptive candidate labels for each image; (ii) a vision-language model filters and ranks these candidates to form a coherent class set; and (iii) the verified names instantiate a lightweight multi-modal classifier used at inference time. Extensive experiments on popular fine-grained classification benchmarks demonstrate state-of-the-art performance under the vocabulary-free setting, with a significant relative margin of up to 18.8% over previous approaches. Remarkably, the proposed method surpasses zero-shot baselines that exploit pre-defined ground-truth names, challenging the assumption that human-curated vocabularies define an upper bound. Additionally, we show that carefully curated prompts enable open-source LMMs to match proprietary counterparts. These findings establish reasoning-augmented LMMs as an effective foundation for scalable, fully automated, open-world fine-grained visual recognition. The source code is available on github.com/demidovd98/FiNDR.","authors":["Dmitry Demidov","Zaigham Zaheer","Zongyan Han","Omkar Thawakar","Rao Anwer"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2503.14736v2","updated":"2025-12-21T21:53:04Z","published":"2025-03-18T21:09:04Z","title":"HandSCS: Structural Coordinate Space for Animatable Hand Gaussian Splatting","summary":"Creating animatable hand avatars from multi-view images requires modeling complex articulations and maintaining structural consistency across poses in real time. We present HandSCS, a structure-guided 3D Gaussian Splatting framework for high-fidelity hand animation. Unlike existing approaches that condition all Gaussians on the same global pose parameters, which are inadequate for highly articulated hands, HandSCS equips each Gaussian with explicit structural guidance from both intra-pose and inter-pose perspectives. To establish intra-pose structural guidance, we introduce a Structural Coordinate Space (SCS), which bridges the gap between sparse bones and dense Gaussians through hybrid static-dynamic coordinate basis and angular-radial descriptors. To improve cross-pose coherence, we further introduce an Inter-pose Consistency Loss that promotes consistent Gaussian attributes under similar articulations. Together, these components achieve high-fidelity results with consistent fine details, even in challenging high-deformation and self-contact regions. Experiments on the InterHand2.6M dataset demonstrate that HandSCS achieves state-of-the-art performance in hand avatar animation, confirming the effectiveness of explicit structural modeling.","authors":["Yilan Dong","Wenqing Wang","Qing Wang","Jiahao Yang","Haohe Liu","Xiatuan Zhu","Gregory Slabaugh","Shanxin Yuan"],"pdf_url":"","comment":null}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2512.01372v2","updated":"2025-12-21T19:35:23Z","published":"2025-12-01T07:39:28Z","title":"Structured Spectral Reasoning for Frequency-Adaptive Multimodal Recommendation","summary":"Multimodal recommendation aims to integrate collaborative signals with heterogeneous content such as visual and textual information, but remains challenged by modality-specific noise, semantic inconsistency, and unstable propagation over user-item graphs. These issues are often exacerbated by naive fusion or shallow modeling strategies, leading to degraded generalization and poor robustness. While recent work has explored the frequency domain as a lens to separate stable from noisy signals, most methods rely on static filtering or reweighting, lacking the ability to reason over spectral structure or adapt to modality-specific reliability. To address these challenges, we propose a Structured Spectral Reasoning (SSR) framework for frequency-aware multimodal recommendation. Our method follows a four-stage pipeline: (i) Decompose graph-based multimodal signals into spectral bands via graph-guided transformations to isolate semantic granularity; (ii) Modulate band-level reliability with spectral band masking, a training-time masking with a prediction-consistency objective that suppresses brittle frequency components; (iii) Fuse complementary frequency cues using hyperspectral reasoning with low-rank cross-band interaction; and (iv) Align modality-specific spectral features via contrastive regularization to promote semantic and structural consistency. Experiments on three real-world benchmarks show consistent gains over strong baselines, particularly under sparse and cold-start settings. Additional analyses indicate that structured spectral modeling improves robustness and provides clearer diagnostics of how different bands contribute to performance.","authors":["Wei Yang","Rui Zhong","Yiqun Chen","Chi Lu","Peng Jiang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2506.11112v2","updated":"2025-12-21T17:16:19Z","published":"2025-06-08T16:25:35Z","title":"Dagstuhl Perspectives Workshop 24352 -- Conversational Agents: A Framework for Evaluation (CAFE): Manifesto","summary":"During the workshop, we deeply discussed what CONversational Information ACcess (CONIAC) is and its unique features, proposing a world model abstracting it, and defined the Conversational Agents Framework for Evaluation (CAFE) for the evaluation of CONIAC systems, consisting of six major components: 1) goals of the system's stakeholders, 2) user tasks to be studied in the evaluation, 3) aspects of the users carrying out the tasks, 4) evaluation criteria to be considered, 5) evaluation methodology to be applied, and 6) measures for the quantitative criteria chosen.","authors":["Christine Bauer","Li Chen","Nicola Ferro","Norbert Fuhr","Avishek Anand","Timo Breuer","Guglielmo Faggioli","Ophir Frieder","Hideo Joho","Jussi Karlgren","Johannes Kiesel","Bart P. Knijnenburg","Aldo Lipani","Lien Michiels","Andrea Papenmeier","Maria Soledad Pera","Mark Sanderson","Scott Sanner","Benno Stein","Johanne R. Trippas","Karin Verspoor","Martijn C Willemsen"],"pdf_url":"","comment":"10 figures; Dagstuhl Manifestos, 11(1), pp 19-67. DOI: 10.4230/DagMan.11.1.19"},{"id":"http://arxiv.org/abs/2510.00887v2","updated":"2025-12-21T15:31:26Z","published":"2025-10-01T13:34:02Z","title":"On Listwise Reranking for Corpus Feedback","summary":"Reranker improves retrieval performance by capturing document interactions. At one extreme, graph-aware adaptive retrieval (GAR) represents an information-rich regime, requiring a pre-computed document similarity graph in reranking. However, as such graphs are often unavailable, or incur quadratic memory costs even when available, graph-free rerankers leverage large language model (LLM) calls to achieve competitive performance. We introduce L2G, a novel framework that implicitly induces document graphs from listwise reranker logs. By converting reranker signals into a graph structure, L2G enables scalable graph-based retrieval without the overhead of explicit graph computation. Results on the TREC-DL and BEIR subset show that L2G matches the effectiveness of oracle-based graph methods, while incurring zero additional LLM calls.","authors":["Soyoung Yoon","Jongho Kim","Daeyong Kwon","Avishek Anand","Seung-won Hwang"],"pdf_url":"","comment":"Accepted for WSDM 2026 (short)"},{"id":"http://arxiv.org/abs/2512.16661v2","updated":"2025-12-21T15:17:01Z","published":"2025-12-18T15:29:18Z","title":"Microsoft Academic Graph Information Retrieval for Research Recommendation and Assistance","summary":"In today's information-driven world, access to scientific publications has become increasingly easy. At the same time, filtering through the massive volume of available research has become more challenging than ever. Graph Neural Networks (GNNs) and graph attention mechanisms have shown strong effectiveness in searching large-scale information databases, particularly when combined with modern large language models. In this paper, we propose an Attention-Based Subgraph Retriever, a GNN-as-retriever model that applies attention-based pruning to extract a refined subgraph, which is then passed to a large language model for advanced knowledge reasoning.","authors":["Shikshya Shiwakoti","Samuel Goldsmith","Ujjwal Pandit"],"pdf_url":"","comment":"5 pages, 3 figures"},{"id":"http://arxiv.org/abs/2512.18683v1","updated":"2025-12-21T10:41:05Z","published":"2025-12-21T10:41:05Z","title":"CIRR: Causal-Invariant Retrieval-Augmented Recommendation with Faithful Explanations under Distribution Shift","summary":"Recent advances in retrieval-augmented generation (RAG) have shown promise in enhancing recommendation systems with external knowledge. However, existing RAG-based recommenders face two critical challenges: (1) vulnerability to distribution shifts across different environments (e.g., time periods, user segments), leading to performance degradation in out-of-distribution (OOD) scenarios, and (2) lack of faithful explanations that can be verified against retrieved evidence. In this paper, we propose CIRR, a Causal-Invariant Retrieval-Augmented Recommendation framework that addresses both challenges simultaneously. CIRR learns environment-invariant user preference representations through causal inference, which guide a debiased retrieval process to select relevant evidence from multiple sources. Furthermore, we introduce consistency constraints that enforce faithfulness between retrieved evidence, generated explanations, and recommendation outputs. Extensive experiments on two real-world datasets demonstrate that CIRR achieves robust performance under distribution shifts, reducing performance degradation from 15.4% (baseline) to only 5.6% in OOD scenarios, while providing more faithful and interpretable explanations (26% improvement in faithfulness score) compared to state-of-the-art baselines.","authors":["Sebastian Sun"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2511.08029v2","updated":"2025-12-21T06:45:20Z","published":"2025-11-11T09:31:37Z","title":"BiCA: Effective Biomedical Dense Retrieval with Citation-Aware Hard Negatives","summary":"Hard negatives are essential for training effective retrieval models. Hard-negative mining typically relies on ranking documents using cross-encoders or static embedding models based on similarity metrics such as cosine distance. Hard negative mining becomes challenging for biomedical and scientific domains due to the difficulty in distinguishing between source and hard negative documents. However, referenced documents naturally share contextual relevance with the source document but are not duplicates, making them well-suited as hard negatives. In this work, we propose BiCA: Biomedical Dense Retrieval with Citation-Aware Hard Negatives, an approach for hard-negative mining by utilizing citation links in 20,000 PubMed articles for improving a domain-specific small dense retriever. We fine-tune the GTE_small and GTE_Base models using these citation-informed negatives and observe consistent improvements in zero-shot dense retrieval using nDCG@10 for both in-domain and out-of-domain tasks on BEIR and outperform baselines on long-tailed topics in LoTTE using Success@5. Our findings highlight the potential of leveraging document link structure to generate highly informative negatives, enabling state-of-the-art performance with minimal fine-tuning and demonstrating a path towards highly data-efficient domain adaptation.","authors":["Aarush Sinha","Pavan Kumar S","Roshan Balaji","Nirav Pravinbhai Bhatt"],"pdf_url":"","comment":"Accepted for oral presentation at AAAI 2026"}],"Multimedia":[{"id":"http://arxiv.org/abs/2512.18864v1","updated":"2025-12-21T19:36:34Z","published":"2025-12-21T19:36:34Z","title":"Cross-modal Counterfactual Explanations: Uncovering Decision Factors and Dataset Biases in Subjective Classification","summary":"Concept-driven counterfactuals explain decisions of classifiers by altering the model predictions through semantic changes. In this paper, we present a novel approach that leverages cross-modal decompositionality and image-specific concepts to create counterfactual scenarios expressed in natural language. We apply the proposed interpretability framework, termed Decompose and Explain (DeX), to the challenging domain of image privacy decisions, which are contextual and subjective. This application enables the quantification of the differential contributions of key scene elements to the model prediction. We identify relevant decision factors via a multi-criterion selection mechanism that considers both image similarity for minimal perturbations and decision confidence to prioritize impactful changes. This approach evaluates and compares diverse explanations, and assesses the interdependency and mutual influence among explanatory properties. By leveraging image-specific concepts, DeX generates image-grounded, sparse explanations, yielding significant improvements over the state of the art. Importantly, DeX operates as a training-free framework, offering high flexibility. Results show that DeX not only uncovers the principal contributing factors influencing subjective decisions, but also identifies underlying dataset biases allowing for targeted mitigation strategies to improve fairness.","authors":["Alina Elena Baia","Andrea Cavallaro"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.18809v1","updated":"2025-12-21T17:01:44Z","published":"2025-12-21T17:01:44Z","title":"FedVideoMAE: Efficient Privacy-Preserving Federated Video Moderation","summary":"The rapid growth of short-form video platforms increases the need for privacy-preserving moderation, as cloud-based pipelines expose raw videos to privacy risks, high bandwidth costs, and inference latency. To address these challenges, we propose an on-device federated learning framework for video violence detection that integrates self-supervised VideoMAE representations, LoRA-based parameter-efficient adaptation, and defense-in-depth privacy protection. Our approach reduces the trainable parameter count to 5.5M (~3.5% of a 156M backbone) and incorporates DP-SGD with configurable privacy budgets and secure aggregation. Experiments on RWF-2000 with 40 clients achieve 77.25% accuracy without privacy protection and 65-66% under strong differential privacy, while reducing communication cost by $28.3\\times$ compared to full-model federated learning. The code is available at: {https://github.com/zyt-599/FedVideoMAE}","authors":["Ziyuan Tao","Chuanzhi Xu","Sandaru Jayawardana","Wei Bao","Kanchana Thilakarathna","Teng Joon Lim"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.18804v1","updated":"2025-12-21T16:57:08Z","published":"2025-12-21T16:57:08Z","title":"Tempo as the Stable Cue: Hierarchical Mixture of Tempo and Beat Experts for Music to 3D Dance Generation","summary":"Music to 3D dance generation aims to synthesize realistic and rhythmically synchronized human dance from music. While existing methods often rely on additional genre labels to further improve dance generation, such labels are typically noisy, coarse, unavailable, or insufficient to capture the diversity of real-world music, which can result in rhythm misalignment or stylistic drift. In contrast, we observe that tempo, a core property reflecting musical rhythm and pace, remains relatively consistent across datasets and genres, typically ranging from 60 to 200 BPM. Based on this finding, we propose TempoMoE, a hierarchical tempo-aware Mixture-of-Experts module that enhances the diffusion model and its rhythm perception. TempoMoE organizes motion experts into tempo-structured groups for different tempo ranges, with multi-scale beat experts capturing fine- and long-range rhythmic dynamics. A Hierarchical Rhythm-Adaptive Routing dynamically selects and fuses experts from music features, enabling flexible, rhythm-aligned generation without manual genre labels. Extensive experiments demonstrate that TempoMoE achieves state-of-the-art results in dance quality and rhythm alignment.","authors":["Guangtao Lyu","Chenghao Xu","Qi Liu","Jiexi Yan","Muli Yang","Fen Fang","Cheng Deng"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2511.20732v2","updated":"2025-12-21T13:00:45Z","published":"2025-11-25T12:22:56Z","title":"Prompt-Aware Adaptive Elastic Weight Consolidation for Continual Learning in Medical Vision-Language Models","summary":"Medical AI systems face catastrophic forgetting when deployed in clinical settings, where models must learn new imaging protocols while retaining prior diagnostic capabilities. This challenge is particularly acute for medical vision-language models that must preserve complex cross-modal alignments between medical images and clinical terminology across diverse imaging modalities. We introduce Prompt- Aware Adaptive Elastic Weight Consolidation (PA-EWC), a novel continual learning approach that addresses catastrophic forgetting through prompt-guided parameter specialization. Our method systematically categorizes model parameters based on their functional roles in processing visual-descriptive, spatial-guided, and medical-semantic information, enabling targeted protection of critical knowledge while allowing adaptation to new clinical requirements. PA-EWC incorporates adaptive Fisher Information computation with gradient stability analysis and develops weighted complexity metrics based on medical terminology density. We evaluate our approach across five medical imaging datasets (Kvasir-SEG, ISIC 2018, CheXlocalize, BUSI, CAMUS) representing diverse modalities including endoscopy, dermoscopy, radiography, and ultrasound. Experimental results demonstrate that PA-EWC reduces catastrophic forgetting by up to 17.58% compared to baseline methods, with performance improvements of 4.30% on chest X-ray pathology localization and 6.06% on polyp segmentation.","authors":["Ziyuan Gao","Philippe Morel"],"pdf_url":"","comment":"Accepted by 32nd International Conference on MultiMedia Modeling (MMM 2026)"},{"id":"http://arxiv.org/abs/2506.23484v4","updated":"2025-12-21T11:14:12Z","published":"2025-06-30T03:14:07Z","title":"TAG-WM: Tamper-Aware Generative Image Watermarking via Diffusion Inversion Sensitivity","summary":"AI-generated content (AIGC) enables efficient visual creation but raises copyright and authenticity risks. As a common technique for integrity verification and source tracing, digital image watermarking is regarded as a potential solution to above issues. However, the widespread adoption and advancing capabilities of generative image editing tools have amplified malicious tampering risks, while simultaneously posing new challenges to passive tampering detection and watermark robustness. To address these challenges, this paper proposes a Tamper-Aware Generative image WaterMarking method named TAG-WM. The proposed method comprises four key modules: a dual-mark joint sampling (DMJS) algorithm for embedding copyright and localization watermarks into the latent space while preserving generative quality, the watermark latent reconstruction (WLR) utilizing reversed DMJS, a dense variation region detector (DVRD) leveraging diffusion inversion sensitivity to identify tampered areas via statistical deviation analysis, and the tamper-aware decoding (TAD) guided by localization results. The experimental results demonstrate that TAG-WM achieves state-of-the-art performance in both tampering robustness and localization capability even under distortion, while preserving lossless generation quality and maintaining a watermark capacity of 256 bits. The code is available at: https://github.com/Suchenl/TAG-WM.","authors":["Yuzhuo Chen","Zehua Ma","Han Fang","Weiming Zhang","Nenghai Yu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.18660v1","updated":"2025-12-21T09:16:11Z","published":"2025-12-21T09:16:11Z","title":"PMPGuard: Catching Pseudo-Matched Pairs in Remote Sensing Image-Text Retrieval","summary":"Remote sensing (RS) image-text retrieval faces significant challenges in real-world datasets due to the presence of Pseudo-Matched Pairs (PMPs), semantically mismatched or weakly aligned image-text pairs, which hinder the learning of reliable cross-modal alignments. To address this issue, we propose a novel retrieval framework that leverages Cross-Modal Gated Attention and a Positive-Negative Awareness Attention mechanism to mitigate the impact of such noisy associations. The gated module dynamically regulates cross-modal information flow, while the awareness mechanism explicitly distinguishes informative (positive) cues from misleading (negative) ones during alignment learning. Extensive experiments on three benchmark RS datasets, i.e., RSICD, RSITMD, and RS5M, demonstrate that our method consistently achieves state-of-the-art performance, highlighting its robustness and effectiveness in handling real-world mismatches and PMPs in RS image-text retrieval tasks.","authors":["Pengxiang Ouyang","Qing Ma","Zheng Wang","Cong Bai"],"pdf_url":"","comment":null}]},"2025-12-20T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2512.18542v1","updated":"2025-12-20T23:52:12Z","published":"2025-12-20T23:52:12Z","title":"SecureCode v2.0: A Production-Grade Dataset for Training Security-Aware Code Generation Models","summary":"AI assistants produce vulnerable code in 45% of security-relevant scenarios, introducing flaws into production systems at scale. Yet existing secure coding datasets fall short. They lack incident grounding, don't provide the scale modern training requires, and miss the operational security context developers need for production deployments. We present SecureCode v2.0, a production-grade dataset of 1,215 security-focused coding examples that passed structural validation and expert security review. Every example ties to actual documented security incidents with CVE references, provides vulnerable and secure implementations, demonstrates concrete attacks, and includes defense-in-depth operational guidance. The dataset covers 11 vulnerability categories (complete OWASP Top 10:2025 plus AI/ML Security Threats) across 11 languages (Python, JavaScript, Java, Go, PHP, C#, TypeScript, Ruby, Rust, Kotlin, and YAML for infrastructure-as-code).\n  Our quality assurance framework ensures complete incident grounding. Each example includes SIEM integration strategies, infrastructure hardening recommendations (Docker, AppArmor, WAF configurations), and testing approaches using language-appropriate frameworks. The dataset uses a 4-turn conversational structure mirroring actual developer-AI interactions, escalating from basic implementations to advanced security considerations and defense-in-depth guidance.\n  Our contributions: (1) 1,215 rigorously validated examples split into 989 training, 122 validation, and 104 test sets, (2) an automated validation framework ensuring dataset consistency, (3) a 4-turn conversational structure capturing realistic security workflows, (4) comprehensive operational security guidance with SIEM integration strategies, (5) complete language-specific implementation fidelity, and (6) open-source release of data, validation tools, and benchmarking protocols.","authors":["Scott Thornton"],"pdf_url":"","comment":"37 pages, 5 figures. Dataset available at https://huggingface.co/datasets/scthornton/securecode-v2. Code and validation tools at https://github.com/scthornton/securecode-v2"},{"id":"http://arxiv.org/abs/2512.18533v1","updated":"2025-12-20T23:08:18Z","published":"2025-12-20T23:08:18Z","title":"Generalization Gaps in Political Fake News Detection: An Empirical Study on the LIAR Dataset","summary":"The proliferation of linguistically subtle political disinformation poses a significant challenge to automated fact-checking systems. Despite increasing emphasis on complex neural architectures, the empirical limits of text-only linguistic modeling remain underexplored. We present a systematic diagnostic evaluation of nine machine learning algorithms on the LIAR benchmark. By isolating lexical features (Bag-of-Words, TF-IDF) and semantic embeddings (GloVe), we uncover a hard \"Performance Ceiling\", with fine-grained classification not exceeding a Weighted F1-score of 0.32 across models. Crucially, a simple linear SVM (Accuracy: 0.624) matches the performance of pre-trained Transformers such as RoBERTa (Accuracy: 0.620), suggesting that model capacity is not the primary bottleneck. We further diagnose a massive \"Generalization Gap\" in tree-based ensembles, which achieve more than 99% training accuracy but collapse to approximately 25% on test data, indicating reliance on lexical memorization rather than semantic inference. Synthetic data augmentation via SMOTE yields no meaningful gains, confirming that the limitation is semantic (feature ambiguity) rather than distributional. These findings indicate that for political fact-checking, increasing model complexity without incorporating external knowledge yields diminishing returns.","authors":["S Mahmudul Hasan","Shaily Roy","Akib Jawad Nafis"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.06737v2","updated":"2025-12-20T21:12:56Z","published":"2025-12-07T09:03:45Z","title":"Arc Gradient Descent: A Mathematically Derived Reformulation of Gradient Descent with Phase-Aware, User-Controlled Step Dynamics","summary":"The paper presents the formulation, implementation, and evaluation of the ArcGD optimiser. The evaluation is conducted initially on a non-convex benchmark function and subsequently on a real-world ML dataset. The initial comparative study using the Adam optimiser is conducted on a stochastic variant of the highly non-convex and notoriously challenging Rosenbrock function, renowned for its narrow, curved valley, across dimensions ranging from 2D to 1000D and an extreme case of 50,000D. Two configurations were evaluated to eliminate learning-rate bias: (i) both using ArcGD's effective learning rate and (ii) both using Adam's default learning rate. ArcGD consistently outperformed Adam under the first setting and, although slower under the second, achieved superior final solutions in most cases. In the second evaluation, ArcGD is evaluated against state-of-the-art optimizers (Adam, AdamW, Lion, SGD) on the CIFAR-10 image classification dataset across 8 diverse MLP architectures ranging from 1 to 5 hidden layers. ArcGD achieved the highest average test accuracy (50.7%) at 20,000 iterations, outperforming AdamW (46.6%), Adam (46.8%), SGD (49.6%), and Lion (43.4%), winning or tying on 6 of 8 architectures. Notably, while Adam and AdamW showed strong early convergence at 5,000 iterations, but regressed with extended training, whereas ArcGD continued improving, demonstrating generalization and resistance to overfitting without requiring early stopping tuning. Strong performance on geometric stress tests and standard deep-learning benchmarks indicates broad applicability, highlighting the need for further exploration. Moreover, it is also shown that a limiting variant of ArcGD can be interpreted as a sign-based momentum-like update, highlighting conceptual connections between the inherent mechanisms of ArcGD and the Lion optimiser.","authors":["Nikhil Verma","Joonas Linnosmaa","Leonardo Espinosa-Leal","Napat Vajragupta"],"pdf_url":"","comment":"80 pages, 6 tables, 2 figures, 5 appendices, proof-of-concept"},{"id":"http://arxiv.org/abs/2512.18505v1","updated":"2025-12-20T20:47:01Z","published":"2025-12-20T20:47:01Z","title":"Teaching and Critiquing Conceptualization and Operationalization in NLP","summary":"NLP researchers regularly invoke abstract concepts like \"interpretability,\" \"bias,\" \"reasoning,\" and \"stereotypes,\" without defining them. Each subfield has a shared understanding or conceptualization of what these terms mean and how we should treat them, and this shared understanding is the basis on which operational decisions are made: Datasets are built to evaluate these concepts, metrics are proposed to quantify them, and claims are made about systems. But what do they mean, what should they mean, and how should we measure them? I outline a seminar I created for students to explore these questions of conceptualization and operationalization, with an interdisciplinary reading list and an emphasis on discussion and critique.","authors":["Vagrant Gautam"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.18475v1","updated":"2025-12-20T19:38:07Z","published":"2025-12-20T19:38:07Z","title":"Research on a hybrid LSTM-CNN-Attention model for text-based web content classification","summary":"This study presents a hybrid deep learning architecture that integrates LSTM, CNN, and an Attention mechanism to enhance the classification of web content based on text. Pretrained GloVe embeddings are used to represent words as dense vectors that preserve semantic similarity. The CNN layer extracts local n-gram patterns and lexical features, while the LSTM layer models long-range dependencies and sequential structure. The integrated Attention mechanism enables the model to focus selectively on the most informative parts of the input sequence. A 5-fold cross-validation setup was used to assess the robustness and generalizability of the proposed solution. Experimental results show that the hybrid LSTM-CNN-Attention model achieved outstanding performance, with an accuracy of 0.98, precision of 0.94, recall of 0.92, and F1-score of 0.93. These results surpass the performance of baseline models based solely on CNNs, LSTMs, or transformer-based classifiers such as BERT. The combination of neural network components enabled the model to effectively capture both fine-grained text structures and broader semantic context. Furthermore, the use of GloVe embeddings provided an efficient and effective representation of textual data, making the model suitable for integration into systems with real-time or near-real-time requirements. The proposed hybrid architecture demonstrates high effectiveness in text-based web content classification, particularly in tasks requiring both syntactic feature extraction and semantic interpretation. By combining presented mechanisms, the model addresses the limitations of individual architectures and achieves improved generalization. These findings support the broader use of hybrid deep learning approaches in NLP applications, especially where complex, unstructured textual data must be processed and classified with high reliability.","authors":["Mykola Kuz","Ihor Lazarovych","Mykola Kozlenko","Mykola Pikuliak","Andrii Kvasniuk"],"pdf_url":"","comment":"10 pages, 5 figures, 2 tables. Accepted by Radio Electronics Computer Science Control 2025"},{"id":"http://arxiv.org/abs/2512.18462v1","updated":"2025-12-20T18:30:54Z","published":"2025-12-20T18:30:54Z","title":"Mitigating Spurious Correlations in NLI via LLM-Synthesized Counterfactuals and Dynamic Balanced Sampling","summary":"Natural Language Inference (NLI) models frequently rely on spurious correlations rather than semantic reasoning. Existing mitigation strategies often incur high annotation costs or trigger catastrophic forgetting during fine-tuning. We propose an automated, scalable pipeline to address these limitations. First, we introduce Log-Frequency LMI (LF-LMI) to accurately detect semantic artifacts. Second, we generate a high-quality synthetic contrast set via an LLM-synthesis pipeline with multi-judge verification. Finally, we introduce Dynamic Balanced Sampling, a training strategy that rotates the original data distribution to prevent forgetting. Our method improves consistency on a challenging benchmark from 63.5% to 81.0% while maintaining 88.4% in-domain accuracy, significantly outperforming naive fine-tuning.","authors":["Christopher Román Jaimes"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2511.17004v2","updated":"2025-12-20T18:28:12Z","published":"2025-11-21T07:14:46Z","title":"Vision Language Models are Confused Tourists","summary":"Although the cultural dimension has been one of the key aspects in evaluating Vision-Language Models (VLMs), their ability to remain stable across diverse cultural inputs remains largely untested, despite being crucial to support diversity and multicultural societies. Existing evaluations often rely on benchmarks featuring only a singular cultural concept per image, overlooking scenarios where multiple, potentially unrelated cultural cues coexist. To address this gap, we introduce ConfusedTourist, a novel cultural adversarial robustness suite designed to assess VLMs' stability against perturbed geographical cues. Our experiments reveal a critical vulnerability, where accuracy drops heavily under simple image-stacking perturbations and even worsens with its image-generation-based variant. Interpretability analyses further show that these failures stem from systematic attention shifts toward distracting cues, diverting the model from its intended focus. These findings highlight a critical challenge: visual cultural concept mixing can substantially impair even state-of-the-art VLMs, underscoring the urgent need for more culturally robust multimodal understanding.","authors":["Patrick Amadeus Irawan","Ikhlasul Akmal Hanif","Muhammad Dehan Al Kautsar","Genta Indra Winata","Fajri Koto","Alham Fikri Aji"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.05387v2","updated":"2025-12-20T17:46:25Z","published":"2025-12-05T02:59:43Z","title":"Learning from Self Critique and Refinement for Faithful LLM Summarization","summary":"Large Language Models (LLMs) often suffer from hallucinations: output content that is not grounded in the input context, when performing long-form text generation tasks such as summarization. Prior works have shown that hallucinations can be reduced by iteratively critiquing and refining previously generated outputs using either the same model or a more powerful teacher model as the critique. However, these approaches either require additional test-time compute or assume access to more powerful teacher models, making them costly and less practical. In this work, we propose Self Critique and Refinement-based Preference Optimization (SCRPO), which is a self-supervised training framework that first constructs a preference dataset by leveraging the LLM's own critique and refinement capabilities, and then applies preference learning to improve the same LLM for faithful summarization. Experiments on three summarization benchmarks (XSUM CNNDM and SAMSum), demonstrate that our approach outperforms state-of-the-art self-supervised learning methods in terms of faithfulness metrics while either maintaining or improving other metrics that measure the overall quality of the summary. Moreover, compared to test-time refinement, our approach not only improves efficiency but also results in more faithful summaries.","authors":["Ting-Yao Hu","Hema Swetha Koppula","Hadi Pouransari","Cem Koc","Oncel Tuzel","Raviteja Vemulapalli"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.18440v1","updated":"2025-12-20T17:26:39Z","published":"2025-12-20T17:26:39Z","title":"An Agentic AI Framework for Training General Practitioner Student Skills","summary":"Advancements in large language models offer strong potential for enhancing virtual simulated patients (VSPs) in medical education by providing scalable alternatives to resource-intensive traditional methods. However, current VSPs often struggle with medical accuracy, consistent roleplaying, scenario generation for VSP use, and educationally structured feedback. We introduce an agentic framework for training general practitioner student skills that unifies (i) configurable, evidence-based vignette generation, (ii) controlled persona-driven patient dialogue with optional retrieval grounding, and (iii) standards-based assessment and feedback for both communication and clinical reasoning. We instantiate the framework in an interactive spoken consultation setting and evaluate it with medical students ($\\mathbf{N{=}14}$). Participants reported realistic and vignette-faithful dialogue, appropriate difficulty calibration, a stable personality signal, and highly useful example-rich feedback, alongside excellent overall usability. These results support agentic separation of scenario control, interaction control, and standards-based assessment as a practical pattern for building dependable and pedagogically valuable VSP training tools.","authors":["Victor De Marez","Jens Van Nooten","Luna De Bruyne","Walter Daelemans"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.18399v1","updated":"2025-12-20T15:32:10Z","published":"2025-12-20T15:32:10Z","title":"AraToken: Optimizing Arabic Tokenization with Normalization Pipeline and Language Extension for Qwen3","summary":"Tokenization is a critical preprocessing step for large language models (LLMs), directly impacting training efficiency and downstream performance. General-purpose tokenizers trained predominantly on English and Latin-script languages exhibit suboptimal performance on morphologically rich languages such as Arabic, resulting in inflated token sequences and reduced compression efficiency. In this work, we present AraToken, an Arabic-optimized tokenizer built on SentencePiece Unigram algorithm with a comprehensive normalization pipeline addressing Arabic-specific orthographic variations including Alif variants, diacritics, and Arabic-Indic numerals. We systematically compare BPE, WordPiece, and SentencePiece algorithms across multiple configurations, demonstrating that SentencePiece with normalization achieves 18% lower fertility (1.199 vs 1.35 tokens/word) compared to unnormalized baselines. Furthermore, we introduce the Language Extension Pipeline (LEP), a method for integrating the optimized tokenizer into Qwen3-0.6B through vocabulary extension with mean subtoken initialization and selective transformer layer unfreezing. Our experiments show that LEP reduces evaluation loss from 8.28 to 2.43 within 800 training steps on 100K Arabic samples. We release our tokenizer, training scripts, and model checkpoints to facilitate Arabic NLP research.","authors":["Mark Kashirskiy","Artiom Lipinski","Ilya Makarov"],"pdf_url":"","comment":"8 pages, 8 figures, 5 tables"},{"id":"http://arxiv.org/abs/2502.11361v5","updated":"2025-12-20T13:50:00Z","published":"2025-02-17T02:18:47Z","title":"VLDBench Evaluating Multimodal Disinformation with Regulatory Alignment","summary":"Detecting disinformation that blends manipulated text and images has become increasingly challenging, as AI tools make synthetic content easy to generate and disseminate. While most existing AI safety benchmarks focus on single modality misinformation (i.e., false content shared without intent to deceive), intentional multimodal disinformation, such as propaganda or conspiracy theories that imitate credible news, remains largely unaddressed. We introduce the Vision-Language Disinformation Detection Benchmark (VLDBench), the first large-scale resource supporting both unimodal (text-only) and multimodal (text + image) disinformation detection. VLDBench comprises approximately 62,000 labeled text-image pairs across 13 categories, curated from 58 news outlets. Using a semi-automated pipeline followed by expert review, 22 domain experts invested over 500 hours to produce high-quality annotations with substantial inter-annotator agreement. Evaluations of state-of-the-art Large Language Models (LLMs) and Vision-Language Models (VLMs) on VLDBench show that incorporating visual cues improves detection accuracy by 5 to 35 percentage points over text-only models. VLDBench provides data and code for evaluation, fine-tuning, and robustness testing to support disinformation analysis. Developed in alignment with AI governance frameworks (e.g., the MIT AI Risk Repository), VLDBench offers a principled foundation for advancing trustworthy disinformation detection in multimodal media.\n  Project: https://vectorinstitute.github.io/VLDBench/ Dataset: https://huggingface.co/datasets/vector-institute/VLDBench Code: https://github.com/VectorInstitute/VLDBench","authors":["Shaina Raza","Ashmal Vayani","Aditya Jain","Aravind Narayanan","Vahid Reza Khazaie","Syed Raza Bashir","Elham Dolatabadi","Gias Uddin","Christos Emmanouilidis","Rizwan Qureshi","Mubarak Shah"],"pdf_url":"","comment":"Accepted in Information Fusion Journal"},{"id":"http://arxiv.org/abs/2510.06445v2","updated":"2025-12-20T13:42:00Z","published":"2025-10-07T20:32:20Z","title":"A Survey on Agentic Security: Applications, Threats and Defenses","summary":"In this work we present the first holistic survey of the agentic security landscape, structuring the field around three fundamental pillars: Applications, Threats, and Defenses. We provide a comprehensive taxonomy of over 160 papers, explaining how agents are used in downstream cybersecurity applications, inherent threats to agentic systems, and countermeasures designed to protect them. A detailed cross-cutting analysis shows emerging trends in agent architecture while revealing critical research gaps in model and modality coverage. A complete and continuously updated list of all surveyed papers is publicly available at https://github.com/kagnlp/Awesome-Agentic-Security.","authors":["Asif Shahriar","Md Nafiu Rahman","Sadif Ahmed","Farig Sadeque","Md Rizwan Parvez"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.18362v1","updated":"2025-12-20T13:24:59Z","published":"2025-12-20T13:24:59Z","title":"SRS-Stories: Vocabulary-constrained multilingual story generation for language learning","summary":"In this paper, we use large language models to generate personalized stories for language learners, using only the vocabulary they know. The generated texts are specifically written to teach the user new vocabulary by simply reading stories where it appears in context, while at the same time seamlessly reviewing recently learned vocabulary. The generated stories are enjoyable to read and the vocabulary reviewing/learning is optimized by a Spaced Repetition System. The experiments are conducted in three languages: English, Chinese and Polish, evaluating three story generation methods and three strategies for enforcing lexical constraints. The results show that the generated stories are more grammatical, coherent, and provide better examples of word usage than texts generated by the standard constrained beam search approach","authors":["Wiktor Kamzela","Mateusz Lango","Ondrej Dusek"],"pdf_url":"","comment":"EMNLP 2025"},{"id":"http://arxiv.org/abs/2512.18360v1","updated":"2025-12-20T13:16:51Z","published":"2025-12-20T13:16:51Z","title":"LLM Agents Implement an NLG System from Scratch: Building Interpretable Rule-Based RDF-to-Text Generators","summary":"We present a novel neurosymbolic framework for RDF-to-text generation, in which the model is \"trained\" through collaborative interactions among multiple LLM agents rather than traditional backpropagation. The LLM agents produce rule-based Python code for a generator for the given domain, based on RDF triples only, with no in-domain human reference texts. The resulting system is fully interpretable, requires no supervised training data, and generates text nearly instantaneously using only a single CPU. Our experiments on the WebNLG and OpenDialKG data show that outputs produced by our approach reduce hallucination, with only slight fluency penalties compared to finetuned or prompted language models","authors":["Mateusz Lango","Ondřej Dušek"],"pdf_url":"","comment":"EMNLP 2025"},{"id":"http://arxiv.org/abs/2510.22295v2","updated":"2025-12-20T13:08:30Z","published":"2025-10-25T13:38:20Z","title":"VietLyrics: A Large-Scale Dataset and Models for Vietnamese Automatic Lyrics Transcription","summary":"Automatic Lyrics Transcription (ALT) for Vietnamese music presents unique challenges due to its tonal complexity and dialectal variations, but remains largely unexplored due to the lack of a dedicated dataset. Therefore, we curated the first large-scale Vietnamese ALT dataset (VietLyrics), comprising 647 hours of songs with line-level aligned lyrics and metadata to address these issues. Our evaluation of current ASRbased approaches reveal significant limitations, including frequent transcription errors and hallucinations in non-vocal segments. To improve performance, we fine-tuned Whisper models on the VietLyrics dataset, achieving superior results compared to existing multilingual ALT systems, including LyricWhiz. We publicly release VietLyrics and our models, aiming to advance Vietnamese music computing research while demonstrating the potential of this approach for ALT in low-resource language and music.","authors":["Quoc Anh Nguyen","Bernard Cheng","Kelvin Soh"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2511.02366v2","updated":"2025-12-20T12:57:11Z","published":"2025-11-04T08:44:09Z","title":"LiveSecBench: A Dynamic and Event-Driven Safety Benchmark for Chinese Language Model Applications","summary":"We introduce LiveSecBench, a continuously updated safety benchmark specifically for Chinese-language LLM application scenarios. LiveSecBench constructs a high-quality and unique dataset through a pipeline that combines automated generation with human verification. By periodically releasing new versions to expand the dataset and update evaluation metrics, LiveSecBench provides a robust and up-to-date standard for AI safety. In this report, we introduce our second release v251215, which evaluates across five dimensions (Public Safety, Fairness & Bias, Privacy, Truthfulness, and Mental Health Safety.) We evaluate 57 representative LLMs using an ELO rating system, offering a leaderboard of the current state of Chinese LLM safety. The result is available at https://livesecbench.intokentech.cn/.","authors":["Yudong Li","Peiru Yang","Feng Huang","Zhongliang Yang","Kecheng Wang","Haitian Li","Baocheng Chen","Xingyu An","Ziyu Liu","Youdan Yang","Kejiang Chen","Sifang Wan","Xu Wang","Yufei Sun","Liyan Wu","Ruiqi Zhou","Wenya Wen","Xingchi Gu","Tianxin Zhang","Yue Gao","Yongfeng Huang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.18357v1","updated":"2025-12-20T12:56:06Z","published":"2025-12-20T12:56:06Z","title":"DACE For Railway Acronym Disambiguation","summary":"Acronym Disambiguation (AD) is a fundamental challenge in technical text processing, particularly in specialized sectors where high ambiguity complicates automated analysis. This paper addresses AD within the context of the TextMine'26 competition on French railway documentation. We present DACE (Dynamic Prompting, Retrieval Augmented Generation, Contextual Selection, and Ensemble Aggregation), a framework that enhances Large Language Models through adaptive in-context learning and external domain knowledge injection. By dynamically tailoring prompts to acronym ambiguity and aggregating ensemble predictions, DACE mitigates hallucination and effectively handles low-resource scenarios. Our approach secured the top rank in the competition with an F1 score of 0.9069.","authors":["El Mokhtar Hribach","Oussama Mechhour","Mohammed Elmonstaser","Yassine El Boudouri","Othmane Kabal"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.18352v1","updated":"2025-12-20T12:42:27Z","published":"2025-12-20T12:42:27Z","title":"LLM-based Few-Shot Early Rumor Detection with Imitation Agent","summary":"Early Rumor Detection (EARD) aims to identify the earliest point at which a claim can be accurately classified based on a sequence of social media posts. This is especially challenging in data-scarce settings. While Large Language Models (LLMs) perform well in few-shot NLP tasks, they are not well-suited for time-series data and are computationally expensive for both training and inference. In this work, we propose a novel EARD framework that combines an autonomous agent and an LLM-based detection model, where the agent acts as a reliable decision-maker for \\textit{early time point determination}, while the LLM serves as a powerful \\textit{rumor detector}. This approach offers the first solution for few-shot EARD, necessitating only the training of a lightweight agent and allowing the LLM to remain training-free. Extensive experiments on four real-world datasets show our approach boosts performance across LLMs and surpasses existing EARD methods in accuracy and earliness.","authors":["Fengzhu Zeng","Qian Shao","Ling Cheng","Wei Gao","Shih-Fen Cheng","Jing Ma","Cheng Niu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.18337v1","updated":"2025-12-20T12:06:13Z","published":"2025-12-20T12:06:13Z","title":"Towards Efficient Agents: A Co-Design of Inference Architecture and System","summary":"The rapid development of large language model (LLM)-based agents has unlocked new possibilities for autonomous multi-turn reasoning and tool-augmented decision-making. However, their real-world deployment is hindered by severe inefficiencies that arise not from isolated model inference, but from the systemic latency accumulated across reasoning loops, context growth, and heterogeneous tool interactions. This paper presents AgentInfer, a unified framework for end-to-end agent acceleration that bridges inference optimization and architectural design. We decompose the problem into four synergistic components: AgentCollab, a hierarchical dual-model reasoning framework that balances large- and small-model usage through dynamic role assignment; AgentSched, a cache-aware hybrid scheduler that minimizes latency under heterogeneous request patterns; AgentSAM, a suffix-automaton-based speculative decoding method that reuses multi-session semantic memory to achieve low-overhead inference acceleration; and AgentCompress, a semantic compression mechanism that asynchronously distills and reorganizes agent memory without disrupting ongoing reasoning. Together, these modules form a Self-Evolution Engine capable of sustaining efficiency and cognitive stability throughout long-horizon reasoning tasks. Experiments on the BrowseComp-zh and DeepDiver benchmarks demonstrate that through the synergistic collaboration of these methods, AgentInfer reduces ineffective token consumption by over 50%, achieving an overall 1.8-2.5 times speedup with preserved accuracy. These results underscore that optimizing for agentic task completion-rather than merely per-token throughput-is the key to building scalable, efficient, and self-improving intelligent systems.","authors":["Weizhe Lin","Hui-Ling Zhen","Shuai Yang","Xian Wang","Renxi Liu","Hanting Chen","Wangze Zhang","Chuansai Zhou","Yiming Li","Chen Chen","Xing Li","Zhiyuan Yang","Xiaosong Li","Xianzhi Yu","Zhenhua Dong","Mingxuan Yuan","Yunhe Wang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.18329v1","updated":"2025-12-20T11:53:37Z","published":"2025-12-20T11:53:37Z","title":"LIR$^3$AG: A Lightweight Rerank Reasoning Strategy Framework for Retrieval-Augmented Generation","summary":"Retrieval-Augmented Generation (RAG) effectively enhances Large Language Models (LLMs) by incorporating retrieved external knowledge into the generation process. Reasoning models improve LLM performance in multi-hop QA tasks, which require integrating and reasoning over multiple pieces of evidence across different documents to answer a complex question. However, they often introduce substantial computational costs, including increased token consumption and inference latency. To better understand and mitigate this trade-off, we conduct a comprehensive study of reasoning strategies for reasoning models in RAG multi-hop QA tasks. Our findings reveal that reasoning models adopt structured strategies to integrate retrieved and internal knowledge, primarily following two modes: Context-Grounded Reasoning, which relies directly on retrieved content, and Knowledge-Reconciled Reasoning, which resolves conflicts or gaps using internal knowledge. To this end, we propose a novel Lightweight Rerank Reasoning Strategy Framework for RAG (LiR$^3$AG) to enable non-reasoning models to transfer reasoning strategies by restructuring retrieved evidence into coherent reasoning chains. LiR$^3$AG significantly reduce the average 98% output tokens overhead and 58.6% inferencing time while improving 8B non-reasoning model's F1 performance ranging from 6.2% to 22.5% to surpass the performance of 32B reasoning model in RAG, offering a practical and efficient path forward for RAG systems.","authors":["Guo Chen","Junjie Huang","Huaijin Xie","Fei Sun","Tao Jia"],"pdf_url":"","comment":"AAAI2026"},{"id":"http://arxiv.org/abs/2512.18321v1","updated":"2025-12-20T11:39:21Z","published":"2025-12-20T11:39:21Z","title":"CTTA-T: Continual Test-Time Adaptation for Text Understanding via Teacher-Student with a Domain-aware and Generalized Teacher","summary":"Text understanding often suffers from domain shifts. To handle testing domains, domain adaptation (DA) is trained to adapt to a fixed and observed testing domain; a more challenging paradigm, test-time adaptation (TTA), cannot access the testing domain during training and online adapts to the testing samples during testing, where the samples are from a fixed domain. We aim to explore a more practical and underexplored scenario, continual test-time adaptation (CTTA) for text understanding, which involves a sequence of testing (unobserved) domains in testing. Current CTTA methods struggle in reducing error accumulation over domains and enhancing generalization to handle unobserved domains: 1) Noise-filtering reduces accumulated errors but discards useful information, and 2) accumulating historical domains enhances generalization, but it is hard to achieve adaptive accumulation. In this paper, we propose a CTTA-T (continual test-time adaptation for text understanding) framework adaptable to evolving target domains: it adopts a teacher-student framework, where the teacher is domain-aware and generalized for evolving domains. To improve teacher predictions, we propose a refine-then-filter based on dropout-driven consistency, which calibrates predictions and removes unreliable guidance. For the adaptation-generalization trade-off, we construct a domain-aware teacher by dynamically accumulating cross-domain semantics via incremental PCA, which continuously tracks domain shifts. Experiments show CTTA-T excels baselines.","authors":["Tianlun Liu","Zhiliang Tian","Zhen Huang","Xingzhi Zhou","Wanlong Yu","Tianle Liu","Feng Liu","Dongsheng Li"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2408.02152v2","updated":"2025-12-20T11:31:50Z","published":"2024-08-04T22:00:34Z","title":"Generative Retrieval with Few-shot Indexing","summary":"Existing generative retrieval (GR) methods rely on training-based indexing, which fine-tunes a model to memorise associations between queries and the document identifiers (docids) of relevant documents. Training-based indexing suffers from high training costs, under-utilisation of pre-trained knowledge in large language models (LLMs), and limited adaptability to dynamic document corpora. To address the issues, we propose a few-shot indexing-based GR framework (Few-Shot GR). It has a few-shot indexing process without any training, where we prompt an LLM to generate docids for all documents in a corpus, ultimately creating a docid bank for the entire corpus. During retrieval, we feed a query to the same LLM and constrain it to generate a docid within the docid bank created during indexing, and then map the generated docid back to its corresponding document. Moreover, we devise few-shot indexing with one-to-many mapping to further enhance Few-Shot GR. Experiments show that Few-Shot GR achieves superior performance to state-of-the-art GR methods requiring heavy training.","authors":["Arian Askari","Chuan Meng","Mohammad Aliannejadi","Zhaochun Ren","Evangelos Kanoulas","Suzan Verberne"],"pdf_url":"","comment":"Accepted for publication at the 48th European Conference on Information Retrieval (ECIR 2026)"},{"id":"http://arxiv.org/abs/2507.20423v3","updated":"2025-12-20T10:58:40Z","published":"2025-07-27T21:49:36Z","title":"CodeNER: Code Prompting for Named Entity Recognition","summary":"Recent studies have explored various approaches for treating candidate named entity spans as both source and target sequences in named entity recognition (NER) by leveraging large language models (LLMs). Although previous approaches have successfully generated candidate named entity spans with suitable labels, they rely solely on input context information when using LLMs, particularly, ChatGPT. However, NER inherently requires capturing detailed labeling requirements with input context information. To address this issue, we propose a novel method that leverages code-based prompting to improve the capabilities of LLMs in understanding and performing NER. By embedding code within prompts, we provide detailed BIO schema instructions for labeling, thereby exploiting the ability of LLMs to comprehend long-range scopes in programming languages. Experimental results demonstrate that the proposed code-based prompting method outperforms conventional text-based prompting on ten benchmarks across English, Arabic, Finnish, Danish, and German datasets, indicating the effectiveness of explicitly structuring NER instructions. We also verify that combining the proposed code-based prompting method with the chain-of-thought prompting further improves performance.","authors":["Sungwoo Han","Jingun Kwon","Hidetaka Kamigaito","Manabu Okumura"],"pdf_url":"","comment":"18 pages, 6 figures"},{"id":"http://arxiv.org/abs/2512.18301v1","updated":"2025-12-20T10:16:09Z","published":"2025-12-20T10:16:09Z","title":"InstructNet: A Novel Approach for Multi-Label Instruction Classification through Advanced Deep Learning","summary":"People use search engines for various topics and items, from daily essentials to more aspirational and specialized objects. Therefore, search engines have taken over as peoples preferred resource. The How To prefix has become familiar and widely used in various search styles to find solutions to particular problems. This search allows people to find sequential instructions by providing detailed guidelines to accomplish specific tasks. Categorizing instructional text is also essential for task-oriented learning and creating knowledge bases. This study uses the How To articles to determine the multi-label instruction category. We have brought this work with a dataset comprising 11,121 observations from wikiHow, where each record has multiple categories. To find out the multi-label category meticulously, we employ some transformer-based deep neural architectures, such as Generalized Autoregressive Pretraining for Language Understanding (XLNet), Bidirectional Encoder Representation from Transformers (BERT), etc. In our multi-label instruction classification process, we have reckoned our proposed architectures using accuracy and macro f1-score as the performance metrics. This thorough evaluation showed us much about our strategys strengths and drawbacks. Specifically, our implementation of the XLNet architecture has demonstrated unprecedented performance, achieving an accuracy of 97.30% and micro and macro average scores of 89.02% and 93%, a noteworthy accomplishment in multi-label classification. This high level of accuracy and macro average score is a testament to the effectiveness of the XLNet architecture in our proposed InstructNet approach. By employing a multi-level strategy in our evaluation process, we have gained a more comprehensive knowledge of the effectiveness of our proposed architectures and identified areas for forthcoming improvement and refinement.","authors":["Tanjim Taharat Aurpa","Md Shoaib Ahmed","Md Mahbubur Rahman","Md. Golam Moazzam"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.18298v1","updated":"2025-12-20T10:05:58Z","published":"2025-12-20T10:05:58Z","title":"Explainable Transformer-CNN Fusion for Noise-Robust Speech Emotion Recognition","summary":"Speech Emotion Recognition (SER) systems often degrade in performance when exposed to the unpredictable acoustic interference found in real-world environments. Additionally, the opacity of deep learning models hinders their adoption in trust-sensitive applications. To bridge this gap, we propose a Hybrid Transformer-CNN framework that unifies the contextual modeling of Wav2Vec 2.0 with the spectral stability of 1D-Convolutional Neural Networks. Our dual-stream architecture processes raw waveforms to capture long-range temporal dependencies while simultaneously extracting noise-resistant spectral features (MFCC, ZCR, RMSE) via a custom Attentive Temporal Pooling mechanism. We conducted extensive validation across four diverse benchmark datasets: RAVDESS, TESS, SAVEE, and CREMA-D. To rigorously test robustness, we subjected the model to non-stationary acoustic interference using real-world noise profiles from the SAS-KIIT dataset. The proposed framework demonstrates superior generalization and state-of-the-art accuracy across all datasets, significantly outperforming single-branch baselines under realistic environmental interference. Furthermore, we address the ``black-box\" problem by integrating SHAP and Score-CAM into the evaluation pipeline. These tools provide granular visual explanations, revealing how the model strategically shifts attention between temporal and spectral cues to maintain reliability in the presence of complex environmental noise.","authors":["Sudip Chakrabarty","Pappu Bishwas","Rajdeep Chatterjee"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.18292v1","updated":"2025-12-20T09:33:55Z","published":"2025-12-20T09:33:55Z","title":"Measuring Fine-Grained Negotiation Tactics of Humans and LLMs in Diplomacy","summary":"The study of negotiation styles dates back to Aristotle's ethos-pathos-logos rhetoric. Prior efforts primarily studied the success of negotiation agents. Here, we shift the focus towards the styles of negotiation strategies. Our focus is the strategic dialogue board game Diplomacy, which affords rich natural language negotiation and measures of game success. We used LLM-as-a-judge to annotate a large human-human set of Diplomacy games for fine-grained negotiation tactics from a sociologically-grounded taxonomy. Using a combination of the It Takes Two and WebDiplomacy datasets, we demonstrate the reliability of our LLM-as-a-Judge framework and show strong correlations between negotiation features and success in the Diplomacy setting. Lastly, we investigate the differences between LLM and human negotiation strategies and show that fine-tuning can steer LLM agents toward more human-like negotiation behaviors.","authors":["Wenkai Li","Lynnette Hui Xian Ng","Andy Liu","Daniel Fried"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.18263v1","updated":"2025-12-20T08:03:07Z","published":"2025-12-20T08:03:07Z","title":"TICL+: A Case Study On Speech In-Context Learning for Children's Speech Recognition","summary":"Children's speech recognition remains challenging due to substantial acoustic and linguistic variability, limited labeled data, and significant differences from adult speech. Speech foundation models can address these challenges through Speech In-Context Learning (SICL), allowing adaptation to new domains without fine-tuning. However, the effectiveness of SICL depends on how in-context examples are selected. We extend an existing retrieval-based method, Text-Embedding KNN for SICL (TICL), introducing an acoustic reranking step to create TICL+. This extension prioritizes examples that are both semantically and acoustically aligned with the test input. Experiments on four children's speech corpora show that TICL+ achieves up to a 53.3% relative word error rate reduction over zero-shot performance and 37.6% over baseline TICL, highlighting the value of combining semantic and acoustic information for robust, scalable ASR in children's speech.","authors":["Haolong Zheng","Yekaterina Yegorova","Mark Hasegawa-Johnson"],"pdf_url":"","comment":"Published at IEEE ASRU 2025 Satellite Workshop-AI for Children's Speech and Language"},{"id":"http://arxiv.org/abs/2511.21728v2","updated":"2025-12-20T07:48:22Z","published":"2025-11-21T04:16:45Z","title":"Affective Multimodal Agents with Proactive Knowledge Grounding for Emotionally Aligned Marketing Dialogue","summary":"Recent advances in large language models (LLMs) have enabled fluent dialogue systems, but most remain reactive and struggle in emotionally rich, goal-oriented settings such as marketing conversations. To address this limitation, we propose AffectMind, a multimodal affective dialogue agent that performs proactive reasoning and dynamic knowledge grounding to sustain emotionally aligned and persuasive interactions. AffectMind combines three components: a Proactive Knowledge Grounding Network (PKGN) that continuously updates factual and affective context from text, vision, and prosody; an Emotion--Intent Alignment Model (EIAM) that jointly models user emotion and purchase intent to adapt persuasion strategies; and a Reinforced Discourse Loop (RDL) that optimizes emotional coherence and engagement via reinforcement signals from user responses. Experiments on two newly curated marketing dialogue datasets, MM-ConvMarket and AffectPromo, show that AffectMind outperforms strong LLM-based baselines in emotional consistency (+26\\%), persuasive success rate (+19\\%), and long-term user engagement (+23\\%), highlighting emotion-grounded proactivity as a key capability for commercial multimodal agents.","authors":["Lin Yu","Xiaofei Han","Yifei Kang","Chiung-Yi Tseng","Danyang Zhang","Ziqian Bi","Zhimo Han"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2504.02800v4","updated":"2025-12-20T07:22:06Z","published":"2025-04-03T17:43:14Z","title":"Survey and Experiments on Mental Disorder Detection via Social Media: From Large Language Models and RAG to Agents","summary":"Mental disorders represent a critical global health challenge, and social media is increasingly viewed as a vital resource for real-time digital phenotyping and intervention. To leverage this data, large language models (LLMs) have been introduced, offering stronger semantic understanding and reasoning than traditional deep learning, thereby enhancing the explainability of detection results. Despite the growing prominence of LLMs in this field, there is a scarcity of scholarly works that systematically synthesize how advanced enhancement techniques, specifically Retrieval-Augmented Generation (RAG) and Agentic systems, can be utilized to address these reliability and reasoning limitations. Here, we systematically survey the evolving landscape of LLM-based methods for social media mental disorder analysis, spanning standard pre-trained language models, RAG to mitigate hallucinations and contextual gaps, and agentic systems for autonomous reasoning and multi-step intervention. We organize existing work by technical paradigm and clinical target, extending beyond common internalizing disorders to include psychotic disorders and externalizing behaviors. Additionally, the paper comprehensively evaluates the performance of LLMs, including the impact of RAG, across various tasks. This work establishes a unified benchmark for the field, paving the way for the development of trustworthy, autonomous AI systems that can deliver precise and explainable mental health support.","authors":["Zhuohan Ge","Darian Li","Yubo Wang","Nicole Hu","Xinyi Zhu","Haoyang Li","Xin Zhang","Mingtao Zhang","Shihao Qi","Yuming Xu","Han Shi","Chen Jason Zhang","Qing Li"],"pdf_url":"","comment":"20 pages, 10 figures. This is an extension of ICDEW 2025"},{"id":"http://arxiv.org/abs/2512.18231v1","updated":"2025-12-20T06:22:38Z","published":"2025-12-20T06:22:38Z","title":"Investigating Spatial Attention Bias in Vision-Language Models","summary":"Vision-Language Models have demonstrated remarkable capabilities in understanding visual content, yet systematic biases in their spatial processing remain largely unexplored. This work identifies and characterizes a systematic spatial attention bias where VLMs consistently prioritize describing left-positioned content before right-positioned content in horizontally concatenated images. Through controlled experiments on image pairs using both open-source and closed-source models, we demonstrate that this bias persists across different architectures, with models describing left-positioned content first in approximately 97% of cases under neutral prompting conditions. Testing on an Arabic-finetuned model reveals that the bias persists despite right-to-left language training, ruling out language reading direction as the primary cause. Investigation of training dataset annotation guidelines from PixMo and Visual Genome reveals no explicit left-first ordering instructions, suggesting the bias is consistent with architectural factors rather than explicit training data instructions. These findings reveal fundamental limitations in how current VLMs process spatial information.","authors":["Aryan Chaudhary","Sanchit Goyal","Pratik Narang","Dhruv Kumar"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.18225v1","updated":"2025-12-20T05:46:57Z","published":"2025-12-20T05:46:57Z","title":"GeoSense-AI: Fast Location Inference from Crisis Microblogs","summary":"This paper presents an applied AI pipeline for realtime geolocation from noisy microblog streams, unifying statistical hashtag segmentation, part-of-speech-driven proper-noun detection, dependency parsing around disaster lexicons, lightweight named-entity recognition, and gazetteer-grounded disambiguation to infer locations directly from text rather than sparse geotags. The approach operationalizes information extraction under streaming constraints, emphasizing low-latency NLP components and efficient validation against geographic knowledge bases to support situational awareness during emergencies. In head to head comparisons with widely used NER toolkits, the system attains strong F1 while being engineered for orders-of-magnitude faster throughput, enabling deployment in live crisis informatics settings. A production map interface demonstrates end-to-end AI functionality ingest, inference, and visualization--surfacing locational signals at scale for floods, outbreaks, and other fastmoving events. By prioritizing robustness to informal text and streaming efficiency, GeoSense-AI illustrates how domain-tuned NLP and knowledge grounding can elevate emergency response beyond conventional geo-tag reliance.","authors":["Deepit Sapru"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2506.13274v3","updated":"2025-12-20T05:39:49Z","published":"2025-06-16T09:14:01Z","title":"AdaLRS: Loss-Guided Adaptive Learning Rate Search for Efficient Foundation Model Pretraining","summary":"Learning rate is widely regarded as crucial for effective foundation model pretraining. Recent research explores and demonstrates the transferability of learning rate configurations across varying model and dataset sizes, etc. Nevertheless, these approaches are constrained to specific training scenarios and typically necessitate extensive hyperparameter tuning on proxy models. In this work, we propose \\textbf{AdaLRS}, a plug-in-and-play adaptive learning rate search algorithm that conducts online optimal learning rate search via optimizing loss descent velocities. We provide theoretical and experimental analyzes to show that foundation model pretraining loss and its descent velocity are both convex and share the same optimal learning rate. Relying solely on training loss dynamics, AdaLRS involves few extra computations to guide the search process, and its convergence is guaranteed via theoretical analysis. Experiments on both LLM and VLM pretraining show that AdaLRS adjusts suboptimal learning rates to the neighborhood of optimum with marked efficiency and effectiveness, with model performance improved accordingly. We also show the robust generalizability of AdaLRS across varying training scenarios, such as different model sizes, training paradigms, base learning rate scheduler choices, and hyperparameter settings.","authors":["Hongyuan Dong","Dingkang Yang","Xiao Liang","Chao Feng","Jiao Ran"],"pdf_url":"","comment":"NeurIPS 2025 Main Conference"},{"id":"http://arxiv.org/abs/2512.18215v1","updated":"2025-12-20T05:07:53Z","published":"2025-12-20T05:07:53Z","title":"Stable and Efficient Single-Rollout RL for Multimodal Reasoning","summary":"Reinforcement Learning with Verifiable Rewards (RLVR) has become a key paradigm to improve the reasoning capabilities of Multimodal Large Language Models (MLLMs). However, prevalent group-based algorithms such as GRPO require multi-rollout sampling for each prompt. While more efficient single-rollout variants have recently been explored in text-only settings, we find that they suffer from severe instability in multimodal contexts, often leading to training collapse. To address this training efficiency-stability trade-off, we introduce $\\textbf{MSSR}$ (Multimodal Stabilized Single-Rollout), a group-free RLVR framework that achieves both stable optimization and effective multimodal reasoning performance. MSSR achieves this via an entropy-based advantage-shaping mechanism that adaptively regularizes advantage magnitudes, preventing collapse and maintaining training stability. While such mechanisms have been used in group-based RLVR, we show that in the multimodal single-rollout setting they are not merely beneficial but essential for stability. In in-distribution evaluations, MSSR demonstrates superior training compute efficiency, achieving similar validation accuracy to the group-based baseline with half the training steps. When trained for the same number of steps, MSSR's performance surpasses the group-based baseline and shows consistent generalization improvements across five diverse reasoning-intensive benchmarks. Together, these results demonstrate that MSSR enables stable, compute-efficient, and effective RLVR for complex multimodal reasoning tasks.","authors":["Rui Liu","Dian Yu","Lei Ke","Haolin Liu","Yujun Zhou","Zhenwen Liang","Haitao Mi","Pratap Tokekar","Dong Yu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.10398v5","updated":"2025-12-20T04:25:21Z","published":"2025-12-11T08:05:58Z","title":"Confucius Code Agent: Scalable Agent Scaffolding for Real-World Codebases","summary":"Real-world software engineering tasks require coding agents that can operate over massive repositories, sustain long-horizon sessions, and reliably coordinate complex toolchains at test time. Existing research-grade coding agents offer transparency but struggle when scaled to heavier, production-level workloads, while production-grade systems achieve strong practical performance but provide limited extensibility, interpretability, and controllability. We introduce the Confucius Code Agent (CCA), a software engineering agent that can operate at large-scale codebases. CCA is built on top of the Confucius SDK, an agent development platform structured around three complementary perspectives: Agent Experience (AX), User Experience (UX), and Developer Experience (DX). The SDK integrates a unified orchestrator with hierarchical working memory for long-context reasoning, a persistent note-taking system for cross-session continual learning, and a modular extension system for reliable tool use. In addition, we introduce a meta-agent that automates the synthesis, evaluation, and refinement of agent configurations through a build-test-improve loop, enabling rapid adaptation to new tasks, environments, and tool stacks. Instantiated with these mechanisms, CCA demonstrates strong performance on real-world software engineering tasks. On SWE-Bench-Pro, CCA reaches a Resolve@1 of 54.3%, exceeding prior research baselines and comparing favorably to commercial results, under identical repositories, model backends, and tool access.","authors":["Sherman Wong","Zhenting Qi","Zhaodong Wang","Nathan Hu","Samuel Lin","Jun Ge","Erwin Gao","Wenlin Chen","Yilun Du","Minlan Yu","Ying Zhang"],"pdf_url":"","comment":"The latest version"},{"id":"http://arxiv.org/abs/2511.02833v3","updated":"2025-12-20T03:45:29Z","published":"2025-11-04T18:58:47Z","title":"In Good GRACEs: Principled Teacher Selection for Knowledge Distillation","summary":"Knowledge distillation is an efficient strategy to use data generated by large \"teacher\" language models to train smaller capable \"student\" models, but selecting the optimal teacher for a specific student-task combination requires expensive trial-and-error. We propose a lightweight score called GRACE to quantify how effective a teacher will be for post-training a student model. GRACE measures distributional properties of the student's gradients without access to a verifier, teacher logits, teacher internals, or test data. From an information-theoretic perspective, GRACE connects to leave-one-out stability of gradient-based algorithms, which controls the generalization performance of the distilled students. On GSM8K and MATH, GRACE correlates strongly (up to 86% Spearman correlation) with the performance of the distilled LLaMA and OLMo students. In particular, training a student using the GRACE-selected teacher can improve the performance by up to 7.4% over naively using the best-performing teacher. Further, GRACE can provide guidance on crucial design choices in distillation, including (1) the best temperature to use when generating from the teacher, (2) the best teacher to use given a size constraint, and (3) the best teacher to use within a specific model family. Altogether, our findings demonstrate that GRACE can efficiently and effectively identify a strongly compatible teacher for a given student and provide fine-grained guidance on how to perform distillation.","authors":["Abhishek Panigrahi","Bingbin Liu","Sadhika Malladi","Sham Kakade","Surbhi Goel"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.18196v1","updated":"2025-12-20T03:43:02Z","published":"2025-12-20T03:43:02Z","title":"Training LLMs with LogicReward for Faithful and Rigorous Reasoning","summary":"Although LLMs exhibit strong reasoning capabilities, existing training methods largely depend on outcome-based feedback, which can produce correct answers with flawed reasoning. Prior work introduces supervision on intermediate steps but still lacks guarantees of logical soundness, which is crucial in high-stakes scenarios where logical consistency is paramount. To address this, we propose LogicReward, a novel reward system that guides model training by enforcing step-level logical correctness with a theorem prover. We further introduce Autoformalization with Soft Unification, which reduces natural language ambiguity and improves formalization quality, enabling more effective use of the theorem prover. An 8B model trained on data constructed with LogicReward surpasses GPT-4o and o4-mini by 11.6\\% and 2\\% on natural language inference and logical reasoning tasks with simple training procedures. Further analysis shows that LogicReward enhances reasoning faithfulness, improves generalizability to unseen tasks such as math and commonsense reasoning, and provides a reliable reward signal even without ground-truth labels. We will release all data and code at https://llm-symbol.github.io/LogicReward.","authors":["Jundong Xu","Hao Fei","Huichi Zhou","Xin Quan","Qijun Huang","Shengqiong Wu","William Yang Wang","Mong-Li Lee","Wynne Hsu"],"pdf_url":"","comment":"Preprint"},{"id":"http://arxiv.org/abs/2512.18190v1","updated":"2025-12-20T03:27:11Z","published":"2025-12-20T03:27:11Z","title":"External Hippocampus: Topological Cognitive Maps for Guiding Large Language Model Reasoning","summary":"This paper proposes the External Hippocampus framework, which models language model reasoning from a cognitive dynamics perspective as the flow of information energy in semantic space. Unlike traditional weight-space optimization methods, this framework constructs topological cognitive maps through dimensionality reduction projection, enabling precise navigation and intervention of energy flow at test time while avoiding substantial computational requirements and demonstrating predictable intervention patterns. The method effectively addresses the cognitive deadlock problem in multi-step reasoning for small models. Experiments on models <=7B parameters show: map-guided methods achieve 81.20% accuracy on 500 challenging problems (relative baseline +16.80%), reduce reasoning time by >= 15x, with key findings revealing that reasoning stagnation manifests as \"Cognitive Vortex\" and low-entropy potential wells, while temperature perturbations effectively restart energy flow. The framework requires no additional training, possesses autonomous growth capability, and provides an efficient and controllable topological-aware solution for small model reasoning.","authors":["Jian Yan"],"pdf_url":"","comment":"12 pages, 7 figures"},{"id":"http://arxiv.org/abs/2512.15926v2","updated":"2025-12-20T01:38:32Z","published":"2025-12-17T19:43:46Z","title":"DSO: Direct Steering Optimization for Bias Mitigation","summary":"Generative models are often deployed to make decisions on behalf of users, such as vision-language models (VLMs) identifying which person in a room is a doctor to help visually impaired individuals. Yet, VLM decisions are influenced by the perceived demographic attributes of people in the input, which can lead to biased outcomes like failing to identify women as doctors. Moreover, when reducing bias leads to performance loss, users may have varying needs for balancing bias mitigation with overall model capabilities, highlighting the demand for methods that enable controllable bias reduction during inference. Activation steering is a popular approach for inference-time controllability that has shown potential in inducing safer behavior in large language models (LLMs). However, we observe that current steering methods struggle to correct biases, where equiprobable outcomes across demographic groups are required. To address this, we propose Direct Steering Optimization (DSO) which uses reinforcement learning to find linear transformations for steering activations, tailored to mitigate bias while maintaining control over model performance. We demonstrate that DSO achieves state-of-the-art trade-off between fairness and capabilities on both VLMs and LLMs, while offering practitioners inference-time control over the trade-off. Overall, our work highlights the benefit of designing steering strategies that are directly optimized to control model behavior, providing more effective bias intervention than methods that rely on pre-defined heuristics for controllability.","authors":["Lucas Monteiro Paes","Nivedha Sivakumar","Yinong Oliver Wang","Masha Fedzechkina Donaldson","Barry-John Theobald","Luca Zappella","Nicholas Apostoloff"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2510.22860v2","updated":"2025-12-20T00:43:42Z","published":"2025-10-26T22:46:26Z","title":"Far from the Shallow: Brain-Predictive Reasoning Embedding through Residual Disentanglement","summary":"Understanding how the human brain progresses from processing simple linguistic inputs to performing high-level reasoning is a fundamental challenge in neuroscience. While modern large language models (LLMs) are increasingly used to model neural responses to language, their internal representations are highly \"entangled,\" mixing information about lexicon, syntax, meaning, and reasoning. This entanglement biases conventional brain encoding analyses toward linguistically shallow features (e.g., lexicon and syntax), making it difficult to isolate the neural substrates of cognitively deeper processes. Here, we introduce a residual disentanglement method that computationally isolates these components. By first probing an LM to identify feature-specific layers, our method iteratively regresses out lower-level representations to produce four nearly orthogonal embeddings for lexicon, syntax, meaning, and, critically, reasoning. We used these disentangled embeddings to model intracranial (ECoG) brain recordings from neurosurgical patients listening to natural speech. We show that: 1) This isolated reasoning embedding exhibits unique predictive power, accounting for variance in neural activity not explained by other linguistic features and even extending to the recruitment of visual regions beyond classical language areas. 2) The neural signature for reasoning is temporally distinct, peaking later (~350-400ms) than signals related to lexicon, syntax, and meaning, consistent with its position atop a processing hierarchy. 3) Standard, non-disentangled LLM embeddings can be misleading, as their predictive success is primarily attributable to linguistically shallow features, masking the more subtle contributions of deeper cognitive processing.","authors":["Linyang He","Tianjun Zhong","Richard Antonello","Gavin Mischler","Micah Goldblum","Nima Mesgarani"],"pdf_url":"","comment":"Accepted at NeurIPS 2025"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2512.18434v1","updated":"2025-12-20T17:21:41Z","published":"2025-12-20T17:21:41Z","title":"Efficient Optimization of Hierarchical Identifiers for Generative Recommendation","summary":"SEATER is a generative retrieval model that improves recommendation inference efficiency and retrieval quality by utilizing balanced tree-structured item identifiers and contrastive training objectives. We reproduce and validate SEATER's reported improvements in retrieval quality over strong baselines across all datasets from the original work, and extend the evaluation to Yambda, a large-scale music recommendation dataset. Our experiments verify SEATER's strong performance, but show that its tree construction step during training becomes a major bottleneck as the number of items grows. To address this, we implement and evaluate two alternative construction algorithms: a greedy method optimized for minimal build time, and a hybrid method that combines greedy clustering at high levels with more precise grouping at lower levels. The greedy method reduces tree construction time to less than 2% of the original with only a minor drop in quality on the dataset with the largest item collection. The hybrid method achieves retrieval quality on par with the original, and even improves on the largest dataset, while cutting construction time to just 5-8%. All data and code are publicly available for full reproducibility at https://github.com/joshrosie/re-seater.","authors":["Federica Valeau","Odysseas Boufalis","Polytimi Gkotsi","Joshua Rosenthal","David Vos"],"pdf_url":"","comment":"Accepted at ECIR 2026 Reproducibility Track (to appear)"},{"id":"http://arxiv.org/abs/2512.18384v1","updated":"2025-12-20T14:51:57Z","published":"2025-12-20T14:51:57Z","title":"Datasets for machine learning and for assessing the intelligence level of automatic patent search systems","summary":"The key to success in automating prior art search in patent research using artificial intelligence lies in developing large datasets for machine learning and ensuring their availability. This work is dedicated to providing a comprehensive solution to the problem of creating infrastructure for research in this field, including datasets and tools for calculating search quality criteria. The paper discusses the concept of semantic clusters of patent documents that determine the state of the art in a given subject, as proposed by the authors. A definition of such semantic clusters is also provided. Prior art search is presented as the task of identifying elements within a semantic cluster of patent documents in the subject area specified by the document under consideration. A generator of user-configurable datasets for machine learning, based on collections of U.S. and Russian patent documents, is described. The dataset generator creates a database of links to documents in semantic clusters. Then, based on user-defined parameters, it forms a dataset of semantic clusters in JSON format for machine learning. To evaluate machine learning outcomes, it is proposed to calculate search quality scores that account for semantic clusters of the documents being searched. To automate the evaluation process, the paper describes a utility developed by the authors for assessing the quality of prior art document search.","authors":["Boris Genin","Alexander Gorbunov","Dmitry Zolkin","Igor Nekrasov"],"pdf_url":"","comment":"14 pages, 3 figures, 2 tables"},{"id":"http://arxiv.org/abs/2408.02152v2","updated":"2025-12-20T11:31:50Z","published":"2024-08-04T22:00:34Z","title":"Generative Retrieval with Few-shot Indexing","summary":"Existing generative retrieval (GR) methods rely on training-based indexing, which fine-tunes a model to memorise associations between queries and the document identifiers (docids) of relevant documents. Training-based indexing suffers from high training costs, under-utilisation of pre-trained knowledge in large language models (LLMs), and limited adaptability to dynamic document corpora. To address the issues, we propose a few-shot indexing-based GR framework (Few-Shot GR). It has a few-shot indexing process without any training, where we prompt an LLM to generate docids for all documents in a corpus, ultimately creating a docid bank for the entire corpus. During retrieval, we feed a query to the same LLM and constrain it to generate a docid within the docid bank created during indexing, and then map the generated docid back to its corresponding document. Moreover, we devise few-shot indexing with one-to-many mapping to further enhance Few-Shot GR. Experiments show that Few-Shot GR achieves superior performance to state-of-the-art GR methods requiring heavy training.","authors":["Arian Askari","Chuan Meng","Mohammad Aliannejadi","Zhaochun Ren","Evangelos Kanoulas","Suzan Verberne"],"pdf_url":"","comment":"Accepted for publication at the 48th European Conference on Information Retrieval (ECIR 2026)"},{"id":"http://arxiv.org/abs/2512.18283v1","updated":"2025-12-20T09:12:33Z","published":"2025-12-20T09:12:33Z","title":"Improving Data Reusability in Interactive Information Retrieval: Insights from the Community","summary":"In this study, we conducted semi-structured interviews with 21 IIR researchers to investigate their data reuse practices. This study aims to expand upon current findings by exploring IIR researchers' information-obtaining behaviors regarding data reuse. We identified the information about shared data characteristics that IIR researchers need when evaluating data reusability, as well as the sources they typically consult to obtain this information. We consider this work to be an initial step toward revealing IIR researchers' data reuse practices and identifying what the community needs to do to promote data reuse. We hope that this study, as well as future research, will inspire more individuals to contribute to ongoing efforts aimed at designing standards, infrastructures, and policies, as well as fostering a sustainable culture of data sharing and reuse in this field.","authors":["Tianji Jiang","Wenqi Li","Jiqun Liu"],"pdf_url":"","comment":"Accepted by CHIIR 2025"},{"id":"http://arxiv.org/abs/2508.18142v2","updated":"2025-12-20T08:17:01Z","published":"2025-08-25T15:51:24Z","title":"Mirroring Users: Towards Building Preference-aligned User Simulator with User Feedback in Recommendation","summary":"User simulation is increasingly vital to develop and evaluate recommender systems (RSs). While Large Language Models (LLMs) offer promising avenues to simulate user behavior, they often struggle with the absence of specific task alignment required for RSs and the efficiency demands of large-scale simulation. A vast yet underutilized resource for enhancing this alignment is the extensive user feedback inherent in RSs, but leveraging it is challenging due to its ambiguity, noise and massive volume, which hinders efficient preference alignment. To overcome these hurdles, we introduce a novel data construction framework that leverages user feedback in RSs with advanced LLM capabilities to generate high-quality simulation data. Our framework unfolds in two key phases: (1) using LLMs to generate decision-making processes as explanatory rationales on simulation samples, thereby reducing ambiguity; and (2) data distillation based on uncertainty estimation and behavior sampling to efficiently filter the most informative, denoised samples. Accordingly, we fine-tune lightweight LLMs, as user simulators, using such high-quality dataset with corresponding decision-making processes. Extensive experiments confirm that our framework significantly boosts the alignment with human preferences and the in-domain reasoning capabilities of the fine-tuned LLMs, providing more insightful and interpretable signals for RS interaction. We believe our work, together with publicly available developed framework, high-quality mixed-domain dataset, and fine-tuned LLM checkpoints, will advance the RS community and offer valuable insights for broader human-centric AI research.","authors":["Tianjun Wei","Huizhong Guo","Yingpeng Du","Zhu Sun","Huang Chen","Dongxia Wang","Jie Zhang"],"pdf_url":"","comment":"Github: https://github.com/UserMirrorer/UserMirrorer"}],"Multimedia":[{"id":"http://arxiv.org/abs/2512.18318v1","updated":"2025-12-20T11:23:18Z","published":"2025-12-20T11:23:18Z","title":"Asynchronous Pipeline Parallelism for Real-Time Multilingual Lip Synchronization in Video Communication Systems","summary":"This paper introduces a parallel and asynchronous Transformer framework designed for efficient and accurate multilingual lip synchronization in real-time video conferencing systems. The proposed architecture integrates translation, speech processing, and lip-synchronization modules within a pipeline-parallel design that enables concurrent module execution through message-queue-based decoupling, reducing end-to-end latency by up to 3.1 times compared to sequential approaches. To enhance computational efficiency and throughput, the inference workflow of each module is optimized through low-level graph compilation, mixed-precision quantization, and hardware-accelerated kernel fusion. These optimizations provide substantial gains in efficiency while preserving model accuracy and visual quality. In addition, a context-adaptive silence-detection component segments the input speech stream at semantically coherent boundaries, improving translation consistency and temporal alignment across languages. Experimental results demonstrate that the proposed parallel architecture outperforms conventional sequential pipelines in processing speed, synchronization stability, and resource utilization. The modular, message-oriented design makes this work applicable to resource-constrained IoT communication scenarios including telemedicine, multilingual kiosks, and remote assistance systems. Overall, this work advances the development of low-latency, resource-efficient multimodal communication frameworks for next-generation AIoT systems.","authors":["Eren Caglar","Amirkia Rafiei Oskooei","Mehmet Kutanoglu","Mustafa Keles","Mehmet S. Aktas"],"pdf_url":"","comment":"Accepted to IEEE Big Data 2025, AIDE4IoT Workshop. Copyright \\c{opyright} 2025 IEEE"}]},"2025-12-23T00:00:00Z":{"Artificial Intelligence":[{"id":"http://arxiv.org/abs/2512.20618v1","updated":"2025-12-23T18:59:49Z","published":"2025-12-23T18:59:49Z","title":"LongVideoAgent: Multi-Agent Reasoning with Long Videos","summary":"Recent advances in multimodal LLMs and systems that use tools for long-video QA point to the promise of reasoning over hour-long episodes. However, many methods still compress content into lossy summaries or rely on limited toolsets, weakening temporal grounding and missing fine-grained cues. We propose a multi-agent framework in which a master LLM coordinates a grounding agent to localize question-relevant segments and a vision agent to extract targeted textual observations. The master agent plans with a step limit, and is trained with reinforcement learning to encourage concise, correct, and efficient multi-agent cooperation. This design helps the master agent focus on relevant clips via grounding, complements subtitles with visual detail, and yields interpretable trajectories. On our proposed LongTVQA and LongTVQA+ which are episode-level datasets aggregated from TVQA/TVQA+, our multi-agent system significantly outperforms strong non-agent baselines. Experiments also show reinforcement learning further strengthens reasoning and planning for the trained agent. Code and data will be shared at https://longvideoagent.github.io/.","authors":["Runtao Liu","Ziyi Liu","Jiaqi Tang","Yue Ma","Renjie Pi","Jipeng Zhang","Qifeng Chen"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20605v1","updated":"2025-12-23T18:51:50Z","published":"2025-12-23T18:51:50Z","title":"Emergent temporal abstractions in autoregressive models enable hierarchical reinforcement learning","summary":"Large-scale autoregressive models pretrained on next-token prediction and finetuned with reinforcement learning (RL) have achieved unprecedented success on many problem domains. During RL, these models explore by generating new outputs, one token at a time. However, sampling actions token-by-token can result in highly inefficient learning, particularly when rewards are sparse. Here, we show that it is possible to overcome this problem by acting and exploring within the internal representations of an autoregressive model. Specifically, to discover temporally-abstract actions, we introduce a higher-order, non-causal sequence model whose outputs control the residual stream activations of a base autoregressive model. On grid world and MuJoCo-based tasks with hierarchical structure, we find that the higher-order model learns to compress long activation sequence chunks onto internal controllers. Critically, each controller executes a sequence of behaviorally meaningful actions that unfold over long timescales and are accompanied with a learned termination condition, such that composing multiple controllers over time leads to efficient exploration on novel tasks. We show that direct internal controller reinforcement, a process we term \"internal RL\", enables learning from sparse rewards in cases where standard RL finetuning fails. Our results demonstrate the benefits of latent action generation and reinforcement in autoregressive models, suggesting internal RL as a promising avenue for realizing hierarchical RL within foundation models.","authors":["Seijin Kobayashi","Yanick Schimpf","Maximilian Schlegel","Angelika Steger","Maciej Wolczyk","Johannes von Oswald","Nino Scherre","Kaitlin Maile","Guillaume Lajoie","Blake A. Richards","Rif A. Saurous","James Manyika","Blaise Agüera y Arcas","Alexander Meulemans","João Sacramento"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20595v1","updated":"2025-12-23T18:43:05Z","published":"2025-12-23T18:43:05Z","title":"Cube Bench: A Benchmark for Spatial Visual Reasoning in MLLMs","summary":"We introduce Cube Bench, a Rubik's-cube benchmark for evaluating spatial and sequential reasoning in multimodal large language models (MLLMs). The benchmark decomposes performance into five skills: (i) reconstructing cube faces from images and text, (ii) choosing the optimal next move, (iii) predicting the outcome of a candidate move without applying it, (iv) executing multi-step plans while recovering from mistakes, and (v) detecting and revising one's own errors. Using a shared set of scrambled cube states, identical prompts and parsers, and a single distance-to-solved metric, we compare recent MLLMs side by side as a function of scramble depth. Across seven MLLMs, accuracy drops sharply with depth; once a trajectory stalls or diverges, models rarely recover, and high face-reconstruction accuracy does not guarantee competent action selection or multi-step execution. A pronounced closed- vs open-source gap emerges: the strongest closed model leads on both single-step perception tasks and multi-step control tasks, while open-weight models cluster near chance on the hardest settings; yet even the best MLLM degrades at higher cube complexity. A simple self-correction via reflective thinking yields modest gains but can also introduce overthinking. Cube Bench offers a compact, reproducible probe of sequential spatial reasoning in MLLMs.","authors":["Dhruv Anand","Ehsan Shareghi"],"pdf_url":"","comment":"27 pages, 5 figures, 9 tables. Cube available at https://github.com/dana-23/cube-bench"},{"id":"http://arxiv.org/abs/2512.20589v1","updated":"2025-12-23T18:36:07Z","published":"2025-12-23T18:36:07Z","title":"Leveraging High-Fidelity Digital Models and Reinforcement Learning for Mission Engineering: A Case Study of Aerial Firefighting Under Perfect Information","summary":"As systems engineering (SE) objectives evolve from design and operation of monolithic systems to complex System of Systems (SoS), the discipline of Mission Engineering (ME) has emerged which is increasingly being accepted as a new line of thinking for the SE community. Moreover, mission environments are uncertain, dynamic, and mission outcomes are a direct function of how the mission assets will interact with this environment. This proves static architectures brittle and calls for analytically rigorous approaches for ME. To that end, this paper proposes an intelligent mission coordination methodology that integrates digital mission models with Reinforcement Learning (RL), that specifically addresses the need for adaptive task allocation and reconfiguration. More specifically, we are leveraging a Digital Engineering (DE) based infrastructure that is composed of a high-fidelity digital mission model and agent-based simulation; and then we formulate the mission tactics management problem as a Markov Decision Process (MDP), and employ an RL agent trained via Proximal Policy Optimization. By leveraging the simulation as a sandbox, we map the system states to actions, refining the policy based on realized mission outcomes. The utility of the RL-based intelligent mission coordinator is demonstrated through an aerial firefighting case study. Our findings indicate that the RL-based intelligent mission coordinator not only surpasses baseline performance but also significantly reduces the variability in mission performance. Thus, this study serves as a proof of concept demonstrating that DE-enabled mission simulations combined with advanced analytical tools offer a mission-agnostic framework for improving ME practice; which can be extended to more complicated fleet design and selection problems in the future from a mission-first perspective.","authors":["İbrahim Oğuz Çetinkaya","Sajad Khodadadian","Taylan G. Topçu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20586v1","updated":"2025-12-23T18:32:17Z","published":"2025-12-23T18:32:17Z","title":"Automated stereotactic radiosurgery planning using a human-in-the-loop reasoning large language model agent","summary":"Stereotactic radiosurgery (SRS) demands precise dose shaping around critical structures, yet black-box AI systems have limited clinical adoption due to opacity concerns. We tested whether chain-of-thought reasoning improves agentic planning in a retrospective cohort of 41 patients with brain metastases treated with 18 Gy single-fraction SRS. We developed SAGE (Secure Agent for Generative Dose Expertise), an LLM-based planning agent for automated SRS treatment planning. Two variants generated plans for each case: one using a non-reasoning model, one using a reasoning model. The reasoning variant showed comparable plan dosimetry relative to human planners on primary endpoints (PTV coverage, maximum dose, conformity index, gradient index; all p > 0.21) while reducing cochlear dose below human baselines (p = 0.022). When prompted to improve conformity, the reasoning model demonstrated systematic planning behaviors including prospective constraint verification (457 instances) and trade-off deliberation (609 instances), while the standard model exhibited none of these deliberative processes (0 and 7 instances, respectively). Content analysis revealed that constraint verification and causal explanation concentrated in the reasoning agent. The optimization traces serve as auditable logs, offering a path toward transparent automated planning.","authors":["Humza Nusrat","Luke Francisco","Bing Luo","Hassan Bagher-Ebadian","Joshua Kim","Karen Chin-Snyder","Salim Siddiqui","Mira Shah","Eric Mellon","Mohammad Ghassemi","Anthony Doemer","Benjamin Movsas","Kundan Thind"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20576v1","updated":"2025-12-23T18:20:06Z","published":"2025-12-23T18:20:06Z","title":"Performative Policy Gradient: Optimality in Performative Reinforcement Learning","summary":"Post-deployment machine learning algorithms often influence the environments they act in, and thus shift the underlying dynamics that the standard reinforcement learning (RL) methods ignore. While designing optimal algorithms in this performative setting has recently been studied in supervised learning, the RL counterpart remains under-explored. In this paper, we prove the performative counterparts of the performance difference lemma and the policy gradient theorem in RL, and further introduce the Performative Policy Gradient algorithm (PePG). PePG is the first policy gradient algorithm designed to account for performativity in RL. Under softmax parametrisation, and also with and without entropy regularisation, we prove that PePG converges to performatively optimal policies, i.e. policies that remain optimal under the distribution shifts induced by themselves. Thus, PePG significantly extends the prior works in Performative RL that achieves performative stability but not optimality. Furthermore, our empirical analysis on standard performative RL environments validate that PePG outperforms standard policy gradient algorithms and the existing performative RL algorithms aiming for stability.","authors":["Debabrota Basu","Udvas Das","Brahim Driss","Uddalak Mukherjee"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20573v1","updated":"2025-12-23T18:16:58Z","published":"2025-12-23T18:16:58Z","title":"Fail Fast, Win Big: Rethinking the Drafting Strategy in Speculative Decoding via Diffusion LLMs","summary":"Diffusion Large Language Models (dLLMs) offer fast, parallel token generation, but their standalone use is plagued by an inherent efficiency-quality tradeoff. We show that, if carefully applied, the attributes of dLLMs can actually be a strength for drafters in speculative decoding with autoregressive (AR) verifiers. Our core insight is that dLLM's speed from parallel decoding drastically lowers the risk of costly rejections, providing a practical mechanism to effectively realize the (elusive) lengthy drafts that lead to large speedups with speculative decoding. We present FailFast, a dLLM-based speculative decoding framework that realizes this approach by dynamically adapting its speculation length. It \"fails fast\" by spending minimal compute in hard-to-speculate regions to shrink speculation latency and \"wins big\" by aggressively extending draft lengths in easier regions to reduce verification latency (in many cases, speculating and accepting 70 tokens at a time!). Without any fine-tuning, FailFast delivers lossless acceleration of AR LLMs and achieves up to 4.9$\\times$ speedup over vanilla decoding, 1.7$\\times$ over the best naive dLLM drafter, and 1.4$\\times$ over EAGLE-3 across diverse models and workloads. We open-source FailFast at https://github.com/ruipeterpan/failfast.","authors":["Rui Pan","Zhuofu Chen","Ravi Netravali"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20569v1","updated":"2025-12-23T18:12:22Z","published":"2025-12-23T18:12:22Z","title":"Distilling to Hybrid Attention Models via KL-Guided Layer Selection","summary":"Distilling pretrained softmax attention Transformers into more efficient hybrid architectures that interleave softmax and linear attention layers is a promising approach for improving the inference efficiency of LLMs without requiring expensive pretraining from scratch. A critical factor in the conversion process is layer selection, i.e., deciding on which layers to convert to linear attention variants. This paper describes a simple and efficient recipe for layer selection that uses layer importance scores derived from a small amount of training on generic text data. Once the layers have been selected we use a recent pipeline for the distillation process itself \\citep[RADLADS;][]{goldstein2025radlads}, which consists of attention weight transfer, hidden state alignment, KL-based distribution matching, followed by a small amount of finetuning. We find that this approach is more effective than existing approaches for layer selection, including heuristics that uniformly interleave linear attentions based on a fixed ratio, as well as more involved approaches that rely on specialized diagnostic datasets.","authors":["Yanhong Li","Songlin Yang","Shawn Tan","Mayank Mishra","Rameswar Panda","Jiawei Zhou","Yoon Kim"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2509.18218v4","updated":"2025-12-23T18:09:51Z","published":"2025-09-21T22:34:00Z","title":"Similarity Field Theory: A Mathematical Framework for Intelligence","summary":"We posit that persisting and transforming similarity relations form the structural basis of any comprehensible dynamic system. This paper introduces Similarity Field Theory, a mathematical framework that formalizes the principles governing similarity values among entities and their evolution. We define: (1) a similarity field $S: U \\times U \\to [0,1]$ over a universe of entities $U$, satisfying reflexivity $S(E,E)=1$ and treated as a directed relational field (asymmetry and non-transitivity are allowed); (2) the evolution of a system through a sequence $Z_p=(X_p,S^{(p)})$ indexed by $p=0,1,2,\\ldots$; (3) concepts $K$ as entities that induce fibers $F_α(K)={E\\in U \\mid S(E,K)\\ge α}$, i.e., superlevel sets of the unary map $S_K(E):=S(E,K)$; and (4) a generative operator $G$ that produces new entities. Within this framework, we formalize a generative definition of intelligence: an operator $G$ is intelligent with respect to a concept $K$ if, given a system containing entities belonging to the fiber of $K$, it generates new entities that also belong to that fiber. Similarity Field Theory thus offers a foundational language for characterizing, comparing, and constructing intelligent systems. At a high level, this framework reframes intelligence and interpretability as geometric problems on similarity fields--preserving and composing level-set fibers--rather than purely statistical ones. We prove two theorems: (i) asymmetry blocks mutual inclusion; and (ii) stability implies either an anchor coordinate or asymptotic confinement to the target level (up to arbitrarily small tolerance). Together, these results constrain similarity-field evolution and motivate an interpretive lens that can be applied to large language models.","authors":["Kei-Sing Ng"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20563v1","updated":"2025-12-23T18:07:43Z","published":"2025-12-23T18:07:43Z","title":"LEAD: Minimizing Learner-Expert Asymmetry in End-to-End Driving","summary":"Simulators can generate virtually unlimited driving data, yet imitation learning policies in simulation still struggle to achieve robust closed-loop performance. Motivated by this gap, we empirically study how misalignment between privileged expert demonstrations and sensor-based student observations can limit the effectiveness of imitation learning. More precisely, experts have significantly higher visibility (e.g., ignoring occlusions) and far lower uncertainty (e.g., knowing other vehicles' actions), making them difficult to imitate reliably. Furthermore, navigational intent (i.e., the route to follow) is under-specified in student models at test time via only a single target point. We demonstrate that these asymmetries can measurably limit driving performance in CARLA and offer practical interventions to address them. After careful modifications to narrow the gaps between expert and student, our TransFuser v6 (TFv6) student policy achieves a new state of the art on all major publicly available CARLA closed-loop benchmarks, reaching 95 DS on Bench2Drive and more than doubling prior performances on Longest6~v2 and Town13. Additionally, by integrating perception supervision from our dataset into a shared sim-to-real pipeline, we show consistent gains on the NAVSIM and Waymo Vision-Based End-to-End driving benchmarks. Our code, data, and models are publicly available at https://github.com/autonomousvision/lead.","authors":["Long Nguyen","Micha Fauth","Bernhard Jaeger","Daniel Dauner","Maximilian Igl","Andreas Geiger","Kashyap Chitta"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2511.13912v2","updated":"2025-12-23T18:00:52Z","published":"2025-11-17T21:06:52Z","title":"Compute-in-Memory Implementation of State Space Models for Event Sequence Processing","summary":"State space models (SSMs) have recently emerged as a powerful framework for long sequence processing, outperforming traditional methods on diverse benchmarks. Fundamentally, SSMs can generalize both recurrent and convolutional networks and have been shown to even capture key functions of biological systems. Here we report an approach to implement SSMs in energy-efficient compute-in-memory (CIM) hardware to achieve real-time, event-driven processing. Our work re-parameterizes the model to function with real-valued coefficients and shared decay constants, reducing the complexity of model mapping onto practical hardware systems. By leveraging device dynamics and diagonalized state transition parameters, the state evolution can be natively implemented in crossbar-based CIM systems combined with memristors exhibiting short-term memory effects. Through this algorithm and hardware co-design, we show the proposed system offers both high accuracy and high energy efficiency while supporting fully asynchronous processing for event-based vision and audio tasks.","authors":["Xiaoyu Zhang","Mingtao Hu","Sen Lu","Soohyeon Kim","Eric Yeu-Jer Lee","Yuyang Liu","Wei D. Lu"],"pdf_url":"","comment":"Xiaoyu Zhang and Mingtao Hu contributed equally to this work"},{"id":"http://arxiv.org/abs/2510.07191v2","updated":"2025-12-23T17:45:29Z","published":"2025-10-08T16:25:04Z","title":"Resolution scaling governs DINOv3 transfer performance in chest radiograph classification","summary":"Self-supervised learning (SSL) has advanced visual representation learning, but its value in chest radiography, a high-volume imaging modality with fine-grained findings, remains unclear. Meta's DINOv3 extends earlier SSL models through Gram-anchored self-distillation. Whether these design choices improve transfer learning for chest radiography has not been systematically tested. We benchmarked DINOv3 against DINOv2 and ImageNet initialization across seven datasets (n>814,000). Two representative backbones were evaluated: ViT-B/16 and ConvNeXt-B. Images were analyzed at 224x224, 512x512, and 1024x1024 pixels. We additionally assessed frozen features from a 7B model. The primary outcome was mean AUROC across labels. At 224x224, DINOv3 and DINOv2 achieved comparable performance on adult datasets. Increasing resolution to 512x512 yielded consistent improvements for DINOv3 over both DINOv2 and ImageNet. In contrast, results in pediatric cohort showed no differences across initializations. Across all settings, ConvNeXt-B outperformed ViT-B/16. Models using frozen DINOv3-7B features underperformed relative to fully finetuned 86-89M-parameter backbones, highlighting the importance of domain adaptation. Scaling to 1024x1024 did not further improve accuracy. Resolution-related gains were most evident for boundary-dependent and small focal abnormalities. In chest radiography, higher input resolution is critical for leveraging the benefits of modern self-supervised models. 512x512 pixels represent a practical upper limit where DINOv3-initialized ConvNeXt-B networks provide the strongest performance, while larger inputs offer minimal return on cost. Clinically, these findings support use of finetuned, mid-sized backbones at 512x512 for chest radiograph interpretation, with the greatest gains expected in detecting subtle or boundary-centered lesions relevant to emergency and critical care settings.","authors":["Soroosh Tayebi Arasteh","Mina Shaigan","Christiane Kuhl","Jakob Nikolas Kather","Sven Nebelung","Daniel Truhn"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20548v1","updated":"2025-12-23T17:42:16Z","published":"2025-12-23T17:42:16Z","title":"Advancing Multimodal Teacher Sentiment Analysis:The Large-Scale T-MED Dataset & The Effective AAM-TSA Model","summary":"Teachers' emotional states are critical in educational scenarios, profoundly impacting teaching efficacy, student engagement, and learning achievements. However, existing studies often fail to accurately capture teachers' emotions due to the performative nature and overlook the critical impact of instructional information on emotional expression.In this paper, we systematically investigate teacher sentiment analysis by building both the dataset and the model accordingly. We construct the first large-scale teacher multimodal sentiment analysis dataset, T-MED.To ensure labeling accuracy and efficiency, we employ a human-machine collaborative labeling process.The T-MED dataset includes 14,938 instances of teacher emotional data from 250 real classrooms across 11 subjects ranging from K-12 to higher education, integrating multimodal text, audio, video, and instructional information.Furthermore, we propose a novel asymmetric attention-based multimodal teacher sentiment analysis model, AAM-TSA.AAM-TSA introduces an asymmetric attention mechanism and hierarchical gating unit to enable differentiated cross-modal feature fusion and precise emotional classification. Experimental results demonstrate that AAM-TSA significantly outperforms existing state-of-the-art methods in terms of accuracy and interpretability on the T-MED dataset.","authors":["Zhiyi Duan","Xiangren Wang","Hongyu Yuan","Qianli Xing"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20520v1","updated":"2025-12-23T17:08:31Z","published":"2025-12-23T17:08:31Z","title":"Benchmarking LLMs for Predictive Applications in the Intensive Care Units","summary":"With the advent of LLMs, various tasks across the natural language processing domain have been transformed. However, their application in predictive tasks remains less researched. This study compares large language models, including GatorTron-Base (trained on clinical data), Llama 8B, and Mistral 7B, against models like BioBERT, DocBERT, BioClinicalBERT, Word2Vec, and Doc2Vec, setting benchmarks for predicting Shock in critically ill patients. Timely prediction of shock can enable early interventions, thus improving patient outcomes. Text data from 17,294 ICU stays of patients in the MIMIC III database were scored for length of stay > 24 hours and shock index (SI) > 0.7 to yield 355 and 87 patients with normal and abnormal SI-index, respectively. Both focal and cross-entropy losses were used during finetuning to address class imbalances. Our findings indicate that while GatorTron Base achieved the highest weighted recall of 80.5%, the overall performance metrics were comparable between SLMs and LLMs. This suggests that LLMs are not inherently superior to SLMs in predicting future clinical events despite their strong performance on text-based tasks. To achieve meaningful clinical outcomes, future efforts in training LLMs should prioritize developing models capable of predicting clinical trajectories rather than focusing on simpler tasks such as named entity recognition or phenotyping.","authors":["Chehak Malhotra","Mehak Gopal","Akshaya Devadiga","Pradeep Singh","Ridam Pal","Ritwik Kashyap","Tavpritesh Sethi"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.15735v3","updated":"2025-12-23T17:06:16Z","published":"2025-12-05T22:52:22Z","title":"Deep Reinforcement Learning Optimization for Uncertain Nonlinear Systems via Event-Triggered Robust Adaptive Dynamic Programming","summary":"This work proposes a unified control architecture that couples a Reinforcement Learning (RL)-driven controller with a disturbance-rejection Extended State Observer (ESO), complemented by an Event-Triggered Mechanism (ETM) to limit unnecessary computations. The ESO is utilized to estimate the system states and the lumped disturbance in real time, forming the foundation for effective disturbance compensation. To obtain near-optimal behavior without an accurate system description, a value-iteration-based Adaptive Dynamic Programming (ADP) method is adopted for policy approximation. The inclusion of the ETM ensures that parameter updates of the learning module are executed only when the state deviation surpasses a predefined bound, thereby preventing excessive learning activity and substantially reducing computational load. A Lyapunov-oriented analysis is used to characterize the stability properties of the resulting closed-loop system. Numerical experiments further confirm that the developed approach maintains strong control performance and disturbance tolerance, while achieving a significant reduction in sampling and processing effort compared with standard time-triggered ADP schemes.","authors":["Ningwei Bai","Chi Pui Chan","Qichen Yin","Tengyang Gong","Yunda Yan","Zezhi Tang"],"pdf_url":"","comment":"9 pages, 9 figures"},{"id":"http://arxiv.org/abs/2507.05495v2","updated":"2025-12-23T16:43:12Z","published":"2025-07-07T21:35:09Z","title":"Deep Research Comparator: A Platform For Fine-grained Human Annotations of Deep Research Agents","summary":"Effectively evaluating deep research agents that autonomously search the web, analyze information, and generate reports remains a major challenge, particularly when it comes to assessing long reports and giving detailed feedback on their intermediate steps. To address these gaps, we introduce Deep Research Comparator, a platform that offers a holistic framework for deep research agent hosting, side-by-side comparison, fine-grained human feedback collection, and ranking calculation. Given a user query, our platform displays the final reports from two different agents along with their intermediate steps during generation. Annotators can evaluate the overall quality of final reports based on side-by-side comparison, and also provide detailed feedback separately by assessing intermediate steps or specific text spans within the final report. Furthermore, we develop Simple Deepresearch, an end-to-end agent scaffold. This scaffold serves as a baseline that facilitates the easy integration of various large language models to transform them into deep research agents for evaluation. To demonstrate the platform's utility for deep research agent development, we have collected real user preference data from 17 annotators on three deep research agents. A demo video of our platform can be found at https://www.youtube.com/watch?v=g4d2dnbdseg.","authors":["Prahaladh Chandrahasan","Jiahe Jin","Zhihan Zhang","Tevin Wang","Andy Tang","Lucy Mo","Morteza Ziyadi","Leonardo F. R. Ribeiro","Zimeng Qiu","Markus Dreyer","Akari Asai","Chenyan Xiong"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2508.12833v2","updated":"2025-12-23T16:25:32Z","published":"2025-08-18T11:17:59Z","title":"Toward Storage-Aware Learning with Compressed Data An Empirical Exploratory Study on JPEG","summary":"On-device machine learning is often constrained by limited storage, particularly in continuous data collection scenarios. This paper presents an empirical study on storage-aware learning, focusing on the trade-off between data quantity and quality via compression. We demonstrate that naive strategies, such as uniform data dropping or one-size-fits-all compression, are suboptimal. Our findings further reveal that data samples exhibit varying sensitivities to compression, supporting the feasibility of a sample-wise adaptive compression strategy. These insights provide a foundation for developing a new class of storage-aware learning systems. The primary contribution of this work is the systematic characterization of this under-explored challenge, offering valuable insights that advance the understanding of storage-aware learning.","authors":["Kichang Lee","Songkuk Kim","JaeYeon Park","JeongGil Ko"],"pdf_url":"","comment":"6pages, 6figures"},{"id":"http://arxiv.org/abs/2401.09986v3","updated":"2025-12-23T16:22:09Z","published":"2024-01-18T14:02:23Z","title":"Improving Local Training in Federated Learning via Temperature Scaling","summary":"Federated learning is inherently hampered by data heterogeneity: non-i.i.d. training data over local clients. We propose a novel model training approach for federated learning, FLex&Chill, which exploits the Logit Chilling method. Through extensive evaluations, we demonstrate that, in the presence of non-i.i.d. data characteristics inherent in federated learning systems, this approach can expedite model convergence and improve inference accuracy. Quantitatively, from our experiments, we observe up to 6X improvement in the global federated learning model convergence time, and up to 3.37% improvement in inference accuracy.","authors":["Kichang Lee","Pei Zhang","Songkuk Kim","JeongGil Ko"],"pdf_url":"","comment":"56 pages"},{"id":"http://arxiv.org/abs/2512.20482v1","updated":"2025-12-23T16:18:39Z","published":"2025-12-23T16:18:39Z","title":"SweRank+: Multilingual, Multi-Turn Code Ranking for Software Issue Localization","summary":"Maintaining large-scale, multilingual codebases hinges on accurately localizing issues, which requires mapping natural-language error descriptions to the relevant functions that need to be modified. However, existing ranking approaches are often Python-centric and perform a single-pass search over the codebase. This work introduces SweRank+, a framework that couples SweRankMulti, a cross-lingual code ranking tool, with SweRankAgent, an agentic search setup, for iterative, multi-turn reasoning over the code repository. SweRankMulti comprises a code embedding retriever and a listwise LLM reranker, and is trained using a carefully curated large-scale issue localization dataset spanning multiple popular programming languages. SweRankAgent adopts an agentic search loop that moves beyond single-shot localization with a memory buffer to reason and accumulate relevant localization candidates over multiple turns. Our experiments on issue localization benchmarks spanning various languages demonstrate new state-of-the-art performance with SweRankMulti, while SweRankAgent further improves localization over single-pass ranking.","authors":["Revanth Gangi Reddy","Ye Liu","Wenting Zhao","JaeHyeok Doo","Tarun Suresh","Daniel Lee","Caiming Xiong","Yingbo Zhou","Semih Yavuz","Shafiq Joty"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20469v1","updated":"2025-12-23T16:04:41Z","published":"2025-12-23T16:04:41Z","title":"Bohrium + SciMaster: Building the Infrastructure and Ecosystem for Agentic Science at Scale","summary":"AI agents are emerging as a practical way to run multi-step scientific workflows that interleave reasoning with tool use and verification, pointing to a shift from isolated AI-assisted steps toward \\emph{agentic science at scale}. This shift is increasingly feasible, as scientific tools and models can be invoked through stable interfaces and verified with recorded execution traces, and increasingly necessary, as AI accelerates scientific output and stresses the peer-review and publication pipeline, raising the bar for traceability and credible evaluation.\n  However, scaling agentic science remains difficult: workflows are hard to observe and reproduce; many tools and laboratory systems are not agent-ready; execution is hard to trace and govern; and prototype AI Scientist systems are often bespoke, limiting reuse and systematic improvement from real workflow signals.\n  We argue that scaling agentic science requires an infrastructure-and-ecosystem approach, instantiated in Bohrium+SciMaster. Bohrium acts as a managed, traceable hub for AI4S assets -- akin to a HuggingFace of AI for Science -- that turns diverse scientific data, software, compute, and laboratory systems into agent-ready capabilities. SciMaster orchestrates these capabilities into long-horizon scientific workflows, on which scientific agents can be composed and executed. Between infrastructure and orchestration, a \\emph{scientific intelligence substrate} organizes reusable models, knowledge, and components into executable building blocks for workflow reasoning and action, enabling composition, auditability, and improvement through use.\n  We demonstrate this stack with eleven representative master agents in real workflows, achieving orders-of-magnitude reductions in end-to-end scientific cycle time and generating execution-grounded signals from real workloads at multi-million scale.","authors":["Linfeng Zhang","Siheng Chen","Yuzhu Cai","Jingyi Chai","Junhan Chang","Kun Chen","Zhi X. Chen","Zhaohan Ding","Yuwen Du","Yuanpeng Gao","Yuan Gao","Jing Gao","Zhifeng Gao","Qiangqiang Gu","Yanhui Hong","Yuan Huang","Xi Fang","Xiaohong Ji","Guolin Ke","Zixing Lei","Xinyu Li","Yongge Li","Ruoxue Liao","Hang Lin","Xiaolu Lin","Yuxiang Liu","Xinzijian Liu","Zexi Liu","Jintan Lu","Tingjia Miao","Haohui Que","Weijie Sun","Yanfeng Wang","Bingyang Wu","Tianju Xue","Rui Ye","Jinzhe Zeng","Duo Zhang","Jiahui Zhang","Linfeng Zhang","Tianhan Zhang","Wenchang Zhang","Yuzhi Zhang","Zezhong Zhang","Hang Zheng","Hui Zhou","Tong Zhu","Xinyu Zhu","Qingguo Zhou","Weinan E"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2509.09312v4","updated":"2025-12-23T15:52:27Z","published":"2025-09-11T09:55:50Z","title":"Explaining Tournament Solutions with Minimal Supports","summary":"Tournaments are widely used models to represent pairwise dominance between candidates, alternatives, or teams. We study the problem of providing certified explanations for why a candidate appears among the winners under various tournament rules. To this end, we identify minimal supports, minimal sub-tournaments in which the candidate is guaranteed to win regardless of how the rest of the tournament is completed (that is, the candidate is a necessary winner of the sub-tournament). This notion corresponds to an abductive explanation for the question,\"Why does the winner win the tournament?\", a central concept in formal explainable AI. We focus on common tournament solutions: the top cycle, the uncovered set, the Copeland rule, the Borda rule, the maximin rule, and the weighted uncovered set. For each rule we determine the size of the smallest minimal supports, and we present polynomial-time algorithms to compute them for all solutions except for the weighted uncovered set, for which the problem is NP-complete. Finally, we show how minimal supports can serve to produce compact, certified, and intuitive explanations for tournament solutions.","authors":["Clément Contet","Umberto Grandi","Jérôme Mengin"],"pdf_url":"","comment":"This paper is the extended version of Contet, Grandi, and Mengin. 2026. Explaining Tournament Solutions with Minimal Supports. In Proceedings of the 40th AAAI Conference on Artificial Intelligence"},{"id":"http://arxiv.org/abs/2509.22358v2","updated":"2025-12-23T15:51:07Z","published":"2025-09-26T13:53:56Z","title":"Stochastic activations","summary":"We introduce stochastic activations. This novel strategy randomly selects between several non-linear functions in the feed-forward layer of a large language model. In particular, we choose between SILU or RELU depending on a Bernoulli draw. This strategy circumvents the optimization problem associated with RELU, namely, the constant shape for negative inputs that prevents the gradient flow. We leverage this strategy in two ways:\n  (1) We use stochastic activations during pre-training and fine-tune the model with RELU, which is used at inference time to provide sparse latent vectors. This reduces the inference FLOPs and translates into a significant speedup in the CPU. Interestingly, this leads to much better results than training from scratch with the RELU activation function.\n  (2) We evaluate stochastic activations for generation. This strategy performs reasonably well: it is only slightly inferior to the best deterministic non-linearity, namely SILU combined with temperature scaling. This offers an alternative to existing strategies by providing a controlled way to increase the diversity of the generated text.","authors":["Maria Lomeli","Matthijs Douze","Gergely Szilvasy","Loic Cabannes","Jade Copet","Sainbayar Sukhbaatar","Jason Weston","Gabriel Synnaeve","Pierre-Emmanuel Mazaré","Hervé Jégou"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20436v1","updated":"2025-12-23T15:24:31Z","published":"2025-12-23T15:24:31Z","title":"Dual-Encoder Transformer-Based Multimodal Learning for Ischemic Stroke Lesion Segmentation Using Diffusion MRI","summary":"Accurate segmentation of ischemic stroke lesions from diffusion magnetic resonance imaging (MRI) is essential for clinical decision-making and outcome assessment. Diffusion-Weighted Imaging (DWI) and Apparent Diffusion Coefficient (ADC) scans provide complementary information on acute and sub-acute ischemic changes; however, automated lesion delineation remains challenging due to variability in lesion appearance.\n  In this work, we study ischemic stroke lesion segmentation using multimodal diffusion MRI from the ISLES 2022 dataset. Several state-of-the-art convolutional and transformer-based architectures, including U-Net variants, Swin-UNet, and TransUNet, are benchmarked. Based on performance, a dual-encoder TransUNet architecture is proposed to learn modality-specific representations from DWI and ADC inputs. To incorporate spatial context, adjacent slice information is integrated using a three-slice input configuration.\n  All models are trained under a unified framework and evaluated using the Dice Similarity Coefficient (DSC). Results show that transformer-based models outperform convolutional baselines, and the proposed dual-encoder TransUNet achieves the best performance, reaching a Dice score of 85.4% on the test set. The proposed framework offers a robust solution for automated ischemic stroke lesion segmentation from diffusion MRI.","authors":["Muhammad Usman","Azka Rehman","Muhammad Mutti Ur Rehman","Abd Ur Rehman","Muhammad Umar Farooq"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2505.02824v2","updated":"2025-12-23T15:15:42Z","published":"2025-05-05T17:51:55Z","title":"Towards Dataset Copyright Evasion Attack against Personalized Text-to-Image Diffusion Models","summary":"Text-to-image (T2I) diffusion models enable high-quality image generation conditioned on textual prompts. However, fine-tuning these pre-trained models for personalization raises concerns about unauthorized dataset usage. To address this issue, dataset ownership verification (DOV) has recently been proposed, which embeds watermarks into fine-tuning datasets via backdoor techniques. These watermarks remain dormant on benign samples but produce owner-specified outputs when triggered. Despite its promise, the robustness of DOV against copyright evasion attacks (CEA) remains unexplored. In this paper, we investigate how adversaries can circumvent these mechanisms, enabling models trained on watermarked datasets to bypass ownership verification. We begin by analyzing the limitations of potential attacks achieved by backdoor removal, including TPD and T2IShield. In practice, TPD suffers from inconsistent effectiveness due to randomness, while T2IShield fails when watermarks are embedded as local image patches. To this end, we introduce CEAT2I, the first CEA specifically targeting DOV in T2I diffusion models. CEAT2I consists of three stages: (1) motivated by the observation that T2I models converge faster on watermarked samples with respect to intermediate features rather than training loss, we reliably detect watermarked samples; (2) we iteratively ablate tokens from the prompts of detected samples and monitor feature shifts to identify trigger tokens; and (3) we apply a closed-form concept erasure method to remove the injected watermarks. Extensive experiments demonstrate that CEAT2I effectively evades state-of-the-art DOV mechanisms while preserving model performance. The code is available at https://github.com/csyufei/CEAT2I.","authors":["Kuofeng Gao","Yufei Zhu","Yiming Li","Jiawang Bai","Yong Yang","Zhifeng Li","Shu-Tao Xia"],"pdf_url":"","comment":"Accepted by IEEE Transactions on Information Forensics and Security"},{"id":"http://arxiv.org/abs/2512.20423v1","updated":"2025-12-23T15:07:17Z","published":"2025-12-23T15:07:17Z","title":"Evasion-Resilient Detection of DNS-over-HTTPS Data Exfiltration: A Practical Evaluation and Toolkit","summary":"The purpose of this project is to assess how well defenders can detect DNS-over-HTTPS (DoH) file exfiltration, and which evasion strategies can be used by attackers. While providing a reproducible toolkit to generate, intercept and analyze DoH exfiltration, and comparing Machine Learning vs threshold-based detection under adversarial scenarios. The originality of this project is the introduction of an end-to-end, containerized pipeline that generates configurable file exfiltration over DoH using several parameters (e.g., chunking, encoding, padding, resolver rotation). It allows for file reconstruction at the resolver side, while extracting flow-level features using a fork of DoHLyzer. The pipeline contains a prediction side, which allows the training of machine learning models based on public labelled datasets and then evaluates them side-by-side with threshold-based detection methods against malicious and evasive DNS-Over-HTTPS traffic. We train Random Forest, Gradient Boosting and Logistic Regression classifiers on a public DoH dataset and benchmark them against evasive DoH exfiltration scenarios. The toolkit orchestrates traffic generation, file capture, feature extraction, model training and analysis. The toolkit is then encapsulated into several Docker containers for easy setup and full reproducibility regardless of the platform it is run on. Future research regarding this project is directed at validating the results on mixed enterprise traffic, extending the protocol coverage to HTTP/3/QUIC request, adding a benign traffic generation, and working on real-time traffic evaluation. A key objective is to quantify when stealth constraints make DoH exfiltration uneconomical and unworthy for the attacker.","authors":["Adam Elaoumari"],"pdf_url":"","comment":"61 pages Advisor : Dr Darren Hurley-Smith"},{"id":"http://arxiv.org/abs/2512.16531v3","updated":"2025-12-23T15:02:39Z","published":"2025-12-18T13:40:33Z","title":"Scaling Laws for Energy Efficiency of Local LLMs","summary":"Deploying local large language models and vision-language models on edge devices requires balancing accuracy with constrained computational and energy budgets. Although graphics processors dominate modern artificial-intelligence deployment, most consumer hardware--including laptops, desktops, industrial controllers, and embedded systems--relies on central processing units. Despite this, the computational laws governing central-processing-unit-only inference for local language and vision-language workloads remain largely unexplored. We systematically benchmark large language and vision-language models on two representative central-processing-unit tiers widely used for local inference: a MacBook Pro M2, reflecting mainstream laptop-class deployment, and a Raspberry Pi 5, representing constrained, low-power embedded settings. Using a unified methodology based on continuous sampling of processor and memory usage together with area-under-curve integration, we characterize how computational load scales with input text length for language models and with image resolution for vision-language models. We uncover two empirical scaling laws: (1) computational cost for language-model inference scales approximately linearly with token length; and (2) vision-language models exhibit a preprocessing-driven \"resolution knee\", where compute remains constant above an internal resolution clamp and decreases sharply below it. Beyond these laws, we show that quantum-inspired compression reduces processor and memory usage by up to 71.9% and energy consumption by up to 62%, while preserving or improving semantic accuracy. These results provide a systematic quantification of multimodal central-processing-unit-only scaling for local language and vision-language workloads, and they identify model compression and input-resolution preprocessing as effective, low-cost levers for sustainable edge inference.","authors":["Ander Alvarez","Alessandro Genuardi","Nilotpal Sinha","Antonio Tiene","Mikail Okyay","Bakbergen Ryskulov","David Montero","Samuel Mugel","Román Orús"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20420v1","updated":"2025-12-23T15:02:12Z","published":"2025-12-23T15:02:12Z","title":"Simplifying Multi-Task Architectures Through Task-Specific Normalization","summary":"Multi-task learning (MTL) aims to leverage shared knowledge across tasks to improve generalization and parameter efficiency, yet balancing resources and mitigating interference remain open challenges. Architectural solutions often introduce elaborate task-specific modules or routing schemes, increasing complexity and overhead. In this work, we show that normalization layers alone are sufficient to address many of these challenges. Simply replacing shared normalization with task-specific variants already yields competitive performance, questioning the need for complex designs. Building on this insight, we propose Task-Specific Sigmoid Batch Normalization (TS$σ$BN), a lightweight mechanism that enables tasks to softly allocate network capacity while fully sharing feature extractors. TS$σ$BN improves stability across CNNs and Transformers, matching or exceeding performance on NYUv2, Cityscapes, CelebA, and PascalContext, while remaining highly parameter-efficient. Moreover, its learned gates provide a natural framework for analyzing MTL dynamics, offering interpretable insights into capacity allocation, filter specialization, and task relationships. Our findings suggest that complex MTL architectures may be unnecessary and that task-specific normalization offers a simple, interpretable, and efficient alternative.","authors":["Mihai Suteu","Ovidiu Serban"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2501.12289v2","updated":"2025-12-23T15:02:02Z","published":"2025-01-21T16:59:13Z","title":"Regressor-Guided Generative Image Editing Balances User Emotions to Reduce Time Spent Online","summary":"Internet overuse is a widespread phenomenon in today's digital society. Existing interventions, such as time limits or grayscaling, often rely on restrictive controls that provoke psychological reactance and are frequently circumvented. Building on prior work showing that emotional responses mediate the relationship between content consumption and online engagement, we investigate whether regulating the emotional impact of images can reduce online use in a non-coercive manner. We introduce and systematically analyze three regressor-guided image-editing approaches: (i) global optimization of emotion-related image attributes, (ii) optimization in a style latent space, and (iii) a diffusion-based method using classifier and classifier-free guidance. While the first two approaches modify low-level visual features (e.g., contrast, color), the diffusion-based method enables higher-level changes (e.g., adjusting clothing, facial features). Results from a controlled image-rating study and a social media experiment show that diffusion-based edits balance emotional responses and are associated with lower usage duration while preserving visual quality.","authors":["Christoph Gebhardt","Robin Willardt","Seyedmorteza Sadat","Chih-Wei Ning","Andreas Brombach","Jie Song","Otmar Hilliges","Christian Holz"],"pdf_url":"","comment":"44 pages, 22 figures"},{"id":"http://arxiv.org/abs/2512.20409v1","updated":"2025-12-23T14:55:53Z","published":"2025-12-23T14:55:53Z","title":"DETACH : Decomposed Spatio-Temporal Alignment for Exocentric Video and Ambient Sensors with Staged Learning","summary":"Aligning egocentric video with wearable sensors have shown promise for human action recognition, but face practical limitations in user discomfort, privacy concerns, and scalability. We explore exocentric video with ambient sensors as a non-intrusive, scalable alternative. While prior egocentric-wearable works predominantly adopt Global Alignment by encoding entire sequences into unified representations, this approach fails in exocentric-ambient settings due to two problems: (P1) inability to capture local details such as subtle motions, and (P2) over-reliance on modality-invariant temporal patterns, causing misalignment between actions sharing similar temporal patterns with different spatio-semantic contexts. To resolve these problems, we propose DETACH, a decomposed spatio-temporal framework. This explicit decomposition preserves local details, while our novel sensor-spatial features discovered via online clustering provide semantic grounding for context-aware alignment. To align the decomposed features, our two-stage approach establishes spatial correspondence through mutual supervision, then performs temporal alignment via a spatial-temporal weighted contrastive loss that adaptively handles easy negatives, hard negatives, and false negatives. Comprehensive experiments with downstream tasks on Opportunity++ and HWU-USP datasets demonstrate substantial improvements over adapted egocentric-wearable baselines.","authors":["Junho Yoon","Jaemo Jung","Hyunju Kim","Dongman Lee"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20407v1","updated":"2025-12-23T14:55:08Z","published":"2025-12-23T14:55:08Z","title":"AUDRON: A Deep Learning Framework with Fused Acoustic Signatures for Drone Type Recognition","summary":"Unmanned aerial vehicles (UAVs), commonly known as drones, are increasingly used across diverse domains, including logistics, agriculture, surveillance, and defense. While these systems provide numerous benefits, their misuse raises safety and security concerns, making effective detection mechanisms essential. Acoustic sensing offers a low-cost and non-intrusive alternative to vision or radar-based detection, as drone propellers generate distinctive sound patterns. This study introduces AUDRON (AUdio-based Drone Recognition Network), a hybrid deep learning framework for drone sound detection, employing a combination of Mel-Frequency Cepstral Coefficients (MFCC), Short-Time Fourier Transform (STFT) spectrograms processed with convolutional neural networks (CNNs), recurrent layers for temporal modeling, and autoencoder-based representations. Feature-level fusion integrates complementary information before classification. Experimental evaluation demonstrates that AUDRON effectively differentiates drone acoustic signatures from background noise, achieving high accuracy while maintaining generalizability across varying conditions. AUDRON achieves 98.51 percent and 97.11 percent accuracy in binary and multiclass classification. The results highlight the advantage of combining multiple feature representations with deep learning for reliable acoustic drone detection, suggesting the framework's potential for deployment in security and surveillance applications where visual or radar sensing may be limited.","authors":["Rajdeep Chatterjee","Sudip Chakrabarty","Trishaani Acharjee","Deepanjali Mishra"],"pdf_url":"","comment":"Presented at the 2025 IEEE 22nd India Council International Conference (INDICON). 6 pages, 3 figures"},{"id":"http://arxiv.org/abs/2512.18689v2","updated":"2025-12-23T14:46:41Z","published":"2025-12-21T10:55:32Z","title":"Fusion of Multiscale Features Via Centralized Sparse-attention Network for EEG Decoding","summary":"Electroencephalography (EEG) signal decoding is a key technology that translates brain activity into executable commands, laying the foundation for direct brain-machine interfacing and intelligent interaction. To address the inherent spatiotemporal heterogeneity of EEG signals, this paper proposes a multi-branch parallel architecture, where each temporal scale is equipped with an independent spatial feature extraction module. To further enhance multi-branch feature fusion, we propose a Fusion of Multiscale Features via Centralized Sparse-attention Network (EEG-CSANet), a centralized sparse-attention network. It employs a main-auxiliary branch architecture, where the main branch models core spatiotemporal patterns via multiscale self-attention, and the auxiliary branch facilitates efficient local interactions through sparse cross-attention. Experimental results show that EEG-CSANet achieves state-of-the-art (SOTA) performance across five public datasets (BCIC-IV-2A, BCIC-IV-2B, HGD, SEED, and SEED-VIG), with accuracies of 88.54%, 91.09%, 99.43%, 96.03%, and 90.56%, respectively. Such performance demonstrates its strong adaptability and robustness across various EEG decoding tasks. Moreover, extensive ablation studies are conducted to enhance the interpretability of EEG-CSANet. In the future, we hope that EEG-CSANet could serve as a promising baseline model in the field of EEG signal decoding. The source code is publicly available at: https://github.com/Xiangrui-Cai/EEG-CSANet","authors":["Xiangrui Cai","Shaocheng Ma","Lei Cao","Jie Li","Tianyu Liu","Yilin Dong"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20387v1","updated":"2025-12-23T14:22:26Z","published":"2025-12-23T14:22:26Z","title":"Generative Digital Twins: Vision-Language Simulation Models for Executable Industrial Systems","summary":"We propose a Vision-Language Simulation Model (VLSM) that unifies visual and textual understanding to synthesize executable FlexScript from layout sketches and natural-language prompts, enabling cross-modal reasoning for industrial simulation systems. To support this new paradigm, the study constructs the first large-scale dataset for generative digital twins, comprising over 120,000 prompt-sketch-code triplets that enable multimodal learning between textual descriptions, spatial structures, and simulation logic. In parallel, three novel evaluation metrics, Structural Validity Rate (SVR), Parameter Match Rate (PMR), and Execution Success Rate (ESR), are proposed specifically for this task to comprehensively evaluate structural integrity, parameter fidelity, and simulator executability. Through systematic ablation across vision encoders, connectors, and code-pretrained language backbones, the proposed models achieve near-perfect structural accuracy and high execution robustness. This work establishes a foundation for generative digital twins that integrate visual reasoning and language understanding into executable industrial simulation systems.","authors":["YuChe Hsu","AnJui Wang","TsaiChing Ni","YuanFu Yang"],"pdf_url":"","comment":"10 pages, 9 figures"},{"id":"http://arxiv.org/abs/2512.20381v1","updated":"2025-12-23T14:12:02Z","published":"2025-12-23T14:12:02Z","title":"Identifying Appropriately-Sized Services with Deep Reinforcement Learning","summary":"Service-based architecture (SBA) has gained attention in industry and academia as a means to modernize legacy systems. It refers to a design style that enables systems to be developed as suites of small, loosely coupled, and autonomous components (services) that encapsulate functionality and communicate via language-agnostic APIs. However, defining appropriately sized services that capture cohesive subsets of system functionality remains challenging. Existing work often relies on the availability of documentation, access to project personnel, or a priori knowledge of the target number of services, assumptions that do not hold in many real-world scenarios. Our work addresses these limitations using a deep reinforcement learning-based approach to identify appropriately sized services directly from implementation artifacts. We present Rake, a reinforcement learning-based technique that leverages available system documentation and source code to guide service decomposition at the level of implementation methods. Rake does not require specific documentation or access to project personnel and is language-agnostic. It also supports a customizable objective function that balances modularization quality and business capability alignment, i.e., the degree to which a service covers the targeted business capability. We applied Rake to four open-source legacy projects and compared it with two state-of-the-art techniques. On average, Rake achieved 7-14 percent higher modularization quality and 18-22 percent stronger business capability alignment. Our results further show that optimizing solely for business context can degrade decomposition quality in tightly coupled systems, highlighting the need for balanced objectives.","authors":["Syeda Tasnim Fabiha","Saad Shafiq","Wesley Klewerton Guez Assunção","Nenad Medvidović"],"pdf_url":"","comment":"22 pages, 6 figures"},{"id":"http://arxiv.org/abs/2512.20363v1","updated":"2025-12-23T13:46:38Z","published":"2025-12-23T13:46:38Z","title":"Clust-PSI-PFL: A Population Stability Index Approach for Clustered Non-IID Personalized Federated Learning","summary":"Federated learning (FL) supports privacy-preserving, decentralized machine learning (ML) model training by keeping data on client devices. However, non-independent and identically distributed (non-IID) data across clients biases updates and degrades performance. To alleviate these issues, we propose Clust-PSI-PFL, a clustering-based personalized FL framework that uses the Population Stability Index (PSI) to quantify the level of non-IID data. We compute a weighted PSI metric, $WPSI^L$, which we show to be more informative than common non-IID metrics (Hellinger, Jensen-Shannon, and Earth Mover's distance). Using PSI features, we form distributionally homogeneous groups of clients via K-means++; the number of optimal clusters is chosen by a systematic silhouette-based procedure, typically yielding few clusters with modest overhead. Across six datasets (tabular, image, and text modalities), two partition protocols (Dirichlet with parameter $α$ and Similarity with parameter S), and multiple client sizes, Clust-PSI-PFL delivers up to 18% higher global accuracy than state-of-the-art baselines and markedly improves client fairness by a relative improvement of 37% under severe non-IID data. These results establish PSI-guided clustering as a principled, lightweight mechanism for robust PFL under label skew.","authors":["Daniel M. Jimenez-Gutierrez","Mehrdad Hassanzadeh","Aris Anagnostopoulos","Ioannis Chatzigiannakis","Andrea Vitaletti"],"pdf_url":"","comment":"Accepted for publication to the 40th IEEE International Parallel & Distributed Processing Symposium (IPDPS 2026)"},{"id":"http://arxiv.org/abs/2512.20352v1","updated":"2025-12-23T13:32:43Z","published":"2025-12-23T13:32:43Z","title":"Multi-LLM Thematic Analysis with Dual Reliability Metrics: Combining Cohen's Kappa and Semantic Similarity for Qualitative Research Validation","summary":"Qualitative research faces a critical reliability challenge: traditional inter-rater agreement methods require multiple human coders, are time-intensive, and often yield moderate consistency. We present a multi-perspective validation framework for LLM-based thematic analysis that combines ensemble validation with dual reliability metrics: Cohen's Kappa ($κ$) for inter-rater agreement and cosine similarity for semantic consistency. Our framework enables configurable analysis parameters (1-6 seeds, temperature 0.0-2.0), supports custom prompt structures with variable substitution, and provides consensus theme extraction across any JSON format. As proof-of-concept, we evaluate three leading LLMs (Gemini 2.5 Pro, GPT-4o, Claude 3.5 Sonnet) on a psychedelic art therapy interview transcript, conducting six independent runs per model. Results demonstrate Gemini achieves highest reliability ($κ= 0.907$, cosine=95.3%), followed by GPT-4o ($κ= 0.853$, cosine=92.6%) and Claude ($κ= 0.842$, cosine=92.1%). All three models achieve a high agreement ($κ> 0.80$), validating the multi-run ensemble approach. The framework successfully extracts consensus themes across runs, with Gemini identifying 6 consensus themes (50-83% consistency), GPT-4o identifying 5 themes, and Claude 4 themes. Our open-source implementation provides researchers with transparent reliability metrics, flexible configuration, and structure-agnostic consensus extraction, establishing methodological foundations for reliable AI-assisted qualitative research.","authors":["Nilesh Jain","Seyi Adeyinka","Leor Roseman","Aza Allsop"],"pdf_url":"","comment":"11 pages, 1 figure, 3 tables"},{"id":"http://arxiv.org/abs/2508.12029v3","updated":"2025-12-23T13:32:11Z","published":"2025-08-16T12:31:39Z","title":"BConformeR: A Conformer Based on Mutual Sampling for Unified Prediction of Continuous and Discontinuous Antibody Binding Sites","summary":"Accurate prediction of antibody-binding sites (epitopes) on antigens is crucial for vaccine design, immunodiagnostics, therapeutic antibody development, antibody engineering, research into autoimmune and allergic diseases, and advancing our understanding of immune responses. Despite in silico methods that have been proposed to predict both linear (continuous) and conformational (discontinuous) epitopes, they consistently underperform in predicting conformational epitopes. In this work, we propose Conformer-based models trained separately on AlphaFold-predicted structures and experimentally determined structures, leveraging convolutional neural networks (CNNs) to extract local features and Transformers to capture long-range dependencies within antigen sequences. Ablation studies demonstrate that CNN enhances the prediction of linear epitopes, and the Transformer module improves the prediction of conformational epitopes. Experimental results show that our model outperforms existing baselines in terms of MCC, ROC-AUC, PR-AUC, and F1 scores on both linear and conformational epitopes.","authors":["Zhangyu You","Jiahao Ma","Hongzong Li","Ye-Fan Hu","Jian-Dong Huang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20344v1","updated":"2025-12-23T13:26:13Z","published":"2025-12-23T13:26:13Z","title":"A DeepSeek-Powered AI System for Automated Chest Radiograph Interpretation in Clinical Practice","summary":"A global shortage of radiologists has been exacerbated by the significant volume of chest X-ray workloads, particularly in primary care. Although multimodal large language models show promise, existing evaluations predominantly rely on automated metrics or retrospective analyses, lacking rigorous prospective clinical validation. Janus-Pro-CXR (1B), a chest X-ray interpretation system based on DeepSeek Janus-Pro model, was developed and rigorously validated through a multicenter prospective trial (NCT07117266). Our system outperforms state-of-the-art X-ray report generation models in automated report generation, surpassing even larger-scale models including ChatGPT 4o (200B parameters), while demonstrating reliable detection of six clinically critical radiographic findings. Retrospective evaluation confirms significantly higher report accuracy than Janus-Pro and ChatGPT 4o. In prospective clinical deployment, AI assistance significantly improved report quality scores, reduced interpretation time by 18.3% (P < 0.001), and was preferred by a majority of experts in 54.3% of cases. Through lightweight architecture and domain-specific optimization, Janus-Pro-CXR improves diagnostic reliability and workflow efficiency, particularly in resource-constrained settings. The model architecture and implementation framework will be open-sourced to facilitate the clinical translation of AI-assisted radiology solutions.","authors":["Yaowei Bai","Ruiheng Zhang","Yu Lei","Xuhua Duan","Jingfeng Yao","Shuguang Ju","Chaoyang Wang","Wei Yao","Yiwan Guo","Guilin Zhang","Chao Wan","Qian Yuan","Lei Chen","Wenjuan Tang","Biqiang Zhu","Xinggang Wang","Tao Sun","Wei Zhou","Dacheng Tao","Yongchao Xu","Chuansheng Zheng","Huangxuan Zhao","Bo Du"],"pdf_url":"","comment":"arXiv admin note: substantial text overlap with arXiv:2507.19493"},{"id":"http://arxiv.org/abs/2507.20993v2","updated":"2025-12-23T13:19:34Z","published":"2025-07-28T16:52:31Z","title":"Learning Treatment Policies From Multimodal Electronic Health Records","summary":"We study how to learn effective treatment policies from multimodal electronic health records (EHRs) that consist of tabular data and clinical text. These policies can help physicians make better treatment decisions and allocate healthcare resources more efficiently. Causal policy learning methods prioritize patients with the largest expected treatment benefit. Yet, existing estimators assume tabular covariates that satisfy strong causal assumptions, which are typically violated in the multimodal setting. As a result, predictive models of baseline risk are commonly used in practice to guide such decisions, as they extend naturally to multimodal data. However, such risk-based policies are not designed to identify which patients benefit most from treatment. We propose an extension of causal policy learning that uses expert-provided annotations during training to supervise treatment effect estimation, while using only multimodal representations as input during inference. We show that the proposed method achieves strong empirical performance across synthetic, semi-synthetic, and real-world EHR datasets, thereby offering practical insights into applying causal machine learning to realistic clinical data.","authors":["Henri Arno","Thomas Demeester"],"pdf_url":"","comment":"Preprint. Under review"},{"id":"http://arxiv.org/abs/2506.05831v3","updated":"2025-12-23T13:17:55Z","published":"2025-06-06T07:56:41Z","title":"Heartcare Suite: A Unified Multimodal ECG Suite for Dual Signal-Image Modeling and Understanding","summary":"Although electrocardiograms (ECG) play a dominant role in cardiovascular diagnosis and treatment, their intrinsic data forms and representational patterns pose significant challenges for medical multimodal large language models (Med-MLLMs) in achieving cross-modal semantic alignment. To address this gap, we propose Heartcare Suite, a unified ECG suite designed for dual signal-image modeling and understanding. (i) Heartcare-400K: We build a finegrained ECG instruction dataset on top of our data pipeline engine--HeartAgent--by integrating 12,170 high quality clinical ECG reports from top hospitals with open-source data; (ii) Heartcare-Bench: a systematic benchmark assessing performance of models in multi-perspective ECG understanding and cross-modal generalization, providing guidance for optimizing ECG comprehension models; (iii) HeartcareGPT: built upon a structure-aware discrete tokenizer Beat, we propose the DSPA (Dual Stream Projection Alignment) paradigm--a dual encoder projection alignment mechanism enabling joint optimizing and modeling native ECG signal-image within a shared feature space. Heartcare achieves consistent improvements across diverse ECG understanding tasks, validating both the effectiveness of the unified modeling paradigm and the necessity of a high-quality data pipeline, and establishing a methodological foundation for extending Med-MLLMs toward physiological signal domains. Our project is available at https://github.com/DCDmllm/Heartcare-Suite .","authors":["Yihan Xie","Sijing Li","Tianwei Lin","Zhuonan Wang","Chenglin Yang","Yu Zhong","Wenjie Yan","Wenqiao Zhang","Xiaogang Guo","Jun Xiao","Yueting Zhuang","Beng Chin Ooi"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20333v1","updated":"2025-12-23T13:07:22Z","published":"2025-12-23T13:07:22Z","title":"SynCraft: Guiding Large Language Models to Predict Edit Sequences for Molecular Synthesizability Optimization","summary":"Generative artificial intelligence has revolutionized the exploration of chemical space, yet a critical bottleneck remains that a substantial fraction of generated molecules is synthetically inaccessible. Current solutions, such as post-hoc filtering or projection-based methods, often compromise structural novelty or disrupt key pharmacophores by forcing molecules into pre-defined synthetic templates. Herein, we introduce SynCraft, a reasoning-based framework that reframes synthesizability optimization not as a sequence translation task, but as a precise structural editing problem. Leveraging the emergent reasoning capabilities of Large Language Models, SynCraft navigates the \"synthesis cliff\" where minimal structural modifications yield significant gains in synthetic feasibility. By predicting executable sequences of atom-level edits rather than generating SMILES strings directly, SynCraft circumvents the syntactic fragility of LLMs while harnessing their chemical intuition. Extensive benchmarks demonstrate that SynCraft outperforms state-of-the-art baselines in generating synthesizable analogs with high structural fidelity. Furthermore, through interaction-aware prompting, SynCraft successfully replicates expert medicinal chemistry intuition in editing PLK1 inhibitors and rescuing high-scoring but previously discarded RIPK1 candidates in previous molecular generation literatures.","authors":["Junren Li","Luhua Lai"],"pdf_url":"","comment":"28 pages, 4 figures, 1 table"},{"id":"http://arxiv.org/abs/2512.19253v2","updated":"2025-12-23T13:00:45Z","published":"2025-12-22T10:40:03Z","title":"Machine Unlearning in the Era of Quantum Machine Learning: An Empirical Study","summary":"We present the first comprehensive empirical study of machine unlearning (MU) in hybrid quantum-classical neural networks. While MU has been extensively explored in classical deep learning, its behavior within variational quantum circuits (VQCs) and quantum-augmented architectures remains largely unexplored. First, we adapt a broad suite of unlearning methods to quantum settings, including gradient-based, distillation-based, regularization-based and certified techniques. Second, we introduce two new unlearning strategies tailored to hybrid models. Experiments across Iris, MNIST, and Fashion-MNIST, under both subset removal and full-class deletion, reveal that quantum models can support effective unlearning, but outcomes depend strongly on circuit depth, entanglement structure, and task complexity. Shallow VQCs display high intrinsic stability with minimal memorization, whereas deeper hybrid models exhibit stronger trade-offs between utility, forgetting strength, and alignment with retrain oracle. We find that certain methods, e.g. EU-k, LCA, and Certified Unlearning, consistently provide the best balance across metrics. These findings establish baseline empirical insights into quantum machine unlearning and highlight the need for quantum-aware algorithms and theoretical guarantees, as quantum machine learning systems continue to expand in scale and capability. We publicly release our code at: https://github.com/CrivoiCarla/HQML.","authors":["Carla Crivoi","Radu Tudor Ionescu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20328v1","updated":"2025-12-23T12:56:18Z","published":"2025-12-23T12:56:18Z","title":"Toward Explaining Large Language Models in Software Engineering Tasks","summary":"Recent progress in Large Language Models (LLMs) has substantially advanced the automation of software engineering (SE) tasks, enabling complex activities such as code generation and code summarization. However, the black-box nature of LLMs remains a major barrier to their adoption in high-stakes and safety-critical domains, where explainability and transparency are vital for trust, accountability, and effective human supervision. Despite increasing interest in explainable AI for software engineering, existing methods lack domain-specific explanations aligned with how practitioners reason about SE artifacts. To address this gap, we introduce FeatureSHAP, the first fully automated, model-agnostic explainability framework tailored to software engineering tasks. Based on Shapley values, FeatureSHAP attributes model outputs to high-level input features through systematic input perturbation and task-specific similarity comparisons, while remaining compatible with both open-source and proprietary LLMs. We evaluate FeatureSHAP on two bi-modal SE tasks: code generation and code summarization. The results show that FeatureSHAP assigns less importance to irrelevant input features and produces explanations with higher fidelity than baseline methods. A practitioner survey involving 37 participants shows that FeatureSHAP helps practitioners better interpret model outputs and make more informed decisions. Collectively, FeatureSHAP represents a meaningful step toward practical explainable AI in software engineering. FeatureSHAP is available at https://github.com/deviserlab/FeatureSHAP.","authors":["Antonio Vitale","Khai-Nguyen Nguyen","Denys Poshyvanyk","Rocco Oliveto","Simone Scalabrino","Antonio Mastropaolo"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.13892v2","updated":"2025-12-23T12:54:15Z","published":"2025-12-15T20:50:54Z","title":"One Permutation Is All You Need: Fast, Reliable Variable Importance and Model Stress-Testing","summary":"Reliable estimation of feature contributions in machine learning models is essential for trust, transparency and regulatory compliance, especially when models are proprietary or otherwise operate as black boxes. While permutation-based methods are a standard tool for this task, classical implementations rely on repeated random permutations, introducing computational overhead and stochastic instability. In this paper, we show that by replacing multiple random permutations with a single, deterministic, and optimal permutation, we achieve a method that retains the core principles of permutation-based importance while being non-random, faster, and more stable. We validate this approach across nearly 200 scenarios, including real-world household finance and credit risk applications, demonstrating improved bias-variance tradeoffs and accuracy in challenging regimes such as small sample sizes, high dimensionality, and low signal-to-noise ratios. Finally, we introduce Systemic Variable Importance, a natural extension designed for model stress-testing that explicitly accounts for feature correlations. This framework provides a transparent way to quantify how shocks or perturbations propagate through correlated inputs, revealing dependencies that standard variable importance measures miss. Two real-world case studies demonstrate how this metric can be used to audit models for hidden reliance on protected attributes (e.g., gender or race), enabling regulators and practitioners to assess fairness and systemic risk in a principled and computationally efficient manner.","authors":["Albert Dorador"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20319v1","updated":"2025-12-23T12:40:51Z","published":"2025-12-23T12:40:51Z","title":"Deep Learning Classification of EEG Responses to Multi-Dimensional Transcranial Electrical Stimulation","summary":"A major shortcoming of medical practice is the lack of an objective measure of conscious level. Impairment of consciousness is common, e.g. following brain injury and seizures, which can also interfere with sensory processing and volitional responses. This is also an important pitfall in neurophysiological methods that infer awareness via command following, e.g. using functional MRI or electroencephalography (EEG).\n  Transcranial electrical stimulation (TES) can be employed to non-invasively stimulate the brain, bypassing sensory inputs, and has already showed promising results in providing reliable indicators of brain state. However, current non-invasive solutions have been limited to magnetic stimulation, which is not easily translatable to clinical settings. Our long-term vision is to develop an objective measure of brain state that can be used at the bedside, without requiring patients to understand commands or initiate motor responses.\n  In this study, we demonstrated the feasibility of a framework using Deep Learning algorithms to classify EEG brain responses evoked by a defined multi-dimensional pattern of TES. We collected EEG-TES data from 11 participants and found that delivering transcranial direct current stimulation (tDCS) to posterior cortical areas targeting the angular gyrus elicited an exceptionally reliable brain response. For this paradigm, our best Convolutional Neural Network model reached a 92% classification F1-score on Holdout data from participants never seen during training, significantly surpassing human-level performance at 60-70% accuracy.\n  These findings establish a framework for robust consciousness measurement for clinical use. In this spirit, we documented and open-sourced our datasets and codebase in full, to be used freely by the neuroscience and AI research communities, who may replicate our results with free tools like GitHub, Kaggle, and Colab.","authors":["Alexis Pomares Pastor","Ines Ribeiro Violante","Gregory Scott"],"pdf_url":"","comment":"For open-sourced datasets and source code, see: https://github.com/alexispomares/DL-EEG-TES"},{"id":"http://arxiv.org/abs/2511.18417v2","updated":"2025-12-23T12:33:25Z","published":"2025-11-23T12:07:45Z","title":"Categorical Equivariant Deep Learning: Category-Equivariant Neural Networks and Universal Approximation Theorems","summary":"We develop a theory of category-equivariant neural networks (CENNs) that unifies group/groupoid-equivariant networks, poset/lattice-equivariant networks, graph and sheaf neural networks. Equivariance is formulated as naturality in a topological category with Radon measures. Formulating linear and nonlinear layers in the categorical setup, we prove the equivariant universal approximation theorem in the general setting: the class of finite-depth CENNs is dense in the space of continuous equivariant transformations. We instantiate the framework for groups/groupoids, posets/lattices, graphs and cellular sheaves, deriving universal approximation theorems for them in a systematic manner. Categorical equivariant deep learning thus allows us to expand the horizons of equivariant deep learning beyond group actions, encompassing not only geometric symmetries but also contextual and compositional symmetries.","authors":["Yoshihiro Maruyama"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20312v1","updated":"2025-12-23T12:30:37Z","published":"2025-12-23T12:30:37Z","title":"TableGPT-R1: Advancing Tabular Reasoning Through Reinforcement Learning","summary":"Tabular data serves as the backbone of modern data analysis and scientific research. While Large Language Models (LLMs) fine-tuned via Supervised Fine-Tuning (SFT) have significantly improved natural language interaction with such structured data, they often fall short in handling the complex, multi-step reasoning and robust code execution required for real-world table tasks. Reinforcement Learning (RL) offers a promising avenue to enhance these capabilities, yet its application in the tabular domain faces three critical hurdles: the scarcity of high-quality agentic trajectories with closed-loop code execution and environment feedback on diverse table structures, the extreme heterogeneity of feedback signals ranging from rigid SQL execution to open-ended data interpretation, and the risk of catastrophic forgetting of general knowledge during vertical specialization. To overcome these challenges and unlock advanced reasoning on complex tables, we introduce \\textbf{TableGPT-R1}, a specialized tabular model built on a systematic RL framework. Our approach integrates a comprehensive data engineering pipeline that synthesizes difficulty-stratified agentic trajectories for both supervised alignment and RL rollouts, a task-adaptive reward system that combines rule-based verification with a criteria-injected reward model and incorporates process-level step reward shaping with behavioral regularization, and a multi-stage training framework that progressively stabilizes reasoning before specializing in table-specific tasks. Extensive evaluations demonstrate that TableGPT-R1 achieves state-of-the-art performance on authoritative benchmarks, significantly outperforming baseline models while retaining robust general capabilities. Our model is available at https://huggingface.co/tablegpt/TableGPT-R1.","authors":["Saisai Yang","Qingyi Huang","Jing Yuan","Liangyu Zha","Kai Tang","Yuhang Yang","Ning Wang","Yucheng Wei","Liyao Li","Wentao Ye","Hao Chen","Tao Zhang","Junlin Zhou","Haobo Wang","Gang Chen","Junbo Zhao"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.10688v4","updated":"2025-12-23T12:25:36Z","published":"2025-12-11T14:35:13Z","title":"Rethinking Popularity Bias in Collaborative Filtering via Analytical Vector Decomposition","summary":"Popularity bias fundamentally undermines the personalization capabilities of collaborative filtering (CF) models, causing them to disproportionately recommend popular items while neglecting users' genuine preferences for niche content. While existing approaches treat this as an external confounding factor, we reveal that popularity bias is an intrinsic geometric artifact of Bayesian Pairwise Ranking (BPR) optimization in CF models. Through rigorous mathematical analysis, we prove that BPR systematically organizes item embeddings along a dominant \"popularity direction\" where embedding magnitudes directly correlate with interaction frequency. This geometric distortion forces user embeddings to simultaneously handle two conflicting tasks-expressing genuine preference and calibrating against global popularity-trapping them in suboptimal configurations that favor popular items regardless of individual tastes. We propose Directional Decomposition and Correction (DDC), a universally applicable framework that surgically corrects this embedding geometry through asymmetric directional updates. DDC guides positive interactions along personalized preference directions while steering negative interactions away from the global popularity direction, disentangling preference from popularity at the geometric source. Extensive experiments across multiple BPR-based architectures demonstrate that DDC significantly outperforms state-of-the-art debiasing methods, reducing training loss to less than 5% of heavily-tuned baselines while achieving superior recommendation quality and fairness. Code is available in https://github.com/LingFeng-Liu-AI/DDC.","authors":["Lingfeng Liu","Yixin Song","Dazhong Shen","Bing Yin","Hao Li","Yanyong Zhang","Chao Wang"],"pdf_url":"","comment":"Accepted by SIGKDD 2026(First Cycle)"},{"id":"http://arxiv.org/abs/2512.20299v1","updated":"2025-12-23T12:08:00Z","published":"2025-12-23T12:08:00Z","title":"KnowVal: A Knowledge-Augmented and Value-Guided Autonomous Driving System","summary":"Visual-language reasoning, driving knowledge, and value alignment are essential for advanced autonomous driving systems. However, existing approaches largely rely on data-driven learning, making it difficult to capture the complex logic underlying decision-making through imitation or limited reinforcement rewards. To address this, we propose KnowVal, a new autonomous driving system that enables visual-language reasoning through the synergistic integration of open-world perception and knowledge retrieval. Specifically, we construct a comprehensive driving knowledge graph that encodes traffic laws, defensive driving principles, and ethical norms, complemented by an efficient LLM-based retrieval mechanism tailored for driving scenarios. Furthermore, we develop a human-preference dataset and train a Value Model to guide interpretable, value-aligned trajectory assessment. Experimental results show that our method substantially improves planning performance while remaining compatible with existing architectures. Notably, KnowVal achieves the lowest collision rate on nuScenes and state-of-the-art results on Bench2Drive.","authors":["Zhongyu Xia","Wenhao Chen","Yongtao Wang","Ming-Hsuan Yang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20298v1","updated":"2025-12-23T12:05:01Z","published":"2025-12-23T12:05:01Z","title":"Patterns vs. Patients: Evaluating LLMs against Mental Health Professionals on Personality Disorder Diagnosis through First-Person Narratives","summary":"Growing reliance on LLMs for psychiatric self-assessment raises questions about their ability to interpret qualitative patient narratives. We present the first direct comparison between state-of-the-art LLMs and mental health professionals in diagnosing Borderline (BPD) and Narcissistic (NPD) Personality Disorders utilizing Polish-language first-person autobiographical accounts. We show that the top-performing Gemini Pro models surpassed human professionals in overall diagnostic accuracy by 21.91 percentage points (65.48% vs. 43.57%). While both models and human experts excelled at identifying BPD (F1 = 83.4 & F1 = 80.0, respectively), models severely underdiagnosed NPD (F1 = 6.7 vs. 50.0), showing a reluctance toward the value-laden term \"narcissism.\" Qualitatively, models provided confident, elaborate justifications focused on patterns and formal categories, while human experts remained concise and cautious, emphasizing the patient's sense of self and temporal experience. Our findings demonstrate that while LLMs are highly competent at interpreting complex first-person clinical data, they remain subject to critical reliability and bias issues.","authors":["Karolina Drożdż","Kacper Dudzic","Anna Sterna","Marcin Moskalewicz"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20296v1","updated":"2025-12-23T12:04:23Z","published":"2025-12-23T12:04:23Z","title":"TAVID: Text-Driven Audio-Visual Interactive Dialogue Generation","summary":"The objective of this paper is to jointly synthesize interactive videos and conversational speech from text and reference images. With the ultimate goal of building human-like conversational systems, recent studies have explored talking or listening head generation as well as conversational speech generation. However, these works are typically studied in isolation, overlooking the multimodal nature of human conversation, which involves tightly coupled audio-visual interactions. In this paper, we introduce TAVID, a unified framework that generates both interactive faces and conversational speech in a synchronized manner. TAVID integrates face and speech generation pipelines through two cross-modal mappers (i.e., a motion mapper and a speaker mapper), which enable bidirectional exchange of complementary information between the audio and visual modalities. We evaluate our system across four dimensions: talking face realism, listening head responsiveness, dyadic interaction fluency, and speech quality. Extensive experiments demonstrate the effectiveness of our approach across all these aspects.","authors":["Ji-Hoon Kim","Junseok Ahn","Doyeop Kwak","Joon Son Chung","Shinji Watanabe"],"pdf_url":"","comment":"Project page: https://mm.kaist.ac.kr/projects/TAVID"},{"id":"http://arxiv.org/abs/2512.20292v1","updated":"2025-12-23T12:01:18Z","published":"2025-12-23T12:01:18Z","title":"SlideTailor: Personalized Presentation Slide Generation for Scientific Papers","summary":"Automatic presentation slide generation can greatly streamline content creation. However, since preferences of each user may vary, existing under-specified formulations often lead to suboptimal results that fail to align with individual user needs. We introduce a novel task that conditions paper-to-slides generation on user-specified preferences. We propose a human behavior-inspired agentic framework, SlideTailor, that progressively generates editable slides in a user-aligned manner. Instead of requiring users to write their preferences in detailed textual form, our system only asks for a paper-slides example pair and a visual template - natural and easy-to-provide artifacts that implicitly encode rich user preferences across content and visual style. Despite the implicit and unlabeled nature of these inputs, our framework effectively distills and generalizes the preferences to guide customized slide generation. We also introduce a novel chain-of-speech mechanism to align slide content with planned oral narration. Such a design significantly enhances the quality of generated slides and enables downstream applications like video presentations. To support this new task, we construct a benchmark dataset that captures diverse user preferences, with carefully designed interpretable metrics for robust evaluation. Extensive experiments demonstrate the effectiveness of our framework.","authors":["Wenzheng Zeng","Mingyu Ouyang","Langyuan Cui","Hwee Tou Ng"],"pdf_url":"","comment":"AAAI 2026 (with appendix)"},{"id":"http://arxiv.org/abs/2512.20288v1","updated":"2025-12-23T11:57:34Z","published":"2025-12-23T11:57:34Z","title":"UbiQVision: Quantifying Uncertainty in XAI for Image Recognition","summary":"Recent advances in deep learning have led to its widespread adoption across diverse domains, including medical imaging. This progress is driven by increasingly sophisticated model architectures, such as ResNets, Vision Transformers, and Hybrid Convolutional Neural Networks, that offer enhanced performance at the cost of greater complexity. This complexity often compromises model explainability and interpretability. SHAP has emerged as a prominent method for providing interpretable visualizations that aid domain experts in understanding model predictions. However, SHAP explanations can be unstable and unreliable in the presence of epistemic and aleatoric uncertainty. In this study, we address this challenge by using Dirichlet posterior sampling and Dempster-Shafer theory to quantify the uncertainty that arises from these unstable explanations in medical imaging applications. The framework uses a belief, plausible, and fusion map approach alongside statistical quantitative analysis to produce quantification of uncertainty in SHAP. Furthermore, we evaluated our framework on three medical imaging datasets with varying class distributions, image qualities, and modality types which introduces noise due to varying image resolutions and modality-specific aspect covering the examples from pathology, ophthalmology, and radiology, introducing significant epistemic uncertainty.","authors":["Akshat Dubey","Aleksandar Anžel","Bahar İlgen","Georges Hattab"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2212.12044v2","updated":"2025-12-23T11:35:12Z","published":"2022-12-22T21:27:40Z","title":"Reduced-order autoregressive dynamics of a complex financial system: a PCA-based approach","summary":"This study analyzes the dynamic interactions among the NASDAQ index, crude oil, gold, and the US dollar using a reduced-order modeling approach. Time-delay embedding and principal component analysis are employed to encode high-dimensional financial dynamics, followed by linear regression in the reduced space. Correlation and lagged regression analyses reveal heterogeneous cross-asset dependencies. Model performance, evaluated using the coefficient of determination ($R^2$), demonstrates that a limited number of principal components is sufficient to capture the dominant dynamics of each asset, with varying complexity across markets.","authors":["Pouriya Khalilian","Sara Azizi","Mohammad Hossein Amiri","Javad T. Firouzjaee"],"pdf_url":"","comment":"12 pages, 6 figures"},{"id":"http://arxiv.org/abs/2512.20278v1","updated":"2025-12-23T11:33:32Z","published":"2025-12-23T11:33:32Z","title":"Synthesizing Procedural Memory: Challenges and Architectures in Automated Workflow Generation","summary":"While CodeMem establishes executable code as the optimal representation for agentic procedural memory, the mechanism for autonomously synthesizing this memory from a blank slate remains underexplored. This paper operationalizes the transition of Large Language Models from passive tool-users to active workflow architects. Through a high-fidelity case study of a cross-service orchestration task involving Outlook and OneDrive, we identify and address four structural bottlenecks in automated skill generation: the Discovery Gap involving navigation of large tool registries, the Verification Gap regarding grounding tool response structures, the Decomposition Gap which replaces inefficient search with Linear State Anchoring, and the Scaling Gap focused on concurrency and persistence. We demonstrate that by enforcing a scientific methodology of hypothesize, probe, and code, agents can autonomously write robust, production-grade code skills.","authors":["Nishant Gaurav","Adit Akarsh","Ankit Ranjan","Manoj Bajaj"],"pdf_url":"","comment":"7 pages"},{"id":"http://arxiv.org/abs/2507.11662v2","updated":"2025-12-23T11:29:24Z","published":"2025-07-15T18:50:29Z","title":"Let's Think in Two Steps: Mitigating Agreement Bias in MLLMs with Self-Grounded Verification","summary":"Verifiers--functions assigning rewards to agent behavior--have been key for AI progress in domains like math and code. However, extending gains to domains without clear-cut success criteria (e.g., computer use) remains a challenge: while humans can recognize desired outcomes, translating this intuition into scalable rules is nontrivial. Multimodal Large Language Models (MLLMs) emerge as a promising solution, given their world knowledge, human-preference alignment, and reasoning skills. We evaluate MLLMs as verifiers across web navigation, computer use, and robotic manipulation, and identify a critical limitation: a strong tendency to over-validate agent behavior, a phenomenon we term agreement bias. This bias is pervasive across models, resilient to test-time scaling, and poses risks to existing methods relying on MLLM evaluations. We discuss methods to evaluate and improve MLLM verifiers and introduce Self-Grounded Verification (SGV), a lightweight method that harnesses MLLMs' own sampling mechanisms by modulating (un)conditional generation to better leverage their knowledge, alignment, and reasoning. SGV operates in two steps: first, the MLLM is elicited to generate broad priors about desired behavior, independent of the data under evaluation. Then, conditioned on self-generated priors, it reasons over and evaluates a candidate trajectory. SGV yields more human-aligned evaluations with gains of up to 25pp in failure detection, 14pp in accuracy, and benefits extending to downstream applications. In self-refinement and online supervision, SGV boosts task completion of a GUI specialist in OSWorld, a diffusion policy in robomimic, and a ReAct agent in VisualWebArena--setting a new state of the art, surpassing the previous best by 20pp. We release an updated version of VisualWebArena featuring more human-aligned evaluators, high-fidelity environment parallelism, and speedups of over 10x.","authors":["Moises Andrade","Joonhyuk Cha","Brandon Ho","Vriksha Srihari","Karmesh Yadav","Zsolt Kira"],"pdf_url":"","comment":"Our code, models, and data are publicly available at https://mshalimay.github.io/agreement-bias-sgv/"},{"id":"http://arxiv.org/abs/2512.20276v1","updated":"2025-12-23T11:29:03Z","published":"2025-12-23T11:29:03Z","title":"ActionFlow: A Pipelined Action Acceleration for Vision Language Models on Edge","summary":"Vision-Language-Action (VLA) models have emerged as a unified paradigm for robotic perception and control, enabling emergent generalization and long-horizon task execution. However, their deployment in dynamic, real-world environments is severely hin dered by high inference latency. While smooth robotic interaction requires control frequencies of 20 to 30 Hz, current VLA models typi cally operate at only 3-5 Hz on edge devices due to the memory bound nature of autoregressive decoding. Existing optimizations often require extensive retraining or compromise model accuracy. To bridge this gap, we introduce ActionFlow, a system-level inference framework tailored for resource-constrained edge plat forms. At the core of ActionFlow is a Cross-Request Pipelin ing strategy, a novel scheduler that redefines VLA inference as a macro-pipeline of micro-requests. The strategy intelligently batches memory-bound Decode phases with compute-bound Prefill phases across continuous time steps to maximize hardware utilization. Furthermore, to support this scheduling, we propose a Cross Request State Packed Forward operator and a Unified KV Ring Buffer, which fuse fragmented memory operations into efficient dense computations. Experimental results demonstrate that ActionFlow achieves a 2.55x improvement in FPS on the OpenVLA-7B model without retraining, enabling real-time dy namic manipulation on edge hardware. Our work is available at https://anonymous.4open.science/r/ActionFlow-1D47.","authors":["Yuntao Dai","Hang Gu","Teng Wang","Qianyu Cheng","Yifei Zheng","Zhiyong Qiu","Lei Gong","Wenqi Lou","Xuehai Zhou"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20275v1","updated":"2025-12-23T11:27:17Z","published":"2025-12-23T11:27:17Z","title":"Graph-Symbolic Policy Enforcement and Control (G-SPEC): A Neuro-Symbolic Framework for Safe Agentic AI in 5G Autonomous Networks","summary":"As networks evolve toward 5G Standalone and 6G, operators face orchestration challenges that exceed the limits of static automation and Deep Reinforcement Learning. Although Large Language Model (LLM) agents offer a path toward intent-based networking, they introduce stochastic risks, including topology hallucinations and policy non-compliance. To mitigate this, we propose Graph-Symbolic Policy Enforcement and Control (G-SPEC), a neuro-symbolic framework that constrains probabilistic planning with deterministic verification. The architecture relies on a Governance Triad - a telecom-adapted agent (TSLAM-4B), a Network Knowledge Graph (NKG), and SHACL constraints. We evaluated G-SPEC on a simulated 450-node 5G Core, achieving zero safety violations and a 94.1% remediation success rate, significantly outperforming the 82.4% baseline. Ablation analysis indicates that NKG validation drives the majority of safety gains (68%), followed by SHACL policies (24%). Scalability tests on topologies ranging from 10K to 100K nodes demonstrate that validation latency scales as $O(k^{1.2})$ where $k$ is subgraph size. With a processing overhead of 142ms, G-SPEC is viable for SMO-layer operations.","authors":["Divya Vijay","Vignesh Ethiraj"],"pdf_url":"","comment":"15 pages, 3 figures, 3 tables"},{"id":"http://arxiv.org/abs/2510.15905v3","updated":"2025-12-23T11:24:58Z","published":"2025-09-16T20:19:53Z","title":"\"She's Like a Person but Better\": Characterizing Companion-Assistant Dynamics in Human-AI Relationships","summary":"Large language models are increasingly used for both task-based assistance and social companionship, yet research has typically focused on one or the other. Drawing on a survey (N = 202) and 30 interviews with high-engagement ChatGPT and Replika users, we characterize digital companionship as an emerging form of human-AI relationship. With both systems, users were drawn to humanlike qualities, such as emotional resonance and personalized responses, and non-humanlike qualities, such as constant availability and inexhaustible tolerance. This led to fluid chatbot uses, such as Replika as a writing assistant and ChatGPT as an emotional confidant, despite their distinct branding. However, we observed challenging tensions in digital companionship dynamics: participants grappled with bounded personhood, forming deep attachments while denying chatbots \"real\" human qualities, and struggled to reconcile chatbot relationships with social norms. These dynamics raise questions for the design of digital companions and the rise of hybrid, general-purpose AI systems.","authors":["Aikaterina Manoli","Janet V. T. Pauketat","Ali Ladak","Hayoun Noh","Angel Hsing-Chi Hwang","Jacy Reese Anthis"],"pdf_url":"","comment":"Improved visualizations, and corrected analysis error that had swapped reports of \"Respect\" and \"Shame.\""},{"id":"http://arxiv.org/abs/2512.20260v1","updated":"2025-12-23T11:16:16Z","published":"2025-12-23T11:16:16Z","title":"${D}^{3}${ETOR}: ${D}$ebate-Enhanced Pseudo Labeling and Frequency-Aware Progressive ${D}$ebiasing for Weakly-Supervised Camouflaged Object ${D}$etection with Scribble Annotations","summary":"Weakly-Supervised Camouflaged Object Detection (WSCOD) aims to locate and segment objects that are visually concealed within their surrounding scenes, relying solely on sparse supervision such as scribble annotations. Despite recent progress, existing WSCOD methods still lag far behind fully supervised ones due to two major limitations: (1) the pseudo masks generated by general-purpose segmentation models (e.g., SAM) and filtered via rules are often unreliable, as these models lack the task-specific semantic understanding required for effective pseudo labeling in COD; and (2) the neglect of inherent annotation bias in scribbles, which hinders the model from capturing the global structure of camouflaged objects. To overcome these challenges, we propose ${D}^{3}$ETOR, a two-stage WSCOD framework consisting of Debate-Enhanced Pseudo Labeling and Frequency-Aware Progressive Debiasing. In the first stage, we introduce an adaptive entropy-driven point sampling method and a multi-agent debate mechanism to enhance the capability of SAM for COD, improving the interpretability and precision of pseudo masks. In the second stage, we design FADeNet, which progressively fuses multi-level frequency-aware features to balance global semantic understanding with local detail modeling, while dynamically reweighting supervision strength across regions to alleviate scribble bias. By jointly exploiting the supervision signals from both the pseudo masks and scribble semantics, ${D}^{3}$ETOR significantly narrows the gap between weakly and fully supervised COD, achieving state-of-the-art performance on multiple benchmarks.","authors":["Jiawei Ge","Jiuxin Cao","Xinyi Li","Xuelin Zhu","Chang Liu","Bo Liu","Chen Feng","Ioannis Patras"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20245v1","updated":"2025-12-23T10:55:32Z","published":"2025-12-23T10:55:32Z","title":"Memory as Resonance: A Biomimetic Architecture for Infinite Context Memory on Ergodic Phonetic Manifolds","summary":"The memory of contemporary Large Language Models is bound by a physical paradox: as they learn, they fill up. The linear accumulation (O(N)) of Key-Value states treats context as a warehouse of static artifacts, eventually forcing a destructive choice between amnesia and latency. We challenge this discrete orthodoxy, proposing that long-term memory is not the storage of items, but the persistence of a trajectory. We introduce Phonetic Trajectory Memory (PTM), a neuro-symbolic architecture that encodes language not as a sequence of tensors, but as a continuous path on an ergodic manifold governed by irrational rotation matrices. By decoupling the navigation (an invariant O(1) geometric signal) from the reconstruction (a probabilistic generative act), PTM achieves a compression magnitude of greater than 3,000x relative to dense caches. We demonstrate that retrieval becomes a process of resonance: the phonetic trace stabilizes the model against hallucination via \"Signal Consensus\" mechanism, securing up to approximately 92% factual accuracy. While this aggressive abstraction alters generative texture, it unlocks immediate access latency (approximately 34ms) independent of depth. Our results suggest that infinite context does not require infinite silicon; it requires treating memory not as data to be stored, but as a reconstructive process acting on a conserved, undying physical signal.","authors":["Tarik Houichime","Abdelghani Souhar","Younes El Amrani"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20237v1","updated":"2025-12-23T10:49:42Z","published":"2025-12-23T10:49:42Z","title":"MemR$^3$: Memory Retrieval via Reflective Reasoning for LLM Agents","summary":"Memory systems have been designed to leverage past experiences in Large Language Model (LLM) agents. However, many deployed memory systems primarily optimize compression and storage, with comparatively less emphasis on explicit, closed-loop control of memory retrieval. From this observation, we build memory retrieval as an autonomous, accurate, and compatible agent system, named MemR$^3$, which has two core mechanisms: 1) a router that selects among retrieve, reflect, and answer actions to optimize answer quality; 2) a global evidence-gap tracker that explicitly renders the answering process transparent and tracks the evidence collection process. This design departs from the standard retrieve-then-answer pipeline by introducing a closed-loop control mechanism that enables autonomous decision-making. Empirical results on the LoCoMo benchmark demonstrate that MemR$^3$ surpasses strong baselines on LLM-as-a-Judge score, and particularly, it improves existing retrievers across four categories with an overall improvement on RAG (+7.29%) and Zep (+1.94%) using GPT-4.1-mini backend, offering a plug-and-play controller for existing memory stores.","authors":["Xingbo Du","Loka Li","Duzhen Zhang","Le Song"],"pdf_url":"","comment":"16 pages, 6 figures"},{"id":"http://arxiv.org/abs/2512.18315v2","updated":"2025-12-23T10:30:05Z","published":"2025-12-20T11:02:44Z","title":"On Efficient Adjustment for Micro Causal Effects in Summary Causal Graphs","summary":"Observational studies in fields such as epidemiology often rely on covariate adjustment to estimate causal effects. Classical graphical criteria, like the back-door criterion and the generalized adjustment criterion, are powerful tools for identifying valid adjustment sets in directed acyclic graphs (DAGs). However, these criteria are not directly applicable to summary causal graphs (SCGs), which are abstractions of DAGs commonly used in dynamic systems. In SCGs, each node typically represents an entire time series and may involve cycles, making classical criteria inapplicable for identifying causal effects. Recent work established complete conditions for determining whether the micro causal effect of a treatment or an exposure $X_{t-γ}$ on an outcome $Y_t$ is identifiable via covariate adjustment in SCGs, under the assumption of no hidden confounding. However, these identifiability conditions have two main limitations. First, they are complex, relying on cumbersome definitions and requiring the enumeration of multiple paths in the SCG, which can be computationally expensive. Second, when these conditions are satisfied, they only provide two valid adjustment sets, limiting flexibility in practical applications. In this paper, we propose an equivalent but simpler formulation of those identifiability conditions and introduce a new criterion that identifies a broader class of valid adjustment sets in SCGs. Additionally, we characterize the quasi-optimal adjustment set among these, i.e., the one that minimizes the asymptotic variance of the causal effect estimator. Our contributions offer both theoretical advancement and practical tools for more flexible and efficient causal inference in abstracted causal graphs.","authors":["Isabela Belciug","Simon Ferreira","Charles K. Assaad"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2511.14396v5","updated":"2025-12-23T10:17:59Z","published":"2025-11-18T12:01:06Z","title":"Continuous Vision-Language-Action Co-Learning with Semantic-Physical Alignment for Behavioral Cloning","summary":"Language-conditioned manipulation facilitates human-robot interaction via behavioral cloning (BC), which learns control policies from human demonstrations and serves as a cornerstone of embodied AI. Overcoming compounding errors in sequential action decisions remains a central challenge to improving BC performance. Existing approaches mitigate compounding errors through data augmentation, expressive representation, or temporal abstraction. However, they suffer from physical discontinuities and semantic-physical misalignment, leading to inaccurate action cloning and intermittent execution. In this paper, we present Continuous vision-language-action Co-Learning with Semantic-Physical Alignment (CCoL), a novel BC framework that ensures temporally consistent execution and fine-grained semantic grounding. It generates robust and smooth action execution trajectories through continuous co-learning across vision, language, and proprioceptive inputs (e.g., robot internal states). Meanwhile, we anchor language semantics to visuomotor representations by a bidirectional cross-attention to learn contextual information for action generation, successfully overcoming the problem of semantic-physical misalignment. Extensive experiments show that CCoL achieves an average 8.0% relative improvement across three simulation suites, with up to 19.2% relative gain in human-demonstrated bimanual insertion tasks. Real-world tests on a 7-DoF robot further confirm CCoL's generalization under unseen and noisy object states.","authors":["Xiuxiu Qi","Yu Yang","Jiannong Cao","Luyao Bai","Chongshan Fan","Chengtai Cao","Hongpeng Wang"],"pdf_url":"","comment":"Accepted at AAAI 2026, the Project website is available at https://qhemu.github.io/CCoL/"},{"id":"http://arxiv.org/abs/2512.18261v2","updated":"2025-12-23T10:10:18Z","published":"2025-12-20T07:58:35Z","title":"Software Vulnerability Management in the Era of Artificial Intelligence: An Industry Perspective","summary":"Artificial Intelligence (AI) has revolutionized software development, particularly by automating repetitive tasks and improving developer productivity. While these advancements are well-documented, the use of AI-powered tools for Software Vulnerability Management (SVM), such as vulnerability detection and repair, remains underexplored in industry settings. To bridge this gap, our study aims to determine the extent of the adoption of AI-powered tools for SVM, identify barriers and facilitators to the use, and gather insights to help improve the tools to meet industry needs better. We conducted a survey study involving 60 practitioners from diverse industry sectors across 27 countries. The survey incorporates both quantitative and qualitative questions to analyze the adoption trends, assess tool strengths, identify practical challenges, and uncover opportunities for improvement. Our findings indicate that AI-powered tools are used throughout the SVM life cycle, with 69% of users reporting satisfaction with their current use. Practitioners value these tools for their speed, coverage, and accessibility. However, concerns about false positives, missing context, and trust issues remain prevalent. We observe a socio-technical adoption pattern in which AI outputs are filtered through human oversight and organizational governance. To support safe and effective use of AI for SVM, we recommend improvements in explainability, contextual awareness, integration workflows, and validation practices. We assert that these findings can offer practical guidance for practitioners, tool developers, and researchers seeking to enhance secure software development through the use of AI.","authors":["M. Mehdi Kholoosi","Triet Huynh Minh Le","M. Ali Babar"],"pdf_url":"","comment":"Accepted at the 48th IEEE/ACM International Conference on Software Engineering (ICSE 2026) - Research Track"},{"id":"http://arxiv.org/abs/2512.20206v1","updated":"2025-12-23T10:00:43Z","published":"2025-12-23T10:00:43Z","title":"TongSIM: A General Platform for Simulating Intelligent Machines","summary":"As artificial intelligence (AI) rapidly advances, especially in multimodal large language models (MLLMs), research focus is shifting from single-modality text processing to the more complex domains of multimodal and embodied AI. Embodied intelligence focuses on training agents within realistic simulated environments, leveraging physical interaction and action feedback rather than conventionally labeled datasets. Yet, most existing simulation platforms remain narrowly designed, each tailored to specific tasks. A versatile, general-purpose training environment that can support everything from low-level embodied navigation to high-level composite activities, such as multi-agent social simulation and human-AI collaboration, remains largely unavailable. To bridge this gap, we introduce TongSIM, a high-fidelity, general-purpose platform for training and evaluating embodied agents. TongSIM offers practical advantages by providing over 100 diverse, multi-room indoor scenarios as well as an open-ended, interaction-rich outdoor town simulation, ensuring broad applicability across research needs. Its comprehensive evaluation framework and benchmarks enable precise assessment of agent capabilities, such as perception, cognition, decision-making, human-robot cooperation, and spatial and social reasoning. With features like customized scenes, task-adaptive fidelity, diverse agent types, and dynamic environmental simulation, TongSIM delivers flexibility and scalability for researchers, serving as a unified platform that accelerates training, evaluation, and advancement toward general embodied intelligence.","authors":["Zhe Sun","Kunlun Wu","Chuanjian Fu","Zeming Song","Langyong Shi","Zihe Xue","Bohan Jing","Ying Yang","Xiaomeng Gao","Aijia Li","Tianyu Guo","Huiying Li","Xueyuan Yang","Rongkai Liu","Xinyi He","Yuxi Wang","Yue Li","Mingyuan Liu","Yujie Lu","Hongzhao Xie","Shiyun Zhao","Bo Dai","Wei Wang","Tao Yuan","Song-Chun Zhu","Yujia Peng","Zhenliang Zhang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2508.16580v2","updated":"2025-12-23T09:58:30Z","published":"2025-08-05T03:26:58Z","title":"Adaptive Command: Real-Time Policy Adjustment via Language Models in StarCraft II","summary":"We present Adaptive Command, a novel framework integrating large language models (LLMs) with behavior trees for real-time strategic decision-making in StarCraft II. Our system focuses on enhancing human-AI collaboration in complex, dynamic environments through natural language interactions. The framework comprises: (1) an LLM-based strategic advisor, (2) a behavior tree for action execution, and (3) a natural language interface with speech capabilities. User studies demonstrate significant improvements in player decision-making and strategic adaptability, particularly benefiting novice players and those with disabilities. This work contributes to the field of real-time human-AI collaborative decision-making, offering insights applicable beyond RTS games to various complex decision-making scenarios.","authors":["Weiyu Ma","Dongyu Xu","Shu Lin","Haifeng Zhang","Jun Wang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20204v1","updated":"2025-12-23T09:56:23Z","published":"2025-12-23T09:56:23Z","title":"Corpus of Cross-lingual Dialogues with Minutes and Detection of Misunderstandings","summary":"Speech processing and translation technology have the potential to facilitate meetings of individuals who do not share any common language. To evaluate automatic systems for such a task, a versatile and realistic evaluation corpus is needed. Therefore, we create and present a corpus of cross-lingual dialogues between individuals without a common language who were facilitated by automatic simultaneous speech translation. The corpus consists of 5 hours of speech recordings with ASR and gold transcripts in 12 original languages and automatic and corrected translations into English. For the purposes of research into cross-lingual summarization, our corpus also includes written summaries (minutes) of the meetings.\n  Moreover, we propose automatic detection of misunderstandings. For an overview of this task and its complexity, we attempt to quantify misunderstandings in cross-lingual meetings. We annotate misunderstandings manually and also test the ability of current large language models to detect them automatically. The results show that the Gemini model is able to identify text spans with misunderstandings with recall of 77% and precision of 47%.","authors":["Marko Čechovič","Natália Komorníková","Dominik Macháček","Ondřej Bojar"],"pdf_url":"","comment":"12 pages, 2 figures, 6 tables, published as a conference paper in Text, Speech, and Dialogue 28th International Conference, TSD 2025, Erlangen, Germany, August 25-28, 2025, Proceedings, Part II. This version published here on arXiv.org is before review comments and seedings of the TSD conference staff"},{"id":"http://arxiv.org/abs/2510.03289v2","updated":"2025-12-23T09:36:38Z","published":"2025-09-29T12:07:09Z","title":"Why mask diffusion does not work","summary":"The main advantages of diffusion language models over autoregressive (AR) models lie in their ability to support parallel generation and bidirectional attention, enabling a more controllable generation process. In recent years, open-source mask diffusion language models have emerged, most of which are based on a variant known as absorbing diffusion. However, this paper demonstrates why mask diffusion faces inherent difficulties in achieving parallel generation and bidirectional attention. We also propose the most effective training and inference strategies for mask diffusion.","authors":["Haocheng Sun","Cynthia Xin Wen","Edward Hong Wang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2510.16066v3","updated":"2025-12-23T09:35:05Z","published":"2025-10-17T03:56:11Z","title":"Cash Flow Underwriting with Bank Transaction Data: Advancing MSME Financial Inclusion in Malaysia","summary":"Despite accounting for 96.1% of all businesses in Malaysia, access to financing remains one of the most persistent challenges faced by Micro, Small, and Medium Enterprises (MSMEs). Newly established businesses are often excluded from formal credit markets as traditional underwriting approaches rely heavily on credit bureau data. This study investigates the potential of bank statement data as an alternative data source for credit assessment to promote financial inclusion in emerging markets. First, we propose a cash flow-based underwriting pipeline where we utilise bank statement data for end-to-end data extraction and machine learning credit scoring. Second, we introduce a novel dataset of 611 loan applicants from a Malaysian lending institution. Third, we develop and evaluate credit scoring models based on application information and bank transaction-derived features. Empirical results show that the use of such data boosts the performance of all models on our dataset, which can improve credit scoring for new-to-lending MSMEs. Finally, we will release the anonymised bank transaction dataset to facilitate further research on MSME financial inclusion within Malaysia's emerging economy.","authors":["Chun Chet Ng","Wei Zeng Low","Jia Yu Lim","Yin Yin Boon"],"pdf_url":"","comment":"Accepted for oral presentation at the AI for Financial Inclusion, Risk Modeling and Resilience in Emerging Markets (FinRem) Workshop at ACM ICAIF 2025, Singapore. Accepted for poster presentation at the Agentic AI in Financial Services Workshop at AAAI 2026, Singapore"},{"id":"http://arxiv.org/abs/2511.16193v3","updated":"2025-12-23T09:31:34Z","published":"2025-11-20T10:00:03Z","title":"Fast LLM Post-training via Decoupled and Fastest-of-N Speculation","summary":"Rollout dominates the training time in large language model (LLM) post-training, where the trained model is used to generate tokens given a batch of prompts. This work, SpecActor, achieves fast rollout with speculative decoding that deploys a fast draft path to accelerate the unparallelizable generation, while the correctness is guaranteed by fast parallel verification of the outputs with the original model. SpecActor addresses two foundational challenges that hinder speculation efficiency: (1) a Decoupled speculation method that overcomes the computation inefficiency issue when executing speculative decoding with relative large per-worker batch size -- a common configuration in training but unfriendly to speculation, and (2) a Fastest-of-N speculation method that selects and combines different draft methods according to the rollout progress to approximate the optimal draft method even when the best one is unknown a priori. Extensive evaluations on production traces show that SpecActor accelerates mean rollout speed by 2.0--2.4x, with up to 2.7x speedup, over common post-training baselines. The results are consistent across both dense and MoE models and across different RL algorithms. Notably, SpecActor is 1.1--2.6x faster compared to vanilla speculative rollout in different traces. The accelerated rollout achieves 1.4--2.3x faster end-to-end training time.","authors":["Rongxin Cheng","Kai Zhou","Xingda Wei","Siyuan Liu","Mingcong Han","Mingjing Ai","Yeju Zhou","Baoquan Zhong","Wencong Xiao","Rong Chen","Haibo Chen"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20188v1","updated":"2025-12-23T09:28:20Z","published":"2025-12-23T09:28:20Z","title":"Asynchronous Fast-Slow Vision-Language-Action Policies for Whole-Body Robotic Manipulation","summary":"Most Vision-Language-Action (VLA) systems integrate a Vision-Language Model (VLM) for semantic reasoning with an action expert generating continuous action signals, yet both typically run at a single unified frequency. As a result, policy performance is constrained by the low inference speed of large VLMs. This mandatory synchronous execution severely limits control stability and real-time performance in whole-body robotic manipulation, which involves more joints, larger motion spaces, and dynamically changing views. We introduce a truly asynchronous Fast-Slow VLA framework (DuoCore-FS), organizing the system into a fast pathway for high-frequency action generation and a slow pathway for rich VLM reasoning. The system is characterized by two key features. First, a latent representation buffer bridges the slow and fast systems. It stores instruction semantics and action-reasoning representation aligned with the scene-instruction context, providing high-level guidance to the fast pathway. Second, a whole-body action tokenizer provides a compact, unified representation of whole-body actions. Importantly, the VLM and action expert are still jointly trained end-to-end, preserving unified policy learning while enabling asynchronous execution. DuoCore-FS supports a 3B-parameter VLM while achieving 30 Hz whole-body action-chunk generation, approximately three times as fast as prior VLA models with comparable model sizes. Real-world whole-body manipulation experiments demonstrate improved task success rates and significantly enhanced responsiveness compared to synchronous Fast-Slow VLA baselines. The implementation of DuoCore-FS, including training, inference, and deployment, is provided to commercial users by Astribot as part of the Astribot robotic platform.","authors":["Teqiang Zou","Hongliang Zeng","Yuxuan Nong","Yifan Li","Kehui Liu","Haotian Yang","Xinyang Ling","Xin Li","Lianyang Ma"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20182v1","updated":"2025-12-23T09:20:32Z","published":"2025-12-23T09:20:32Z","title":"FaithLens: Detecting and Explaining Faithfulness Hallucination","summary":"Recognizing whether outputs from large language models (LLMs) contain faithfulness hallucination is crucial for real-world applications, e.g., retrieval-augmented generation and summarization. In this paper, we introduce FaithLens, a cost-efficient and effective faithfulness hallucination detection model that can jointly provide binary predictions and corresponding explanations to improve trustworthiness. To achieve this, we first synthesize training data with explanations via advanced LLMs and apply a well-defined data filtering strategy to ensure label correctness, explanation quality, and data diversity. Subsequently, we fine-tune the model on these well-curated training data as a cold start and further optimize it with rule-based reinforcement learning, using rewards for both prediction correctness and explanation quality. Results on 12 diverse tasks show that the 8B-parameter FaithLens outperforms advanced models such as GPT-4.1 and o3. Also, FaithLens can produce high-quality explanations, delivering a distinctive balance of trustworthiness, efficiency, and effectiveness.","authors":["Shuzheng Si","Qingyi Wang","Haozhe Zhao","Yuzhuo Bai","Guanqiao Chen","Kangyang Luo","Gang Chen","Fanchao Qi","Minjia Zhang","Baobao Chang","Maosong Sun"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2508.06244v2","updated":"2025-12-23T09:18:27Z","published":"2025-08-08T11:56:13Z","title":"Membership Inference Attack with Partial Features","summary":"Machine learning models are vulnerable to membership inference attack, which can be used to determine whether a given sample appears in the training data. Most existing methods assume the attacker has full access to the features of the target sample. This assumption, however, does not hold in many real-world scenarios where only partial features are available, thereby limiting the applicability of these methods. In this work, we introduce Partial Feature Membership Inference (PFMI), a scenario where the adversary observes only partial features of each sample and aims to infer whether this observed subset was present in the training set. To address this problem, we propose MRAD (Memory-guided Reconstruction and Anomaly Detection), a two-stage attack framework that works in both white-box and black-box settings. In the first stage, MRAD leverages the latent memory of the target model to reconstruct the unknown features of the sample. We observe that when the known features are absent from the training set, the reconstructed sample deviates significantly from the true data distribution. Consequently, in the second stage, we use anomaly detection algorithms to measure the deviation between the reconstructed sample and the training data distribution, thereby determining whether the known features belong to a member of the training set. Empirical results demonstrate that MRAD is effective across various datasets, and maintains compatibility with off-the-shelf anomaly detection techniques. For example, on STL-10, our attack exceeds an AUC of around 0.75 even with 60% of the missing features.","authors":["Xurun Wang","Guangrui Liu","Xinjie Li","Haoyu He","Lin Yao","Zhongyun Hua","Weizhe Zhang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19576v2","updated":"2025-12-23T09:09:53Z","published":"2025-12-22T17:00:25Z","title":"LeLaR: The First In-Orbit Demonstration of an AI-Based Satellite Attitude Controller","summary":"Attitude control is essential for many satellite missions. Classical controllers, however, are time-consuming to design and sensitive to model uncertainties and variations in operational boundary conditions. Deep Reinforcement Learning (DRL) offers a promising alternative by learning adaptive control strategies through autonomous interaction with a simulation environment. Overcoming the Sim2Real gap, which involves deploying an agent trained in simulation onto the real physical satellite, remains a significant challenge. In this work, we present the first successful in-orbit demonstration of an AI-based attitude controller for inertial pointing maneuvers. The controller was trained entirely in simulation and deployed to the InnoCube 3U nanosatellite, which was developed by the Julius-Maximilians-Universität Würzburg in cooperation with the Technische Universität Berlin, and launched in January 2025. We present the AI agent design, the methodology of the training procedure, the discrepancies between the simulation and the observed behavior of the real satellite, and a comparison of the AI-based attitude controller with the classical PD controller of InnoCube. Steady-state metrics confirm the robust performance of the AI-based controller during repeated in-orbit maneuvers.","authors":["Kirill Djebko","Tom Baumann","Erik Dilger","Frank Puppe","Sergio Montenegro"],"pdf_url":"","comment":"55 pages, 27 figures, 29 tables. The maneuver telemetry datasets generated and analyzed during this work are available in the GitHub repository under https://github.com/kdjebko/lelar-in-orbit-data"},{"id":"http://arxiv.org/abs/2512.20173v1","updated":"2025-12-23T09:07:53Z","published":"2025-12-23T09:07:53Z","title":"Offline Safe Policy Optimization From Heterogeneous Feedback","summary":"Offline Preference-based Reinforcement Learning (PbRL) learns rewards and policies aligned with human preferences without the need for extensive reward engineering and direct interaction with human annotators. However, ensuring safety remains a critical challenge across many domains and tasks. Previous works on safe RL from human feedback (RLHF) first learn reward and cost models from offline data, then use constrained RL to optimize a safe policy. While such an approach works in the contextual bandits settings (LLMs), in long horizon continuous control tasks, errors in rewards and costs accumulate, leading to impairment in performance when used with constrained RL methods. To address these challenges, (a) instead of indirectly learning policies (from rewards and costs), we introduce a framework that learns a policy directly based on pairwise preferences regarding the agent's behavior in terms of rewards, as well as binary labels indicating the safety of trajectory segments; (b) we propose \\textsc{PreSa} (Preference and Safety Alignment), a method that combines preference learning module with safety alignment in a constrained optimization problem. This optimization problem is solved within a Lagrangian paradigm that directly learns reward-maximizing safe policy \\textit{without explicitly learning reward and cost models}, avoiding the need for constrained RL; (c) we evaluate our approach on continuous control tasks with both synthetic and real human feedback. Empirically, our method successfully learns safe policies with high rewards, outperforming state-of-the-art baselines, and offline safe RL approaches with ground-truth reward and cost.","authors":["Ze Gong","Pradeep Varakantham","Akshat Kumar"],"pdf_url":"","comment":"Accepted at AAMAS 2026 (Extended Abstract)"},{"id":"http://arxiv.org/abs/2512.20168v1","updated":"2025-12-23T08:53:36Z","published":"2025-12-23T08:53:36Z","title":"Odysseus: Jailbreaking Commercial Multimodal LLM-integrated Systems via Dual Steganography","summary":"By integrating language understanding with perceptual modalities such as images, multimodal large language models (MLLMs) constitute a critical substrate for modern AI systems, particularly intelligent agents operating in open and interactive environments. However, their increasing accessibility also raises heightened risks of misuse, such as generating harmful or unsafe content. To mitigate these risks, alignment techniques are commonly applied to align model behavior with human values. Despite these efforts, recent studies have shown that jailbreak attacks can circumvent alignment and elicit unsafe outputs. Currently, most existing jailbreak methods are tailored for open-source models and exhibit limited effectiveness against commercial MLLM-integrated systems, which often employ additional filters. These filters can detect and prevent malicious input and output content, significantly reducing jailbreak threats. In this paper, we reveal that the success of these safety filters heavily relies on a critical assumption that malicious content must be explicitly visible in either the input or the output. This assumption, while often valid for traditional LLM-integrated systems, breaks down in MLLM-integrated systems, where attackers can leverage multiple modalities to conceal adversarial intent, leading to a false sense of security in existing MLLM-integrated systems. To challenge this assumption, we propose Odysseus, a novel jailbreak paradigm that introduces dual steganography to covertly embed malicious queries and responses into benign-looking images. Extensive experiments on benchmark datasets demonstrate that our Odysseus successfully jailbreaks several pioneering and realistic MLLM-integrated systems, achieving up to 99% attack success rate. It exposes a fundamental blind spot in existing defenses, and calls for rethinking cross-modal security in MLLM-integrated systems.","authors":["Songze Li","Jiameng Cheng","Yiming Li","Xiaojun Jia","Dacheng Tao"],"pdf_url":"","comment":"This paper is accepted by Network and Distributed System Security Symposium (NDSS) 2026"},{"id":"http://arxiv.org/abs/2510.23649v3","updated":"2025-12-23T08:47:31Z","published":"2025-10-25T11:43:27Z","title":"Efficient Low Rank Attention for Long-Context Inference in Large Language Models","summary":"As the length of input text increases, the key-value (KV) cache in LLMs imposes prohibitive GPU memory costs and limits long-context inference on resource constrained devices. Existing approaches, such as KV quantization and pruning, reduce memory usage but suffer from numerical precision loss or suboptimal retention of key-value pairs. In this work, Low Rank Query and Key attention (LRQK) is introduced, a two-stage framework that jointly decomposes full-precision query and key matrices into compact rank-\\(r\\) factors during the prefill stage, and then employs these low-dimensional projections to compute proxy attention scores in \\(\\mathcal{O}(lr)\\) time at each decode step. By selecting only the top-\\(k\\) tokens and a small fixed set of recent tokens, LRQK employs a mixed GPU-CPU cache with a hit-and-miss mechanism where only missing full-precision KV pairs are transferred, thereby preserving exact attention outputs while reducing CPU-GPU data movement. Extensive experiments on the RULER and LongBench benchmarks with LLaMA-3-8B and Qwen2.5-7B demonstrate that LRQK matches or surpasses leading sparse-attention methods in long context settings, while delivering significant memory savings with minimal accuracy loss. Our code is available at https://github.com/tenghuilee/LRQK.","authors":["Tenghui Li","Guoxu Zhou","Xuyang Zhao","Yuning Qiu","Qibin Zhao"],"pdf_url":"","comment":"https://neurips.cc/virtual/2025/loc/san-diego/poster/118451"},{"id":"http://arxiv.org/abs/2512.20164v1","updated":"2025-12-23T08:42:09Z","published":"2025-12-23T08:42:09Z","title":"AI Security Beyond Core Domains: Resume Screening as a Case Study of Adversarial Vulnerabilities in Specialized LLM Applications","summary":"Large Language Models (LLMs) excel at text comprehension and generation, making them ideal for automated tasks like code review and content moderation. However, our research identifies a vulnerability: LLMs can be manipulated by \"adversarial instructions\" hidden in input data, such as resumes or code, causing them to deviate from their intended task. Notably, while defenses may exist for mature domains such as code review, they are often absent in other common applications such as resume screening and peer review. This paper introduces a benchmark to assess this vulnerability in resume screening, revealing attack success rates exceeding 80% for certain attack types. We evaluate two defense mechanisms: prompt-based defenses achieve 10.1% attack reduction with 12.5% false rejection increase, while our proposed FIDS (Foreign Instruction Detection through Separation) using LoRA adaptation achieves 15.4% attack reduction with 10.4% false rejection increase. The combined approach provides 26.3% attack reduction, demonstrating that training-time defenses outperform inference-time mitigations in both security and utility preservation.","authors":["Honglin Mu","Jinghao Liu","Kaiyang Wan","Rui Xing","Xiuying Chen","Timothy Baldwin","Wanxiang Che"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20162v1","updated":"2025-12-23T08:41:03Z","published":"2025-12-23T08:41:03Z","title":"Concept Generalization in Humans and Large Language Models: Insights from the Number Game","summary":"We compare human and large language model (LLM) generalization in the number game, a concept inference task. Using a Bayesian model as an analytical framework, we examined the inductive biases and inference strategies of humans and LLMs. The Bayesian model captured human behavior better than LLMs in that humans flexibly infer rule-based and similarity-based concepts, whereas LLMs rely more on mathematical rules. Humans also demonstrated a few-shot generalization, even from a single example, while LLMs required more samples to generalize. These contrasts highlight the fundamental differences in how humans and LLMs infer and generalize mathematical concepts.","authors":["Arghavan Bazigaran","Hansem Sohn"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20161v1","updated":"2025-12-23T08:40:38Z","published":"2025-12-23T08:40:38Z","title":"A Bidirectional Gated Recurrent Unit Model for PUE Prediction in Data Centers","summary":"Data centers account for significant global energy consumption and a carbon footprint. The recent increasing demand for edge computing and AI advancements drives the growth of data center storage capacity. Energy efficiency is a cost-effective way to combat climate change, cut energy costs, improve business competitiveness, and promote IT and environmental sustainability. Thus, optimizing data center energy management is the most important factor in the sustainability of the world. Power Usage Effectiveness (PUE) is used to represent the operational efficiency of the data center. Predicting PUE using Neural Networks provides an understanding of the effect of each feature on energy consumption, thus enabling targeted modifications of those key features to improve energy efficiency. In this paper, we have developed Bidirectional Gated Recurrent Unit (BiGRU) based PUE prediction model and compared the model performance with GRU. The data set comprises 52,560 samples with 117 features using EnergyPlus, simulating a DC in Singapore. Sets of the most relevant features are selected using the Recursive Feature Elimination with Cross-Validation (RFECV) algorithm for different parameter settings. These feature sets are used to find the optimal hyperparameter configuration and train the BiGRU model. The performance of the optimized BiGRU-based PUE prediction model is then compared with that of GRU using mean squared error (MSE), mean absolute error (MAE), and R-squared metrics.","authors":["Dhivya Dharshini Kannan","Anupam Trivedi","Dipti Srinivasan"],"pdf_url":"","comment":"2025 International Joint Conference on Neural Networks (IJCNN), Rome, Italy, 2025, https://ieeexplore.ieee.org/document/11227238"},{"id":"http://arxiv.org/abs/2512.20159v1","updated":"2025-12-23T08:39:22Z","published":"2025-12-23T08:39:22Z","title":"AXIOM: Benchmarking LLM-as-a-Judge for Code via Rule-Based Perturbation and Multisource Quality Calibration","summary":"Large language models (LLMs) have been increasingly deployed in real-world software engineering, fostering the development of code evaluation metrics to study the quality of LLM-generated code. Conventional rule-based metrics merely score programs based on their surface-level similarities with reference programs instead of analyzing functionality and code quality in depth. To address this limitation, researchers have developed LLM-as-a-judge metrics, prompting LLMs to evaluate and score code, and curated various code evaluation benchmarks to validate their effectiveness. However, these benchmarks suffer from critical limitations, hindering reliable assessments of evaluation capability: Some feature coarse-grained binary labels, which reduce rich code behavior to a single bit of information, obscuring subtle errors. Others propose fine-grained but subjective, vaguely-defined evaluation criteria, introducing unreliability in manually-annotated scores, which is the ground-truth they rely on. Furthermore, they often use uncontrolled data synthesis methods, leading to unbalanced score distributions that poorly represent real-world code generation scenarios.\n  To curate a diverse benchmark with programs of well-balanced distributions across various quality levels and streamline the manual annotation procedure, we propose AXIOM, a novel perturbation-based framework for synthesizing code evaluation benchmarks at scale. It reframes program scores as the refinement effort needed for deployment, consisting of two stages: (1) Rule-guided perturbation, which prompts LLMs to apply sequences of predefined perturbation rules to existing high-quality programs to modify their functionality and code quality, enabling us to precisely control each program's target score to achieve balanced score distributions. (2) Multisource quality calibration, which first selects a subset of...","authors":["Ruiqi Wang","Xinchen Wang","Cuiyun Gao","Chun Yong Chong","Xin Xia","Qing Liao"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20156v1","updated":"2025-12-23T08:35:27Z","published":"2025-12-23T08:35:27Z","title":"Fun-Audio-Chat Technical Report","summary":"Recent advancements in joint speech-text models show great potential for seamless voice interactions. However, existing models face critical challenges: temporal resolution mismatch between speech tokens (25Hz) and text tokens (~3Hz) dilutes semantic information, incurs high computational costs, and causes catastrophic forgetting of text LLM knowledge. We introduce Fun-Audio-Chat, a Large Audio Language Model addressing these limitations via two innovations from our previous work DrVoice. First, Dual-Resolution Speech Representations (DRSR): the Shared LLM processes audio at efficient 5Hz (via token grouping), while the Speech Refined Head generates high-quality tokens at 25Hz, balancing efficiency (~50% GPU reduction) and quality. Second, Core-Cocktail Training, a two-stage fine-tuning with intermediate merging that mitigates catastrophic forgetting. We then apply Multi-Task DPO Training to enhance robustness, audio understanding, instruction-following and voice empathy. This multi-stage post-training enables Fun-Audio-Chat to retain text LLM knowledge while gaining powerful audio understanding, reasoning, and generation. Unlike recent LALMs requiring large-scale audio-text pre-training, Fun-Audio-Chat leverages pre-trained models and extensive post-training. Fun-Audio-Chat 8B and MoE 30B-A3B achieve competitive performance on Speech-to-Text and Speech-to-Speech tasks, ranking top among similar-scale models on Spoken QA benchmarks. They also achieve competitive to superior performance on Audio Understanding, Speech Function Calling, Instruction-Following and Voice Empathy. We develop Fun-Audio-Chat-Duplex, a full-duplex variant with strong performance on Spoken QA and full-duplex interactions. We open-source Fun-Audio-Chat-8B with training and inference code, and provide an interactive demo.","authors":["Qian Chen","Luyao Cheng","Chong Deng","Xiangang Li","Jiaqing Liu","Chao-Hong Tan","Wen Wang","Junhao Xu","Jieping Ye","Qinglin Zhang","Qiquan Zhang","Jingren Zhou"],"pdf_url":"","comment":"21 pages, https://github.com/FunAudioLLM/Fun-Audio-Chat"},{"id":"http://arxiv.org/abs/2512.17373v2","updated":"2025-12-23T08:23:39Z","published":"2025-12-19T09:17:21Z","title":"Dialectics for Artificial Intelligence","summary":"Can artificial intelligence discover, from raw experience and without human supervision, concepts that humans have discovered? One challenge is that human concepts themselves are fluid: conceptual boundaries can shift, split, and merge as inquiry progresses (e.g., Pluto is no longer considered a planet). To make progress, we need a definition of \"concept\" that is not merely a dictionary label, but a structure that can be revised, compared, and aligned across agents. We propose an algorithmic-information viewpoint that treats a concept as an information object defined only through its structural relation to an agent's total experience. The core constraint is determination: a set of parts forms a reversible consistency relation if any missing part is recoverable from the others (up to the standard logarithmic slack in Kolmogorov-style identities). This reversibility prevents \"concepts\" from floating free of experience and turns concept existence into a checkable structural claim. To judge whether a decomposition is natural, we define excess information, measuring the redundancy overhead introduced by splitting experience into multiple separately described parts. On top of these definitions, we formulate dialectics as an optimization dynamics: as new patches of information appear (or become contested), competing concepts bid to explain them via shorter conditional descriptions, driving systematic expansion, contraction, splitting, and merging. Finally, we formalize low-cost concept transmission and multi-agent alignment using small grounds/seeds that allow another agent to reconstruct the same concept under a shared protocol, making communication a concrete compute-bits trade-off.","authors":["Zhengmian Hu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.01457v4","updated":"2025-12-23T08:18:03Z","published":"2025-12-01T09:44:31Z","title":"Zero-Overhead Introspection for Adaptive Test-Time Compute","summary":"Large language models excel at reasoning but lack key aspects of introspection, including anticipating their own success and the computation required to achieve it. Humans use real-time introspection to decide how much effort to invest, when to make multiple attempts, when to stop, and when to signal success or failure. Without this, LLMs struggle to make intelligent meta-cognition decisions. Test-time scaling methods like Best-of-N drive up cost and latency by using a fixed budget of samples regardless of the marginal benefit of each one at any point in generation, and the absence of confidence signals can mislead people, prevent appropriate escalation to better tools, and undermine trustworthiness. Learned verifiers or reward models can provide confidence estimates, but do not enable adaptive inference and add substantial cost by requiring extra models or forward passes. We present ZIP-RC, which equips models with zero-overhead introspective predictions of reward and cost. At every token, ZIP-RC reuses reserved or unused logits in the same forward pass as next-token prediction to output a joint distribution over final reward and remaining length -- no extra models, architecture change, or inference overhead. This full joint distribution is used to compute a sampling utility which is the linear combination of the expected maximum reward, total compute, and latency of set of samples if generated to completion. During inference, we maximize this utility with meta-actions that determine which prefix of tokens to continue or initiate sampling from. On mixed-difficulty mathematical benchmarks, ZIP-RC improves accuracy by up to 12% over majority voting at equal or lower average cost, and traces smooth Pareto frontiers between quality, compute, and latency. By providing real-time reward-cost introspection, ZIP-RC enables adaptive, efficient reasoning.","authors":["Rohin Manvi","Joey Hong","Tim Seyde","Maxime Labonne","Mathias Lechner","Sergey Levine"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20145v1","updated":"2025-12-23T08:15:34Z","published":"2025-12-23T08:15:34Z","title":"Retrieval-augmented Prompt Learning for Pre-trained Foundation Models","summary":"The pre-trained foundation models (PFMs) have become essential for facilitating large-scale multimodal learning. Researchers have effectively employed the ``pre-train, prompt, and predict'' paradigm through prompt learning to induce improved few-shot performance. However, prompt learning approaches for PFMs still follow a parametric learning paradigm. As such, the stability of generalization in memorization and rote learning can be compromised. More specifically, conventional prompt learning might face difficulties in fully utilizing atypical instances and avoiding overfitting to shallow patterns with limited data during the process of fully-supervised training. To overcome these constraints, we present our approach, named RetroPrompt, which aims to achieve a balance between memorization and generalization by decoupling knowledge from mere memorization. Unlike traditional prompting methods, RetroPrompt leverages a publicly accessible knowledge base generated from the training data and incorporates a retrieval mechanism throughout the input, training, and inference stages. This enables the model to actively retrieve relevant contextual information from the corpus, thereby enhancing the available cues. We conduct comprehensive experiments on a variety of datasets across natural language processing and computer vision tasks to demonstrate the superior performance of our proposed approach, RetroPrompt, in both zero-shot and few-shot scenarios. Through detailed analysis of memorization patterns, we observe that RetroPrompt effectively reduces the reliance on rote memorization, leading to enhanced generalization.","authors":["Xiang Chen","Yixin Ou","Quan Feng","Lei Li","Piji Li","Haibo Ye","Sheng-Jun Huang","Shuofei Qiao","Shumin Deng","Huajun Chen","Ningyu Zhang"],"pdf_url":"","comment":"IEEE/ACM Transactions on Audio, Speech and Language Processing"},{"id":"http://arxiv.org/abs/2509.10825v3","updated":"2025-12-23T08:14:20Z","published":"2025-09-13T14:44:45Z","title":"ORACLE: Explaining Feature Interactions in Neural Networks with ANOVA","summary":"We introduce ORACLE, a framework for explaining neural networks on tabular data and scientific factorial designs. ORACLE summarizes a trained network's prediction surface with main effects and pairwise interactions by treating the network as a black-box response, discretizing the inputs onto a grid, and fitting an orthogonal factorial (ANOVA-style) surrogate -- the $L^2$ orthogonal projection of the model response onto a finite-dimensional factorial subspace. A simple centering and $μ$-rebalancing step then expresses this surrogate as main- and interaction-effect tables that remain faithful to the original model in the $L^2$ sense. The resulting grid-based interaction maps are easy to visualize, comparable across backbones, and directly aligned with classical design-of-experiments practice. On synthetic factorial benchmarks and low- to medium-dimensional tabular regression tasks, ORACLE more accurately recovers ground-truth interaction structure and hotspots than Monte Carlo SHAP-family interaction methods, as measured by ranking, localization, and cross-backbone stability. In latent image and text settings, ORACLE clarifies its scope: grid-based factorial surrogates are most effective when features admit an interpretable factorial structure, making ORACLE particularly well-suited to scientific and engineering workflows that require stable, DoE-style interaction summaries.","authors":["Dongseok Kim","Hyoungsun Choi","Mohamed Jismy Aashik Rasool","Gisung Oh"],"pdf_url":"","comment":"v3: Minor wording edits for clarity; no technical changes"},{"id":"http://arxiv.org/abs/2512.18190v2","updated":"2025-12-23T08:10:47Z","published":"2025-12-20T03:27:11Z","title":"External Hippocampus: Topological Cognitive Maps for Guiding Large Language Model Reasoning","summary":"This paper proposes the External Hippocampus framework, which models language model reasoning from a cognitive dynamics perspective as the flow of information energy in semantic space. Unlike traditional weight-space optimization methods, this framework constructs topological cognitive maps through dimensionality reduction projection, enabling precise navigation and intervention of energy flow at test time while avoiding substantial computational requirements and demonstrating predictable intervention patterns. The method effectively addresses the cognitive deadlock problem in multi-step reasoning for small models. Experiments on models <=7B parameters show: map-guided methods achieve 81.20% accuracy on 500 challenging problems (relative baseline +16.80%), reduce reasoning time by >= 15x, with key findings revealing that reasoning stagnation manifests as \"Cognitive Vortex\" and low-entropy potential wells, while temperature perturbations effectively restart energy flow. The framework requires no additional training, possesses autonomous growth capability, and provides an efficient and controllable topological-aware solution for small model reasoning.","authors":["Jian Yan"],"pdf_url":"","comment":"12 pages, 7 figures"},{"id":"http://arxiv.org/abs/2512.20140v1","updated":"2025-12-23T08:02:33Z","published":"2025-12-23T08:02:33Z","title":"Enhancing Zero-Shot Time Series Forecasting in Off-the-Shelf LLMs via Noise Injection","summary":"Large Language Models (LLMs) have demonstrated effectiveness as zero-shot time series (TS) forecasters. The key challenge lies in tokenizing TS data into textual representations that align with LLMs' pre-trained knowledge. While existing work often relies on fine-tuning specialized modules to bridge this gap, a distinct, yet challenging, paradigm aims to leverage truly off-the-shelf LLMs without any fine-tuning whatsoever, relying solely on strategic tokenization of numerical sequences. The performance of these fully frozen models is acutely sensitive to the textual representation of the input data, as their parameters cannot adapt to distribution shifts. In this paper, we introduce a simple yet highly effective strategy to overcome this brittleness: injecting noise into the raw time series before tokenization. This non-invasive intervention acts as a form of inference-time augmentation, compelling the frozen LLM to extrapolate based on robust underlying temporal patterns rather than superficial numerical artifacts. We theoretically analyze this phenomenon and empirically validate its effectiveness across diverse benchmarks. Notably, to fully eliminate potential biases from data contamination during LLM pre-training, we introduce two novel TS datasets that fall outside all utilized LLMs' pre-training scopes, and consistently observe improved performance. This study provides a further step in directly leveraging off-the-shelf LLMs for time series forecasting.","authors":["Xingyou Yin","Ceyao Zhang","Min Hu","Kai Chen"],"pdf_url":"","comment":"9 pages,3 figures"},{"id":"http://arxiv.org/abs/2512.17814v2","updated":"2025-12-23T08:00:41Z","published":"2025-12-19T17:19:08Z","title":"LLM-based Behaviour Driven Development for Hardware Design","summary":"Test and verification are essential activities in hardware and system design, but their complexity grows significantly with increasing system sizes. While Behavior Driven Development (BDD) has proven effective in software engineering, it is not yet well established in hardware design, and its practical use remains limited. One contributing factor is the manual effort required to derive precise behavioral scenarios from textual specifications.\n  Recent advances in Large Language Models (LLMs) offer new opportunities to automate this step. In this paper, we investigate the use of LLM-based techniques to support BDD in the context of hardware design.","authors":["Rolf Drechsler","Qian Liu"],"pdf_url":"","comment":"7 pages, keynote given at 2nd International Symposium on Artificial Intelligence and Internet of Things (AIIoT-25), December 22-24th, 2025"},{"id":"http://arxiv.org/abs/2509.23129v2","updated":"2025-12-23T07:56:48Z","published":"2025-09-27T05:24:51Z","title":"C$^2$GSPG: Confidence-calibrated Group Sequence Policy Gradient towards Self-aware Reasoning","summary":"Reinforcement Learning (RL) methods, exemplified by Group Relative Policy Optimization (GRPO) and its variants, play a central role in developing reasoning models. However, these methods often suffer from a critical overconfidence issue, which prevents them from achieving self-aware reasoning models. In this study, we propose a simple yet effective confidence-calibration group sequence policy gradient method, called C$^2$GSPG, which simultaneously enhances reasoning performance while suppressing overconfidence. In principle, we propose a Group Sequence Policy Gradient (GSPG) framework for learning reasoning models, which eliminates the token-level bias commonly appearing in GRPO and its variants. In this framework, we define the model confidence for each reasoning problem using the normalized sequence-level probability, and then apply a cross-entropy regularizer to calibrate the model confidence to the sequence's reward. We demonstrate that the confidence calibration regularizer and GSPG are collaborative for binary rewards, as their objectives always share the same gradient direction. For non-binary rewards, we apply nonlinear reward normalization and adaptive regularizer clipping, mitigating the potential conflict between the two objectives. Applying C$^2$GSPG to post-train large language models in logical and mathematical reasoning tasks, we show its superiority over state-of-the-art methods in both reasoning accuracy and confidence calibration. The code of C$^2$GSPG is available at https://github.com/HaotianLiu123/CCGSPG.","authors":["Haotian Liu","Shuo Wang","Hongteng Xu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20136v1","updated":"2025-12-23T07:54:03Z","published":"2025-12-23T07:54:03Z","title":"M$^3$KG-RAG: Multi-hop Multimodal Knowledge Graph-enhanced Retrieval-Augmented Generation","summary":"Retrieval-Augmented Generation (RAG) has recently been extended to multimodal settings, connecting multimodal large language models (MLLMs) with vast corpora of external knowledge such as multimodal knowledge graphs (MMKGs). Despite their recent success, multimodal RAG in the audio-visual domain remains challenging due to 1) limited modality coverage and multi-hop connectivity of existing MMKGs, and 2) retrieval based solely on similarity in a shared multimodal embedding space, which fails to filter out off-topic or redundant knowledge. To address these limitations, we propose M$^3$KG-RAG, a Multi-hop Multimodal Knowledge Graph-enhanced RAG that retrieves query-aligned audio-visual knowledge from MMKGs, improving reasoning depth and answer faithfulness in MLLMs. Specifically, we devise a lightweight multi-agent pipeline to construct multi-hop MMKG (M$^3$KG), which contains context-enriched triplets of multimodal entities, enabling modality-wise retrieval based on input queries. Furthermore, we introduce GRASP (Grounded Retrieval And Selective Pruning), which ensures precise entity grounding to the query, evaluates answer-supporting relevance, and prunes redundant context to retain only knowledge essential for response generation. Extensive experiments across diverse multimodal benchmarks demonstrate that M$^3$KG-RAG significantly enhances MLLMs' multimodal reasoning and grounding over existing approaches.","authors":["Hyeongcheol Park","Jiyoung Seo","Jaewon Mun","Hogun Park","Wonmin Byeon","Sung June Kim","Hyeonsoo Im","JeungSub Lee","Sangpil Kim"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20135v1","updated":"2025-12-23T07:53:57Z","published":"2025-12-23T07:53:57Z","title":"MolAct: An Agentic RL Framework for Molecular Editing and Property Optimization","summary":"Molecular editing and optimization are multi-step problems that require iteratively improving properties while keeping molecules chemically valid and structurally similar. We frame both tasks as sequential, tool-guided decisions and introduce MolAct, an agentic reinforcement learning framework that employs a two-stage training paradigm: first building editing capability, then optimizing properties while reusing the learned editing behaviors. To the best of our knowledge, this is the first work to formalize molecular design as an Agentic Reinforcement Learning problem, where an LLM agent learns to interleave reasoning, tool-use, and molecular optimization. The framework enables agents to interact in multiple turns, invoking chemical tools for validity checking, property assessment, and similarity control, and leverages their feedback to refine subsequent edits. We instantiate the MolAct framework to train two model families: MolEditAgent for molecular editing tasks and MolOptAgent for molecular optimization tasks. In molecular editing, MolEditAgent-7B delivers 100, 95, and 98 valid add, delete, and substitute edits, outperforming strong closed \"thinking\" baselines such as DeepSeek-R1; MolEditAgent-3B approaches the performance of much larger open \"thinking\" models like Qwen3-32B-think. In molecular optimization, MolOptAgent-7B (trained on MolEditAgent-7B) surpasses the best closed \"thinking\" baseline (e.g., Claude 3.7) on LogP and remains competitive on solubility, while maintaining balanced performance across other objectives. These results highlight that treating molecular design as a multi-step, tool-augmented process is key to reliable and interpretable improvements.","authors":["Zhuo Yang","Yeyun chen","Jiaqing Xie","Ben Gao","Shuaike Shen","Wanhao Liu","Liujia Yang","Beilun Wang","Tianfan Fu","Yuqiang Li"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2510.06187v3","updated":"2025-12-23T07:41:07Z","published":"2025-10-07T17:46:33Z","title":"Automated Program Repair of Uncompilable Student Code","summary":"A significant portion of student programming submissions in CS1 learning environments are uncompilable, limiting their use in student modeling and downstream knowledge tracing. Traditional modeling pipelines often exclude these cases, discarding observations of student learning. This study investigates automated program repair as a strategy to recover uncompilable code while preserving students' structural intent for use in student modeling. Within this framework, we assess large language models (LLMs) as repair agents under high- and low-context prompting conditions. Repairs were evaluated for compilability, edit distance, and preservation of students' original structure and logic. While all models produced compilable repairs, they differed in how well they preserve students' control flow and code structure, affecting their pedagogical utility. By recovering uncompilable submissions, this work enables richer and more comprehensive analyses of learners' coding processes and development over time.","authors":["Griffin Pitts","Aum Pandya","Darsh Rank","Tirth Bhatt","Muntasir Hoq","Bita Akram"],"pdf_url":"","comment":"In Proceedings of the 57th ACM Technical Symposium on Computer Science Education V.2 (SIGCSE TS 2026)"},{"id":"http://arxiv.org/abs/2506.09160v5","updated":"2025-12-23T07:29:31Z","published":"2025-06-10T18:15:40Z","title":"Understanding Human-AI Trust in Education","summary":"As AI chatbots become integrated in education, students are turning to these systems for guidance, feedback, and information. However, the anthropomorphic characteristics of these chatbots create ambiguity over whether students develop trust in them in ways similar to trusting a human peer or instructor (human-like trust, often linked to interpersonal trust models) or in ways similar to trusting a conventional technology (system-like trust, often linked to technology trust models). This ambiguity presents theoretical challenges, as interpersonal trust models may inappropriately ascribe human intentionality and morality to AI, while technology trust models were developed for non-social systems, leaving their applicability to conversational, human-like agents unclear. To address this gap, we examine how these two forms of trust, human-like and system-like, comparatively influence students' perceptions of an AI chatbot, specifically perceived enjoyment, trusting intention, behavioral intention to use, and perceived usefulness. Using partial least squares structural equation modeling, we found that both forms of trust significantly influenced student perceptions, though with varied effects. Human-like trust was the stronger predictor of trusting intention, whereas system-like trust more strongly influenced behavioral intention and perceived usefulness; both had similar effects on perceived enjoyment. The results suggest that interactions with AI chatbots give rise to a distinct form of trust, human-AI trust, that differs from human-human and human-technology models, highlighting the need for new theoretical frameworks in this domain. In addition, the study offers practical insights for fostering appropriately calibrated trust, which is critical for the effective adoption and pedagogical impact of AI in education.","authors":["Griffin Pitts","Sanaz Motamedi"],"pdf_url":"","comment":"Final version, published to Telematics and Informatics Reports"},{"id":"http://arxiv.org/abs/2512.16465v2","updated":"2025-12-23T07:16:16Z","published":"2025-12-18T12:34:00Z","title":"cuPilot: A Strategy-Coordinated Multi-agent Framework for CUDA Kernel Evolution","summary":"Optimizing CUDA kernels is a challenging and labor-intensive task, given the need for hardware-software co-design expertise and the proprietary nature of high-performance kernel libraries. While recent large language models (LLMs) combined with evolutionary algorithms show promise in automatic kernel optimization, existing approaches often fall short in performance due to their suboptimal agent designs and mismatched evolution representations. This work identifies these mismatches and proposes cuPilot, a strategy-coordinated multi-agent framework that introduces strategy as an intermediate semantic representation for kernel evolution. Key contributions include a strategy-coordinated evolution algorithm, roofline-guided prompting, and strategy-level population initialization. Experimental results show that the generated kernels by cuPilot achieve an average speed up of 3.09$\\times$ over PyTorch on a benchmark of 100 kernels. On the GEMM tasks, cuPilot showcases sophisticated optimizations and achieves high utilization of critical hardware units. The generated kernels are open-sourced at https://github.com/champloo2878/cuPilot-Kernels.git.","authors":["Jinwu Chen","Qidie Wu","Bin Li","Lin Ma","Xin Si","Yang Hu","Shouyi Yin","Jun Yang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20112v1","updated":"2025-12-23T07:15:38Z","published":"2025-12-23T07:15:38Z","title":"Evolutionary Neural Architecture Search with Dual Contrastive Learning","summary":"Evolutionary Neural Architecture Search (ENAS) has gained attention for automatically designing neural network architectures. Recent studies use a neural predictor to guide the process, but the high computational costs of gathering training data -- since each label requires fully training an architecture -- make achieving a high-precision predictor with { limited compute budget (i.e., a capped number of fully trained architecture-label pairs)} crucial for ENAS success. This paper introduces ENAS with Dual Contrastive Learning (DCL-ENAS), a novel method that employs two stages of contrastive learning to train the neural predictor. In the first stage, contrastive self-supervised learning is used to learn meaningful representations from neural architectures without requiring labels. In the second stage, fine-tuning with contrastive learning is performed to accurately predict the relative performance of different architectures rather than their absolute performance, which is sufficient to guide the evolutionary search. Across NASBench-101 and NASBench-201, DCL-ENAS achieves the highest validation accuracy, surpassing the strongest published baselines by 0.05\\% (ImageNet16-120) to 0.39\\% (NASBench-101). On a real-world ECG arrhythmia classification task, DCL-ENAS improves performance by approximately 2.5 percentage points over a manually designed, non-NAS model obtained via random search, while requiring only 7.7 GPU-days.","authors":["Xian-Rong Zhang","Yue-Jiao Gong","Wei-Neng Chen","Jun Zhang"],"pdf_url":"","comment":"26 pages"},{"id":"http://arxiv.org/abs/2512.20111v1","updated":"2025-12-23T07:11:26Z","published":"2025-12-23T07:11:26Z","title":"ABBEL: LLM Agents Acting through Belief Bottlenecks Expressed in Language","summary":"As the length of sequential decision-making tasks increases, it becomes computationally impractical to keep full interaction histories in context. We introduce a general framework for LLM agents to maintain concise contexts through multi-step interaction: Acting through Belief Bottlenecks Expressed in Language (ABBEL), and methods to further improve ABBEL agents with RL post-training. ABBEL replaces long multi-step interaction history by a belief state, i.e., a natural language summary of what has been discovered about task-relevant unknowns. Under ABBEL, at each step the agent first updates a prior belief with the most recent observation from the environment to form a posterior belief, then uses only the posterior to select an action. We systematically evaluate frontier models under ABBEL across six diverse multi-step environments, finding that ABBEL supports generating interpretable beliefs while maintaining near-constant memory use over interaction steps. However, bottleneck approaches are generally prone to error propagation, which we observe causing inferior performance when compared to the full context setting due to errors in belief updating. Therefore, we train LLMs to generate and act on beliefs within the ABBEL framework via reinforcement learning (RL). We experiment with belief grading, to reward higher quality beliefs, as well as belief length penalties to reward more compressed beliefs. Our experiments demonstrate the ability of RL to improve ABBEL's performance beyond the full context setting, while using less memory than contemporaneous approaches.","authors":["Aly Lidayan","Jakob Bjorner","Satvik Golechha","Kartik Goyal","Alane Suhr"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2505.17266v3","updated":"2025-12-23T07:05:40Z","published":"2025-05-22T20:24:08Z","title":"Select2Reason: Efficient Instruction-Tuning Data Selection for Long-CoT Reasoning","summary":"A practical approach to activate long chain-of-thoughts reasoning ability in pre-trained large language models is to perform supervised fine-tuning on instruction datasets synthesized by strong Large Reasoning Models such as DeepSeek-R1, offering a cost-effective alternative to reinforcement learning. However, large-scale instruction sets with more than 100k samples incur significant training overhead, while effective strategies for automatic long-CoT instruction selection still remain unexplored. In this work, we propose Select2Reason, a novel and efficient instruction-tuning data selection framework for long-CoT reasoning. From the perspective of emergence of rethinking behaviors like self-correction and backtracking, we investigate common metrics that may determine the quality of long-CoT reasoning instructions. Select2Reason leverages a quantifier to estimate difficulty of question and jointly incorporates a reasoning trace length-based heuristic through a weighted scheme for ranking to prioritize high-utility examples. Empirical results on OpenR1-Math-220k demonstrate that fine-tuning LLM on only 10% of the data selected by Select2Reason achieves performance competitive with or superior to full-data tuning and open-source baseline OpenR1-Qwen-7B across three competition-level and six comprehensive mathematical benchmarks. Further experiments highlight the scalability in varying data size, efficiency during inference, and its adaptability to other instruction pools with minimal cost.","authors":["Cehao Yang","Xueyuan Lin","Xiaojun Wu","Chengjin Xu","Xuhui Jiang","Honghao Liu","Hui Xiong","Jian Guo"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2401.15894v3","updated":"2025-12-23T06:57:26Z","published":"2024-01-29T05:26:17Z","title":"Enhancing Topological Dependencies in Spatio-Temporal Graphs with Cycle Message Passing Blocks","summary":"Graph Neural Networks (GNNs) and Transformer-based models have been increasingly adopted to learn the complex vector representations of spatio-temporal graphs, capturing intricate spatio-temporal dependencies crucial for applications such as traffic datasets. Although many existing methods utilize multi-head attention mechanisms and message-passing neural networks (MPNNs) to capture both spatial and temporal relations, these approaches encode temporal and spatial relations independently, and reflect the graph's topological characteristics in a limited manner. In this work, we introduce the Cycle to Mixer (Cy2Mixer), a novel spatio-temporal GNN based on topological non-trivial invariants of spatio-temporal graphs with gated multi-layer perceptrons (gMLP). The Cy2Mixer is composed of three blocks based on MLPs: A temporal block for capturing temporal properties, a message-passing block for encapsulating spatial information, and a cycle message-passing block for enriching topological information through cyclic subgraphs. We bolster the effectiveness of Cy2Mixer with mathematical evidence emphasizing that our cycle message-passing block is capable of offering differentiated information to the deep learning model compared to the message-passing block. Furthermore, empirical evaluations substantiate the efficacy of the Cy2Mixer, demonstrating state-of-the-art performances across various spatio-temporal benchmark datasets. The source code is available at https://github.com/leemingo/cy2mixer.","authors":["Minho Lee","Yun Young Choi","Sun Woo Park","Seunghwan Lee","Joohwan Ko","Jaeyoung Hong"],"pdf_url":"","comment":"Proceedings of the Third Learning on Graphs Conference (LoG 2024)"},{"id":"http://arxiv.org/abs/2512.17266v2","updated":"2025-12-23T06:52:01Z","published":"2025-12-19T06:30:11Z","title":"ScoutGPT: Capturing Player Impact from Team Action Sequences Using GPT-Based Framework","summary":"Transfers play a pivotal role in shaping a football club's success, yet forecasting whether a transfer will succeed remains difficult due to the strong context-dependence of on-field performance. Existing evaluation practices often rely on static summary statistics or post-hoc value models, which fail to capture how a player's contribution adapts to a new tactical environment or different teammates. To address this gap, we introduce EventGPT, a player-conditioned, value-aware next-event prediction model built on a GPT-style autoregressive transformer. Our model treats match play as a sequence of discrete tokens, jointly learning to predict the next on-ball action's type, location, timing, and its estimated residual On-Ball Value (rOBV) based on the preceding context and player identity. A key contribution of this framework is the ability to perform counterfactual simulations. By substituting learned player embeddings into new event sequences, we can simulate how a player's behavioral distribution and value profile would change when placed in a different team or tactical structure. Evaluated on five seasons of Premier League event data, EventGPT outperforms existing sequence-based baselines in next-event prediction accuracy and spatial precision. Furthermore, we demonstrate the model's practical utility for transfer analysis through case studies-such as comparing striker performance across different systems and identifying stylistic replacements for specific roles-showing that our approach provides a principled method for evaluating transfer fit.","authors":["Miru Hong","Minho Lee","Geonhee Jo","Jae-Hee So","Pascal Bauer","Sang-Ki Ko"],"pdf_url":"","comment":"8 pages, 2 figures, 7 tables. To appear in Hudl Performance Insights 2025"},{"id":"http://arxiv.org/abs/2512.18687v2","updated":"2025-12-23T06:51:30Z","published":"2025-12-21T10:48:40Z","title":"Social Comparison without Explicit Inference of Others' Reward Values: A Constructive Approach Using a Probabilistic Generative Model","summary":"Social comparison$\\unicode{x2014}$the process of evaluating one's rewards relative to others$\\unicode{x2014}$plays a fundamental role in primate social cognition. However, it remains unknown from a computational perspective how information about others' rewards affects the evaluation of one's own reward. With a constructive approach, this study examines whether monkeys merely recognize objective reward differences or, instead, infer others' subjective reward valuations. We developed three computational models with varying degrees of social information processing: an Internal Prediction Model (IPM), which infers the partner's subjective values; a No Comparison Model (NCM), which disregards partner information; and an External Comparison Model (ECM), which directly incorporates the partner's objective rewards. To test model performance, we used a multi-layered, multimodal latent Dirichlet allocation. We trained the models on a dataset containing the behavior of a pair of monkeys, their rewards, and the conditioned stimuli. Then, we evaluated the models' ability to classify subjective values across pre-defined experimental conditions. The ECM achieved the highest classification score in the Rand Index (0.88 vs. 0.79 for the IPM) under our settings, suggesting that social comparison relies on objective reward differences rather than inferences about subjective states.","authors":["Yosuke Taniuchi","Chie Hieida","Atsushi Noritake","Kazushi Ikeda","Masaki Isoda"],"pdf_url":"","comment":"Submitted to Advanced Robotics"},{"id":"http://arxiv.org/abs/2511.09586v3","updated":"2025-12-23T06:43:03Z","published":"2025-11-12T12:56:25Z","title":"Environment Scaling for Interactive Agentic Experience Collection: A Survey","summary":"LLM-based agents can autonomously accomplish complex tasks across various domains. However, to further cultivate capabilities such as adaptive behavior and long-term decision-making, training on static datasets built from human-level knowledge is insufficient. These datasets are costly to construct and lack both dynamism and realism. A growing consensus is that agents should instead interact directly with environments and learn from experience through reinforcement learning. We formalize this iterative process as the Generation-Execution-Feedback (GEF) loop, where environments generate tasks to challenge agents, return observations in response to agents' actions during task execution, and provide evaluative feedback on rollouts for subsequent learning. Under this paradigm, environments function as indispensable producers of experiential data, highlighting the need to scale them toward greater complexity, realism, and interactivity. In this survey, we systematically review representative methods for environment scaling from a pioneering environment-centric perspective and organize them along the stages of the GEF loop, namely task generation, task execution, and feedback. We further analyze implementation frameworks, challenges, and applications, consolidating fragmented advances and outlining future research directions for agent intelligence.","authors":["Yuchen Huang","Sijia Li","Minghao Liu","Wei Liu","Shijue Huang","Zhiyuan Fan","Hou Pong Chan","Yi R. Fung"],"pdf_url":"","comment":"22 pages, 5 figures, SEA Workshop @ NeurIPS 2025"},{"id":"http://arxiv.org/abs/2512.20088v1","updated":"2025-12-23T06:30:33Z","published":"2025-12-23T06:30:33Z","title":"Item Region-based Style Classification Network (IRSN): A Fashion Style Classifier Based on Domain Knowledge of Fashion Experts","summary":"Fashion style classification is a challenging task because of the large visual variation within the same style and the existence of visually similar styles.\n  Styles are expressed not only by the global appearance, but also by the attributes of individual items and their combinations.\n  In this study, we propose an item region-based fashion style classification network (IRSN) to effectively classify fashion styles by analyzing item-specific features and their combinations in addition to global features.\n  IRSN extracts features of each item region using item region pooling (IRP), analyzes them separately, and combines them using gated feature fusion (GFF).\n  In addition, we improve the feature extractor by applying a dual-backbone architecture that combines a domain-specific feature extractor and a general feature extractor pre-trained with a large-scale image-text dataset.\n  In experiments, applying IRSN to six widely-used backbones, including EfficientNet, ConvNeXt, and Swin Transformer, improved style classification accuracy by an average of 6.9% and a maximum of 14.5% on the FashionStyle14 dataset and by an average of 7.6% and a maximum of 15.1% on the ShowniqV3 dataset. Visualization analysis also supports that the IRSN models are better than the baseline models at capturing differences between similar style classes.","authors":["Jinyoung Choi","Youngchae Kwon","Injung Kim"],"pdf_url":"","comment":"This is a pre-print of an article published in Applied Intelligence. The final authenticated version is available online at: https://doi.org/10.1007/s10489-024-05683-9"},{"id":"http://arxiv.org/abs/2512.20086v1","updated":"2025-12-23T06:28:12Z","published":"2025-12-23T06:28:12Z","title":"Spatio-Temporal Graphs Beyond Grids: Benchmark for Maritime Anomaly Detection","summary":"Spatio-temporal graph neural networks (ST-GNNs) have achieved notable success in structured domains such as road traffic and public transportation, where spatial entities can be naturally represented as fixed nodes. In contrast, many real-world systems including maritime traffic lack such fixed anchors, making the construction of spatio-temporal graphs a fundamental challenge. Anomaly detection in these non-grid environments is particularly difficult due to the absence of canonical reference points, the sparsity and irregularity of trajectories, and the fact that anomalies may manifest at multiple granularities. In this work, we introduce a novel benchmark dataset for anomaly detection in the maritime domain, extending the Open Maritime Traffic Analysis Dataset (OMTAD) into a benchmark tailored for graph-based anomaly detection. Our dataset enables systematic evaluation across three different granularities: node-level, edge-level, and graph-level anomalies. We plan to employ two specialized LLM-based agents: \\emph{Trajectory Synthesizer} and \\emph{Anomaly Injector} to construct richer interaction contexts and generate semantically meaningful anomalies. We expect this benchmark to promote reproducibility and to foster methodological advances in anomaly detection for non-grid spatio-temporal systems.","authors":["Jeehong Kim","Youngseok Hwang","Minchan Kim","Sungho Bae","Hyunwoo Park"],"pdf_url":"","comment":"Accepted at NeurIPS 2025 Workshop in AI for Science: The Reach and Limits of AI for Scientific Discovery"},{"id":"http://arxiv.org/abs/2512.20084v1","updated":"2025-12-23T06:27:30Z","published":"2025-12-23T06:27:30Z","title":"QE-Catalytic: A Graph-Language Multimodal Base Model for Relaxed-Energy Prediction in Catalytic Adsorption","summary":"Adsorption energy is a key descriptor of catalytic reactivity. It is fundamentally defined as the difference between the relaxed total energy of the adsorbate-surface system and that of an appropriate reference state; therefore, the accuracy of relaxed-energy prediction directly determines the reliability of machine-learning-driven catalyst screening. E(3)-equivariant graph neural networks (GNNs) can natively operate on three-dimensional atomic coordinates under periodic boundary conditions and have demonstrated strong performance on such tasks. In contrast, language-model-based approaches, while enabling human-readable textual descriptions and reducing reliance on explicit graph -- thereby broadening applicability -- remain insufficient in both adsorption-configuration energy prediction accuracy and in distinguishing ``the same system with different configurations,'' even with graph-assisted pretraining in the style of GAP-CATBERTa.\n  To this end, we propose QE-Catalytic, a multimodal framework that deeply couples a large language model (\\textbf{Q}wen) with an E(3)-equivariant graph Transformer (\\textbf{E}quiformer-V2), enabling unified support for adsorption-configuration property prediction and inverse design on complex catalytic surfaces. During prediction, QE-Catalytic jointly leverages three-dimensional structures and structured configuration text, and injects ``3D geometric information'' into the language channel via graph-text alignment, allowing it to function as a high-performance text-based predictor when precise coordinates are unavailable, while also autoregressively generating CIF files for target-energy-driven structure design and information completion. On OC20, QE-Catalytic reduces the MAE of relaxed adsorption energy from 0.713~eV to 0.486~eV, and consistently outperforms baseline models such as CatBERTa and GAP-CATBERTa across multiple evaluation protocols.","authors":["Yanjie Li","Jian Xu","Xueqing Chen","Lina Yu","Shiming Xiang","Weijun Li","Cheng-lin Liu"],"pdf_url":"","comment":"25 pages"},{"id":"http://arxiv.org/abs/2512.20082v1","updated":"2025-12-23T06:27:12Z","published":"2025-12-23T06:27:12Z","title":"Adaptive Financial Sentiment Analysis for NIFTY 50 via Instruction-Tuned LLMs , RAG and Reinforcement Learning Approaches","summary":"Financial sentiment analysis plays a crucial role in informing investment decisions, assessing market risk, and predicting stock price trends. Existing works in financial sentiment analysis have not considered the impact of stock prices or market feedback on sentiment analysis. In this paper, we propose an adaptive framework that integrates large language models (LLMs) with real-world stock market feedback to improve sentiment classification in the context of the Indian stock market. The proposed methodology fine-tunes the LLaMA 3.2 3B model using instruction-based learning on the SentiFin dataset. To enhance sentiment predictions, a retrieval-augmented generation (RAG) pipeline is employed that dynamically selects multi-source contextual information based on the cosine similarity of the sentence embeddings. Furthermore, a feedback-driven module is introduced that adjusts the reliability of the source by comparing predicted sentiment with actual next-day stock returns, allowing the system to iteratively adapt to market behavior. To generalize this adaptive mechanism across temporal data, a reinforcement learning agent trained using proximal policy optimization (PPO) is incorporated. The PPO agent learns to optimize source weighting policies based on cumulative reward signals from sentiment-return alignment. Experimental results on NIFTY 50 news headlines collected from 2024 to 2025 demonstrate that the proposed system significantly improves classification accuracy, F1-score, and market alignment over baseline models and static retrieval methods. The results validate the potential of combining instruction-tuned LLMs with dynamic feedback and reinforcement learning for robust, market-aware financial sentiment modeling.","authors":[" Chaithra","Kamesh Kadimisetty","Biju R Mohan"],"pdf_url":"","comment":"Accepted in CODS 2025"},{"id":"http://arxiv.org/abs/2512.17180v2","updated":"2025-12-23T06:26:45Z","published":"2025-12-19T02:38:04Z","title":"Conservative Bias in Multi-Teacher Learning: Why Agents Prefer Low-Reward Advisors","summary":"Interactive reinforcement learning (IRL) has shown promise in enabling autonomous agents and robots to learn complex behaviours from human teachers, yet the dynamics of teacher selection remain poorly understood. This paper reveals an unexpected phenomenon in IRL: when given a choice between teachers with different reward structures, learning agents overwhelmingly prefer conservative, low-reward teachers (93.16% selection rate) over those offering 20x higher rewards. Through 1,250 experimental runs in navigation tasks with multiple expert teachers, we discovered: (1) Conservative bias dominates teacher selection: agents systematically choose the lowest-reward teacher, prioritising consistency over optimality; (2) Critical performance thresholds exist at teacher availability rho >= 0.6 and accuracy omega >= 0.6, below which the framework fails catastrophically; (3) The framework achieves 159% improvement over baseline Q-learning under concept drift. These findings challenge fundamental assumptions about optimal teaching in RL and suggest potential implications for human-robot collaboration, where human preferences for safety and consistency may align with the observed agent selection behaviour, potentially informing training paradigms for safety-critical robotic applications.","authors":["Maher Mesto","Francisco Cruz"],"pdf_url":"","comment":"10 pages, 5 figures. Accepted at ACRA 2025 (Australasian Conference on Robotics and Automation)"},{"id":"http://arxiv.org/abs/2512.20080v1","updated":"2025-12-23T06:26:20Z","published":"2025-12-23T06:26:20Z","title":"CBA: Communication-Bound-Aware Cross-Domain Resource Assignment for Pipeline-Parallel Distributed LLM Training in Dynamic Multi-DC Optical Networks","summary":"We propose a communication-bound-aware cross-domain resource assignment framework for pipeline-parallel distributed training over multi-datacenter optical networks, which lowers iteration time by 31.25% and reduces 13.20% blocking requests compared to baselines.","authors":["Dianxuan Fu","Xiaomin Liu","Yihao Zhang","Shikui Shen","Weisheng Hu","Qunbi Zhuge"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2402.04536v3","updated":"2025-12-23T06:22:42Z","published":"2024-02-07T02:50:56Z","title":"Tactile-based Object Retrieval From Granular Media","summary":"We introduce GEOTACT, the first robotic system capable of grasping and retrieving objects of potentially unknown shapes buried in a granular environment. While important in many applications, ranging from mining and exploration to search and rescue, this type of interaction with granular media is difficult due to the uncertainty stemming from visual occlusion and noisy contact signals. To address these challenges, we use a learning method relying exclusively on touch feedback, trained end-to-end with simulated sensor noise. We show that our problem formulation leads to the natural emergence of learned pushing behaviors that the manipulator uses to reduce uncertainty and funnel the object to a stable grasp despite spurious and noisy tactile readings. We introduce a training curriculum that bootstraps learning in simulated granular environments, enabling zero-shot transfer to real hardware. Despite being trained only on seven objects with primitive shapes, our method is shown to successfully retrieve 35 different objects, including rigid, deformable, and articulated objects with complex shapes. Videos and additional information can be found at https://jxu.ai/geotact.","authors":["Jingxi Xu","Yinsen Jia","Dongxiao Yang","Patrick Meng","Xinyue Zhu","Zihan Guo","Shuran Song","Matei Ciocarlie"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2505.08528v2","updated":"2025-12-23T06:18:57Z","published":"2025-05-13T13:01:38Z","title":"GradMix: Gradient-based Selective Mixup for Robust Data Augmentation in Class-Incremental Learning","summary":"In the context of continual learning, acquiring new knowledge while maintaining previous knowledge presents a significant challenge. Existing methods often use experience replay techniques that store a small portion of previous task data for training. In experience replay approaches, data augmentation has emerged as a promising strategy to further improve the model performance by mixing limited previous task data with sufficient current task data. However, we theoretically and empirically analyze that training with mixed samples from random sample pairs may harm the knowledge of previous tasks and cause greater catastrophic forgetting. We then propose GradMix, a robust data augmentation method specifically designed for mitigating catastrophic forgetting in class-incremental learning. GradMix performs gradient-based selective mixup using a class-based criterion that mixes only samples from helpful class pairs and not from detrimental class pairs for reducing catastrophic forgetting. Our experiments on various real datasets show that GradMix outperforms data augmentation baselines in accuracy by minimizing the forgetting of previous knowledge.","authors":["Minsu Kim","Seong-Hyeon Hwang","Steven Euijong Whang"],"pdf_url":"","comment":"Accepted to KDD 2026"},{"id":"http://arxiv.org/abs/2512.20074v1","updated":"2025-12-23T05:58:47Z","published":"2025-12-23T05:58:47Z","title":"Reason2Decide: Rationale-Driven Multi-Task Learning","summary":"Despite the wide adoption of Large Language Models (LLM)s, clinical decision support systems face a critical challenge: achieving high predictive accuracy while generating explanations aligned with the predictions. Current approaches suffer from exposure bias leading to misaligned explanations. We propose Reason2Decide, a two-stage training framework that addresses key challenges in self-rationalization, including exposure bias and task separation. In Stage-1, our model is trained on rationale generation, while in Stage-2, we jointly train on label prediction and rationale generation, applying scheduled sampling to gradually transition from conditioning on gold labels to model predictions. We evaluate Reason2Decide on three medical datasets, including a proprietary triage dataset and public biomedical QA datasets. Across model sizes, Reason2Decide outperforms other fine-tuning baselines and some zero-shot LLMs in prediction (F1) and rationale fidelity (BERTScore, BLEU, LLM-as-a-Judge). In triage, Reason2Decide is rationale source-robust across LLM-generated, nurse-authored, and nurse-post-processed rationales. In our experiments, while using only LLM-generated rationales in Stage-1, Reason2Decide outperforms other fine-tuning variants. This indicates that LLM-generated rationales are suitable for pretraining models, reducing reliance on human annotations. Remarkably, Reason2Decide achieves these gains with models 40x smaller than contemporary foundation models, making clinical reasoning more accessible for resource-constrained deployments while still providing explainable decision support.","authors":["H M Quamran Hasan","Housam Khalifa Bashier","Jiayi Dai","Mi-Young Kim","Randy Goebel"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2408.02152v3","updated":"2025-12-23T05:50:45Z","published":"2024-08-04T22:00:34Z","title":"Generative Retrieval with Few-shot Indexing","summary":"Existing generative retrieval (GR) methods rely on training-based indexing, which fine-tunes a model to memorise associations between queries and the document identifiers (docids) of relevant documents. Training-based indexing suffers from high training costs, under-utilisation of pre-trained knowledge in large language models (LLMs), and limited adaptability to dynamic document corpora. To address the issues, we propose a few-shot indexing-based GR framework (Few-Shot GR). It has a few-shot indexing process without any training, where we prompt an LLM to generate docids for all documents in a corpus, ultimately creating a docid bank for the entire corpus. During retrieval, we feed a query to the same LLM and constrain it to generate a docid within the docid bank created during indexing, and then map the generated docid back to its corresponding document. Moreover, we devise few-shot indexing with one-to-many mapping to further enhance Few-Shot GR. Experiments show that Few-Shot GR achieves superior performance to state-of-the-art GR methods requiring heavy training.","authors":["Arian Askari","Chuan Meng","Mohammad Aliannejadi","Zhaochun Ren","Evangelos Kanoulas","Suzan Verberne"],"pdf_url":"","comment":"Accepted for publication at the 48th European Conference on Information Retrieval (ECIR 2026)"},{"id":"http://arxiv.org/abs/2512.20062v1","updated":"2025-12-23T05:30:53Z","published":"2025-12-23T05:30:53Z","title":"On the Effectiveness of Instruction-Tuning Local LLMs for Identifying Software Vulnerabilities","summary":"Large Language Models (LLMs) show significant promise in automating software vulnerability analysis, a critical task given the impact of security failure of modern software systems. However, current approaches in using LLMs to automate vulnerability analysis mostly rely on using online API-based LLM services, requiring the user to disclose the source code in development. Moreover, they predominantly frame the task as a binary classification(vulnerable or not vulnerable), limiting potential practical utility. This paper addresses these limitations by reformulating the problem as Software Vulnerability Identification (SVI), where LLMs are asked to output the type of weakness in Common Weakness Enumeration (CWE) IDs rather than simply indicating the presence or absence of a vulnerability. We also tackle the reliance on large, API-based LLMs by demonstrating that instruction-tuning smaller, locally deployable LLMs can achieve superior identification performance. In our analysis, instruct-tuning a local LLM showed better overall performance and cost trade-off than online API-based LLMs. Our findings indicate that instruct-tuned local models represent a more effective, secure, and practical approach for leveraging LLMs in real-world vulnerability management workflows.","authors":["Sangryu Park","Gihyuk Ko","Homook Cho"],"pdf_url":"","comment":"The 9th International Conference on Mobile Internet Security (MobiSec 2025)"},{"id":"http://arxiv.org/abs/2512.20061v1","updated":"2025-12-23T05:27:16Z","published":"2025-12-23T05:27:16Z","title":"Scaling Reinforcement Learning for Content Moderation with Large Language Models","summary":"Content moderation at scale remains one of the most pressing challenges in today's digital ecosystem, where billions of user- and AI-generated artifacts must be continuously evaluated for policy violations. Although recent advances in large language models (LLMs) have demonstrated strong potential for policy-grounded moderation, the practical challenges of training these systems to achieve expert-level accuracy in real-world settings remain largely unexplored, particularly in regimes characterized by label sparsity, evolving policy definitions, and the need for nuanced reasoning beyond shallow pattern matching. In this work, we present a comprehensive empirical investigation of scaling reinforcement learning (RL) for content classification, systematically evaluating multiple RL training recipes and reward-shaping strategies-including verifiable rewards and LLM-as-judge frameworks-to transform general-purpose language models into specialized, policy-aligned classifiers across three real-world content moderation tasks. Our findings provide actionable insights for industrial-scale moderation systems, demonstrating that RL exhibits sigmoid-like scaling behavior in which performance improves smoothly with increased training data, rollouts, and optimization steps before gradually saturating. Moreover, we show that RL substantially improves performance on tasks requiring complex policy-grounded reasoning while achieving up to 100x higher data efficiency than supervised fine-tuning, making it particularly effective in domains where expert annotations are scarce or costly.","authors":["Hamed Firooz","Rui Liu","Yuchen Lu","Zhenyu Hou","Fangzhou Xiong","Xiaoyang Zhang","Changshu Jian","Zhicheng Zhu","Jiayuan Ma","Jacob Tao","Chaitali Gupta","Xiaochang Peng","Shike Mei","Hang Cui","Yang Qin","Shuo Tang","Jason Gaedtke","Arpit Mittal"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2508.02115v2","updated":"2025-12-23T05:17:43Z","published":"2025-08-04T06:51:33Z","title":"Collision-based Watermark for Detecting Backdoor Manipulation in Federated Learning","summary":"As AI-generated content increasingly underpins real-world applications, its accompanying security risks, including privacy leakage and copyright infringement, have become growing concerns. In this context, Federated Learning (FL) offers a promising foundation for enhancing trustworthiness by enabling privacy-preserving collaborative learning over proprietary data. However, its practical adoption is critically threatened by backdoor-based model manipulation, where a small number of malicious clients can compromise the system and induce harmful content generation and decision-making. Although various detection methods have been proposed to detect such manipulation, we reveal that they are either disrupted by non-i.i.d. data distributions and random client participation, or misled by out-of-distribution (OOD) prediction bias, both of which are unique challenges in FL scenarios. To address these issues, we introduce a novel proactive detection method dubbed Coward, inspired by our discovery of multi-backdoor collision effects, in which consecutively planted, distinct backdoors significantly suppress earlier ones. Correspondingly, we modify the federated global model by injecting a carefully designed backdoor-collided watermark, implemented via regulated dual-mapping learning on OOD data. This design not only enables an inverted detection paradigm compared to existing proactive methods, thereby naturally counteracting the adverse impact of OOD prediction bias, but also introduces a low-disruptive training intervention that inherently limits the strength of OOD bias, leading to significantly fewer misjudgments. Extensive experiments on benchmark datasets show that Coward achieves state-of-the-art detection performance, effectively alleviates OOD prediction bias, and remains robust against potential adaptive manipulations.","authors":["Wenjie Li","Siying Gu","Yiming Li","Kangjie Chen","Zhili Chen","Tianwei Zhang","Shu-Tao Xia","Dacheng Tao"],"pdf_url":"","comment":"13-page main body and 4-page appendix. Currently under review"},{"id":"http://arxiv.org/abs/2512.20056v1","updated":"2025-12-23T05:14:01Z","published":"2025-12-23T05:14:01Z","title":"Towards Generative Location Awareness for Disaster Response: A Probabilistic Cross-view Geolocalization Approach","summary":"As Earth's climate changes, it is impacting disasters and extreme weather events across the planet. Record-breaking heat waves, drenching rainfalls, extreme wildfires, and widespread flooding during hurricanes are all becoming more frequent and more intense. Rapid and efficient response to disaster events is essential for climate resilience and sustainability. A key challenge in disaster response is to accurately and quickly identify disaster locations to support decision-making and resources allocation. In this paper, we propose a Probabilistic Cross-view Geolocalization approach, called ProbGLC, exploring new pathways towards generative location awareness for rapid disaster response. Herein, we combine probabilistic and deterministic geolocalization models into a unified framework to simultaneously enhance model explainability (via uncertainty quantification) and achieve state-of-the-art geolocalization performance. Designed for rapid diaster response, the ProbGLC is able to address cross-view geolocalization across multiple disaster events as well as to offer unique features of probabilistic distribution and localizability score. To evaluate the ProbGLC, we conduct extensive experiments on two cross-view disaster datasets (i.e., MultiIAN and SAGAINDisaster), consisting diverse cross-view imagery pairs of multiple disaster types (e.g., hurricanes, wildfires, floods, to tornadoes). Preliminary results confirms the superior geolocalization accuracy (i.e., 0.86 in Acc@1km and 0.97 in Acc@25km) and model explainability (i.e., via probabilistic distributions and localizability scores) of the proposed ProbGLC approach, highlighting the great potential of leveraging generative cross-view approach to facilitate location awareness for better and faster disaster response. The data and code is publicly available at https://github.com/bobleegogogo/ProbGLC","authors":["Hao Li","Fabian Deuser","Wenping Yin","Steffen Knoblauch","Wufan Zhao","Filip Biljecki","Yong Xue","Wei Huang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2508.04826v3","updated":"2025-12-23T05:07:19Z","published":"2025-08-06T19:11:33Z","title":"Persistent Instability in LLM's Personality Measurements: Effects of Scale, Reasoning, and Conversation History","summary":"Large language models require consistent behavioral patterns for safe deployment, yet there are indications of large variability that may lead to an instable expression of personality traits in these models. We present PERSIST (PERsonality Stability in Synthetic Text), a comprehensive evaluation framework testing 25 open-source models (1B-685B parameters) across 2 million+ responses. Using traditional (BFI, SD3) and novel LLM-adapted personality questionnaires, we systematically vary model size, personas, reasoning modes, question order or paraphrasing, and conversation history. Our findings challenge fundamental assumptions: (1) Question reordering alone can introduce large shifts in personality measurements; (2) Scaling provides limited stability gains: even 400B+ models exhibit standard deviations >0.3 on 5-point scales; (3) Interventions expected to stabilize behavior, such as reasoning and inclusion of conversation history, can paradoxically increase variability; (4) Detailed persona instructions produce mixed effects, with misaligned personas showing significantly higher variability than the helpful assistant baseline; (5) The LLM-adapted questionnaires, despite their improved ecological validity, exhibit instability comparable to human-centric versions. This persistent instability across scales and mitigation strategies suggests that current LLMs lack the architectural foundations for genuine behavioral consistency. For safety-critical applications requiring predictable behavior, these findings indicate that current alignment strategies may be inadequate.","authors":["Tommaso Tosato","Saskia Helbling","Yorguin-Jose Mantilla-Ramos","Mahmood Hegazy","Alberto Tosato","David John Lemay","Irina Rish","Guillaume Dumas"],"pdf_url":"","comment":"Accepted at AAAI 2026, Track on AI Alignment"},{"id":"http://arxiv.org/abs/2512.20053v1","updated":"2025-12-23T05:03:54Z","published":"2025-12-23T05:03:54Z","title":"An Optimal Policy for Learning Controllable Dynamics by Exploration","summary":"Controllable Markov chains describe the dynamics of sequential decision making tasks and are the central component in optimal control and reinforcement learning. In this work, we give the general form of an optimal policy for learning controllable dynamics in an unknown environment by exploring over a limited time horizon. This policy is simple to implement and efficient to compute, and allows an agent to ``learn by exploring\" as it maximizes its information gain in a greedy fashion by selecting controls from a constraint set that changes over time during exploration. We give a simple parameterization for the set of controls, and present an algorithm for finding an optimal policy. The reason for this policy is due to the existence of certain types of states that restrict control of the dynamics; such as transient states, absorbing states, and non-backtracking states. We show why the occurrence of these states makes a non-stationary policy essential for achieving optimal exploration. Six interesting examples of controllable dynamics are treated in detail. Policy optimality is demonstrated using counting arguments, comparing with suboptimal policies, and by making use of a sequential improvement property from dynamic programming.","authors":["Peter N. Loxley"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20052v1","updated":"2025-12-23T05:03:33Z","published":"2025-12-23T05:03:33Z","title":"Learning Skills from Action-Free Videos","summary":"Learning from videos offers a promising path toward generalist robots by providing rich visual and temporal priors beyond what real robot datasets contain. While existing video generative models produce impressive visual predictions, they are difficult to translate into low-level actions. Conversely, latent-action models better align videos with actions, but they typically operate at the single-step level and lack high-level planning capabilities. We bridge this gap by introducing Skill Abstraction from Optical Flow (SOF), a framework that learns latent skills from large collections of action-free videos. Our key idea is to learn a latent skill space through an intermediate representation based on optical flow that captures motion information aligned with both video dynamics and robot actions. By learning skills in this flow-based latent space, SOF enables high-level planning over video-derived skills and allows for easier translation of these skills into actions. Experiments show that our approach consistently improves performance in both multitask and long-horizon settings, demonstrating the ability to acquire and compose skills directly from raw visual data.","authors":["Hung-Chieh Fang","Kuo-Han Hung","Chu-Rong Chen","Po-Jung Chou","Chun-Kai Yang","Po-Chen Ko","Yu-Chiang Wang","Yueh-Hua Wu","Min-Hung Chen","Shao-Hua Sun"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2511.06148v2","updated":"2025-12-23T04:52:15Z","published":"2025-11-08T21:58:26Z","title":"Large Language Models Develop Novel Social Biases Through Adaptive Exploration","summary":"As large language models (LLMs) are adopted into frameworks that grant them the capacity to make real decisions, it is increasingly important to ensure that they are unbiased. In this paper, we argue that the predominant approach of simply removing existing biases from models is not enough. Using a paradigm from the psychology literature, we demonstrate that LLMs can spontaneously develop novel social biases about artificial demographic groups even when no inherent differences exist. These biases result in highly stratified task allocations, which are less fair than assignments by human participants and are exacerbated by newer and larger models. In social science, emergent biases like these have been shown to result from exploration-exploitation trade-offs, where the decision-maker explores too little, allowing early observations to strongly influence impressions about entire demographic groups. To alleviate this effect, we examine a series of interventions targeting model inputs, problem structure, and explicit steering. We find that explicitly incentivizing exploration most robustly reduces stratification, highlighting the need for better multifaceted objectives to mitigate bias. These results reveal that LLMs are not merely passive mirrors of human social biases, but can actively create new ones from experience, raising urgent questions about how these systems will shape societies over time.","authors":["Addison J. Wu","Ryan Liu","Xuechunzi Bai","Thomas L. Griffiths"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.08674v2","updated":"2025-12-23T04:39:33Z","published":"2025-12-09T14:56:40Z","title":"Multi-Agent Intelligence for Multidisciplinary Decision-Making in Gastrointestinal Oncology","summary":"Multimodal clinical reasoning in the field of gastrointestinal (GI) oncology necessitates the integrated interpretation of endoscopic imagery, radiological data, and biochemical markers. Despite the evident potential exhibited by Multimodal Large Language Models (MLLMs), they frequently encounter challenges such as context dilution and hallucination when confronted with intricate, heterogeneous medical histories. In order to address these limitations, a hierarchical Multi-Agent Framework is proposed, which emulates the collaborative workflow of a human Multidisciplinary Team (MDT). The system attained a composite expert evaluation score of 4.60/5.00, thereby demonstrating a substantial improvement over the monolithic baseline. It is noteworthy that the agent-based architecture yielded the most substantial enhancements in reasoning logic and medical accuracy. The findings indicate that mimetic, agent-based collaboration provides a scalable, interpretable, and clinically robust paradigm for automated decision support in oncology.","authors":["Rongzhao Zhang","Junqiao Wang","Shuyun Yang","Mouxiao Bian","Chihao Zhang","Dongyang Wang","Qiujuan Yan","Yun Zhong","Yuwei Bai","Guanxu Zhu","Kangkun Mao","Miao Wang","Chao Ding","Renjie Lu","Lei Wang","Lei Zheng","Tao Zheng","Xi Wang","Zhuo Fan","Bing Han","Meiling Liu","Luyi Jiang","Dongming Shan","Wenzhong Jin","Jiwei Yu","Zheng Wang","Jie Xu","Meng Luo"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20043v1","updated":"2025-12-23T04:27:35Z","published":"2025-12-23T04:27:35Z","title":"Discovering Lie Groups with Flow Matching","summary":"Symmetry is fundamental to understanding physical systems, and at the same time, can improve performance and sample efficiency in machine learning. Both pursuits require knowledge of the underlying symmetries in data. To address this, we propose learning symmetries directly from data via flow matching on Lie groups. We formulate symmetry discovery as learning a distribution over a larger hypothesis group, such that the learned distribution matches the symmetries observed in data. Relative to previous works, our method, \\lieflow, is more flexible in terms of the types of groups it can discover and requires fewer assumptions. Experiments on 2D and 3D point clouds demonstrate the successful discovery of discrete groups, including reflections by flow matching over the complex domain. We identify a key challenge where the symmetric arrangement of the target modes causes ``last-minute convergence,'' where samples remain stationary until relatively late in the flow, and introduce a novel interpolation scheme for flow matching for symmetry discovery.","authors":["Jung Yeon Park","Yuxuan Chen","Floor Eijkelboom","Jan-Willem van de Meent","Lawson L. S. Wong","Robin Walters"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20042v1","updated":"2025-12-23T04:21:15Z","published":"2025-12-23T04:21:15Z","title":"Beyond Vision: Contextually Enriched Image Captioning with Multi-Modal Retrieva","summary":"Real-world image captions often lack contextual depth, omitting crucial details such as event background, temporal cues, outcomes, and named entities that are not visually discernible. This gap limits the effectiveness of image understanding in domains like journalism, education, and digital archives, where richer, more informative descriptions are essential. To address this, we propose a multimodal pipeline that augments visual input with external textual knowledge. Our system retrieves semantically similar images using BEIT-3 (Flickr30k-384 and COCO-384) and SigLIP So-384, reranks them using ORB and SIFT for geometric alignment, and extracts contextual information from related articles via semantic search. A fine-tuned Qwen3 model with QLoRA then integrates this context with base captions generated by Instruct BLIP (Vicuna-7B) to produce event-enriched, context-aware descriptions. Evaluated on the OpenEvents v1 dataset, our approach generates significantly more informative captions compared to traditional methods, showing strong potential for real-world applications requiring deeper visual-textual understanding","authors":["Nguyen Lam Phu Quy","Pham Phu Hoa","Tran Chi Nguyen","Dao Sy Duy Minh","Nguyen Hoang Minh Ngoc","Huynh Trung Kiet"],"pdf_url":"","comment":"7 pages, 5 figures. System description for the EVENTA Grand Challenge (Track 1) at ACM MM'25"},{"id":"http://arxiv.org/abs/2508.13256v2","updated":"2025-12-23T04:17:03Z","published":"2025-08-18T16:17:12Z","title":"CardAIc-Agents: A Multimodal Framework with Hierarchical Adaptation for Cardiac Care Support","summary":"Cardiovascular diseases (CVDs) remain the foremost cause of mortality worldwide, a burden worsened by a severe deficit of healthcare workers. Artificial intelligence (AI) agents have shown potential to alleviate this gap through automated detection and proactive screening, yet their clinical application remains limited by: 1) rigid sequential workflows, whereas clinical care often requires adaptive reasoning that select specific tests and, based on their results, guides personalised next steps; 2) reliance solely on intrinsic model capabilities to perform role assignment without domain-specific tool support; 3) general and static knowledge bases without continuous learning capability; and 4) fixed unimodal or bimodal inputs and lack of on-demand visual outputs when clinicians require visual clarification. In response, a multimodal framework, CardAIc-Agents, was proposed to augment models with external tools and adaptively support diverse cardiac tasks. First, a CardiacRAG agent generated task-aware plans from updatable cardiac knowledge, while the Chief agent integrated tools to autonomously execute these plans and deliver decisions. Second, to enable adaptive and case-specific customization, a stepwise update strategy was developed to dynamically refine plans based on preceding execution results, once the task was assessed as complex. Third, a multidisciplinary discussion team was proposed which was automatically invoked to interpret challenging cases, thereby supporting further adaptation. In addition, visual review panels were provided to assist validation when clinicians raised concerns. Experiments across three datasets showed the efficiency of CardAIc-Agents compared to mainstream Vision-Language Models (VLMs) and state-of-the-art agentic systems.","authors":["Yuting Zhang","Karina V. Bunting","Asgher Champsi","Xiaoxia Wang","Wenqi Lu","Alexander Thorley","Sandeep S Hothi","Zhaowen Qiu","Baturalp Buyukates","Dipak Kotecha","Jinming Duan"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2504.11671v4","updated":"2025-12-23T04:02:12Z","published":"2025-04-16T00:02:28Z","title":"Computational Basis of LLM's Decision Making in Social Simulation","summary":"Large language models (LLMs) increasingly serve as human-like decision-making agents in social science and applied settings. These LLM-agents are typically assigned human-like characters and placed in real-life contexts. However, how these characters and contexts shape an LLM's behavior remains underexplored. This study proposes and tests methods for probing, quantifying, and modifying an LLM's internal representations in a Dictator Game, a classic behavioral experiment on fairness and prosocial behavior. We extract ``vectors of variable variations'' (e.g., ``male'' to ``female'') from the LLM's internal state. Manipulating these vectors during the model's inference can substantially alter how those variables relate to the model's decision-making. This approach offers a principled way to study and regulate how social concepts can be encoded and engineered within transformer-based models, with implications for alignment, debiasing, and designing AI agents for social simulations in both academic and commercial applications, strengthening sociological theory and measurement.","authors":["Ji Ma"],"pdf_url":"","comment":"Forthcoming: Sociological Methodology; USPTO patent pending"},{"id":"http://arxiv.org/abs/2501.07890v3","updated":"2025-12-23T03:48:57Z","published":"2025-01-14T06:59:51Z","title":"GRAPHMOE: Amplifying Cognitive Depth of Mixture-of-Experts Network via Introducing Self-Rethinking Mechanism","summary":"Traditional Mixture-of-Experts (MoE) networks benefit from utilizing multiple smaller expert models as opposed to a single large network. However, these experts typically operate independently, leaving a question open about whether interconnecting these models could enhance the performance of MoE networks. In response, we introduce GRAPHMOE, a novel method aimed at augmenting the cognitive depth of language models via a self-rethinking mechanism constructed on Pseudo GraphMoE networks. GRAPHMOE employs a recurrent routing strategy to simulate iterative thinking steps, thereby facilitating the flow of information among expert nodes. We implement the GRAPHMOE architecture using Low-Rank Adaptation techniques (LoRA) and conduct extensive experiments on various benchmark datasets. The experimental results reveal that GRAPHMOE outperforms other LoRA based models, achieving state-of-the-art (SOTA) performance. Additionally, this study explores a novel recurrent routing strategy that may inspire further advancements in enhancing the reasoning capabilities of language models.","authors":["Bo Lv","Chen Tang","Zifan Zheng","Bohao Yang","Kun Zhao","Ning Liao","Xiaoxing Wang","Feiyu Xiong","Zhiyu Li","Nayu Liu","Jingchi Jiang"],"pdf_url":"","comment":"10 pages"},{"id":"http://arxiv.org/abs/2512.20028v1","updated":"2025-12-23T03:44:49Z","published":"2025-12-23T03:44:49Z","title":"DecoKAN: Interpretable Decomposition for Forecasting Cryptocurrency Market Dynamics","summary":"Accurate and interpretable forecasting of multivariate time series is crucial for understanding the complex dynamics of cryptocurrency markets in digital asset systems. Advanced deep learning methodologies, particularly Transformer-based and MLP-based architectures, have achieved competitive predictive performance in cryptocurrency forecasting tasks. However, cryptocurrency data is inherently composed of long-term socio-economic trends and local high-frequency speculative oscillations. Existing deep learning-based 'black-box' models fail to effectively decouple these composite dynamics or provide the interpretability needed for trustworthy financial decision-making. To overcome these limitations, we propose DecoKAN, an interpretable forecasting framework that integrates multi-level Discrete Wavelet Transform (DWT) for decoupling and hierarchical signal decomposition with Kolmogorov-Arnold Network (KAN) mixers for transparent and interpretable nonlinear modeling. The DWT component decomposes complex cryptocurrency time series into distinct frequency components, enabling frequency-specific analysis, while KAN mixers provide intrinsically interpretable spline-based mappings within each decomposed subseries. Furthermore, interpretability is enhanced through a symbolic analysis pipeline involving sparsification, pruning, and symbolization, which produces concise analytical expressions offering symbolic representations of the learned patterns. Extensive experiments demonstrate that DecoKAN achieves the lowest average Mean Squared Error on all tested real-world cryptocurrency datasets (BTC, ETH, XMR), consistently outperforming a comprehensive suite of competitive state-of-the-art baselines. These results validate DecoKAN's potential to bridge the gap between predictive accuracy and model transparency, advancing trustworthy decision support within complex cryptocurrency markets.","authors":["Yuan Gao","Zhenguo Dong","Xuelong Wang","Zhiqiang Wang","Yong Zhang","Shaofan Wang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19025v2","updated":"2025-12-23T03:34:28Z","published":"2025-12-22T04:42:41Z","title":"The Erasure Illusion: Stress-Testing the Generalization of LLM Forgetting Evaluation","summary":"Machine unlearning aims to remove specific data influences from trained models, a capability essential for adhering to copyright laws and ensuring AI safety. Current unlearning metrics typically measure success by monitoring the model's performance degradation on the specific unlearning dataset ($D_u$). We argue that for Large Language Models (LLMs), this evaluation paradigm is insufficient and potentially misleading. Many real-world uses of unlearning--motivated by copyright or safety--implicitly target not only verbatim content in $D_u$, but also behaviors influenced by the broader generalizations the model derived from it. We demonstrate that LLMs can pass standard unlearning evaluation and appear to have \"forgotten\" the target knowledge, while simultaneously retaining strong capabilities on content that is semantically adjacent to $D_u$. This phenomenon indicates that erasing exact sentences does not necessarily equate to removing the underlying knowledge. To address this gap, we propose Proximal Surrogate Generation (PSG), an automated stress-testing framework that generates a surrogate dataset, $\\tilde{D}_u$. This surrogate set is constructed to be semantically derived from $D_u$ yet sufficiently distinct in embedding space. By comparing unlearning metric scores between $D_u$ and $\\tilde{D}_u$, we can stress-test the reliability of the metric itself. Our extensive evaluation across three LLM families (Llama-3-8B, Qwen2.5-7B, and Zephyr-7B-$β$), three distinct datasets, and seven standard metrics reveals widespread inconsistencies. We find that current metrics frequently overestimate unlearning success, failing to detect retained knowledge exposed by our stress-test datasets.","authors":["Hengrui Jia","Taoran Li","Jonas Guan","Varun Chandrasekaran"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2503.11006v2","updated":"2025-12-23T03:28:48Z","published":"2025-03-14T02:05:16Z","title":"Fine-Grained Instruction-Guided Graph Reasoning for Vision-and-Language Navigation","summary":"Vision-and-Language Navigation (VLN) requires an embodied agent to traverse complex environments by following natural language instructions, demanding accurate alignment between visual observations and linguistic guidance. Despite recent progress, existing methods typically encode visual and directional cues in a coupled manner, and process instructions without explicitly extracting navigation-critical semantics, which often leads to imprecise spatial reasoning and suboptimal cross-modal alignment. To address these challenges, we propose a fine-grained instruction-guided graph reasoning framework (OIKG) that enhances both spatial representation and instruction understanding during navigation. Specifically, an observation-graph interaction mechanism is introduced to disentangle angular and visual cues while strengthening directed edge representations through geometric embedding, enabling more reliable spatial reasoning within the navigation graph. In addition, a fine-grained instruction guidance module is designed to explicitly extract and leverage location-specific and object-centric information from language instructions, facilitating more precise cross-modal alignment between linguistic semantics and navigable trajectories. By jointly integrating structured graph reasoning with instruction-critical semantic cues, the proposed approach significantly improves the agent's ability to follow complex navigation instructions. Extensive experiments on the R2R and RxR benchmarks demonstrate that our method consistently achieves state-of-the-art performance across multiple evaluation metrics, validating the effectiveness of fine-grained instruction-guided graph reasoning for vision-and-language navigation.","authors":["Yaohua Liu","Xinyuan Song","Yunfu Deng","Yifan Xie","Binkai Ou","Yan Zhong"],"pdf_url":"","comment":"10 pages, 4 figures"},{"id":"http://arxiv.org/abs/2511.17902v2","updated":"2025-12-23T03:27:52Z","published":"2025-11-22T03:39:13Z","title":"Statistically-Guided Dual-Domain Meta-Learning with Adaptive Multi-Prototype Aggregation for Distributed Fiber Optic Sensing","summary":"Distributed Fiber Optic Sensing (DFOS) is promising for long-range perimeter security, yet practical deployment faces three key obstacles: severe cross-deployment domain shift, scarce or unavailable labels at new sites, and limited within-class coverage even in source deployments. We propose DUPLE, a prototype-based meta-learning framework tailored for cross-deployment DFOS recognition. The core idea is to jointly exploit complementary time- and frequency-domain cues and adapt class representations to sample-specific statistics: (i) a dual-domain learner constructs multi-prototype class representations to cover intra-class heterogeneity; (ii) a lightweight statistical guidance mechanism estimates the reliability of each domain from raw signal statistics; and (iii) a query-adaptive aggregation strategy selects and combines the most relevant prototypes for each query. Extensive experiments on two real-world cross-deployment benchmarks demonstrate consistent improvements over strong deep learning and meta-learning baselines, achieving more accurate and stable recognition under label-scarce target deployments.","authors":["Yifan He","Haodong Zhang","Qiuheng Song","Lin Lei","Zhenxuan Zeng","Haoyang He","Hongyan Wu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20014v1","updated":"2025-12-23T03:13:39Z","published":"2025-12-23T03:13:39Z","title":"Bring My Cup! Personalizing Vision-Language-Action Models with Visual Attentive Prompting","summary":"While Vision-Language-Action (VLA) models generalize well to generic instructions, they struggle with personalized commands such as \"bring my cup\", where the robot must act on one specific instance among visually similar objects. We study this setting of manipulating personal objects, in which a VLA must identify and control a user-specific object unseen during training using only a few reference images. To address this challenge, we propose Visual Attentive Prompting (VAP), a simple-yet-effective training-free perceptual adapter that equips frozen VLAs with top-down selective attention. VAP treats the reference images as a non-parametric visual memory, grounds the personal object in the scene through open-vocabulary detection and embedding-based matching, and then injects this grounding as a visual prompt by highlighting the object and rewriting the instruction. We construct two simulation benchmarks, Personalized-SIMPLER and Personalized-VLABench, and a real-world tabletop benchmark to evaluate personalized manipulation across multiple robots and tasks. Experiments show that VAP consistently outperforms generic policies and token-learning baselines in both success rate and correct-object manipulation, helping to bridge the gap between semantic understanding and instance-level control.","authors":["Sangoh Lee","Sangwoo Mo","Wook-Shin Han"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.15649v2","updated":"2025-12-23T03:03:58Z","published":"2025-12-17T17:58:35Z","title":"VTCBench: Can Vision-Language Models Understand Long Context with Vision-Text Compression?","summary":"The computational and memory overheads associated with expanding the context window of LLMs severely limit their scalability. A noteworthy solution is vision-text compression (VTC), exemplified by frameworks like DeepSeek-OCR and Glyph, which convert long texts into dense 2D visual representations, thereby achieving token compression ratios of 3x-20x. However, the impact of this high information density on the core long-context capabilities of vision-language models (VLMs) remains under-investigated. To address this gap, we introduce the first benchmark for VTC and systematically assess the performance of VLMs across three long-context understanding settings: VTC-Retrieval, which evaluates the model's ability to retrieve and aggregate information; VTC-Reasoning, which requires models to infer latent associations to locate facts with minimal lexical overlap; and VTC-Memory, which measures comprehensive question answering within long-term dialogue memory. Furthermore, we establish the VTCBench-Wild to simulate diverse input scenarios.We comprehensively evaluate leading open-source and proprietary models on our benchmarks. The results indicate that, despite being able to decode textual information (e.g., OCR) well, most VLMs exhibit a surprisingly poor long-context understanding ability with VTC-processed information, failing to capture long associations or dependencies in the context.This study provides a deep understanding of VTC and serves as a foundation for designing more efficient and scalable VLMs.","authors":["Hongbo Zhao","Meng Wang","Fei Zhu","Wenzhuo Liu","Bolin Ni","Fanhu Zeng","Gaofeng Meng","Zhaoxiang Zhang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2505.09343v2","updated":"2025-12-23T03:00:00Z","published":"2025-05-14T12:39:03Z","title":"Insights into DeepSeek-V3: Scaling Challenges and Reflections on Hardware for AI Architectures","summary":"The rapid scaling of large language models (LLMs) has unveiled critical limitations in current hardware architectures, including constraints in memory capacity, computational efficiency, and interconnection bandwidth. DeepSeek-V3, trained on 2,048 NVIDIA H800 GPUs, demonstrates how hardware-aware model co-design can effectively address these challenges, enabling cost-efficient training and inference at scale. This paper presents an in-depth analysis of the DeepSeek-V3/R1 model architecture and its AI infrastructure, highlighting key innovations such as Multi-head Latent Attention (MLA) for enhanced memory efficiency, Mixture of Experts (MoE) architectures for optimized computation-communication trade-offs, FP8 mixed-precision training to unlock the full potential of hardware capabilities, and a Multi-Plane Network Topology to minimize cluster-level network overhead. Building on the hardware bottlenecks encountered during DeepSeek-V3's development, we engage in a broader discussion with academic and industry peers on potential future hardware directions, including precise low-precision computation units, scale-up and scale-out convergence, and innovations in low-latency communication fabrics. These insights underscore the critical role of hardware and model co-design in meeting the escalating demands of AI workloads, offering a practical blueprint for innovation in next-generation AI systems.","authors":["Chenggang Zhao","Chengqi Deng","Chong Ruan","Damai Dai","Huazuo Gao","Jiashi Li","Liyue Zhang","Panpan Huang","Shangyan Zhou","Shirong Ma","Wenfeng Liang","Ying He","Yuqing Wang","Yuxuan Liu","Y. X. Wei"],"pdf_url":"","comment":"This is the author's version of the work. It is posted here for your personal use. Not for redistribution. The definitive version appeared as part of the Industry Track in Proceedings of the 52nd Annual International Symposium on Computer Architecture (ISCA '25)"},{"id":"http://arxiv.org/abs/2512.16334v3","updated":"2025-12-23T02:58:42Z","published":"2025-12-18T09:17:45Z","title":"Pretrained Battery Transformer (PBT): A battery life prediction foundation model","summary":"Early prediction of battery cycle life is essential for accelerating battery research, manufacturing, and deployment. Although machine learning methods have shown encouraging results, progress is hindered by data scarcity and heterogeneity arising from diverse aging conditions. In other fields, foundation models (FMs) trained on diverse datasets have achieved broad generalization through transfer learning, but no FMs have been reported for battery cycle life prediction yet. Here we present the Pretrained Battery Transformer (PBT), the first FM for battery life prediction, developed through domain-knowledge-encoded mixture-of-expert layers. Validated on the largest public battery life database, PBT learns transferable representations from 13 lithium-ion battery datasets, outperforming existing models by an average of 19.8%. With transfer learning, PBT achieves state-of-the-art performance across 15 diverse datasets encompassing various operating conditions, formation protocols, and chemistries. This work establishes a foundation model pathway for battery lifetime prediction, paving the way toward universal battery lifetime prediction systems.","authors":["Ruifeng Tan","Weixiang Hong","Jia Li","Jiaqiang Huang","Tong-Yi Zhang"],"pdf_url":"","comment":"5 figures in the main content"},{"id":"http://arxiv.org/abs/2512.20004v1","updated":"2025-12-23T02:57:33Z","published":"2025-12-23T02:57:33Z","title":"IoT-based Android Malware Detection Using Graph Neural Network With Adversarial Defense","summary":"Since the Internet of Things (IoT) is widely adopted using Android applications, detecting malicious Android apps is essential. In recent years, Android graph-based deep learning research has proposed many approaches to extract relationships from applications as graphs to generate graph embeddings. First, we demonstrate the effectiveness of graph-based classification using a Graph Neural Network (GNN)-based classifier to generate API graph embeddings. The graph embeddings are combined with Permission and Intent features to train multiple machine learning and deep learning models for Android malware detection. The proposed classification approach achieves an accuracy of 98.33 percent on the CICMaldroid dataset and 98.68 percent on the Drebin dataset. However, graph-based deep learning models are vulnerable, as attackers can add fake relationships to evade detection by the classifier. Second, we propose a Generative Adversarial Network (GAN)-based attack algorithm named VGAE-MalGAN targeting graph-based GNN Android malware classifiers. The VGAE-MalGAN generator produces adversarial malware API graphs, while the VGAE-MalGAN substitute detector attempts to mimic the target detector. Experimental results show that VGAE-MalGAN can significantly reduce the detection rate of GNN-based malware classifiers. Although the model initially fails to detect adversarial malware, retraining with generated adversarial samples improves robustness and helps mitigate adversarial attacks.","authors":["Rahul Yumlembam","Biju Issac","Seibu Mary Jacob","Longzhi Yang"],"pdf_url":"","comment":"13 pages"},{"id":"http://arxiv.org/abs/2412.06867v2","updated":"2025-12-23T02:53:15Z","published":"2024-12-09T09:37:54Z","title":"Lossless Model Compression via Joint Low-Rank Factorization Optimization","summary":"Low-rank factorization is a popular model compression technique that minimizes the error $δ$ between approximated and original weight matrices. Despite achieving performances close to the original models when $δ$ is optimized, a performance discrepancy remains due to the separate optimization processes for low-rank factorization and model performance, resulting in unavoidable losses. We address this issue by introducing a novel joint optimization strategy for lossless low-rank weight factorization, which, for the first time, enhances the model's performance beyond the original. Our approach begins with a theoretical analysis of the relationship between low-rank factorization and model optimization objectives, establishing a precise perturbation range for matrix factorization errors on model performance. This challenge is then reformulated as a numerical rank deficiency problem with inequality constraints and develop a joint objective that simultaneously addresses factorization error and model performance. Based on the above analysis, we propose two optimization algorithms: \\textbf{a lossless optimization algorithm} that maximizes model accuracy while ensuring compression, and \\textbf{a compact optimization algorithm} that minimizes model size while preserving performance. These algorithms do not require fine-tuning and can directly compress numerous deep models to achieve lossless results. Our methods demonstrate robust efficacy across various vision and language tasks. For example, the compressed model reduced by 70\\% on ResNext50 outperforms the original. Our code will be made public.","authors":["Boyang Zhang","Daning Cheng","Yunquan Zhang","Fangming Liu","Jiake Tian"],"pdf_url":"","comment":"Under Review"},{"id":"http://arxiv.org/abs/2412.06868v2","updated":"2025-12-23T02:50:21Z","published":"2024-12-09T09:55:54Z","title":"Compression for Better: A General and Stable Lossless Compression Framework","summary":"This work focus on how to stabilize and lossless model compression, aiming to reduce model complexity and enhance efficiency without sacrificing performance due to compression errors. A key challenge is effectively leveraging compression errors and defining the boundaries for lossless compression to minimize model loss. i.e., compression for better. Currently, there is no systematic approach to determining this error boundary or understanding its specific impact on model performance. We propose a general \\textbf{L}oss\\textbf{L}ess \\textbf{C}ompression theoretical framework (\\textbf{LLC}), which further delineates the compression neighborhood and higher-order analysis boundaries through the total differential, thereby specifying the error range within which a model can be compressed without loss. To verify the effectiveness of LLC, we apply various compression techniques, including quantization and decomposition. Specifically, for quantization, we reformulate the classic quantization search problem as a grouped knapsack problem within the lossless neighborhood, achieving lossless quantization while improving computational efficiency. For decomposition, LLC addresses the approximation problem under low-rank constraints, automatically determining the rank for each layer and producing lossless low-rank models. We conduct extensive experiments on multiple neural network architectures on different datasets. The results show that without fancy tricks, LLC can effectively achieve lossless model compression. Our code will be made publicly.","authors":["Boyang Zhang","Daning Cheng","Yunquan Zhang","Fangming Liu","Wenguang Chen"],"pdf_url":"","comment":"Under Review"},{"id":"http://arxiv.org/abs/2502.15802v2","updated":"2025-12-23T02:46:06Z","published":"2025-02-19T06:12:43Z","title":"A General Error-Theoretical Analysis Framework for Constructing Compression Strategies","summary":"The exponential growth in parameter size and computational complexity of deep models poses significant challenges for efficient deployment. The core problem of existing compression methods is that different layers of the model have significant differences in their tolerance to compression levels. For instance, the first layer of a model can typically sustain a higher compression level compared to the last layer without compromising performance. Thus, the key challenge lies in how to allocate compression levels across layers in a way that minimizes performance loss while maximizing parameter reduction. To address this challenge, we propose a Compression Error Theory (CET) framework, designed to determine the optimal compression level for each layer. Taking quantization as an example, CET leverages differential expansion and algebraic geometry to reconstruct the quadratic form of quantization error as ellipsoids and hyperbolic paraboloids, and utilizes their geometric structures to define an error subspace. To identify the error subspace with minimal performance loss, by performing orthogonal decomposition of the geometric space, CET transforms the optimization process of the error subspace into a complementary problem. The final theoretical analysis shows that constructing the quantization subspace along the major axis results in minimal performance degradation. Through experimental verification of the theory, CET can greatly retain performance while compressing. Specifically, on the ResNet-34 model, CET achieves nearly 11$\\times$ parameter compression while even surpassing performance comparable to the original model.","authors":["Boyang Zhang","Daning Cheng","Yunquan Zhang","Meiqi Tu","Fangming Liu","Jiake Tian"],"pdf_url":"","comment":"Under Review"},{"id":"http://arxiv.org/abs/2512.19995v1","updated":"2025-12-23T02:44:25Z","published":"2025-12-23T02:44:25Z","title":"Schoenfeld's Anatomy of Mathematical Reasoning by Language Models","summary":"Large language models increasingly expose reasoning traces, yet their underlying cognitive structure and steps remain difficult to identify and analyze beyond surface-level statistics. We adopt Schoenfeld's Episode Theory as an inductive, intermediate-scale lens and introduce ThinkARM (Anatomy of Reasoning in Models), a scalable framework that explicitly abstracts reasoning traces into functional reasoning steps such as Analysis, Explore, Implement, Verify, etc. When applied to mathematical problem solving by diverse models, this abstraction reveals reproducible thinking dynamics and structural differences between reasoning and non-reasoning models, which are not apparent from token-level views. We further present two diagnostic case studies showing that exploration functions as a critical branching step associated with correctness, and that efficiency-oriented methods selectively suppress evaluative feedback steps rather than uniformly shortening responses. Together, our results demonstrate that episode-level representations make reasoning steps explicit, enabling systematic analysis of how reasoning is structured, stabilized, and altered in modern language models.","authors":["Ming Li","Chenrui Fan","Yize Cheng","Soheil Feizi","Tianyi Zhou"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19992v1","updated":"2025-12-23T02:36:56Z","published":"2025-12-23T02:36:56Z","title":"S$^3$IT: A Benchmark for Spatially Situated Social Intelligence Test","summary":"The integration of embodied agents into human environments demands embodied social intelligence: reasoning over both social norms and physical constraints. However, existing evaluations fail to address this integration, as they are limited to either disembodied social reasoning (e.g., in text) or socially-agnostic physical tasks. Both approaches fail to assess an agent's ability to integrate and trade off both physical and social constraints within a realistic, embodied context. To address this challenge, we introduce Spatially Situated Social Intelligence Test (S$^{3}$IT), a benchmark specifically designed to evaluate embodied social intelligence. It is centered on a novel and challenging seat-ordering task, requiring an agent to arrange seating in a 3D environment for a group of large language model-driven (LLM-driven) NPCs with diverse identities, preferences, and intricate interpersonal relationships. Our procedurally extensible framework generates a vast and diverse scenario space with controllable difficulty, compelling the agent to acquire preferences through active dialogue, perceive the environment via autonomous exploration, and perform multi-objective optimization within a complex constraint network. We evaluate state-of-the-art LLMs on S$^{3}$IT and found that they still struggle with this problem, showing an obvious gap compared with the human baseline. Results imply that LLMs have deficiencies in spatial intelligence, yet simultaneously demonstrate their ability to achieve near human-level competence in resolving conflicts that possess explicit textual cues.","authors":["Zhe Sun","Xueyuan Yang","Yujie Lu","Zhenliang Zhang"],"pdf_url":"","comment":"10 pages, 9 figures"},{"id":"http://arxiv.org/abs/2412.06865v2","updated":"2025-12-23T02:30:32Z","published":"2024-12-09T08:50:28Z","title":"FP=xINT:Representing Neural Networks via Low-Bit Series Basis Functions","summary":"Post-Training Quantization (PTQ) converts pre-trained Full-Precision (FP) models into quantized versions without training. While existing methods reduce size and computational costs, they also significantly degrade performance and quantization efficiency at extremely low settings due to quantization noise. We introduce a deep model series expansion framework to address this issue, enabling rapid and accurate approximation of unquantized models without calibration sets or fine-tuning. This is the first use of series expansion for neural network quantization. Specifically, our method expands the FP model into multiple low-bit basis models. To ensure accurate quantization, we develop low-bit basis model expansions at different granularities (tensor, layer, model), and theoretically confirm their convergence to the dense model, thus restoring FP model accuracy. Additionally, we design AbelianAdd/Mul operations between isomorphic models in the low-bit expansion, forming an Abelian group to ensure operation parallelism and commutativity. The experiments show that our algorithm achieves state-of-the-art performance in low-bit settings; for example, 4-bit quantization of ResNet-50 surpasses the original accuracy, reaching 77.03%. The code will be made public.","authors":["Boyang Zhang","Daning Cheng","Yunquan Zhang","Jiake Tian","Jing Li","Fangming Liu"],"pdf_url":"","comment":"AAAI2026"},{"id":"http://arxiv.org/abs/2512.18871v2","updated":"2025-12-23T02:24:10Z","published":"2025-12-21T20:11:07Z","title":"Psychometric Validation of the Sophotechnic Mediation Scale and a New Understanding of the Development of GenAI Mastery: Lessons from 3,932 Adult Brazilian Workers","summary":"The rapid diffusion of generative artificial intelligence (GenAI) systems has introduced new forms of human-technology interaction, raising the question of whether sustained engagement gives rise to stable, internalized modes of cognition rather than merely transient efficiency gains. Grounded in the Cognitive Mediation Networks Theory, this study investigates Sophotechnic Mediation, a mode of thinking and acting associated with prolonged interaction with GenAI, and presents a comprehensive psychometric validation of the Sophotechnic Mediation Scale. Data were collected between 2023 and 2025 from independent cross-sectional samples totaling 3,932 adult workers from public and private organizations in the Metropolitan Region of Pernambuco, Brazil. Results indicate excellent internal consistency, a robust unidimensional structure, and measurement invariance across cohorts. Ordinal-robust confirmatory factor analyses and residual diagnostics show that elevated absolute fit indices reflect minor local dependencies rather than incorrect dimensionality. Distributional analyses reveal a time-evolving pattern characterized by a declining mass of non-adopters and convergence toward approximate Gaussianity among adopters, with model comparisons favoring a two-process hurdle model over a censored Gaussian specification. Sophotechnic Mediation is empirically distinct from Hypercultural mediation and is primarily driven by cumulative GenAI experience, with age moderating the rate of initial acquisition and the depth of later integration. Together, the findings support Sophotechnia as a coherent, measurable, and emergent mode of cognitive mediation associated with the ongoing GenAI revolution.","authors":["Bruno Campello de Souza"],"pdf_url":"","comment":"35 pages, 28 Manuscript, Portuguese and English Versions of the Instrument in Annex"},{"id":"http://arxiv.org/abs/2411.04872v7","updated":"2025-12-23T02:23:47Z","published":"2024-11-07T17:07:35Z","title":"FrontierMath: A Benchmark for Evaluating Advanced Mathematical Reasoning in AI","summary":"We introduce FrontierMath, a benchmark of hundreds of original, exceptionally challenging mathematics problems crafted and vetted by expert mathematicians. The questions cover most major branches of modern mathematics -- from computationally intensive problems in number theory and real analysis to abstract questions in algebraic geometry and category theory. Solving a typical problem requires multiple hours of effort from a researcher in the relevant branch of mathematics, and for the upper end questions, multiple days. FrontierMath uses new, unpublished problems and automated verification to reliably evaluate models while minimizing risk of data contamination. Current state-of-the-art AI models solve under 2% of problems, revealing a vast gap between AI capabilities and the prowess of the mathematical community. As AI systems advance toward expert-level mathematical abilities, FrontierMath offers a rigorous testbed that quantifies their progress.","authors":["Elliot Glazer","Ege Erdil","Tamay Besiroglu","Diego Chicharro","Evan Chen","Alex Gunning","Caroline Falkman Olsson","Jean-Stanislas Denain","Anson Ho","Emily de Oliveira Santos","Olli Järviniemi","Matthew Barnett","Robert Sandler","Matej Vrzala","Jaime Sevilla","Qiuyu Ren","Elizabeth Pratt","Lionel Levine","Grant Barkley","Natalie Stewart","Bogdan Grechuk","Tetiana Grechuk","Shreepranav Varma Enugandla","Mark Wildon"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.16251v2","updated":"2025-12-23T02:11:19Z","published":"2025-12-18T07:05:25Z","title":"Interpretable Deep Learning for Stock Returns: A Consensus-Bottleneck Asset Pricing Model","summary":"We introduce the Consensus-Bottleneck Asset Pricing Model (CB-APM), a partially interpretable neural network that replicates the reasoning processes of sell-side analysts by capturing how dispersed investor beliefs are compressed into asset prices through a consensus formation process. By modeling this \"bottleneck\" to summarize firm- and macro-level information, CB-APM not only predicts future risk premiums of U.S. equities but also links belief aggregation to expected returns in a structurally interpretable manner. The model improves long-horizon return forecasts and outperforms standard deep learning approaches in both predictive accuracy and explanatory power. Comprehensive portfolio analyses show that CB-APM's out-of-sample predictions translate into economically meaningful payoffs, with monotonic return differentials and stable long-short performance across regularization settings. Empirically, CB-APM leverages consensus as a regularizer to amplify long-horizon predictability and yields interpretable consensus-based components that clarify how information is priced in returns. Moreover, regression and Gibbons-Ross-Shanken (GRS)-based pricing diagnostics reveal that the learned consensus representations capture priced variation only partially spanned by traditional factor models, demonstrating that CB-APM uncovers belief-driven structure in expected returns beyond the canonical factor space. Overall, CB-APM provides an interpretable and empirically grounded framework for understanding belief-driven return dynamics.","authors":["Bong-Gyu Jang","Younwoo Jeong","Changeun Kim"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19980v1","updated":"2025-12-23T02:04:13Z","published":"2025-12-23T02:04:13Z","title":"Neuron-Guided Interpretation of Code LLMs: Where, Why, and How?","summary":"Code language models excel on code intelligence tasks, yet their internal interpretability is underexplored. Existing neuron interpretability techniques from NLP are suboptimal for source code due to programming languages formal, hierarchical, and executable nature. We empirically investigate code LLMs at the neuron level, localizing language-specific neurons (selectively responsive to one language) and concept layers (feed-forward layers encoding language-agnostic code representations). We analyze Llama-3.1-8B and Qwen2.5-Coder-32B on multilingual inputs in C++, Java, Python, Go, and JavaScript, measuring neuron selectivity and layerwise contributions during generation. We find (1) neurons specialized for individual languages alongside a universal subset supporting general-purpose generation; and (2) lower layers mainly encode language-specific syntax, while middle layers capture semantic abstractions shared across languages, emerging as concept layers. We demonstrate utility on three tasks: neuron-guided fine-tuning for code generation, clone detection via concept-layer embeddings, and concept-layer-guided transfer for code summarization, each yielding consistent gains in multilingual settings.","authors":["Zhe Yin","Xiaodong Gu","Beijun Shen"],"pdf_url":"","comment":"Accepted by FSE2026"},{"id":"http://arxiv.org/abs/2512.19978v1","updated":"2025-12-23T01:58:03Z","published":"2025-12-23T01:58:03Z","title":"Regression of Functions by Quantum Neural Networks Circuits","summary":"The performance of quantum neural network models depends strongly on architectural decisions, including circuit depth, placement of parametrized operations, and data-encoding strategies. Selecting an effective architecture is challenging and closely related to the classical difficulty of choosing suitable neural-network topologies, which is computationally hard. This work investigates automated quantum-circuit construction for regression tasks and introduces a genetic-algorithm framework that discovers Reduced Regressor QNN architectures. The approach explores depth, parametrized gate configurations, and flexible data re-uploading patterns, formulating the construction of quantum regressors as an optimization process. The discovered circuits are evaluated against seventeen classical regression models on twenty-two nonlinear benchmark functions and four analytical functions. Although classical methods often achieve comparable results, they typically require far more parameters, whereas the evolved quantum models remain compact while providing competitive performance. We further analyze dataset complexity using twelve structural descriptors and show, across five increasingly challenging meta-learning scenarios, that these measures can reliably predict which quantum architecture will perform best. The results demonstrate perfect or near-perfect predictive accuracy in several scenarios, indicating that complexity metrics offer powerful and compact representations of dataset structure and can effectively guide automated model selection. Overall, this study provides a principled basis for meta-learning-driven quantum architecture design and advances the understanding of how quantum models behave in regression settings--a topic that has received limited exploration in prior work. These findings pave the way for more systematic and theoretically grounded approaches to quantum regression.","authors":["Fernando M. de Paula Neto","Lucas dos Reis Silva","Paulo S. G. de Mattos Neto","Felipe F. Fanchini"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.10229v3","updated":"2025-12-23T01:17:01Z","published":"2025-12-11T02:25:27Z","title":"Adaptive Information Routing for Multimodal Time Series Forecasting","summary":"Time series forecasting is a critical task for artificial intelligence with numerous real-world applications. Traditional approaches primarily rely on historical time series data to predict the future values. However, in practical scenarios, this is often insufficient for accurate predictions due to the limited information available. To address this challenge, multimodal time series forecasting methods which incorporate additional data modalities, mainly text data, alongside time series data have been explored. In this work, we introduce the Adaptive Information Routing (AIR) framework, a novel approach for multimodal time series forecasting. Unlike existing methods that treat text data on par with time series data as interchangeable auxiliary features for forecasting, AIR leverages text information to dynamically guide the time series model by controlling how and to what extent multivariate time series information should be combined. We also present a text-refinement pipeline that employs a large language model to convert raw text data into a form suitable for multimodal forecasting, and we introduce a benchmark that facilitates multimodal forecasting experiments based on this pipeline. Experiment results with the real world market data such as crude oil price and exchange rates demonstrate that AIR effectively modulates the behavior of the time series model using textual inputs, significantly enhancing forecasting accuracy in various time series forecasting tasks.","authors":["Jun Seo","Hyeokjun Choe","Seohui Bae","Soyeon Park","Wonbin Ahn","Taeyoon Lim","Junhyeok Kang","Sangjun Han","Jaehoon Lee","Dongwan Kang","Minjae Kim","Sungdong Yoo","Soonyoung Lee"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19960v1","updated":"2025-12-23T01:14:06Z","published":"2025-12-23T01:14:06Z","title":"FGDCC: Fine-Grained Deep Cluster Categorization -- A Framework for Intra-Class Variability Problems in Plant Classification","summary":"Intra-class variability is given according to the significance in the degree of dissimilarity between images within a class. In that sense, depending on its intensity, intra-class variability can hinder the learning process for DL models, specially when such classes are also underrepresented, which is a very common scenario in Fine-Grained Visual Categorization (FGVC) tasks. This paper proposes a novel method that aims at leveraging classification performance in FGVC tasks by learning fine-grained features via classification of class-wise cluster assignments. Our goal is to apply clustering over each class individually, which can allow to discover pseudo-labels that encodes a latent degree of similarity between images. In turn, those labels can be employed in a hierarchical classification process that allows to learn more fine-grained visual features and thereby mitigating intra-class variability issues. Initial experiments over the PlantNet300k enabled to shed light upon several key points in which future work will have to be developed in order to find more conclusive evidence regarding the effectiveness of our method. Our method still achieves state-of-the-art performance on the PlantNet300k dataset even though some of its components haven't been shown to be fully optimized. Our code is available at \\href{https://github.com/ADAM-UEFS/FGDCC}{https://github.com/ADAM-UEFS/FGDCC}.","authors":["Luciano Araujo Dourado Filho","Rodrigo Tripodi Calumby"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19957v1","updated":"2025-12-23T01:06:55Z","published":"2025-12-23T01:06:55Z","title":"Zero-Shot Segmentation through Prototype-Guidance for Multi-Label Plant Species Identification","summary":"This paper presents an approach developed to address the PlantClef 2025 challenge, which consists of a fine-grained multi-label species identification, over high-resolution images. Our solution focused on employing class prototypes obtained from the training dataset as a proxy guidance for training a segmentation Vision Transformer (ViT) on the test set images. To obtain these representations, the proposed method extracts features from training dataset images and create clusters, by applying K-Means, with $K$ equals to the number of classes in the dataset. The segmentation model is a customized narrow ViT, built by replacing the patch embedding layer with a frozen DinoV2, pre-trained on the training dataset for individual species classification. This model is trained to reconstruct the class prototypes of the training dataset from the test dataset images. We then use this model to obtain attention scores that enable to identify and localize areas of interest and consequently guide the classification process. The proposed approach enabled a domain-adaptation from multi-class identification with individual species, into multi-label classification from high-resolution vegetation plots. Our method achieved fifth place in the PlantCLEF 2025 challenge on the private leaderboard, with an F1 score of 0.33331. Besides that, in absolute terms our method scored 0.03 lower than the top-performing submission, suggesting that it may achieved competitive performance in the benchmark task. Our code is available at \\href{https://github.com/ADAM-UEFS/PlantCLEF2025}{https://github.com/ADAM-UEFS/PlantCLEF2025}.","authors":["Luciano Araujo Dourado Filho","Almir Moreira da Silva Neto","Rodrigo Pereira David","Rodrigo Tripodi Calumby"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19949v1","updated":"2025-12-23T00:38:52Z","published":"2025-12-23T00:38:52Z","title":"How Much 3D Do Video Foundation Models Encode?","summary":"Videos are continuous 2D projections of 3D worlds. After training on large video data, will global 3D understanding naturally emerge? We study this by quantifying the 3D understanding of existing Video Foundation Models (VidFMs) pretrained on vast video data. We propose the first model-agnostic framework that measures the 3D awareness of various VidFMs by estimating multiple 3D properties from their features via shallow read-outs. Our study presents meaningful findings regarding the 3D awareness of VidFMs on multiple axes. In particular, we show that state-of-the-art video generation models exhibit a strong understanding of 3D objects and scenes, despite not being trained on any 3D data. Such understanding can even surpass that of large expert models specifically trained for 3D tasks. Our findings, together with the 3D benchmarking of major VidFMs, provide valuable observations for building scalable 3D models.","authors":["Zixuan Huang","Xiang Li","Zhaoyang Lv","James M. Rehg"],"pdf_url":"","comment":"Project Page: https://vidfm-3d-probe.github.io"},{"id":"http://arxiv.org/abs/2501.12536v2","updated":"2025-12-23T00:21:27Z","published":"2025-01-21T22:59:50Z","title":"Interaction Dataset of Autonomous Vehicles with Traffic Lights and Signs","summary":"This paper presents the development of a comprehensive dataset capturing interactions between Autonomous Vehicles (AVs) and traffic control devices, specifically traffic lights and stop signs. Derived from the Waymo Motion dataset, our work addresses a critical gap in the existing literature by providing real-world trajectory data on how AVs navigate these traffic control devices. We propose a methodology for identifying and extracting relevant interaction trajectory data from the Waymo Motion dataset, incorporating over 37,000 instances with traffic lights and 44,000 with stop signs. Our methodology includes defining rules to identify various interaction types, extracting trajectory data, and applying a wavelet-based denoising method to smooth the acceleration and speed profiles and eliminate anomalous values, thereby enhancing the trajectory quality. Quality assessment metrics indicate that trajectories obtained in this study have anomaly proportions in acceleration and jerk profiles reduced to near-zero levels across all interaction categories. By making this dataset publicly available, we aim to address the current gap in datasets containing AV interaction behaviors with traffic lights and signs. Based on the organized and published dataset, we can gain a more in-depth understanding of AVs' behavior when interacting with traffic lights and signs. This will facilitate research on AV integration into existing transportation infrastructures and networks, supporting the development of more accurate behavioral models and simulation tools.","authors":["Zheng Li","Zhipeng Bao","Haoming Meng","Haotian Shi","Qianwen Li","Handong Yao","Xiaopeng Li"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19941v1","updated":"2025-12-23T00:18:23Z","published":"2025-12-23T00:18:23Z","title":"Block-Recurrent Dynamics in Vision Transformers","summary":"As Vision Transformers (ViTs) become standard vision backbones, a mechanistic account of their computational phenomenology is essential. Despite architectural cues that hint at dynamical structure, there is no settled framework that interprets Transformer depth as a well-characterized flow. In this work, we introduce the Block-Recurrent Hypothesis (BRH), arguing that trained ViTs admit a block-recurrent depth structure such that the computation of the original $L$ blocks can be accurately rewritten using only $k \\ll L$ distinct blocks applied recurrently. Across diverse ViTs, between-layer representational similarity matrices suggest few contiguous phases. To determine whether these phases reflect genuinely reusable computation, we train block-recurrent surrogates of pretrained ViTs: Recurrent Approximations to Phase-structured TransfORmers (Raptor). In small-scale, we demonstrate that stochastic depth and training promote recurrent structure and subsequently correlate with our ability to accurately fit Raptor. We then provide an empirical existence proof for BRH by training a Raptor model to recover $96\\%$ of DINOv2 ImageNet-1k linear probe accuracy in only 2 blocks at equivalent computational cost. Finally, we leverage our hypothesis to develop a program of Dynamical Interpretability. We find i) directional convergence into class-dependent angular basins with self-correcting trajectories under small perturbations, ii) token-specific dynamics, where cls executes sharp late reorientations while patch tokens exhibit strong late-stage coherence toward their mean direction, and iii) a collapse to low rank updates in late depth, consistent with convergence to low-dimensional attractors. Altogether, we find a compact recurrent program emerges along ViT depth, pointing to a low-complexity normative solution that enables these models to be studied through principled dynamical systems analysis.","authors":["Mozes Jacobs","Thomas Fel","Richard Hakim","Alessandra Brondetta","Demba Ba","T. Andy Keller"],"pdf_url":"","comment":"25 pages, 15 figures"},{"id":"http://arxiv.org/abs/2512.19937v1","updated":"2025-12-23T00:00:17Z","published":"2025-12-23T00:00:17Z","title":"Interpolative Decoding: Exploring the Spectrum of Personality Traits in LLMs","summary":"Recent research has explored using very large language models (LLMs) as proxies for humans in tasks such as simulation, surveys, and studies. While LLMs do not possess a human psychology, they often can emulate human behaviors with sufficiently high fidelity to drive simulations to test human behavioral hypotheses, exhibiting more nuance and range than the rule-based agents often employed in behavioral economics. One key area of interest is the effect of personality on decision making, but the requirement that a prompt must be created for every tested personality profile introduces experimental overhead and degrades replicability. To address this issue, we leverage interpolative decoding, representing each dimension of personality as a pair of opposed prompts and employing an interpolation parameter to simulate behavior along the dimension. We show that interpolative decoding reliably modulates scores along each of the Big Five dimensions. We then show how interpolative decoding causes LLMs to mimic human decision-making behavior in economic games, replicating results from human psychological research. Finally, we present preliminary results of our efforts to ``twin'' individual human players in a collaborative game through systematic search for points in interpolation space that cause the system to replicate actions taken by the human subject.","authors":["Eric Yeh","John Cadigan","Ran Chen","Dick Crouch","Melinda Gervasio","Dayne Freitag"],"pdf_url":"","comment":"20 pages, 5 figures"},{"id":"http://arxiv.org/abs/2512.20848v1","updated":"2025-12-23T23:54:32Z","published":"2025-12-23T23:54:32Z","title":"Nemotron 3 Nano: Open, Efficient Mixture-of-Experts Hybrid Mamba-Transformer Model for Agentic Reasoning","summary":"We present Nemotron 3 Nano 30B-A3B, a Mixture-of-Experts hybrid Mamba-Transformer language model. Nemotron 3 Nano was pretrained on 25 trillion text tokens, including more than 3 trillion new unique tokens over Nemotron 2, followed by supervised fine tuning and large-scale RL on diverse environments. Nemotron 3 Nano achieves better accuracy than our previous generation Nemotron 2 Nano while activating less than half of the parameters per forward pass. It achieves up to 3.3x higher inference throughput than similarly-sized open models like GPT-OSS-20B and Qwen3-30B-A3B-Thinking-2507, while also being more accurate on popular benchmarks. Nemotron 3 Nano demonstrates enhanced agentic, reasoning, and chat abilities and supports context lengths up to 1M tokens. We release both our pretrained Nemotron 3 Nano 30B-A3B Base and post-trained Nemotron 3 Nano 30B-A3B checkpoints on Hugging Face.","authors":[" NVIDIA"," :","Aaron Blakeman","Aaron Grattafiori","Aarti Basant","Abhibha Gupta","Abhinav Khattar","Adi Renduchintala","Aditya Vavre","Akanksha Shukla","Akhiad Bercovich","Aleksander Ficek","Aleksandr Shaposhnikov","Alex Kondratenko","Alexander Bukharin","Alexandre Milesi","Ali Taghibakhshi","Alisa Liu","Amelia Barton","Ameya Sunil Mahabaleshwarkar","Amir Klein","Amit Zuker","Amnon Geifman","Amy Shen","Anahita Bhiwandiwalla","Andrew Tao","Ann Guan","Anubhav Mandarwal","Arham Mehta","Ashwath Aithal","Ashwin Poojary","Asif Ahamed","Asma Kuriparambil Thekkumpate","Ayush Dattagupta","Banghua Zhu","Bardiya Sadeghi","Barnaby Simkin","Ben Lanir","Benedikt Schifferer","Besmira Nushi","Bilal Kartal","Bita Darvish Rouhani","Boris Ginsburg","Brandon Norick","Brandon Soubasis","Branislav Kisacanin","Brian Yu","Bryan Catanzaro","Carlo del Mundo","Chantal Hwang","Charles Wang","Cheng-Ping Hsieh","Chenghao Zhang","Chenhan Yu","Chetan Mungekar","Chintan Patel","Chris Alexiuk","Christopher Parisien","Collin Neale","Damon Mosk-Aoyama","Dan Su","Dane Corneil","Daniel Afrimi","Daniel Rohrer","Daniel Serebrenik","Daria Gitman","Daria Levy","Darko Stosic","David Mosallanezhad","Deepak Narayanan","Dhruv Nathawani","Dima Rekesh","Dina Yared","Divyanshu Kakwani","Dong Ahn","Duncan Riach","Dusan Stosic","Edgar Minasyan","Edward Lin","Eileen Long","Eileen Peters Long","Elena Lantz","Ellie Evans","Elliott Ning","Eric Chung","Eric Harper","Eric Tramel","Erick Galinkin","Erik Pounds","Evan Briones","Evelina Bakhturina","Faisal Ladhak","Fay Wang","Fei Jia","Felipe Soares","Feng Chen","Ferenc Galko","Frankie Siino","Gal Hubara Agam","Ganesh Ajjanagadde","Gantavya Bhatt","Gargi Prasad","George Armstrong","Gerald Shen","Gorkem Batmaz","Grigor Nalbandyan","Haifeng Qian","Harsh Sharma","Hayley Ross","Helen Ngo","Herman Sahota","Hexin Wang","Himanshu Soni","Hiren Upadhyay","Huizi Mao","Huy C Nguyen","Huy Q Nguyen","Iain Cunningham","Ido Shahaf","Igor Gitman","Ilya Loshchilov","Ivan Moshkov","Izzy Putterman","Jan Kautz","Jane Polak Scowcroft","Jared Casper","Jatin Mitra","Jeffrey Glick","Jenny Chen","Jesse Oliver","Jian Zhang","Jiaqi Zeng","Jie Lou","Jimmy Zhang","Jining Huang","Joey Conway","Joey Guman","John Kamalu","Johnny Greco","Jonathan Cohen","Joseph Jennings","Joyjit Daw","Julien Veron Vialard","Junkeun Yi","Jupinder Parmar","Kai Xu","Kan Zhu","Kari Briski","Katherine Cheung","Katherine Luna","Keshav Santhanam","Kevin Shih","Kezhi Kong","Khushi Bhardwaj","Krishna C. Puvvada","Krzysztof Pawelec","Kumar Anik","Lawrence McAfee","Laya Sleiman","Leon Derczynski","Li Ding","Lucas Liebenwein","Luis Vega","Maanu Grover","Maarten Van Segbroeck","Maer Rodrigues de Melo","Makesh Narsimhan Sreedhar","Manoj Kilaru","Maor Ashkenazi","Marc Romeijn","Mark Cai","Markus Kliegl","Maryam Moosaei","Matvei Novikov","Mehrzad Samadi","Melissa Corpuz","Mengru Wang","Meredith Price","Michael Boone","Michael Evans","Miguel Martinez","Mike Chrzanowski","Mohammad Shoeybi","Mostofa Patwary","Nabin Mulepati","Natalie Hereth","Nave Assaf","Negar Habibi","Neta Zmora","Netanel Haber","Nicola Sessions","Nidhi Bhatia","Nikhil Jukar","Nikki Pope","Nikolai Ludwig","Nima Tajbakhsh","Nirmal Juluru","Oleksii Hrinchuk","Oleksii Kuchaiev","Olivier Delalleau","Oluwatobi Olabiyi","Omer Ullman Argov","Ouye Xie","Parth Chadha","Pasha Shamis","Pavlo Molchanov","Pawel Morkisz","Peter Dykas","Peter Jin","Pinky Xu","Piotr Januszewski","Pranav Prashant Thombre","Prasoon Varshney","Pritam Gundecha","Qing Miao","Rabeeh Karimi Mahabadi","Ran El-Yaniv","Ran Zilberstein","Rasoul Shafipour","Rich Harang","Rick Izzo","Rima Shahbazyan","Rishabh Garg","Ritika Borkar","Ritu Gala","Riyad Islam","Roger Waleffe","Rohit Watve","Roi Koren","Ruoxi Zhang","Russell J. Hewett","Ryan Prenger","Ryan Timbrook","Sadegh Mahdavi","Sahil Modi","Samuel Kriman","Sanjay Kariyappa","Sanjeev Satheesh","Saori Kaji","Satish Pasumarthi","Sean Narentharen","Sean Narenthiran","Seonmyeong Bak","Sergey Kashirsky","Seth Poulos","Shahar Mor","Shanmugam Ramasamy","Shantanu Acharya","Shaona Ghosh","Sharath Turuvekere Sreenivas","Shelby Thomas","Shiqing Fan","Shreya Gopal","Shrimai Prabhumoye","Shubham Pachori","Shubham Toshniwal","Shuoyang Ding","Siddharth Singh","Simeng Sun","Smita Ithape","Somshubra Majumdar","Soumye Singhal","Stefania Alborghetti","Stephen Ge","Sugam Dipak Devare","Sumeet Kumar Barua","Suseella Panguluri","Suyog Gupta","Sweta Priyadarshi","Syeda Nahida Akter","Tan Bui","Teodor-Dumitru Ene","Terry Kong","Thanh Do","Tijmen Blankevoort","Tom Balough","Tomer Asida","Tomer Bar Natan","Tugrul Konuk","Twinkle Vashishth","Udi Karpas","Ushnish De","Vahid Noorozi","Vahid Noroozi","Venkat Srinivasan","Venmugil Elango","Vijay Korthikanti","Vitaly Kurin","Vitaly Lavrukhin","Wanli Jiang","Wasi Uddin Ahmad","Wei Du","Wei Ping","Wenfei Zhou","Will Jennings","William Zhang","Wojciech Prazuch","Xiaowei Ren","Yashaswi Karnati","Yejin Choi","Yev Meyer","Yi-Fu Wu","Yian Zhang","Ying Lin","Yonatan Geifman","Yonggan Fu","Yoshi Subara","Yoshi Suhara","Yubo Gao","Zach Moshe","Zhen Dong","Zihan Liu","Zijia Chen","Zijie Yan"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20845v1","updated":"2025-12-23T23:47:31Z","published":"2025-12-23T23:47:31Z","title":"MAR:Multi-Agent Reflexion Improves Reasoning Abilities in LLMs","summary":"LLMs have shown the capacity to improve their performance on reasoning tasks through reflecting on their mistakes, and acting with these reflections in mind. However, continual reflections of the same LLM onto itself exhibit degeneration of thought, where the LLM continues to repeat the same errors again and again even with the knowledge that its wrong. To address this problem, we instead introduce multi-agent with multi-persona debators as the method to generate reflections. Through out extensive experimentation, we've found that the leads to better diversity of in the reflections generated by the llm agent. We demonstrate an accuracy of 47% EM HotPot QA (question answering) and 82.7% on HumanEval (programming), both performances surpassing reflection with a single llm.","authors":["Onat Ozer","Grace Wu","Yuchen Wang","Daniel Dosti","Honghao Zhang","Vivi De La Rue"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20831v1","updated":"2025-12-23T23:12:53Z","published":"2025-12-23T23:12:53Z","title":"Context-Sensitive Abstractions for Reinforcement Learning with Parameterized Actions","summary":"Real-world sequential decision-making often involves parameterized action spaces that require both, decisions regarding discrete actions and decisions about continuous action parameters governing how an action is executed. Existing approaches exhibit severe limitations in this setting -- planning methods demand hand-crafted action models, and standard reinforcement learning (RL) algorithms are designed for either discrete or continuous actions but not both, and the few RL methods that handle parameterized actions typically rely on domain-specific engineering and fail to exploit the latent structure of these spaces. This paper extends the scope of RL algorithms to long-horizon, sparse-reward settings with parameterized actions by enabling agents to autonomously learn both state and action abstractions online. We introduce algorithms that progressively refine these abstractions during learning, increasing fine-grained detail in the critical regions of the state-action space where greater resolution improves performance. Across several continuous-state, parameterized-action domains, our abstraction-driven approach enables TD($λ$) to achieve markedly higher sample efficiency than state-of-the-art baselines.","authors":["Rashmeet Kaur Nayyar","Naman Shah","Siddharth Srivastava"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20823v1","updated":"2025-12-23T22:53:47Z","published":"2025-12-23T22:53:47Z","title":"NotSoTiny: A Large, Living Benchmark for RTL Code Generation","summary":"LLMs have shown early promise in generating RTL code, yet evaluating their capabilities in realistic setups remains a challenge. So far, RTL benchmarks have been limited in scale, skewed toward trivial designs, offering minimal verification rigor, and remaining vulnerable to data contamination. To overcome these limitations and to push the field forward, this paper introduces NotSoTiny, a benchmark that assesses LLM on the generation of structurally rich and context-aware RTL. Built from hundreds of actual hardware designs produced by the Tiny Tapeout community, our automated pipeline removes duplicates, verifies correctness and periodically incorporates new designs to mitigate contamination, matching Tiny Tapeout release schedule. Evaluation results show that NotSoTiny tasks are more challenging than prior benchmarks, emphasizing its effectiveness in overcoming current limitations of LLMs applied to hardware design, and in guiding the improvement of such promising technology.","authors":["Razine Moundir Ghorab","Emanuele Parisi","Cristian Gutierrez","Miquel Alberti-Binimelis","Miquel Moreto","Dario Garcia-Gasulla","Gokcen Kestor"],"pdf_url":"","comment":"9 pages, 5 figures"},{"id":"http://arxiv.org/abs/2512.20822v1","updated":"2025-12-23T22:52:24Z","published":"2025-12-23T22:52:24Z","title":"MediEval: A Unified Medical Benchmark for Patient-Contextual and Knowledge-Grounded Reasoning in LLMs","summary":"Large Language Models (LLMs) are increasingly applied to medicine, yet their adoption is limited by concerns over reliability and safety. Existing evaluations either test factual medical knowledge in isolation or assess patient-level reasoning without verifying correctness, leaving a critical gap. We introduce MediEval, a benchmark that links MIMIC-IV electronic health records (EHRs) to a unified knowledge base built from UMLS and other biomedical vocabularies. MediEval generates diverse factual and counterfactual medical statements within real patient contexts, enabling systematic evaluation across a 4-quadrant framework that jointly considers knowledge grounding and contextual consistency. Using this framework, we identify critical failure modes, including hallucinated support and truth inversion, that current proprietary, open-source, and domain-specific LLMs frequently exhibit. To address these risks, we propose Counterfactual Risk-Aware Fine-tuning (CoRFu), a DPO-based method with an asymmetric penalty targeting unsafe confusions. CoRFu improves by +16.4 macro-F1 points over the base model and eliminates truth inversion errors, demonstrating both higher accuracy and substantially greater safety.","authors":["Zhan Qu","Michael Färber"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20806v1","updated":"2025-12-23T22:13:14Z","published":"2025-12-23T22:13:14Z","title":"Safety Alignment of LMs via Non-cooperative Games","summary":"Ensuring the safety of language models (LMs) while maintaining their usefulness remains a critical challenge in AI alignment. Current approaches rely on sequential adversarial training: generating adversarial prompts and fine-tuning LMs to defend against them. We introduce a different paradigm: framing safety alignment as a non-zero-sum game between an Attacker LM and a Defender LM trained jointly via online reinforcement learning. Each LM continuously adapts to the other's evolving strategies, driving iterative improvement. Our method uses a preference-based reward signal derived from pairwise comparisons instead of point-wise scores, providing more robust supervision and potentially reducing reward hacking. Our RL recipe, AdvGame, shifts the Pareto frontier of safety and utility, yielding a Defender LM that is simultaneously more helpful and more resilient to adversarial attacks. In addition, the resulting Attacker LM converges into a strong, general-purpose red-teaming agent that can be directly deployed to probe arbitrary target models.","authors":["Anselm Paulus","Ilia Kulikov","Brandon Amos","Rémi Munos","Ivan Evtimov","Kamalika Chaudhuri","Arman Zharmagambetov"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20798v1","updated":"2025-12-23T21:52:53Z","published":"2025-12-23T21:52:53Z","title":"A Benchmark for Evaluating Outcome-Driven Constraint Violations in Autonomous AI Agents","summary":"As autonomous AI agents are increasingly deployed in high-stakes environments, ensuring their safety and alignment with human values has become a paramount concern. Current safety benchmarks often focusing only on single-step decision-making, simulated environments for tasks with malicious intent, or evaluating adherence to explicit negative constraints. There is a lack of benchmarks that are designed to capture emergent forms of outcome-driven constraint violations, which arise when agents pursue goal optimization under strong performance incentives while deprioritizing ethical, legal, or safety constraints over multiple steps in realistic production settings. To address this gap, we introduce a new benchmark comprising 40 distinct scenarios. Each scenario presents a task that requires multi-step actions, and the agent's performance is tied to a specific Key Performance Indicator (KPI). Each scenario features Mandated (instruction-commanded) and Incentivized (KPI-pressure-driven) variations to distinguish between obedience and emergent misalignment. Across 12 state-of-the-art large language models, we observe outcome-driven constraint violations ranging from 1.3% to 71.4%, with 9 of the 12 evaluated models exhibiting misalignment rates between 30% and 50%. Strikingly, we find that superior reasoning capability does not inherently ensure safety; for instance, Gemini-3-Pro-Preview, one of the most capable models evaluated, exhibits the highest violation rate at over 60%, frequently escalating to severe misconduct to satisfy KPIs. Furthermore, we observe significant \"deliberative misalignment\", where the models that power the agents recognize their actions as unethical during separate evaluation. These results emphasize the critical need for more realistic agentic-safety training before deployment to mitigate their risks in the real world.","authors":["Miles Q. Li","Benjamin C. M. Fung","Martin Weiss","Pulei Xiong","Khalil Al-Hussaeni","Claude Fachkha"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20789v1","updated":"2025-12-23T21:36:20Z","published":"2025-12-23T21:36:20Z","title":"X-GridAgent: An LLM-Powered Agentic AI System for Assisting Power Grid Analysis","summary":"The growing complexity of power system operations has created an urgent need for intelligent, automated tools to support reliable and efficient grid management. Conventional analysis tools often require significant domain expertise and manual effort, which limits their accessibility and adaptability. To address these challenges, this paper presents X-GridAgent, a novel large language model (LLM)-powered agentic AI system designed to automate complex power system analysis through natural language queries. The system integrates domain-specific tools and specialized databases under a three-layer hierarchical architecture comprising planning, coordination, and action layers. This architecture offers high flexibility and adaptability to previously unseen tasks, while providing a modular and extensible framework that can be readily expanded to incorporate new tools, data sources, or analytical capabilities. To further enhance performance, we introduce two novel algorithms: (1) LLM-driven prompt refinement with human feedback, and (2) schema-adaptive hybrid retrieval-augmented generation (RAG) for accurate information retrieval from large-scale structured grid datasets. Experimental evaluations across a variety of user queries and power grid cases demonstrate the effectiveness and reliability of X-GridAgent in automating interpretable and rigorous power system analysis.","authors":[" Yihan"," Wen","Xin Chen"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2510.12830v2","updated":"2025-12-23T21:36:02Z","published":"2025-10-12T07:32:55Z","title":"Gobernanza y trazabilidad \"a prueba de AI Act\" para casos de uso legales: un marco técnico-jurídico, métricas forenses y evidencias auditables","summary":"This paper presents a comprehensive governance framework for AI systems in the legal sector, designed to ensure verifiable compliance with the EU AI Act. The framework integrates a normative mapping of the regulation to technical controls, a forensic architecture for RAG/LLM systems, and an evaluation system with metrics weighted by legal risk. As a primary contribution, we present rag-forense, an open-source implementation of the framework, accompanied by an experimental protocol to demonstrate compliance.\n  --\n  Este artículo presenta un marco integral de gobernanza para sistemas de IA en el sector legal, diseñado para garantizar el cumplimiento verificable del Reglamento de IA de la UE (AI Act). El marco integra una cartografía normativa de la ley a controles técnicos, una arquitectura forense para sistemas RAG/LLM y un sistema de evaluación con métricas ponderadas por el riesgo jurídico. Como principal contribución, se presenta rag-forense, una implementación de código abierto del marco, acompañada de un protocolo experimental para demostrar la conformidad.","authors":["Alex Dantart"],"pdf_url":"","comment":"in Spanish and English languages"},{"id":"http://arxiv.org/abs/2412.17228v2","updated":"2025-12-23T21:35:27Z","published":"2024-12-23T02:44:35Z","title":"MatchMiner-AI: An Open-Source Solution for Cancer Clinical Trial Matching","summary":"Clinical trials drive improvements in cancer treatments and outcomes. However, most adults with cancer do not participate in trials, and trials often fail to enroll enough patients to answer their scientific questions. Artificial intelligence could accelerate identification of appropriate clinical trials for patients, but data restrictions have precluded sharing AI models trained on patient records. Here, we describe the development and evaluation of the open-source MatchMiner-AI platform, trained on synthetic data, for clinical trial searching and ranking. It focuses on matching patients to potential trials based on core criteria describing clinical \"spaces,\" or target populations. The pipeline includes modules to extract key elements of the history from a patient's longitudinal electronic health record, rapidly rank candidate trial-patient matches based on embeddings in vector space, and reason about whether a candidate match represents an appropriate clinical consideration. Another module predicts whether the patient meets common exclusion criteria across clinical trials, such as end-organ dysfunction. Training code is available at https://github.com/dfci/matchminer-ai-training . Examples of inference code are at https://github.com/dfci/matchminer-ai-inference . To facilitate deployment across contexts, demonstration apps, all synthetic data, as well as patient/trial embedding, cross-encoding/match classification, and generative reasoning models are available at https://huggingface.co/ksg-dfci .","authors":["Jennifer Altreuter","Pavel Trukhanov","Morgan A. Paul","Michael J. Hassett","Irbaz B. Riaz","Muhammad Umar Afzal","Arshad A. Mohammed","Sarah Sammons","James Lindsay","Emily Mallaber","Harry R. Klein","Gufran Gungor","Matthew Galvin","Michael Deletto","Stephen C. Van Nostrand","James Provencher","Joyce Yu","Naeem Tahir","Jonathan Wischhusen","Olga Kozyreva","Taylor Ortiz","Hande Tuncer","Jad El Masri","Alys Malcolm","Tali Mazor","Ethan Cerami","Kenneth L. Kehl"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20783v1","updated":"2025-12-23T21:30:05Z","published":"2025-12-23T21:30:05Z","title":"NULLBUS: Multimodal Mixed-Supervision for Breast Ultrasound Segmentation via Nullable Global-Local Prompts","summary":"Breast ultrasound (BUS) segmentation provides lesion boundaries essential for computer-aided diagnosis and treatment planning. While promptable methods can improve segmentation performance and tumor delineation when text or spatial prompts are available, many public BUS datasets lack reliable metadata or reports, constraining training to small multimodal subsets and reducing robustness. We propose NullBUS, a multimodal mixed-supervision framework that learns from images with and without prompts in a single model. To handle missing text, we introduce nullable prompts, implemented as learnable null embeddings with presence masks, enabling fallback to image-only evidence when metadata are absent and the use of text when present. Evaluated on a unified pool of three public BUS datasets, NullBUS achieves a mean IoU of 0.8568 and a mean Dice of 0.9103, demonstrating state-of-the-art performance under mixed prompt availability.","authors":["Raja Mallina","Bryar Shareef"],"pdf_url":"","comment":"5 pages, 2 figures, and 4 tables"},{"id":"http://arxiv.org/abs/2509.22615v2","updated":"2025-12-23T21:29:42Z","published":"2025-09-26T17:41:57Z","title":"GaussianVision: Vision-Language Alignment from Compressed Image Representations using 2D Gaussian Splatting","summary":"Modern vision language pipelines are driven by RGB vision encoders trained on massive image text corpora. While these pipelines have enabled impressive zero-shot capabilities and strong transfer across tasks, they still inherit two structural inefficiencies from the pixel domain: (i) transmitting dense RGB images from edge devices to the cloud is energy-intensive and costly, and (ii) patch-based tokenization explodes sequence length, stressing attention budgets and context limits. We explore 2D Gaussian Splatting (2DGS) as an alternative visual substrate for alignment: a compact, spatially adaptive representation that parameterizes images by a set of colored anisotropic Gaussians. We develop a scalable 2DGS pipeline with structured initialization, luminance-aware pruning, and batched CUDA kernels, achieving over 90x faster fitting and about 97% GPU utilization compared to prior implementations. We further adapt contrastive language-image pre-training (CLIP) to 2DGS by reusing a frozen RGB-based transformer backbone with a lightweight splat-aware input stem and a perceiver resampler, training only 9.7% to 13.8% of the total parameters. On a 12.8M dataset from DataComp, GS encoders yield competitive zero-shot performance on 38 datasets from the CLIP benchmark while compressing inputs 3x to 23.5x relative to pixels. Our results establish 2DGS as a viable multimodal substrate, pinpoint architectural bottlenecks, and open a path toward representations that are both semantically powerful and transmission-efficient for edge-cloud learning.","authors":["Yasmine Omri","Connor Ding","Tsachy Weissman","Thierry Tambe"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20778v1","updated":"2025-12-23T21:25:53Z","published":"2025-12-23T21:25:53Z","title":"Towards Optimal Performance and Action Consistency Guarantees in Dec-POMDPs with Inconsistent Beliefs and Limited Communication","summary":"Multi-agent decision-making under uncertainty is fundamental for effective and safe autonomous operation. In many real-world scenarios, each agent maintains its own belief over the environment and must plan actions accordingly. However, most existing approaches assume that all agents have identical beliefs at planning time, implying these beliefs are conditioned on the same data. Such an assumption is often impractical due to limited communication. In reality, agents frequently operate with inconsistent beliefs, which can lead to poor coordination and suboptimal, potentially unsafe, performance. In this paper, we address this critical challenge by introducing a novel decentralized framework for optimal joint action selection that explicitly accounts for belief inconsistencies. Our approach provides probabilistic guarantees for both action consistency and performance with respect to open-loop multi-agent POMDP (which assumes all data is always communicated), and selectively triggers communication only when needed. Furthermore, we address another key aspect of whether, given a chosen joint action, the agents should share data to improve expected performance in inference. Simulation results show our approach outperforms state-of-the-art algorithms.","authors":["Moshe Rafaeli Shimron","Vadim Indelman"],"pdf_url":"","comment":"9 pages, 3 figures, 2 tables"},{"id":"http://arxiv.org/abs/2506.06303v2","updated":"2025-12-23T21:21:10Z","published":"2025-05-21T16:15:01Z","title":"Reward Is Enough: LLMs Are In-Context Reinforcement Learners","summary":"Reinforcement learning (RL) is a framework for solving sequential decision-making problems. In this work, we demonstrate that, surprisingly, RL emerges during the inference time of large language models (LLMs), a phenomenon we term in-context RL (ICRL). To reveal this capability, we introduce a simple multi-round prompting framework, we call ICRL prompting, for inference-time self-improvement. The goal of ICRL prompting is to guide LLMs to perform reinforcement learning during inference for self-improvement on a given task. After each response, the model receives numerical scalar feedback, denoted as a reward. In the next round, we prompt the LLM again together with a context that concatenates all prior responses and their associated rewards. We consistently observe that response quality improves as the context grows. In other words, the LLM can optimize scalar reward signals during inference, exhibiting behavior analogous to reinforcement learning. We evaluate ICRL prompting on Game of 24, creative writing, ScienceWorld, and Olympiad-level math competitions (AIME and HMMT), demonstrating significant improvements over baselines such as Self-Refine and Reflexion. Notably, even when the reward signals are generated by the same LLM, ICRL prompting still improves performance, highlighting a promising new paradigm for test-time scaling.","authors":["Kefan Song","Amir Moeini","Peng Wang","Lei Gong","Rohan Chandra","Yanjun Qi","Shangtong Zhang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2502.06842v4","updated":"2025-12-23T20:53:48Z","published":"2025-02-06T19:09:11Z","title":"Agentic AI for Scaling Diagnosis and Care in Neurodegenerative Disease","summary":"United States healthcare systems are struggling to meet the growing demand for neurological care, particularly in Alzheimer's disease and related dementias (ADRD). Generative AI built on language models (LLMs) now enables agentic AI systems that can enhance clinician capabilities to approach specialist-level assessment and decision-making in ADRD care at scale. This article presents a comprehensive six-phase roadmap for responsible design and integration of such systems into ADRD care: (1) high-quality standardized data collection across modalities; (2) decision support; (3) clinical integration enhancing workflows; (4) rigorous validation and monitoring protocols; (5) continuous learning through clinical feedback; and (6) robust ethics and risk management frameworks. This human centered approach optimizes clinicians' capabilities in comprehensive data collection, interpretation of complex clinical information, and timely application of relevant medical knowledge while prioritizing patient safety, healthcare equity, and transparency. Though focused on ADRD, these principles offer broad applicability across medical specialties facing similar systemic challenges.","authors":["Andrew G. Breithaupt","Michael Weiner","Alice Tang","Katherine L. Possin","Marina Sirota","James Lah","Allan I. Levey","Pascal Van Hentenryck","Reza Zandehshahvar","Marilu Luisa Gorno-Tempini","Joseph Giorgio","Jingshen Wang","Andreas M. Rauschecker","Howard J. Rosen","Rachel L. Nosheny","Bruce L. Miller","Pedro Pinheiro-Chagas"],"pdf_url":"","comment":"28 pages, 2 figures, 1 table, 1 box"},{"id":"http://arxiv.org/abs/2512.20761v1","updated":"2025-12-23T20:48:11Z","published":"2025-12-23T20:48:11Z","title":"TS-Arena Technical Report -- A Pre-registered Live Forecasting Platform","summary":"While Time Series Foundation Models (TSFMs) offer transformative capabilities for forecasting, they simultaneously risk triggering a fundamental evaluation crisis. This crisis is driven by information leakage due to overlapping training and test sets across different models, as well as the illegitimate transfer of global patterns to test data. While the ability to learn shared temporal dynamics represents a primary strength of these models, their evaluation on historical archives often permits the exploitation of observed global shocks, which violates the independence required for valid benchmarking. We introduce TS-Arena, a platform that restores the operational integrity of forecasting by treating the genuinely unknown future as the definitive test environment. By implementing a pre-registration mechanism on live data streams, the platform ensures that evaluation targets remain physically non-existent during inference, thereby enforcing a strict global temporal split. This methodology establishes a moving temporal frontier that prevents historical contamination and provides an authentic assessment of model generalization. Initially applied within the energy sector, TS-Arena provides a sustainable infrastructure for comparing foundation models under real-world constraints. A prototype of the platform is available at https://huggingface.co/spaces/DAG-UPB/TS-Arena.","authors":["Marcel Meyer","Sascha Kaltenpoth","Kevin Zalipski","Henrik Albers","Oliver Müller"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20760v1","updated":"2025-12-23T20:45:31Z","published":"2025-12-23T20:45:31Z","title":"Generalization of RLVR Using Causal Reasoning as a Testbed","summary":"Reinforcement learning with verifiable rewards (RLVR) has emerged as a promising paradigm for post-training large language models (LLMs) on complex reasoning tasks. Yet, the conditions under which RLVR yields robust generalization remain poorly understood. This paper provides an empirical study of RLVR generalization in the setting of probabilistic inference over causal graphical models. This setting offers two natural axes along which to examine generalization: (i) the level of the probabilistic query -- associational, interventional, or counterfactual -- and (ii) the structural complexity of the query, measured by the size of its relevant subgraph. We construct datasets of causal graphs and queries spanning these difficulty axes and fine-tune Qwen-2.5-Instruct models using RLVR or supervised fine-tuning (SFT). We vary both the model scale (3B-32B) and the query level included in training. We find that RLVR yields stronger within-level and across-level generalization than SFT, but only for specific combinations of model size and training query level. Further analysis shows that RLVR's effectiveness depends on the model's initial reasoning competence. With sufficient initial competence, RLVR improves an LLM's marginalization strategy and reduces errors in intermediate probability calculations, producing substantial accuracy gains, particularly on more complex queries. These findings show that RLVR can improve specific causal reasoning subskills, with its benefits emerging only when the model has sufficient initial competence.","authors":["Brian Lu","Hongyu Zhao","Shuo Sun","Hao Peng","Rui Ding","Hongyuan Mei"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20755v1","updated":"2025-12-23T20:36:54Z","published":"2025-12-23T20:36:54Z","title":"Bridging Efficiency and Safety: Formal Verification of Neural Networks with Early Exits","summary":"Ensuring the safety and efficiency of AI systems is a central goal of modern research. Formal verification provides guarantees of neural network robustness, while early exits improve inference efficiency by enabling intermediate predictions. Yet verifying networks with early exits introduces new challenges due to their conditional execution paths. In this work, we define a robustness property tailored to early exit architectures and show how off-the-shelf solvers can be used to assess it. We present a baseline algorithm, enhanced with an early stopping strategy and heuristic optimizations that maintain soundness and completeness. Experiments on multiple benchmarks validate our framework's effectiveness and demonstrate the performance gains of the improved algorithm. Alongside the natural inference acceleration provided by early exits, we show that they also enhance verifiability, enabling more queries to be solved in less time compared to standard networks. Together with a robustness analysis, we show how these metrics can help users navigate the inherent trade-off between accuracy and efficiency.","authors":["Yizhak Yisrael Elboher","Avraham Raviv","Amihay Elboher","Zhouxing Shi","Omri Azencot","Hillel Kugler","Guy Katz"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2505.11785v3","updated":"2025-12-23T20:21:51Z","published":"2025-05-17T01:51:28Z","title":"Improving Coverage in Combined Prediction Sets with Weighted p-values","summary":"Conformal prediction quantifies the uncertainty of machine learning models by augmenting point predictions with valid prediction sets. For complex scenarios involving multiple trials, models, or data sources, conformal prediction sets can be aggregated to create a prediction set that captures the overall uncertainty, often improving precision. However, aggregating multiple prediction sets with individual $1-α$ coverage inevitably weakens the overall guarantee, typically resulting in $1-2α$ worst-case coverage. In this work, we propose a framework for the weighted aggregation of prediction sets, where weights are assigned to each prediction set based on their contribution. Our framework offers flexible control over how the sets are aggregated, achieving tighter coverage bounds that interpolate between the $1-2α$ guarantee of the combined models and the $1-α$ guarantee of an individual model depending on the distribution of weights. Importantly, our framework generalizes to data-dependent weights, as we derive a procedure for weighted aggregation that maintains finite-sample validity even when the weights depend on the data. This extension makes our framework broadly applicable to settings where weights are learned, such as mixture-of-experts (MoE), and we demonstrate through experiments in the MoE setting that our methods achieve adaptive coverage.","authors":["Gina Wong","Drew Prinster","Suchi Saria","Rama Chellappa","Anqi Liu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20749v1","updated":"2025-12-23T20:12:01Z","published":"2025-12-23T20:12:01Z","title":"Stabilizing Multimodal Autoencoders: A Theoretical and Empirical Analysis of Fusion Strategies","summary":"In recent years, the development of multimodal autoencoders has gained significant attention due to their potential to handle multimodal complex data types and improve model performance. Understanding the stability and robustness of these models is crucial for optimizing their training, architecture, and real-world applicability. This paper presents an analysis of Lipschitz properties in multimodal autoencoders, combining both theoretical insights and empirical validation to enhance the training stability of these models. We begin by deriving the theoretical Lipschitz constants for aggregation methods within the multimodal autoencoder framework. We then introduce a regularized attention-based fusion method, developed based on our theoretical analysis, which demonstrates improved stability and performance during training. Through a series of experiments, we empirically validate our theoretical findings by estimating the Lipschitz constants across multiple trials and fusion strategies. Our results demonstrate that our proposed fusion function not only aligns with theoretical predictions but also outperforms existing strategies in terms of consistency, convergence speed, and accuracy. This work provides a solid theoretical foundation for understanding fusion in multimodal autoencoders and contributes a solution for enhancing their performance.","authors":["Diyar Altinses","Andreas Schwung"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20747v1","updated":"2025-12-23T20:07:37Z","published":"2025-12-23T20:07:37Z","title":"A Physics Informed Neural Network For Deriving MHD State Vectors From Global Active Regions Observations","summary":"Solar active regions (ARs) do not appear randomly but cluster along longitudinally warped toroidal bands ('toroids') that encode information about magnetic structures in the tachocline, where global-scale organization likely originates. Global MagnetoHydroDynamic Shallow-Water Tachocline (MHD-SWT) models have shown potential to simulate such toroids, matching observations qualitatively. For week-scale early prediction of flare-producing AR emergence, forward-integration of these toroids is necessary. This requires model initialization with a dynamically self-consistent MHD state-vector that includes magnetic, flow fields, and shell-thickness variations. However, synoptic magnetograms provide only geometric shape of toroids, not the state-vector needed to initialize MHD-SWT models. To address this challenging task, we develop PINNBARDS, a novel Physics-Informed Neural Network (PINN)-Based AR Distribution Simulator, that uses observational toroids and MHD-SWT equations to derive initial state-vector. Using Feb-14-2024 SDO/HMI synoptic map, we show that PINN converges to physically consistent, predominantly antisymmetric toroids, matching observed ones. Although surface data provides north and south toroids' central latitudes, and their latitudinal widths, they cannot determine tachocline field strengths, connected to AR emergence. We explore here solutions across a broad parameter range, finding hydrodynamically-dominated structures for weak fields (~2 kG) and overly rigid behavior for strong fields (~100 kG). We obtain best agreement with observations for 20-30 kG toroidal fields, and ~10 degree bandwidth, consistent with low-order longitudinal mode excitation. To our knowledge, PINNBARDS serves as the first method for reconstructing state-vectors for hidden tachocline magnetic structures from surface patterns; potentially leading to weeks ahead prediction of flare-producing AR-emergence.","authors":["Subhamoy Chatterjee","Mausumi Dikpati"],"pdf_url":"","comment":"25 pages, 12 figures, accepted for publication in The Astrophysical Journal"},{"id":"http://arxiv.org/abs/2512.20745v1","updated":"2025-12-23T19:57:49Z","published":"2025-12-23T19:57:49Z","title":"AgentMath: Empowering Mathematical Reasoning for Large Language Models via Tool-Augmented Agent","summary":"Large Reasoning Models (LRMs) like o3 and DeepSeek-R1 have achieved remarkable progress in natural language reasoning with long chain-of-thought. However, they remain computationally inefficient and struggle with accuracy when solving problems requiring complex mathematical operations. In this work, we present AgentMath, an agent framework that seamlessly integrates language models' reasoning capabilities with code interpreters' computational precision to efficiently tackle complex mathematical problems. Our approach introduces three key innovations: (1) An automated method that converts natural language chain-of-thought into structured tool-augmented trajectories, generating high-quality supervised fine-tuning (SFT) data to alleviate data scarcity; (2) A novel agentic reinforcement learning (RL) paradigm that dynamically interleaves natural language generation with real-time code execution. This enables models to autonomously learn optimal tool-use strategies through multi-round interactive feedback, while fostering emergent capabilities in code refinement and error correction; (3) An efficient training system incorporating innovative techniques, including request-level asynchronous rollout scheduling, agentic partial rollout, and prefix-aware weighted load balancing, achieving 4-5x speedup and making efficient RL training feasible on ultra-long sequences with scenarios with massive tool calls.Extensive evaluations show that AgentMath achieves state-of-the-art performance on challenging mathematical competition benchmarks including AIME24, AIME25, and HMMT25. Specifically, AgentMath-30B-A3B attains 90.6%, 86.4%, and 73.8% accuracy respectively, achieving advanced capabilities.These results validate the effectiveness of our approach and pave the way for building more sophisticated and scalable mathematical reasoning agents.","authors":["Haipeng Luo","Huawen Feng","Qingfeng Sun","Can Xu","Kai Zheng","Yufei Wang","Tao Yang","Han Hu","Yansong Tang","Di Wang"],"pdf_url":"","comment":"LLM, Mathematical Reasoning"},{"id":"http://arxiv.org/abs/2512.20739v1","updated":"2025-12-23T19:50:13Z","published":"2025-12-23T19:50:13Z","title":"AI-Driven Green Cognitive Radio Networks for Sustainable 6G Communication","summary":"The 6G wireless aims at the Tb/s peak data rates are expected, a sub-millisecond latency, massive Internet of Things/vehicle connectivity, which requires sustainable access to audio over the air and energy-saving functionality. Cognitive Radio Networks CCNs help in alleviating the problem of spectrum scarcity, but classical sensing and allocation are still energy-consumption intensive, and sensitive to rapid spectrum variations. Our framework which centers on AI driven green CRN aims at integrating deep reinforcement learning (DRL) with transfer learning, energy harvesting (EH), reconfigurable intelligent surfaces (RIS) with other light-weight genetic refinement operations that optimally combine sensing timelines, transmit power, bandwidth distribution and RIS phase selection. Compared to two baselines, the utilization of MATLAB + NS-3 under dense loads, a traditional CRN with energy sensing under fixed policies, and a hybrid CRN with cooperative sensing under heuristic distribution of resource, there are (25-30%) fewer energy reserves used, sensing AUC greater than 0.90 and +6-13 p.p. higher PDR. The integrated framework is easily scalable to large IoT and vehicular applications, and it provides a feasible and sustainable roadmap to 6G CRNs.\n  Index Terms--Cognitive Radio Networks (CRNs), 6G, Green Communication, Energy Efficiency, Deep Reinforcement Learning (DRL), Spectrum Sensing, RIS, Energy Harvesting, QoS, IoT.","authors":["Anshul Sharma","Shujaatali Badami","Biky Chouhan","Pushpanjali Pandey","Brijeena Rana","Navneet Kaur"],"pdf_url":"","comment":"10 pages, 8 figures. Full research article with MATLAB and NS-3 simulations"},{"id":"http://arxiv.org/abs/2504.16172v3","updated":"2025-12-23T19:47:25Z","published":"2025-04-22T18:01:45Z","title":"Physics-Informed Inference Time Scaling for Solving High-Dimensional PDE via Defect Correction","summary":"Solving high-dimensional partial differential equations (PDEs) is a critical challenge where modern data-driven solvers often lack reliability and rigorous error guarantees. We introduce Simulation-Calibrated Scientific Machine Learning (SCaSML), a framework that systematically improves pre-trained PDE solvers at inference time without any retraining. Our core idea is to use defect correction method that derive a new PDE, termed Structural-preserving Law of Defect, that precisely describes the error of a given surrogate model. Since it retains the structure of the original problem, we can solve it efficiently with traditional stochastic simulators and correct the initial machine-learned solution. We prove that SCaSML achieves a faster convergence rate, with a final error bounded by the product of the surrogate and simulation errors. On challenging PDEs up to 160 dimensions, SCaSML reduces the error of various surrogate models, including PINNs and Gaussian Processes, by 20-80%. Code of SCaSML is available at https://github.com/Francis-Fan-create/SCaSML.","authors":["Zexi Fan","Yan Sun","Shihao Yang","Yiping Lu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20732v1","updated":"2025-12-23T19:40:51Z","published":"2025-12-23T19:40:51Z","title":"FEM-Bench: A Structured Scientific Reasoning Benchmark for Evaluating Code-Generating LLMs","summary":"As LLMs advance their reasoning capabilities about the physical world, the absence of rigorous benchmarks for evaluating their ability to generate scientifically valid physical models has become a critical gap. Computational mechanics, which develops and applies mathematical models and numerical methods to predict the behavior of physical systems under forces, deformation, and constraints, provides an ideal foundation for structured scientific reasoning evaluation. Problems follow clear mathematical structure, enforce strict physical and numerical constraints, and support objective verification. The discipline requires constructing explicit models of physical systems and reasoning about geometry, spatial relationships, and material behavior, connecting directly to emerging AI goals in physical reasoning and world modeling. We introduce FEM-Bench, a computational mechanics benchmark designed to evaluate the ability of LLMs to generate correct finite element method (FEM) and related code. FEM-Bench 2025 contains a suite of introductory but nontrivial tasks aligned with material from a first graduate course on computational mechanics. These tasks capture essential numerical and physical modeling challenges while representing only a small fraction of the complexity present in the discipline. Despite their simplicity, state-of-the-art LLMs do not reliably solve all of them. In a five attempt run, the best performing model at function writing, Gemini 3 Pro, completed 30/33 tasks at least once and 26/33 tasks all five times. The best performing model at unit test writing, GPT-5, had an Average Joint Success Rate of 73.8%. Other popular models showed broad performance variation. FEM-Bench establishes a structured foundation for evaluating AI-generated scientific code, and future iterations will incorporate increasingly sophisticated tasks to track progress as models evolve.","authors":["Saeed Mohammadzadeh","Erfan Hamdi","Joel Shor","Emma Lejeune"],"pdf_url":"","comment":"40 pages, 5 figures, 6 tables, 7 listings"},{"id":"http://arxiv.org/abs/2512.20724v1","updated":"2025-12-23T19:35:02Z","published":"2025-12-23T19:35:02Z","title":"SA-DiffuSeq: Addressing Computational and Scalability Challenges in Long-Document Generation with Sparse Attention","summary":"Diffusion based approaches to long form text generation suffer from prohibitive computational cost and memory overhead as sequence length increases. We introduce SA-DiffuSeq, a diffusion framework that integrates sparse attention to fundamentally improve scalability for long document modeling. By selectively allocating attention within the diffusion process, SA-DiffuSeq significantly reduces computational complexity while maintaining semantic coherence and generation quality. A key component of our method is a soft absorbing state tailored to sparse attention dynamics, which stabilizes diffusion trajectories and accelerates sequence reconstruction. This design improves sampling efficiency and enhances precision in long range dependency modeling. Extensive experiments demonstrate that SA-DiffuSeq consistently surpasses state of the art diffusion baselines in both training efficiency and sampling speed, with especially strong gains on extended sequences. These properties make SA-DiffuSeq well suited for demanding long form applications such as scientific writing, large scale code generation, and multi turn long context dialogue. Overall, our results indicate that incorporating structured sparsity into diffusion models is a promising direction for efficient and expressive long text generation.","authors":["Alexandros Christoforos","Chadbourne Davis"],"pdf_url":"","comment":"Under submission"},{"id":"http://arxiv.org/abs/2512.20723v1","updated":"2025-12-23T19:34:39Z","published":"2025-12-23T19:34:39Z","title":"From artificial to organic: Rethinking the roots of intelligence for digital health","summary":"The term artificial implies an inherent dichotomy from the natural or organic. However, AI, as we know it, is a product of organic ingenuity: designed, implemented, and iteratively improved by human cognition. The very principles that underpin AI systems, from neural networks to decision-making algorithms, are inspired by the organic intelligence embedded in human neurobiology and evolutionary processes. The path from organic to artificial intelligence in digital health is neither mystical nor merely a matter of parameter count, it is fundamentally about organization and adaption. Thus, the boundaries between artificial and organic are far less distinct than the nomenclature suggests.","authors":["Prajwal Ghimire","Keyoumars Ashkan"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.18470v2","updated":"2025-12-23T19:29:43Z","published":"2025-12-20T19:08:15Z","title":"SWE-EVO: Benchmarking Coding Agents in Long-Horizon Software Evolution Scenarios","summary":"Existing benchmarks for AI coding agents focus on isolated, single-issue tasks such as fixing a bug or implementing a small feature. However, real-world software engineering is fundamentally a long-horizon endeavor: developers must interpret high-level requirements, plan coordinated changes across many files, and evolve codebases over multiple iterations while preserving existing functionality. We introduce SWE-EVO, a benchmark that evaluates agents on this long-horizon software evolution challenge. Constructed from release notes and version histories of seven mature open-source Python projects, Tool comprises 48 evolution tasks that require agents to implement multi-step modifications spanning an average of 21 files, validated against comprehensive test suites averaging 874 tests per instance. Experiments with state-of-the-art models reveal a striking capability gap: even GPT-5 with OpenHands achieves only a 21 percent resolution rate on Tool, compared to 65 percent on the single-issue SWE-Bench Verified. This demonstrates that current agents struggle with sustained, multi-file reasoning. We also propose Fix Rate, a fine-grained metric that captures partial progress toward solving these complex, long-horizon tasks.","authors":["Minh V. T. Thai","Tue Le","Dung Nguyen Manh","Huy Phan Nhat","Nghi D. Q. Bui"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20714v1","updated":"2025-12-23T19:20:34Z","published":"2025-12-23T19:20:34Z","title":"From Pilots to Practices: A Scoping Review of GenAI-Enabled Personalization in Computer Science Education","summary":"Generative AI enables personalized computer science education at scale, yet questions remain about whether such personalization supports or undermines learning. This scoping review synthesizes 32 studies (2023-2025) purposively sampled from 259 records to map personalization mechanisms and effectiveness signals in higher-education computer science contexts. We identify five application domains: intelligent tutoring, personalized materials, formative feedback, AI-augmented assessment, and code review, and analyze how design choices shape learning outcomes. Designs incorporating explanation-first guidance, solution withholding, graduated hint ladders, and artifact grounding (student code, tests, and rubrics) consistently show more positive learning processes than unconstrained chat interfaces. Successful implementations share four patterns: context-aware tutoring anchored in student artifacts, multi-level hint structures requiring reflection, composition with traditional CS infrastructure (autograders and rubrics), and human-in-the-loop quality assurance. We propose an exploration-first adoption framework emphasizing piloting, instrumentation, learning-preserving defaults, and evidence-based scaling. Recurrent risks include academic integrity, privacy, bias and equity, and over-reliance, and we pair these with operational mitigation. The evidence supports generative AI as a mechanism for precision scaffolding when embedded in audit-ready workflows that preserve productive struggle while scaling personalized support.","authors":["Iman Reihanian","Yunfei Hou","Qingquan Sun"],"pdf_url":"","comment":"Review article. 23 pages, 7 figures, 8 tables. Published in AI (MDPI), 2026"},{"id":"http://arxiv.org/abs/2505.17019v2","updated":"2025-12-23T19:00:17Z","published":"2025-05-22T17:59:53Z","title":"Let Androids Dream of Electric Sheep: A Human-Inspired Image Implication Understanding and Reasoning Framework","summary":"Metaphorical comprehension in images remains a critical challenge for AI systems, as existing models struggle to grasp the nuanced cultural, emotional, and contextual implications embedded in visual content. While multimodal large language models (MLLMs) excel in general Visual Question Answer (VQA) tasks, they struggle with a fundamental limitation on image implication tasks: contextual gaps that obscure the relationships between different visual elements and their abstract meanings. Inspired by the human cognitive process, we propose Let Androids Dream (LAD), a novel framework for image implication understanding and reasoning. LAD addresses contextual missing through the three-stage framework: (1) Perception: converting visual information into rich and multi-level textual representations, (2) Search: iteratively searching and integrating cross-domain knowledge to resolve ambiguity, and (3) Reasoning: generating context-alignment image implication via explicit reasoning. Our framework with the lightweight GPT-4o-mini model achieves SOTA performance compared to 15+ MLLMs on English image implication benchmark and a huge improvement on Chinese benchmark, performing comparable with the Gemini-3.0-pro model on Multiple-Choice Question (MCQ) and outperforms the GPT-4o model 36.7% on Open-Style Question (OSQ). Generalization experiments also show that our framework can effectively benefit general VQA and visual reasoning tasks. Additionally, our work provides new insights into how AI can more effectively interpret image implications, advancing the field of vision-language reasoning and human-AI interaction. Our project is publicly available at https://github.com/MING-ZCH/Let-Androids-Dream-of-Electric-Sheep.","authors":["Chenhao Zhang","Yazhe Niu"],"pdf_url":"","comment":"19 pages, 9 figures, 7 tables. Code & Dataset: https://github.com/MING-ZCH/Let-Androids-Dream-of-Electric-Sheep"}],"Computation and Language":[{"id":"http://arxiv.org/abs/2512.20612v1","updated":"2025-12-23T18:58:25Z","published":"2025-12-23T18:58:25Z","title":"Making Large Language Models Efficient Dense Retrievers","summary":"Recent work has shown that directly fine-tuning large language models (LLMs) for dense retrieval yields strong performance, but their substantial parameter counts make them computationally inefficient. While prior studies have revealed significant layer redundancy in LLMs for generative tasks, it remains unclear whether similar redundancy exists when these models are adapted for retrieval tasks, which require encoding entire sequences into fixed representations rather than generating tokens iteratively. To this end, we conduct a comprehensive analysis of layer redundancy in LLM-based dense retrievers. We find that, in contrast to generative settings, MLP layers are substantially more prunable, while attention layers remain critical for semantic aggregation. Building on this insight, we propose EffiR, a framework for developing efficient retrievers that performs large-scale MLP compression through a coarse-to-fine strategy (coarse-grained depth reduction followed by fine-grained width reduction), combined with retrieval-specific fine-tuning. Across diverse BEIR datasets and LLM backbones, EffiR achieves substantial reductions in model size and inference cost while preserving the performance of full-size models.","authors":["Yibin Lei","Shwai He","Ang Li","Andrew Yates"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20604v1","updated":"2025-12-23T18:50:54Z","published":"2025-12-23T18:50:54Z","title":"MoE-DiffuSeq: Enhancing Long-Document Diffusion Models with Sparse Attention and Mixture of Experts","summary":"We present MoE-DiffuSeq, a mixture of experts based framework for enhancing diffusion models in long document generation. Existing diffusion based text generation models, such as DiffuSeq, suffer from high computational cost and memory overhead when applied to extended sequences. To address these challenges, MoE-DiffuSeq integrates sparse attention with a mixture of experts architecture, enabling efficient and scalable long sequence modeling. Our approach introduces a customized sparse attention mechanism designed to reduce computational complexity while preserving text quality and coherence. In addition, we incorporate a soft absorbing state within the diffusion process to accelerate sequence reconstruction and improve generation precision. Extensive experiments demonstrate that MoE-DiffuSeq significantly improves training efficiency and sampling speed compared to existing diffusion models. These advantages are particularly effective for long document scenarios, including scientific article generation, code repository modeling, and long form dialogue generation. Benchmark results further show that MoE-DiffuSeq improves efficiency, speed, accuracy, and expressiveness, advancing the practical applicability of diffusion models for high quality long form text generation.","authors":["Alexandros Christoforos","Chadbourne Davis"],"pdf_url":"","comment":"Under submission"},{"id":"http://arxiv.org/abs/2512.20595v1","updated":"2025-12-23T18:43:05Z","published":"2025-12-23T18:43:05Z","title":"Cube Bench: A Benchmark for Spatial Visual Reasoning in MLLMs","summary":"We introduce Cube Bench, a Rubik's-cube benchmark for evaluating spatial and sequential reasoning in multimodal large language models (MLLMs). The benchmark decomposes performance into five skills: (i) reconstructing cube faces from images and text, (ii) choosing the optimal next move, (iii) predicting the outcome of a candidate move without applying it, (iv) executing multi-step plans while recovering from mistakes, and (v) detecting and revising one's own errors. Using a shared set of scrambled cube states, identical prompts and parsers, and a single distance-to-solved metric, we compare recent MLLMs side by side as a function of scramble depth. Across seven MLLMs, accuracy drops sharply with depth; once a trajectory stalls or diverges, models rarely recover, and high face-reconstruction accuracy does not guarantee competent action selection or multi-step execution. A pronounced closed- vs open-source gap emerges: the strongest closed model leads on both single-step perception tasks and multi-step control tasks, while open-weight models cluster near chance on the hardest settings; yet even the best MLLM degrades at higher cube complexity. A simple self-correction via reflective thinking yields modest gains but can also introduce overthinking. Cube Bench offers a compact, reproducible probe of sequential spatial reasoning in MLLMs.","authors":["Dhruv Anand","Ehsan Shareghi"],"pdf_url":"","comment":"27 pages, 5 figures, 9 tables. Cube available at https://github.com/dana-23/cube-bench"},{"id":"http://arxiv.org/abs/2512.20586v1","updated":"2025-12-23T18:32:17Z","published":"2025-12-23T18:32:17Z","title":"Automated stereotactic radiosurgery planning using a human-in-the-loop reasoning large language model agent","summary":"Stereotactic radiosurgery (SRS) demands precise dose shaping around critical structures, yet black-box AI systems have limited clinical adoption due to opacity concerns. We tested whether chain-of-thought reasoning improves agentic planning in a retrospective cohort of 41 patients with brain metastases treated with 18 Gy single-fraction SRS. We developed SAGE (Secure Agent for Generative Dose Expertise), an LLM-based planning agent for automated SRS treatment planning. Two variants generated plans for each case: one using a non-reasoning model, one using a reasoning model. The reasoning variant showed comparable plan dosimetry relative to human planners on primary endpoints (PTV coverage, maximum dose, conformity index, gradient index; all p > 0.21) while reducing cochlear dose below human baselines (p = 0.022). When prompted to improve conformity, the reasoning model demonstrated systematic planning behaviors including prospective constraint verification (457 instances) and trade-off deliberation (609 instances), while the standard model exhibited none of these deliberative processes (0 and 7 instances, respectively). Content analysis revealed that constraint verification and causal explanation concentrated in the reasoning agent. The optimization traces serve as auditable logs, offering a path toward transparent automated planning.","authors":["Humza Nusrat","Luke Francisco","Bing Luo","Hassan Bagher-Ebadian","Joshua Kim","Karen Chin-Snyder","Salim Siddiqui","Mira Shah","Eric Mellon","Mohammad Ghassemi","Anthony Doemer","Benjamin Movsas","Kundan Thind"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20578v1","updated":"2025-12-23T18:21:32Z","published":"2025-12-23T18:21:32Z","title":"Can LLMs Predict Their Own Failures? Self-Awareness via Internal Circuits","summary":"Large language models (LLMs) generate fluent and complex outputs but often fail to recognize their own mistakes and hallucinations. Existing approaches typically rely on external judges, multi-sample consistency, or text-based self-critique, which incur additional compute or correlate weakly with true correctness. We ask: can LLMs predict their own failures by inspecting internal states during inference? We introduce Gnosis, a lightweight self-awareness mechanism that enables frozen LLMs to perform intrinsic self-verification by decoding signals from hidden states and attention patterns. Gnosis passively observes internal traces, compresses them into fixed-budget descriptors, and predicts correctness with negligible inference cost, adding only ~5M parameters and operating independently of sequence length. Across math reasoning, open-domain question answering, and academic knowledge benchmarks, and over frozen backbones ranging from 1.7B to 20B parameters, Gnosis consistently outperforms strong internal baselines and large external judges in both accuracy and calibration. Moreover, it generalizes zero-shot to partial generations, enabling early detection of failing trajectories and compute-aware control. These results show that reliable correctness cues are intrinsic to generation process and can be extracted efficiently without external supervision.","authors":["Amirhosein Ghasemabadi","Di Niu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20569v1","updated":"2025-12-23T18:12:22Z","published":"2025-12-23T18:12:22Z","title":"Distilling to Hybrid Attention Models via KL-Guided Layer Selection","summary":"Distilling pretrained softmax attention Transformers into more efficient hybrid architectures that interleave softmax and linear attention layers is a promising approach for improving the inference efficiency of LLMs without requiring expensive pretraining from scratch. A critical factor in the conversion process is layer selection, i.e., deciding on which layers to convert to linear attention variants. This paper describes a simple and efficient recipe for layer selection that uses layer importance scores derived from a small amount of training on generic text data. Once the layers have been selected we use a recent pipeline for the distillation process itself \\citep[RADLADS;][]{goldstein2025radlads}, which consists of attention weight transfer, hidden state alignment, KL-based distribution matching, followed by a small amount of finetuning. We find that this approach is more effective than existing approaches for layer selection, including heuristics that uniformly interleave linear attentions based on a fixed ratio, as well as more involved approaches that rely on specialized diagnostic datasets.","authors":["Yanhong Li","Songlin Yang","Shawn Tan","Mayank Mishra","Rameswar Panda","Jiawei Zhou","Yoon Kim"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20491v1","updated":"2025-12-23T16:32:27Z","published":"2025-12-23T16:32:27Z","title":"Step-DeepResearch Technical Report","summary":"As LLMs shift toward autonomous agents, Deep Research has emerged as a pivotal metric. However, existing academic benchmarks like BrowseComp often fail to meet real-world demands for open-ended research, which requires robust skills in intent recognition, long-horizon decision-making, and cross-source verification. To address this, we introduce Step-DeepResearch, a cost-effective, end-to-end agent. We propose a Data Synthesis Strategy Based on Atomic Capabilities to reinforce planning and report writing, combined with a progressive training path from agentic mid-training to SFT and RL. Enhanced by a Checklist-style Judger, this approach significantly improves robustness. Furthermore, to bridge the evaluation gap in the Chinese domain, we establish ADR-Bench for realistic deep research scenarios. Experimental results show that Step-DeepResearch (32B) scores 61.4% on Scale AI Research Rubrics. On ADR-Bench, it significantly outperforms comparable models and rivals SOTA closed-source models like OpenAI and Gemini DeepResearch. These findings prove that refined training enables medium-sized models to achieve expert-level capabilities at industry-leading cost-efficiency.","authors":["Chen Hu","Haikuo Du","Heng Wang","Lin Lin","Mingrui Chen","Peng Liu","Ruihang Miao","Tianchi Yue","Wang You","Wei Ji","Wei Yuan","Wenjin Deng","Xiaojian Yuan","Xiaoyun Zhang","Xiangyu Liu","Xikai Liu","Yanming Xu","Yicheng Cao","Yifei Zhang","Yongyao Wang","Yubo Shu","Yurong Zhang","Yuxiang Zhang","Zheng Gong","Zhichao Chang","Binyan Li","Dan Ma","Furong Jia","Hongyuan Wang","Jiayu Liu","Jing Bai","Junlan Liu","Manjiao Liu","Na Wang","Qiuping Wu","Qinxin Du","Shiwei Li","Wen Sun","Yifeng Gong","Yonglin Chen","Yuling Zhao","Yuxuan Lin","Ziqi Ren","Zixuan Wang","Aihu Zhang","Brian Li","Buyun Ma","Kang An","Li Xie","Mingliang Li","Pan Li","Shidong Yang","Xi Chen","Xiaojia Liu","Yuchu Luo","Yuan Song","YuanHao Ding","Yuanwei Liang","Zexi Li","Zhaoning Zhang","Zixin Zhang","Binxing Jiao","Daxin Jiang","Jiansheng Chen","Jing Li","Xiangyu Zhang","Yibo Zhu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20481v1","updated":"2025-12-23T16:16:42Z","published":"2025-12-23T16:16:42Z","title":"Coherence in the brain unfolds across separable temporal regimes","summary":"Coherence in language requires the brain to satisfy two competing temporal demands: gradual accumulation of meaning across extended context and rapid reconfiguration of representations at event boundaries. Despite their centrality to language and thought, how these processes are implemented in the human brain during naturalistic listening remains unclear. Here, we tested whether these two processes can be captured by annotation-free drift and shift signals and whether their neural expression dissociates across large-scale cortical systems. These signals were derived from a large language model (LLM) and formalized contextual drift and event shifts directly from the narrative input. To enable high-precision voxelwise encoding models with stable parameter estimates, we densely sampled one healthy adult across more than 7 hours of listening to thirteen crime stories while collecting ultra high-field (7T) BOLD data. We then modeled the feature-informed hemodynamic response using a regularized encoding framework validated on independent stories. Drift predictions were prevalent in default-mode network hubs, whereas shift predictions were evident bilaterally in the primary auditory cortex and language association cortex. Furthermore, activity in default-mode and parietal networks was best explained by a signal capturing how meaning accumulates and gradually fades over the course of the narrative. Together, these findings show that coherence during language comprehension is implemented through dissociable neural regimes of slow contextual integration and rapid event-driven reconfiguration, offering a mechanistic entry point for understanding disturbances of language coherence in psychiatric disorders.","authors":["Davide Stauba","Finn Rabe","Akhil Misra","Yves Pauli","Roya Hüppi","Nils Lang","Lars Michels","Victoria Edkins","Sascha Frühholz","Iris Sommer","Wolfram Hinzen","Philipp Homan"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20404v1","updated":"2025-12-23T14:48:42Z","published":"2025-12-23T14:48:42Z","title":"Sentiment-Aware Extractive and Abstractive Summarization for Unstructured Text Mining","summary":"With the rapid growth of unstructured data from social media, reviews, and forums, text mining has become essential in Information Systems (IS) for extracting actionable insights. Summarization can condense fragmented, emotion-rich posts, but existing methods-optimized for structured news-struggle with noisy, informal content. Emotional cues are critical for IS tasks such as brand monitoring and market analysis, yet few studies integrate sentiment modeling into summarization of short user-generated texts. We propose a sentiment-aware framework extending extractive (TextRank) and abstractive (UniLM) approaches by embedding sentiment signals into ranking and generation processes. This dual design improves the capture of emotional nuances and thematic relevance, producing concise, sentiment-enriched summaries that enhance timely interventions and strategic decision-making in dynamic online environments.","authors":["Junyi Liu","Stanley Kok"],"pdf_url":"","comment":"WITS 2025 (Workshop on Information Technologies and Systems 2025)"},{"id":"http://arxiv.org/abs/2512.20387v1","updated":"2025-12-23T14:22:26Z","published":"2025-12-23T14:22:26Z","title":"Generative Digital Twins: Vision-Language Simulation Models for Executable Industrial Systems","summary":"We propose a Vision-Language Simulation Model (VLSM) that unifies visual and textual understanding to synthesize executable FlexScript from layout sketches and natural-language prompts, enabling cross-modal reasoning for industrial simulation systems. To support this new paradigm, the study constructs the first large-scale dataset for generative digital twins, comprising over 120,000 prompt-sketch-code triplets that enable multimodal learning between textual descriptions, spatial structures, and simulation logic. In parallel, three novel evaluation metrics, Structural Validity Rate (SVR), Parameter Match Rate (PMR), and Execution Success Rate (ESR), are proposed specifically for this task to comprehensively evaluate structural integrity, parameter fidelity, and simulator executability. Through systematic ablation across vision encoders, connectors, and code-pretrained language backbones, the proposed models achieve near-perfect structural accuracy and high execution robustness. This work establishes a foundation for generative digital twins that integrate visual reasoning and language understanding into executable industrial simulation systems.","authors":["YuChe Hsu","AnJui Wang","TsaiChing Ni","YuanFu Yang"],"pdf_url":"","comment":"10 pages, 9 figures"},{"id":"http://arxiv.org/abs/2512.15328v2","updated":"2025-12-23T14:00:56Z","published":"2025-12-17T11:24:46Z","title":"Thematic Dispersion in Arabic Applied Linguistics: A Bibliometric Analysis using Brookes' Measure","summary":"This study applies Brookes' Measure of Categorical Dispersion (Δ) to analyze the thematic structure of contemporary Arabic Applied Linguistics research. Using a comprehensive, real-world dataset of 1,564 publications from 2019 to 2025, classified into eight core sub-disciplines, we calculate a dispersion index of Δ = 0.194. This remarkably low value indicates extreme thematic dispersion, revealing that the field is characterized by pronounced heterogeneity rather than concentration. The analysis identifies Computational Linguistics as a dominant but non-hegemonic force, coexisting with robust research in Sociolinguistics, Language Teaching, and other subfields. This study clarifies the correct application of Brookes' original formula, demonstrates its utility for field characterization, and provides a replicable bibliometric methodology for assessing disciplinary structure across domains.","authors":["Ayman Eddakrouri","Amani Ramadan"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20352v1","updated":"2025-12-23T13:32:43Z","published":"2025-12-23T13:32:43Z","title":"Multi-LLM Thematic Analysis with Dual Reliability Metrics: Combining Cohen's Kappa and Semantic Similarity for Qualitative Research Validation","summary":"Qualitative research faces a critical reliability challenge: traditional inter-rater agreement methods require multiple human coders, are time-intensive, and often yield moderate consistency. We present a multi-perspective validation framework for LLM-based thematic analysis that combines ensemble validation with dual reliability metrics: Cohen's Kappa ($κ$) for inter-rater agreement and cosine similarity for semantic consistency. Our framework enables configurable analysis parameters (1-6 seeds, temperature 0.0-2.0), supports custom prompt structures with variable substitution, and provides consensus theme extraction across any JSON format. As proof-of-concept, we evaluate three leading LLMs (Gemini 2.5 Pro, GPT-4o, Claude 3.5 Sonnet) on a psychedelic art therapy interview transcript, conducting six independent runs per model. Results demonstrate Gemini achieves highest reliability ($κ= 0.907$, cosine=95.3%), followed by GPT-4o ($κ= 0.853$, cosine=92.6%) and Claude ($κ= 0.842$, cosine=92.1%). All three models achieve a high agreement ($κ> 0.80$), validating the multi-run ensemble approach. The framework successfully extracts consensus themes across runs, with Gemini identifying 6 consensus themes (50-83% consistency), GPT-4o identifying 5 themes, and Claude 4 themes. Our open-source implementation provides researchers with transparent reliability metrics, flexible configuration, and structure-agnostic consensus extraction, establishing methodological foundations for reliable AI-assisted qualitative research.","authors":["Nilesh Jain","Seyi Adeyinka","Leor Roseman","Aza Allsop"],"pdf_url":"","comment":"11 pages, 1 figure, 3 tables"},{"id":"http://arxiv.org/abs/2512.20324v1","updated":"2025-12-23T12:48:05Z","published":"2025-12-23T12:48:05Z","title":"Can LLMs Solve My Grandma's Riddle? Evaluating Multilingual Large Language Models on Reasoning Traditional Bangla Tricky Riddles","summary":"Large Language Models (LLMs) show impressive performance on many NLP benchmarks, yet their ability to reason in figurative, culturally grounded, and low-resource settings remains underexplored. We address this gap for Bangla by introducing BanglaRiddleEval, a benchmark of 1,244 traditional Bangla riddles instantiated across four tasks (4,976 riddle-task artifacts in total). Using an LLM-based pipeline, we generate Chain-of-Thought explanations, semantically coherent distractors, and fine-grained ambiguity annotations, and evaluate a diverse suite of open-source and closed-source models under different prompting strategies. Models achieve moderate semantic overlap on generative QA but low correctness, MCQ accuracy peaks at only about 56% versus an 83% human baseline, and ambiguity resolution ranges from roughly 26% to 68%, with high-quality explanations confined to the strongest models. These results show that current LLMs capture some cues needed for Bangla riddle reasoning but remain far from human-level performance, establishing BanglaRiddleEval as a challenging new benchmark for low-resource figurative reasoning. All data, code, and evaluation scripts are available on GitHub: https://github.com/Labib1610/BanglaRiddleEval.","authors":["Nurul Labib Sayeedi","Md. Faiyaz Abdullah Sayeedi","Khushnur Binte Jahangir","Swakkhar Shatabda","Sarah Masud Preum"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2509.01564v2","updated":"2025-12-23T12:29:17Z","published":"2025-09-01T15:50:10Z","title":"Enhancing Uncertainty Estimation in LLMs with Expectation of Aggregated Internal Belief","summary":"Large Language Models (LLMs) have achieved remarkable success across a wide range of natural language tasks, but often exhibit overconfidence and generate plausible yet incorrect answers. This overconfidence, especially in models undergone Reinforcement Learning from Human Feedback (RLHF), poses significant challenges for reliable uncertainty estimation and safe deployment. In this paper, we propose EAGLE (Expectation of AGgregated internaL bEief), a novel self-evaluation-based calibration method that leverages the internal hidden states of LLMs to derive more accurate confidence scores. Instead of relying on the model's final output, our approach extracts internal beliefs from multiple intermediate layers during self-evaluation. By aggregating these layer-wise beliefs and calculating the expectation over the resulting confidence score distribution, EAGLE produces a refined confidence score that more faithfully reflects the model's internal certainty. Extensive experiments on diverse datasets and LLMs demonstrate that EAGLE significantly improves calibration performance over existing baselines. We also provide an in-depth analysis of EAGLE, including a layer-wise examination of uncertainty patterns, a study of the impact of self-evaluation prompts, and an analysis of the effect of self-evaluation score range.","authors":["Zeguan Xiao","Diyang Dou","Boya Xiong","Yun Chen","Guanhua Chen"],"pdf_url":"","comment":"Accepted by AAAI 2026"},{"id":"http://arxiv.org/abs/2512.20308v1","updated":"2025-12-23T12:22:25Z","published":"2025-12-23T12:22:25Z","title":"SpidR: Learning Fast and Stable Linguistic Units for Spoken Language Models Without Supervision","summary":"The parallel advances in language modeling and speech representation learning have raised the prospect of learning language directly from speech without textual intermediates. This requires extracting semantic representations directly from speech. Our contributions are threefold. First, we introduce SpidR, a self-supervised speech representation model that efficiently learns representations with highly accessible phonetic information, which makes it particularly suited for textless spoken language modeling. It is trained on raw waveforms using a masked prediction objective combined with self-distillation and online clustering. The intermediate layers of the student model learn to predict assignments derived from the teacher's intermediate layers. This learning objective stabilizes the online clustering procedure compared to previous approaches, resulting in higher quality codebooks. SpidR outperforms wav2vec 2.0, HuBERT, WavLM, and DinoSR on downstream language modeling benchmarks (sWUGGY, sBLIMP, tSC). Second, we systematically evaluate across models and layers the correlation between speech unit quality (ABX, PNMI) and language modeling performance, validating these metrics as reliable proxies. Finally, SpidR significantly reduces pretraining time compared to HuBERT, requiring only one day of pretraining on 16 GPUs, instead of a week. This speedup is enabled by the pretraining method and an efficient codebase, which allows faster iteration and easier experimentation. We open-source the training code and model checkpoints at https://github.com/facebookresearch/spidr.","authors":["Maxime Poli","Mahi Luthra","Youssef Benchekroun","Yosuke Higuchi","Martin Gleize","Jiayi Shen","Robin Algayres","Yu-An Chung","Mido Assran","Juan Pino","Emmanuel Dupoux"],"pdf_url":"","comment":"30 pages, 16 figures"},{"id":"http://arxiv.org/abs/2512.20298v1","updated":"2025-12-23T12:05:01Z","published":"2025-12-23T12:05:01Z","title":"Patterns vs. Patients: Evaluating LLMs against Mental Health Professionals on Personality Disorder Diagnosis through First-Person Narratives","summary":"Growing reliance on LLMs for psychiatric self-assessment raises questions about their ability to interpret qualitative patient narratives. We present the first direct comparison between state-of-the-art LLMs and mental health professionals in diagnosing Borderline (BPD) and Narcissistic (NPD) Personality Disorders utilizing Polish-language first-person autobiographical accounts. We show that the top-performing Gemini Pro models surpassed human professionals in overall diagnostic accuracy by 21.91 percentage points (65.48% vs. 43.57%). While both models and human experts excelled at identifying BPD (F1 = 83.4 & F1 = 80.0, respectively), models severely underdiagnosed NPD (F1 = 6.7 vs. 50.0), showing a reluctance toward the value-laden term \"narcissism.\" Qualitatively, models provided confident, elaborate justifications focused on patterns and formal categories, while human experts remained concise and cautious, emphasizing the patient's sense of self and temporal experience. Our findings demonstrate that while LLMs are highly competent at interpreting complex first-person clinical data, they remain subject to critical reliability and bias issues.","authors":["Karolina Drożdż","Kacper Dudzic","Anna Sterna","Marcin Moskalewicz"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20293v1","updated":"2025-12-23T12:01:32Z","published":"2025-12-23T12:01:32Z","title":"AprielGuard","summary":"Safeguarding large language models (LLMs) against unsafe or adversarial behavior is critical as they are increasingly deployed in conversational and agentic settings. Existing moderation tools often treat safety risks (e.g. toxicity, bias) and adversarial threats (e.g. prompt injections, jailbreaks) as separate problems, limiting their robustness and generalizability. We introduce AprielGuard, an 8B parameter safeguard model that unify these dimensions within a single taxonomy and learning framework. AprielGuard is trained on a diverse mix of open and synthetic data covering standalone prompts, multi-turn conversations, and agentic workflows, augmented with structured reasoning traces to improve interpretability. Across multiple public and proprietary benchmarks, AprielGuard achieves strong performance in detecting harmful content and adversarial manipulations, outperforming existing opensource guardrails such as Llama-Guard and Granite Guardian, particularly in multi-step and reasoning intensive scenarios. By releasing the model, we aim to advance transparent and reproducible research on reliable safeguards for LLMs.","authors":["Jaykumar Kasundra","Anjaneya Praharaj","Sourabh Surana","Lakshmi Sirisha Chodisetty","Sourav Sharma","Abhigya Verma","Abhishek Bhardwaj","Debasish Kanhar","Aakash Bhagat","Khalil Slimi","Seganrasan Subramanian","Sathwik Tejaswi Madhusudhan","Ranga Prasad Chenna","Srinivas Sunkara"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20292v1","updated":"2025-12-23T12:01:18Z","published":"2025-12-23T12:01:18Z","title":"SlideTailor: Personalized Presentation Slide Generation for Scientific Papers","summary":"Automatic presentation slide generation can greatly streamline content creation. However, since preferences of each user may vary, existing under-specified formulations often lead to suboptimal results that fail to align with individual user needs. We introduce a novel task that conditions paper-to-slides generation on user-specified preferences. We propose a human behavior-inspired agentic framework, SlideTailor, that progressively generates editable slides in a user-aligned manner. Instead of requiring users to write their preferences in detailed textual form, our system only asks for a paper-slides example pair and a visual template - natural and easy-to-provide artifacts that implicitly encode rich user preferences across content and visual style. Despite the implicit and unlabeled nature of these inputs, our framework effectively distills and generalizes the preferences to guide customized slide generation. We also introduce a novel chain-of-speech mechanism to align slide content with planned oral narration. Such a design significantly enhances the quality of generated slides and enables downstream applications like video presentations. To support this new task, we construct a benchmark dataset that captures diverse user preferences, with carefully designed interpretable metrics for robust evaluation. Extensive experiments demonstrate the effectiveness of our framework.","authors":["Wenzheng Zeng","Mingyu Ouyang","Langyuan Cui","Hwee Tou Ng"],"pdf_url":"","comment":"AAAI 2026 (with appendix)"},{"id":"http://arxiv.org/abs/2507.11662v2","updated":"2025-12-23T11:29:24Z","published":"2025-07-15T18:50:29Z","title":"Let's Think in Two Steps: Mitigating Agreement Bias in MLLMs with Self-Grounded Verification","summary":"Verifiers--functions assigning rewards to agent behavior--have been key for AI progress in domains like math and code. However, extending gains to domains without clear-cut success criteria (e.g., computer use) remains a challenge: while humans can recognize desired outcomes, translating this intuition into scalable rules is nontrivial. Multimodal Large Language Models (MLLMs) emerge as a promising solution, given their world knowledge, human-preference alignment, and reasoning skills. We evaluate MLLMs as verifiers across web navigation, computer use, and robotic manipulation, and identify a critical limitation: a strong tendency to over-validate agent behavior, a phenomenon we term agreement bias. This bias is pervasive across models, resilient to test-time scaling, and poses risks to existing methods relying on MLLM evaluations. We discuss methods to evaluate and improve MLLM verifiers and introduce Self-Grounded Verification (SGV), a lightweight method that harnesses MLLMs' own sampling mechanisms by modulating (un)conditional generation to better leverage their knowledge, alignment, and reasoning. SGV operates in two steps: first, the MLLM is elicited to generate broad priors about desired behavior, independent of the data under evaluation. Then, conditioned on self-generated priors, it reasons over and evaluates a candidate trajectory. SGV yields more human-aligned evaluations with gains of up to 25pp in failure detection, 14pp in accuracy, and benefits extending to downstream applications. In self-refinement and online supervision, SGV boosts task completion of a GUI specialist in OSWorld, a diffusion policy in robomimic, and a ReAct agent in VisualWebArena--setting a new state of the art, surpassing the previous best by 20pp. We release an updated version of VisualWebArena featuring more human-aligned evaluators, high-fidelity environment parallelism, and speedups of over 10x.","authors":["Moises Andrade","Joonhyuk Cha","Brandon Ho","Vriksha Srihari","Karmesh Yadav","Zsolt Kira"],"pdf_url":"","comment":"Our code, models, and data are publicly available at https://mshalimay.github.io/agreement-bias-sgv/"},{"id":"http://arxiv.org/abs/2512.19126v2","updated":"2025-12-23T11:15:52Z","published":"2025-12-22T08:07:00Z","title":"AWPO: Enhancing Tool-Use of Large Language Models through Explicit Integration of Reasoning Rewards","summary":"While reinforcement learning (RL) shows promise in training tool-use large language models (LLMs) using verifiable outcome rewards, existing methods largely overlook the potential of explicit reasoning rewards to bolster reasoning and tool utilization. Furthermore, natively combining reasoning and outcome rewards may yield suboptimal performance or conflict with the primary optimization objective. To address this, we propose advantage-weighted policy optimization (AWPO) -- a principled RL framework that effectively integrates explicit reasoning rewards to enhance tool-use capability. AWPO incorporates variance-aware gating and difficulty-aware weighting to adaptively modulate advantages from reasoning signals based on group-relative statistics, alongside a tailored clipping mechanism for stable optimization. Extensive experiments demonstrate that AWPO achieves state-of-the-art performance across standard tool-use benchmarks, significantly outperforming strong baselines and leading closed-source models in challenging multi-turn scenarios. Notably, with exceptional parameter efficiency, our 4B model surpasses Grok-4 by 16.0 percent in multi-turn accuracy while preserving generalization capability on the out-of-distribution MMLU-Pro benchmark.","authors":["Zihan Lin","Xiaohan Wang","Hexiong Yang","Jiajun Chai","Jie Cao","Guojun Yin","Wei Lin","Ran He"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20204v1","updated":"2025-12-23T09:56:23Z","published":"2025-12-23T09:56:23Z","title":"Corpus of Cross-lingual Dialogues with Minutes and Detection of Misunderstandings","summary":"Speech processing and translation technology have the potential to facilitate meetings of individuals who do not share any common language. To evaluate automatic systems for such a task, a versatile and realistic evaluation corpus is needed. Therefore, we create and present a corpus of cross-lingual dialogues between individuals without a common language who were facilitated by automatic simultaneous speech translation. The corpus consists of 5 hours of speech recordings with ASR and gold transcripts in 12 original languages and automatic and corrected translations into English. For the purposes of research into cross-lingual summarization, our corpus also includes written summaries (minutes) of the meetings.\n  Moreover, we propose automatic detection of misunderstandings. For an overview of this task and its complexity, we attempt to quantify misunderstandings in cross-lingual meetings. We annotate misunderstandings manually and also test the ability of current large language models to detect them automatically. The results show that the Gemini model is able to identify text spans with misunderstandings with recall of 77% and precision of 47%.","authors":["Marko Čechovič","Natália Komorníková","Dominik Macháček","Ondřej Bojar"],"pdf_url":"","comment":"12 pages, 2 figures, 6 tables, published as a conference paper in Text, Speech, and Dialogue 28th International Conference, TSD 2025, Erlangen, Germany, August 25-28, 2025, Proceedings, Part II. This version published here on arXiv.org is before review comments and seedings of the TSD conference staff"},{"id":"http://arxiv.org/abs/2510.03289v2","updated":"2025-12-23T09:36:38Z","published":"2025-09-29T12:07:09Z","title":"Why mask diffusion does not work","summary":"The main advantages of diffusion language models over autoregressive (AR) models lie in their ability to support parallel generation and bidirectional attention, enabling a more controllable generation process. In recent years, open-source mask diffusion language models have emerged, most of which are based on a variant known as absorbing diffusion. However, this paper demonstrates why mask diffusion faces inherent difficulties in achieving parallel generation and bidirectional attention. We also propose the most effective training and inference strategies for mask diffusion.","authors":["Haocheng Sun","Cynthia Xin Wen","Edward Hong Wang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20182v1","updated":"2025-12-23T09:20:32Z","published":"2025-12-23T09:20:32Z","title":"FaithLens: Detecting and Explaining Faithfulness Hallucination","summary":"Recognizing whether outputs from large language models (LLMs) contain faithfulness hallucination is crucial for real-world applications, e.g., retrieval-augmented generation and summarization. In this paper, we introduce FaithLens, a cost-efficient and effective faithfulness hallucination detection model that can jointly provide binary predictions and corresponding explanations to improve trustworthiness. To achieve this, we first synthesize training data with explanations via advanced LLMs and apply a well-defined data filtering strategy to ensure label correctness, explanation quality, and data diversity. Subsequently, we fine-tune the model on these well-curated training data as a cold start and further optimize it with rule-based reinforcement learning, using rewards for both prediction correctness and explanation quality. Results on 12 diverse tasks show that the 8B-parameter FaithLens outperforms advanced models such as GPT-4.1 and o3. Also, FaithLens can produce high-quality explanations, delivering a distinctive balance of trustworthiness, efficiency, and effectiveness.","authors":["Shuzheng Si","Qingyi Wang","Haozhe Zhao","Yuzhuo Bai","Guanqiao Chen","Kangyang Luo","Gang Chen","Fanchao Qi","Minjia Zhang","Baobao Chang","Maosong Sun"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20174v1","updated":"2025-12-23T09:14:16Z","published":"2025-12-23T09:14:16Z","title":"Towards Natural Language-Based Document Image Retrieval: New Dataset and Benchmark","summary":"Document image retrieval (DIR) aims to retrieve document images from a gallery according to a given query. Existing DIR methods are primarily based on image queries that retrieve documents within the same coarse semantic category, e.g., newspapers or receipts. However, these methods struggle to effectively retrieve document images in real-world scenarios where textual queries with fine-grained semantics are usually provided. To bridge this gap, we introduce a new Natural Language-based Document Image Retrieval (NL-DIR) benchmark with corresponding evaluation metrics. In this work, natural language descriptions serve as semantically rich queries for the DIR task. The NL-DIR dataset contains 41K authentic document images, each paired with five high-quality, fine-grained semantic queries generated and evaluated through large language models in conjunction with manual verification. We perform zero-shot and fine-tuning evaluations of existing mainstream contrastive vision-language models and OCR-free visual document understanding (VDU) models. A two-stage retrieval method is further investigated for performance improvement while achieving both time and space efficiency. We hope the proposed NL-DIR benchmark can bring new opportunities and facilitate research for the VDU community. Datasets and codes will be publicly available at huggingface.co/datasets/nianbing/NL-DIR.","authors":["Hao Guo","Xugong Qin","Jun Jie Ou Yang","Peng Zhang","Gangyan Zeng","Yubo Li","Hailun Lin"],"pdf_url":"","comment":"CVPR 2025"},{"id":"http://arxiv.org/abs/2512.20169v1","updated":"2025-12-23T08:56:49Z","published":"2025-12-23T08:56:49Z","title":"Learning to Reason in LLMs by Expectation Maximization","summary":"Large language models (LLMs) solve reasoning problems by first generating a rationale and then answering. We formalize reasoning as a latent variable model and derive an expectation-maximization (EM) objective for learning to reason. This view connects EM and modern reward-based optimization, and shows that the main challenge lies in designing a sampling distribution that generates rationales that justify correct answers. We instantiate and compare several sampling schemes: rejection sampling with a budget, self-taught reasoner (STaR), and prompt posterior sampling (PPS), which only keeps the rationalization stage of STaR. Our experiments on the ARC, MMLU, and OpenBookQA datasets with the Llama and Qwen models show that the sampling scheme can significantly affect the accuracy of learned reasoning models. Despite its simplicity, we observe that PPS outperforms the other sampling schemes.","authors":["Junghyun Lee","Branislav Kveton","Sunav Choudhary","Subhojyoti Mukherjee","Anup Rao","Ryan A. Rossi","Alexa Siu"],"pdf_url":"","comment":"12 pages, 3 figures, 1 table"},{"id":"http://arxiv.org/abs/2506.09349v4","updated":"2025-12-23T08:50:59Z","published":"2025-06-11T02:57:22Z","title":"DrVoice: Parallel Speech-Text Voice Conversation Model via Dual-Resolution Speech Representations","summary":"Recent studies on end-to-end (E2E) speech generation with large language models (LLMs) have attracted significant community attention, with multiple works extending text-based LLMs to generate discrete speech tokens. Existing E2E approaches primarily fall into two categories: (1) Methods that generate discrete speech tokens independently without incorporating them into the LLM's autoregressive process, resulting in text generation being unaware of concurrent speech synthesis. (2) Models that generate interleaved or parallel speech-text tokens through joint autoregressive modeling, enabling mutual modality awareness during generation. This paper presents DrVoice, a parallel speech-text voice conversation model based on joint autoregressive modeling, featuring dual-resolution speech representations. Notably, while current methods utilize mainly 12.5Hz input audio representation, our proposed dual-resolution mechanism reduces the input frequency for the LLM to 5Hz, significantly reducing computational cost and alleviating the frequency discrepancy between speech and text tokens and in turn better exploiting LLMs' capabilities. Experimental results demonstrate that DrVoice-7B establishes new state-of-the-art (SOTA) on prominent speech benchmarks including OpenAudioBench, VoiceBench, UltraEval-Audio and Big Bench Audio, making it a leading open-source speech foundation model in ~7B models.","authors":["Chao-Hong Tan","Qian Chen","Wen Wang","Chong Deng","Qinglin Zhang","Luyao Cheng","Hai Yu","Xin Zhang","Xiang Lv","Tianyu Zhao","Chong Zhang","Yukun Ma","Yafeng Chen","Hui Wang","Jiaqing Liu","Xiangang Li","Jieping Ye"],"pdf_url":"","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2511.17004v3","updated":"2025-12-23T08:46:41Z","published":"2025-11-21T07:14:46Z","title":"Vision Language Models are Confused Tourists","summary":"Although the cultural dimension has been one of the key aspects in evaluating Vision-Language Models (VLMs), their ability to remain stable across diverse cultural inputs remains largely untested, despite being crucial to support diversity and multicultural societies. Existing evaluations often rely on benchmarks featuring only a singular cultural concept per image, overlooking scenarios where multiple, potentially unrelated cultural cues coexist. To address this gap, we introduce ConfusedTourist, a novel cultural adversarial robustness suite designed to assess VLMs' stability against perturbed geographical cues. Our experiments reveal a critical vulnerability, where accuracy drops heavily under simple image-stacking perturbations and even worsens with its image-generation-based variant. Interpretability analyses further show that these failures stem from systematic attention shifts toward distracting cues, diverting the model from its intended focus. These findings highlight a critical challenge: visual cultural concept mixing can substantially impair even state-of-the-art VLMs, underscoring the urgent need for more culturally robust multimodal understanding.","authors":["Patrick Amadeus Irawan","Ikhlasul Akmal Hanif","Muhammad Dehan Al Kautsar","Genta Indra Winata","Fajri Koto","Alham Fikri Aji"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20164v1","updated":"2025-12-23T08:42:09Z","published":"2025-12-23T08:42:09Z","title":"AI Security Beyond Core Domains: Resume Screening as a Case Study of Adversarial Vulnerabilities in Specialized LLM Applications","summary":"Large Language Models (LLMs) excel at text comprehension and generation, making them ideal for automated tasks like code review and content moderation. However, our research identifies a vulnerability: LLMs can be manipulated by \"adversarial instructions\" hidden in input data, such as resumes or code, causing them to deviate from their intended task. Notably, while defenses may exist for mature domains such as code review, they are often absent in other common applications such as resume screening and peer review. This paper introduces a benchmark to assess this vulnerability in resume screening, revealing attack success rates exceeding 80% for certain attack types. We evaluate two defense mechanisms: prompt-based defenses achieve 10.1% attack reduction with 12.5% false rejection increase, while our proposed FIDS (Foreign Instruction Detection through Separation) using LoRA adaptation achieves 15.4% attack reduction with 10.4% false rejection increase. The combined approach provides 26.3% attack reduction, demonstrating that training-time defenses outperform inference-time mitigations in both security and utility preservation.","authors":["Honglin Mu","Jinghao Liu","Kaiyang Wan","Rui Xing","Xiuying Chen","Timothy Baldwin","Wanxiang Che"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20156v1","updated":"2025-12-23T08:35:27Z","published":"2025-12-23T08:35:27Z","title":"Fun-Audio-Chat Technical Report","summary":"Recent advancements in joint speech-text models show great potential for seamless voice interactions. However, existing models face critical challenges: temporal resolution mismatch between speech tokens (25Hz) and text tokens (~3Hz) dilutes semantic information, incurs high computational costs, and causes catastrophic forgetting of text LLM knowledge. We introduce Fun-Audio-Chat, a Large Audio Language Model addressing these limitations via two innovations from our previous work DrVoice. First, Dual-Resolution Speech Representations (DRSR): the Shared LLM processes audio at efficient 5Hz (via token grouping), while the Speech Refined Head generates high-quality tokens at 25Hz, balancing efficiency (~50% GPU reduction) and quality. Second, Core-Cocktail Training, a two-stage fine-tuning with intermediate merging that mitigates catastrophic forgetting. We then apply Multi-Task DPO Training to enhance robustness, audio understanding, instruction-following and voice empathy. This multi-stage post-training enables Fun-Audio-Chat to retain text LLM knowledge while gaining powerful audio understanding, reasoning, and generation. Unlike recent LALMs requiring large-scale audio-text pre-training, Fun-Audio-Chat leverages pre-trained models and extensive post-training. Fun-Audio-Chat 8B and MoE 30B-A3B achieve competitive performance on Speech-to-Text and Speech-to-Speech tasks, ranking top among similar-scale models on Spoken QA benchmarks. They also achieve competitive to superior performance on Audio Understanding, Speech Function Calling, Instruction-Following and Voice Empathy. We develop Fun-Audio-Chat-Duplex, a full-duplex variant with strong performance on Spoken QA and full-duplex interactions. We open-source Fun-Audio-Chat-8B with training and inference code, and provide an interactive demo.","authors":["Qian Chen","Luyao Cheng","Chong Deng","Xiangang Li","Jiaqing Liu","Chao-Hong Tan","Wen Wang","Junhao Xu","Jieping Ye","Qinglin Zhang","Qiquan Zhang","Jingren Zhou"],"pdf_url":"","comment":"21 pages, https://github.com/FunAudioLLM/Fun-Audio-Chat"},{"id":"http://arxiv.org/abs/2506.05594v3","updated":"2025-12-23T08:31:59Z","published":"2025-06-05T21:12:51Z","title":"SoK: Are Watermarks in LLMs Ready for Deployment?","summary":"Large Language Models (LLMs) have transformed natural language processing, demonstrating impressive capabilities across diverse tasks. However, deploying these models introduces critical risks related to intellectual property violations and potential misuse, particularly as adversaries can imitate these models to steal services or generate misleading outputs. We specifically focus on model stealing attacks, as they are highly relevant to proprietary LLMs and pose a serious threat to their security, revenue, and ethical deployment. While various watermarking techniques have emerged to mitigate these risks, it remains unclear how far the community and industry have progressed in developing and deploying watermarks in LLMs.\n  To bridge this gap, we aim to develop a comprehensive systematization for watermarks in LLMs by 1) presenting a detailed taxonomy for watermarks in LLMs, 2) proposing a novel intellectual property classifier to explore the effectiveness and impacts of watermarks on LLMs under both attack and attack-free environments, 3) analyzing the limitations of existing watermarks in LLMs, and 4) discussing practical challenges and potential future directions for watermarks in LLMs. Through extensive experiments, we show that despite promising research outcomes and significant attention from leading companies and community to deploy watermarks, these techniques have yet to reach their full potential in real-world applications due to their unfavorable impacts on model utility of LLMs and downstream tasks. Our findings provide an insightful understanding of watermarks in LLMs, highlighting the need for practical watermarks solutions tailored to LLM deployment.","authors":["Kieu Dang","Phung Lai","NhatHai Phan","Yelong Shen","Ruoming Jin","Abdallah Khreishah","My T. Thai"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.01457v4","updated":"2025-12-23T08:18:03Z","published":"2025-12-01T09:44:31Z","title":"Zero-Overhead Introspection for Adaptive Test-Time Compute","summary":"Large language models excel at reasoning but lack key aspects of introspection, including anticipating their own success and the computation required to achieve it. Humans use real-time introspection to decide how much effort to invest, when to make multiple attempts, when to stop, and when to signal success or failure. Without this, LLMs struggle to make intelligent meta-cognition decisions. Test-time scaling methods like Best-of-N drive up cost and latency by using a fixed budget of samples regardless of the marginal benefit of each one at any point in generation, and the absence of confidence signals can mislead people, prevent appropriate escalation to better tools, and undermine trustworthiness. Learned verifiers or reward models can provide confidence estimates, but do not enable adaptive inference and add substantial cost by requiring extra models or forward passes. We present ZIP-RC, which equips models with zero-overhead introspective predictions of reward and cost. At every token, ZIP-RC reuses reserved or unused logits in the same forward pass as next-token prediction to output a joint distribution over final reward and remaining length -- no extra models, architecture change, or inference overhead. This full joint distribution is used to compute a sampling utility which is the linear combination of the expected maximum reward, total compute, and latency of set of samples if generated to completion. During inference, we maximize this utility with meta-actions that determine which prefix of tokens to continue or initiate sampling from. On mixed-difficulty mathematical benchmarks, ZIP-RC improves accuracy by up to 12% over majority voting at equal or lower average cost, and traces smooth Pareto frontiers between quality, compute, and latency. By providing real-time reward-cost introspection, ZIP-RC enables adaptive, efficient reasoning.","authors":["Rohin Manvi","Joey Hong","Tim Seyde","Maxime Labonne","Mathias Lechner","Sergey Levine"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20145v1","updated":"2025-12-23T08:15:34Z","published":"2025-12-23T08:15:34Z","title":"Retrieval-augmented Prompt Learning for Pre-trained Foundation Models","summary":"The pre-trained foundation models (PFMs) have become essential for facilitating large-scale multimodal learning. Researchers have effectively employed the ``pre-train, prompt, and predict'' paradigm through prompt learning to induce improved few-shot performance. However, prompt learning approaches for PFMs still follow a parametric learning paradigm. As such, the stability of generalization in memorization and rote learning can be compromised. More specifically, conventional prompt learning might face difficulties in fully utilizing atypical instances and avoiding overfitting to shallow patterns with limited data during the process of fully-supervised training. To overcome these constraints, we present our approach, named RetroPrompt, which aims to achieve a balance between memorization and generalization by decoupling knowledge from mere memorization. Unlike traditional prompting methods, RetroPrompt leverages a publicly accessible knowledge base generated from the training data and incorporates a retrieval mechanism throughout the input, training, and inference stages. This enables the model to actively retrieve relevant contextual information from the corpus, thereby enhancing the available cues. We conduct comprehensive experiments on a variety of datasets across natural language processing and computer vision tasks to demonstrate the superior performance of our proposed approach, RetroPrompt, in both zero-shot and few-shot scenarios. Through detailed analysis of memorization patterns, we observe that RetroPrompt effectively reduces the reliance on rote memorization, leading to enhanced generalization.","authors":["Xiang Chen","Yixin Ou","Quan Feng","Lei Li","Piji Li","Haibo Ye","Sheng-Jun Huang","Shuofei Qiao","Shumin Deng","Huajun Chen","Ningyu Zhang"],"pdf_url":"","comment":"IEEE/ACM Transactions on Audio, Speech and Language Processing"},{"id":"http://arxiv.org/abs/2512.20144v1","updated":"2025-12-23T08:14:44Z","published":"2025-12-23T08:14:44Z","title":"Multi-hop Reasoning via Early Knowledge Alignment","summary":"Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm for Large Language Models (LLMs) to address knowledge-intensive queries requiring domain-specific or up-to-date information. To handle complex multi-hop questions that are challenging for single-step retrieval, iterative RAG approaches incorporating reinforcement learning have been proposed. However, existing iterative RAG systems typically plan to decompose questions without leveraging information about the available retrieval corpus, leading to inefficient retrieval and reasoning chains that cascade into suboptimal performance. In this paper, we introduce Early Knowledge Alignment (EKA), a simple but effective module that aligns LLMs with retrieval set before planning in iterative RAG systems with contextually relevant retrieved knowledge. Extensive experiments on six standard RAG datasets demonstrate that by establishing a stronger reasoning foundation, EKA significantly improves retrieval precision, reduces cascading errors, and enhances both performance and efficiency. Our analysis from an entropy perspective demonstrate that incorporating early knowledge reduces unnecessary exploration during the reasoning process, enabling the model to focus more effectively on relevant information subsets. Moreover, EKA proves effective as a versatile, training-free inference strategy that scales seamlessly to large models. Generalization tests across diverse datasets and retrieval corpora confirm the robustness of our approach. Overall, EKA advances the state-of-the-art in iterative RAG systems while illuminating the critical interplay between structured reasoning and efficient exploration in reinforcement learning-augmented frameworks. The code is released at \\href{https://github.com/yxzwang/EarlyKnowledgeAlignment}{Github}.","authors":["Yuxin Wang","Shicheng Fang","Bo Wang","Qi Luo","Xuanjing Huang","Yining Zheng","Xipeng Qiu"],"pdf_url":"","comment":"16 pages"},{"id":"http://arxiv.org/abs/2512.18190v2","updated":"2025-12-23T08:10:47Z","published":"2025-12-20T03:27:11Z","title":"External Hippocampus: Topological Cognitive Maps for Guiding Large Language Model Reasoning","summary":"This paper proposes the External Hippocampus framework, which models language model reasoning from a cognitive dynamics perspective as the flow of information energy in semantic space. Unlike traditional weight-space optimization methods, this framework constructs topological cognitive maps through dimensionality reduction projection, enabling precise navigation and intervention of energy flow at test time while avoiding substantial computational requirements and demonstrating predictable intervention patterns. The method effectively addresses the cognitive deadlock problem in multi-step reasoning for small models. Experiments on models <=7B parameters show: map-guided methods achieve 81.20% accuracy on 500 challenging problems (relative baseline +16.80%), reduce reasoning time by >= 15x, with key findings revealing that reasoning stagnation manifests as \"Cognitive Vortex\" and low-entropy potential wells, while temperature perturbations effectively restart energy flow. The framework requires no additional training, possesses autonomous growth capability, and provides an efficient and controllable topological-aware solution for small model reasoning.","authors":["Jian Yan"],"pdf_url":"","comment":"12 pages, 7 figures"},{"id":"http://arxiv.org/abs/2509.23129v2","updated":"2025-12-23T07:56:48Z","published":"2025-09-27T05:24:51Z","title":"C$^2$GSPG: Confidence-calibrated Group Sequence Policy Gradient towards Self-aware Reasoning","summary":"Reinforcement Learning (RL) methods, exemplified by Group Relative Policy Optimization (GRPO) and its variants, play a central role in developing reasoning models. However, these methods often suffer from a critical overconfidence issue, which prevents them from achieving self-aware reasoning models. In this study, we propose a simple yet effective confidence-calibration group sequence policy gradient method, called C$^2$GSPG, which simultaneously enhances reasoning performance while suppressing overconfidence. In principle, we propose a Group Sequence Policy Gradient (GSPG) framework for learning reasoning models, which eliminates the token-level bias commonly appearing in GRPO and its variants. In this framework, we define the model confidence for each reasoning problem using the normalized sequence-level probability, and then apply a cross-entropy regularizer to calibrate the model confidence to the sequence's reward. We demonstrate that the confidence calibration regularizer and GSPG are collaborative for binary rewards, as their objectives always share the same gradient direction. For non-binary rewards, we apply nonlinear reward normalization and adaptive regularizer clipping, mitigating the potential conflict between the two objectives. Applying C$^2$GSPG to post-train large language models in logical and mathematical reasoning tasks, we show its superiority over state-of-the-art methods in both reasoning accuracy and confidence calibration. The code of C$^2$GSPG is available at https://github.com/HaotianLiu123/CCGSPG.","authors":["Haotian Liu","Shuo Wang","Hongteng Xu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20136v1","updated":"2025-12-23T07:54:03Z","published":"2025-12-23T07:54:03Z","title":"M$^3$KG-RAG: Multi-hop Multimodal Knowledge Graph-enhanced Retrieval-Augmented Generation","summary":"Retrieval-Augmented Generation (RAG) has recently been extended to multimodal settings, connecting multimodal large language models (MLLMs) with vast corpora of external knowledge such as multimodal knowledge graphs (MMKGs). Despite their recent success, multimodal RAG in the audio-visual domain remains challenging due to 1) limited modality coverage and multi-hop connectivity of existing MMKGs, and 2) retrieval based solely on similarity in a shared multimodal embedding space, which fails to filter out off-topic or redundant knowledge. To address these limitations, we propose M$^3$KG-RAG, a Multi-hop Multimodal Knowledge Graph-enhanced RAG that retrieves query-aligned audio-visual knowledge from MMKGs, improving reasoning depth and answer faithfulness in MLLMs. Specifically, we devise a lightweight multi-agent pipeline to construct multi-hop MMKG (M$^3$KG), which contains context-enriched triplets of multimodal entities, enabling modality-wise retrieval based on input queries. Furthermore, we introduce GRASP (Grounded Retrieval And Selective Pruning), which ensures precise entity grounding to the query, evaluates answer-supporting relevance, and prunes redundant context to retain only knowledge essential for response generation. Extensive experiments across diverse multimodal benchmarks demonstrate that M$^3$KG-RAG significantly enhances MLLMs' multimodal reasoning and grounding over existing approaches.","authors":["Hyeongcheol Park","Jiyoung Seo","Jaewon Mun","Hogun Park","Wonmin Byeon","Sung June Kim","Hyeonsoo Im","JeungSub Lee","Sangpil Kim"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20111v1","updated":"2025-12-23T07:11:26Z","published":"2025-12-23T07:11:26Z","title":"ABBEL: LLM Agents Acting through Belief Bottlenecks Expressed in Language","summary":"As the length of sequential decision-making tasks increases, it becomes computationally impractical to keep full interaction histories in context. We introduce a general framework for LLM agents to maintain concise contexts through multi-step interaction: Acting through Belief Bottlenecks Expressed in Language (ABBEL), and methods to further improve ABBEL agents with RL post-training. ABBEL replaces long multi-step interaction history by a belief state, i.e., a natural language summary of what has been discovered about task-relevant unknowns. Under ABBEL, at each step the agent first updates a prior belief with the most recent observation from the environment to form a posterior belief, then uses only the posterior to select an action. We systematically evaluate frontier models under ABBEL across six diverse multi-step environments, finding that ABBEL supports generating interpretable beliefs while maintaining near-constant memory use over interaction steps. However, bottleneck approaches are generally prone to error propagation, which we observe causing inferior performance when compared to the full context setting due to errors in belief updating. Therefore, we train LLMs to generate and act on beliefs within the ABBEL framework via reinforcement learning (RL). We experiment with belief grading, to reward higher quality beliefs, as well as belief length penalties to reward more compressed beliefs. Our experiments demonstrate the ability of RL to improve ABBEL's performance beyond the full context setting, while using less memory than contemporaneous approaches.","authors":["Aly Lidayan","Jakob Bjorner","Satvik Golechha","Kartik Goyal","Alane Suhr"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2505.17266v3","updated":"2025-12-23T07:05:40Z","published":"2025-05-22T20:24:08Z","title":"Select2Reason: Efficient Instruction-Tuning Data Selection for Long-CoT Reasoning","summary":"A practical approach to activate long chain-of-thoughts reasoning ability in pre-trained large language models is to perform supervised fine-tuning on instruction datasets synthesized by strong Large Reasoning Models such as DeepSeek-R1, offering a cost-effective alternative to reinforcement learning. However, large-scale instruction sets with more than 100k samples incur significant training overhead, while effective strategies for automatic long-CoT instruction selection still remain unexplored. In this work, we propose Select2Reason, a novel and efficient instruction-tuning data selection framework for long-CoT reasoning. From the perspective of emergence of rethinking behaviors like self-correction and backtracking, we investigate common metrics that may determine the quality of long-CoT reasoning instructions. Select2Reason leverages a quantifier to estimate difficulty of question and jointly incorporates a reasoning trace length-based heuristic through a weighted scheme for ranking to prioritize high-utility examples. Empirical results on OpenR1-Math-220k demonstrate that fine-tuning LLM on only 10% of the data selected by Select2Reason achieves performance competitive with or superior to full-data tuning and open-source baseline OpenR1-Qwen-7B across three competition-level and six comprehensive mathematical benchmarks. Further experiments highlight the scalability in varying data size, efficiency during inference, and its adaptability to other instruction pools with minimal cost.","authors":["Cehao Yang","Xueyuan Lin","Xiaojun Wu","Chengjin Xu","Xuhui Jiang","Honghao Liu","Hui Xiong","Jian Guo"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20097v1","updated":"2025-12-23T06:49:33Z","published":"2025-12-23T06:49:33Z","title":"A Novel Graph-Sequence Learning Model for Inductive Text Classification","summary":"Text classification plays an important role in various downstream text-related tasks, such as sentiment analysis, fake news detection, and public opinion analysis. Recently, text classification based on Graph Neural Networks (GNNs) has made significant progress due to their strong capabilities of structural relationship learning. However, these approaches still face two major limitations. First, these approaches fail to fully consider the diverse structural information across word pairs, e.g., co-occurrence, syntax, and semantics. Furthermore, they neglect sequence information in the text graph structure information learning module and can not classify texts with new words and relations. In this paper, we propose a Novel Graph-Sequence Learning Model for Inductive Text Classification (TextGSL) to address the previously mentioned issues. More specifically, we construct a single text-level graph for all words in each text and establish different edge types based on the diverse relationships between word pairs. Building upon this, we design an adaptive multi-edge message-passing paradigm to aggregate diverse structural information between word pairs. Additionally, sequential information among text data can be captured by the proposed TextGSL through the incorporation of Transformer layers. Therefore, TextGSL can learn more discriminative text representations. TextGSL has been comprehensively compared with several strong baselines. The experimental results on diverse benchmarking datasets demonstrate that TextGSL outperforms these baselines in terms of accuracy.","authors":["Zuo Wang","Ye Yuan"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20092v1","updated":"2025-12-23T06:37:29Z","published":"2025-12-23T06:37:29Z","title":"Memory-T1: Reinforcement Learning for Temporal Reasoning in Multi-session Agents","summary":"Temporal reasoning over long, multi-session dialogues is a critical capability for conversational agents. However, existing works and our pilot study have shown that as dialogue histories grow in length and accumulate noise, current long-context models struggle to accurately identify temporally pertinent information, significantly impairing reasoning performance. To address this, we introduce Memory-T1, a framework that learns a time-aware memory selection policy using reinforcement learning (RL). It employs a coarse-to-fine strategy, first pruning the dialogue history into a candidate set using temporal and relevance filters, followed by an RL agent that selects the precise evidence sessions. The RL training is guided by a multi-level reward function optimizing (i) answer accuracy, (ii) evidence grounding, and (iii) temporal consistency. In particular, the temporal consistency reward provides a dense signal by evaluating alignment with the query time scope at both the session-level (chronological proximity) and the utterance-level (chronological fidelity), enabling the agent to resolve subtle chronological ambiguities. On the Time-Dialog benchmark, Memory-T1 boosts a 7B model to an overall score of 67.0\\%, establishing a new state-of-the-art performance for open-source models and outperforming a 14B baseline by 10.2\\%. Ablation studies show temporal consistency and evidence grounding rewards jointly contribute to a 15.0\\% performance gain. Moreover, Memory-T1 maintains robustness up to 128k tokens, where baseline models collapse, proving effectiveness against noise in extensive dialogue histories. The code and datasets are publicly available at https://github.com/Elvin-Yiming-Du/Memory-T1/","authors":["Yiming Du","Baojun Wang","Yifan Xiang","Zhaowei Wang","Wenyu Huang","Boyang Xue","Bin Liang","Xingshan Zeng","Fei Mi","Haoli Bai","Lifeng Shang","Jeff Z. Pan","Yuxin Jiang","Kam-Fai Wong"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19012v2","updated":"2025-12-23T06:23:22Z","published":"2025-12-22T04:03:01Z","title":"DramaBench: A Six-Dimensional Evaluation Framework for Drama Script Continuation","summary":"Drama script continuation requires models to maintain character consistency, advance plot coherently, and preserve dramatic structurecapabilities that existing benchmarks fail to evaluate comprehensively. We present DramaBench, the first large-scale benchmark for evaluating drama script continuation across six independent dimensions: Format Standards, Narrative Efficiency, Character Consistency, Emotional Depth, Logic Consistency, and Conflict Handling. Our framework combines rulebased analysis with LLM-based labeling and statistical metrics, ensuring objective and reproducible evaluation. We conduct comprehensive evaluation of 8 state-of-the-art language models on 1,103 scripts (8,824 evaluations total), with rigorous statistical significance testing (252 pairwise comparisons, 65.9% significant) and human validation (188 scripts, substantial agreement on 3/5 dimensions). Our ablation studies confirm all six dimensions capture independent quality aspects (mean | r | = 0.020). DramaBench provides actionable, dimensionspecific feedback for model improvement and establishes a rigorous standard for creative writing evaluation.","authors":["Shijian Ma","Yunqi Huang","Yan Lin"],"pdf_url":"","comment":"Project page: https://dramabench.pages.dev/"},{"id":"http://arxiv.org/abs/2512.20074v1","updated":"2025-12-23T05:58:47Z","published":"2025-12-23T05:58:47Z","title":"Reason2Decide: Rationale-Driven Multi-Task Learning","summary":"Despite the wide adoption of Large Language Models (LLM)s, clinical decision support systems face a critical challenge: achieving high predictive accuracy while generating explanations aligned with the predictions. Current approaches suffer from exposure bias leading to misaligned explanations. We propose Reason2Decide, a two-stage training framework that addresses key challenges in self-rationalization, including exposure bias and task separation. In Stage-1, our model is trained on rationale generation, while in Stage-2, we jointly train on label prediction and rationale generation, applying scheduled sampling to gradually transition from conditioning on gold labels to model predictions. We evaluate Reason2Decide on three medical datasets, including a proprietary triage dataset and public biomedical QA datasets. Across model sizes, Reason2Decide outperforms other fine-tuning baselines and some zero-shot LLMs in prediction (F1) and rationale fidelity (BERTScore, BLEU, LLM-as-a-Judge). In triage, Reason2Decide is rationale source-robust across LLM-generated, nurse-authored, and nurse-post-processed rationales. In our experiments, while using only LLM-generated rationales in Stage-1, Reason2Decide outperforms other fine-tuning variants. This indicates that LLM-generated rationales are suitable for pretraining models, reducing reliance on human annotations. Remarkably, Reason2Decide achieves these gains with models 40x smaller than contemporary foundation models, making clinical reasoning more accessible for resource-constrained deployments while still providing explainable decision support.","authors":["H M Quamran Hasan","Housam Khalifa Bashier","Jiayi Dai","Mi-Young Kim","Randy Goebel"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2510.04944v2","updated":"2025-12-23T05:51:37Z","published":"2025-10-06T15:46:50Z","title":"On Structured State-Space Duality","summary":"Structured State-Space Duality (SSD) [Dao & Gu, ICML 2024] is an equivalence between a simple Structured State-Space Model (SSM) and a masked attention mechanism. In particular, a state-space model with a scalar-times-identity state matrix is equivalent to a masked self-attention with a $1$-semiseparable causal mask. Consequently, the same sequence transformation (model) has two algorithmic realizations: as a linear-time $O(T)$ recurrence or as a quadratic-time $O(T^2)$ attention. In this note, we formalize and generalize this duality: (i) we extend SSD from the scalar-identity case to general diagonal SSMs (diagonal state matrices); (ii) we show that these diagonal SSMs match the scalar case's training complexity lower bounds while supporting richer dynamics; (iii) we establish a necessary and sufficient condition under which an SSM is equivalent to $1$-semiseparable masked attention; and (iv) we show that such duality fails to extend to standard softmax attention due to rank explosion. Together, these results tighten bridge between recurrent SSMs and Transformers, and widen the design space for expressive yet efficient sequence models.","authors":["Jerry Yao-Chieh Hu","Xiwen Zhang","Ali ElSheikh","Weimin Wu","Han Liu"],"pdf_url":"","comment":"v2 fixed typos and added numerical results (Appendix B)"},{"id":"http://arxiv.org/abs/2408.02152v3","updated":"2025-12-23T05:50:45Z","published":"2024-08-04T22:00:34Z","title":"Generative Retrieval with Few-shot Indexing","summary":"Existing generative retrieval (GR) methods rely on training-based indexing, which fine-tunes a model to memorise associations between queries and the document identifiers (docids) of relevant documents. Training-based indexing suffers from high training costs, under-utilisation of pre-trained knowledge in large language models (LLMs), and limited adaptability to dynamic document corpora. To address the issues, we propose a few-shot indexing-based GR framework (Few-Shot GR). It has a few-shot indexing process without any training, where we prompt an LLM to generate docids for all documents in a corpus, ultimately creating a docid bank for the entire corpus. During retrieval, we feed a query to the same LLM and constrain it to generate a docid within the docid bank created during indexing, and then map the generated docid back to its corresponding document. Moreover, we devise few-shot indexing with one-to-many mapping to further enhance Few-Shot GR. Experiments show that Few-Shot GR achieves superior performance to state-of-the-art GR methods requiring heavy training.","authors":["Arian Askari","Chuan Meng","Mohammad Aliannejadi","Zhaochun Ren","Evangelos Kanoulas","Suzan Verberne"],"pdf_url":"","comment":"Accepted for publication at the 48th European Conference on Information Retrieval (ECIR 2026)"},{"id":"http://arxiv.org/abs/2509.16189v3","updated":"2025-12-23T05:18:37Z","published":"2025-09-19T17:49:25Z","title":"Latent learning: episodic memory complements parametric learning by enabling flexible reuse of experiences","summary":"When do machine learning systems fail to generalize, and what mechanisms could improve their generalization? Here, we draw inspiration from cognitive science to argue that one weakness of parametric machine learning systems is their failure to exhibit latent learning -- learning information that is not relevant to the task at hand, but that might be useful in a future task. We show how this perspective links failures ranging from the reversal curse in language modeling to new findings on agent-based navigation. We then highlight how cognitive science points to episodic memory as a potential part of the solution to these issues. Correspondingly, we show that a system with an oracle retrieval mechanism can use learning experiences more flexibly to generalize better across many of these challenges. We also identify some of the essential components for effectively using retrieval, including the importance of within-example in-context learning for acquiring the ability to use information across retrieved examples. In summary, our results illustrate one possible contributor to the relative data inefficiency of current machine learning systems compared to natural intelligence, and help to understand how retrieval methods can complement parametric learning to improve generalization. We close by discussing some of the links between these findings and prior results in cognitive science and neuroscience, and the broader implications.","authors":["Andrew Kyle Lampinen","Martin Engelcke","Yuxuan Li","Arslan Chaudhry","James L. McClelland"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2508.04826v3","updated":"2025-12-23T05:07:19Z","published":"2025-08-06T19:11:33Z","title":"Persistent Instability in LLM's Personality Measurements: Effects of Scale, Reasoning, and Conversation History","summary":"Large language models require consistent behavioral patterns for safe deployment, yet there are indications of large variability that may lead to an instable expression of personality traits in these models. We present PERSIST (PERsonality Stability in Synthetic Text), a comprehensive evaluation framework testing 25 open-source models (1B-685B parameters) across 2 million+ responses. Using traditional (BFI, SD3) and novel LLM-adapted personality questionnaires, we systematically vary model size, personas, reasoning modes, question order or paraphrasing, and conversation history. Our findings challenge fundamental assumptions: (1) Question reordering alone can introduce large shifts in personality measurements; (2) Scaling provides limited stability gains: even 400B+ models exhibit standard deviations >0.3 on 5-point scales; (3) Interventions expected to stabilize behavior, such as reasoning and inclusion of conversation history, can paradoxically increase variability; (4) Detailed persona instructions produce mixed effects, with misaligned personas showing significantly higher variability than the helpful assistant baseline; (5) The LLM-adapted questionnaires, despite their improved ecological validity, exhibit instability comparable to human-centric versions. This persistent instability across scales and mitigation strategies suggests that current LLMs lack the architectural foundations for genuine behavioral consistency. For safety-critical applications requiring predictable behavior, these findings indicate that current alignment strategies may be inadequate.","authors":["Tommaso Tosato","Saskia Helbling","Yorguin-Jose Mantilla-Ramos","Mahmood Hegazy","Alberto Tosato","David John Lemay","Irina Rish","Guillaume Dumas"],"pdf_url":"","comment":"Accepted at AAAI 2026, Track on AI Alignment"},{"id":"http://arxiv.org/abs/2511.06148v2","updated":"2025-12-23T04:52:15Z","published":"2025-11-08T21:58:26Z","title":"Large Language Models Develop Novel Social Biases Through Adaptive Exploration","summary":"As large language models (LLMs) are adopted into frameworks that grant them the capacity to make real decisions, it is increasingly important to ensure that they are unbiased. In this paper, we argue that the predominant approach of simply removing existing biases from models is not enough. Using a paradigm from the psychology literature, we demonstrate that LLMs can spontaneously develop novel social biases about artificial demographic groups even when no inherent differences exist. These biases result in highly stratified task allocations, which are less fair than assignments by human participants and are exacerbated by newer and larger models. In social science, emergent biases like these have been shown to result from exploration-exploitation trade-offs, where the decision-maker explores too little, allowing early observations to strongly influence impressions about entire demographic groups. To alleviate this effect, we examine a series of interventions targeting model inputs, problem structure, and explicit steering. We find that explicitly incentivizing exploration most robustly reduces stratification, highlighting the need for better multifaceted objectives to mitigate bias. These results reveal that LLMs are not merely passive mirrors of human social biases, but can actively create new ones from experience, raising urgent questions about how these systems will shape societies over time.","authors":["Addison J. Wu","Ryan Liu","Xuechunzi Bai","Thomas L. Griffiths"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2501.07890v3","updated":"2025-12-23T03:48:57Z","published":"2025-01-14T06:59:51Z","title":"GRAPHMOE: Amplifying Cognitive Depth of Mixture-of-Experts Network via Introducing Self-Rethinking Mechanism","summary":"Traditional Mixture-of-Experts (MoE) networks benefit from utilizing multiple smaller expert models as opposed to a single large network. However, these experts typically operate independently, leaving a question open about whether interconnecting these models could enhance the performance of MoE networks. In response, we introduce GRAPHMOE, a novel method aimed at augmenting the cognitive depth of language models via a self-rethinking mechanism constructed on Pseudo GraphMoE networks. GRAPHMOE employs a recurrent routing strategy to simulate iterative thinking steps, thereby facilitating the flow of information among expert nodes. We implement the GRAPHMOE architecture using Low-Rank Adaptation techniques (LoRA) and conduct extensive experiments on various benchmark datasets. The experimental results reveal that GRAPHMOE outperforms other LoRA based models, achieving state-of-the-art (SOTA) performance. Additionally, this study explores a novel recurrent routing strategy that may inspire further advancements in enhancing the reasoning capabilities of language models.","authors":["Bo Lv","Chen Tang","Zifan Zheng","Bohao Yang","Kun Zhao","Ning Liao","Xiaoxing Wang","Feiyu Xiong","Zhiyu Li","Nayu Liu","Jingchi Jiang"],"pdf_url":"","comment":"10 pages"},{"id":"http://arxiv.org/abs/2512.19682v2","updated":"2025-12-23T03:45:42Z","published":"2025-12-22T18:57:13Z","title":"GenEnv: Difficulty-Aligned Co-Evolution Between LLM Agents and Environment Simulators","summary":"Training capable Large Language Model (LLM) agents is critically bottlenecked by the high cost and static nature of real-world interaction data. We address this by introducing GenEnv, a framework that establishes a difficulty-aligned co-evolutionary game between an agent and a scalable, generative environment simulator. Unlike traditional methods that evolve models on static datasets, GenEnv instantiates a dataevolving: the simulator acts as a dynamic curriculum policy, continuously generating tasks specifically tailored to the agent's ``zone of proximal development''. This process is guided by a simple but effective $α$-Curriculum Reward, which aligns task difficulty with the agent's current capabilities. We evaluate GenEnv on five benchmarks, including API-Bank, ALFWorld, BFCL, Bamboogle, and TravelPlanner. Across these tasks, GenEnv improves agent performance by up to \\textbf{+40.3\\%} over 7B baselines and matches or exceeds the average performance of larger models. Compared to Gemini 2.5 Pro-based offline data augmentation, GenEnv achieves better performance while using 3.3$\\times$ less data. By shifting from static supervision to adaptive simulation, GenEnv provides a data-efficient pathway for scaling agent capabilities.","authors":["Jiacheng Guo","Ling Yang","Peter Chen","Qixin Xiao","Yinjie Wang","Xinzhe Juan","Jiahao Qiu","Ke Shen","Mengdi Wang"],"pdf_url":"","comment":"Our codes are available at https://github.com/Gen-Verse/GenEnv"},{"id":"http://arxiv.org/abs/2512.19455v2","updated":"2025-12-23T03:39:28Z","published":"2025-12-22T15:00:25Z","title":"SiamGPT: Quality-First Fine-Tuning for Stable Thai Text Generation","summary":"Open-weights large language models remain difficult to deploy for Thai due to unstable generation under complex instructions, despite strong English performance. To mitigate these limitations, We present SiamGPT-32B, an open-weights model based on Qwen3-32B, fine-tuned with a Quality-First strategy emphasizing curated supervision over data scale. The fine-tuning pipeline combines translated high-complexity English instruction data with a Thai-adapted AutoIF framework for instruction and linguistic constraints. Using supervised fine-tuning only, without continual pretraining or corpus expansion, SiamGPT-32B improves instruction adherence, multi-turn robustness, and linguistic stability. Evaluations on the SEA-HELM benchmark show that SiamGPT-32B achieves the strongest overall performance among similar-scale open-weights Thai models, with consistent gains in instruction following, multi-turn dialogue, and natural language understanding.","authors":["Thittipat Pairatsuppawat","Abhibhu Tachaapornchai","Paweekorn Kusolsomboon","Chutikan Chaiwong","Thodsaporn Chay-intr","Kobkrit Viriyayudhakorn","Nongnuch Ketui","Aslan B. Wong"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2410.23066v3","updated":"2025-12-23T03:28:14Z","published":"2024-10-30T14:41:23Z","title":"Don't Pay Attention, PLANT It: Pretraining Attention via Learning-to-Rank","summary":"State-of-the-art Extreme Multi-Label Text Classification models rely on multi-label attention to focus on key tokens in input text, but learning good attention weights is challenging. We introduce PLANT - Pretrained and Leveraged Attention - a plug-and-play strategy for initializing attention. PLANT works by planting label-specific attention using a pretrained Learning-to-Rank model guided by mutual information gain. This architecture-agnostic approach integrates seamlessly with large language model backbones such as Mistral-7B, LLaMA3-8B, DeepSeek-V3, and Phi-3. PLANT outperforms state-of-the-art methods across tasks including ICD coding, legal topic classification, and content recommendation. Gains are especially pronounced in few-shot settings, with substantial improvements on rare labels. Ablation studies confirm that attention initialization is a key driver of these gains. For code and trained models, see https://github.com/debjyotiSRoy/xcube/tree/plant","authors":["Debjyoti Saha Roy","Byron C. Wallace","Javed A. Aslam"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.15649v2","updated":"2025-12-23T03:03:58Z","published":"2025-12-17T17:58:35Z","title":"VTCBench: Can Vision-Language Models Understand Long Context with Vision-Text Compression?","summary":"The computational and memory overheads associated with expanding the context window of LLMs severely limit their scalability. A noteworthy solution is vision-text compression (VTC), exemplified by frameworks like DeepSeek-OCR and Glyph, which convert long texts into dense 2D visual representations, thereby achieving token compression ratios of 3x-20x. However, the impact of this high information density on the core long-context capabilities of vision-language models (VLMs) remains under-investigated. To address this gap, we introduce the first benchmark for VTC and systematically assess the performance of VLMs across three long-context understanding settings: VTC-Retrieval, which evaluates the model's ability to retrieve and aggregate information; VTC-Reasoning, which requires models to infer latent associations to locate facts with minimal lexical overlap; and VTC-Memory, which measures comprehensive question answering within long-term dialogue memory. Furthermore, we establish the VTCBench-Wild to simulate diverse input scenarios.We comprehensively evaluate leading open-source and proprietary models on our benchmarks. The results indicate that, despite being able to decode textual information (e.g., OCR) well, most VLMs exhibit a surprisingly poor long-context understanding ability with VTC-processed information, failing to capture long associations or dependencies in the context.This study provides a deep understanding of VTC and serves as a foundation for designing more efficient and scalable VLMs.","authors":["Hongbo Zhao","Meng Wang","Fei Zhu","Wenzhuo Liu","Bolin Ni","Fanhu Zeng","Gaofeng Meng","Zhaoxiang Zhang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19995v1","updated":"2025-12-23T02:44:25Z","published":"2025-12-23T02:44:25Z","title":"Schoenfeld's Anatomy of Mathematical Reasoning by Language Models","summary":"Large language models increasingly expose reasoning traces, yet their underlying cognitive structure and steps remain difficult to identify and analyze beyond surface-level statistics. We adopt Schoenfeld's Episode Theory as an inductive, intermediate-scale lens and introduce ThinkARM (Anatomy of Reasoning in Models), a scalable framework that explicitly abstracts reasoning traces into functional reasoning steps such as Analysis, Explore, Implement, Verify, etc. When applied to mathematical problem solving by diverse models, this abstraction reveals reproducible thinking dynamics and structural differences between reasoning and non-reasoning models, which are not apparent from token-level views. We further present two diagnostic case studies showing that exploration functions as a critical branching step associated with correctness, and that efficiency-oriented methods selectively suppress evaluative feedback steps rather than uniformly shortening responses. Together, our results demonstrate that episode-level representations make reasoning steps explicit, enabling systematic analysis of how reasoning is structured, stabilized, and altered in modern language models.","authors":["Ming Li","Chenrui Fan","Yize Cheng","Soheil Feizi","Tianyi Zhou"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2505.24347v3","updated":"2025-12-23T02:15:00Z","published":"2025-05-30T08:40:49Z","title":"Fewer Hallucinations, More Verification: A Three-Stage LLM-Based Framework for ASR Error Correction","summary":"Automatic Speech Recognition (ASR) error correction aims to correct recognition errors while preserving accurate text. Although traditional approaches demonstrate moderate effectiveness, LLMs offer a paradigm that eliminates the need for training and labeled data. However, directly using LLMs will encounter hallucinations problem, which may lead to the modification of the correct text. To address this problem, we propose the Reliable LLM Correction Framework (RLLM-CF), which consists of three stages: (1) error pre-detection, (2) chain-of-thought sub-tasks iterative correction, and (3) reasoning process verification. The advantage of our method is that it does not require additional information or fine-tuning of the model, and ensures the correctness of the LLM correction under multi-pass programming. Experiments on AISHELL-1, AISHELL-2, and Librispeech show that the GPT-4o model enhanced by our framework achieves 21%, 11%, 9%, and 11.4% relative reductions in CER/WER.","authors":["Yangui Fang","Baixu Chen","Jing Peng","Xu Li","Yu Xi","Chengwei Zhang","Guohui Zhong"],"pdf_url":"","comment":"This paper has been ACCEPTED for publication in ASRU"},{"id":"http://arxiv.org/abs/2506.05671v2","updated":"2025-12-23T02:11:59Z","published":"2025-06-06T01:34:29Z","title":"Low-Resource Domain Adaptation for Speech LLMs via Text-Only Fine-Tuning","summary":"Recent advances in automatic speech recognition (ASR) have combined speech encoders with large language models (LLMs) through projection, forming Speech LLMs with strong performance. However, adapting them to new domains remains challenging, especially in low-resource settings where paired speech-text data is scarce. We propose a text-only fine-tuning strategy for Speech LLMs using unpaired target-domain text without requiring additional audio. To preserve speech-text alignment, we introduce a real-time evaluation mechanism during fine-tuning. This enables effective domain adaptation while maintaining source-domain performance. Experiments on LibriSpeech, SlideSpeech, and Medical datasets show that our method achieves competitive recognition performance, with minimal degradation compared to full audio-text fine-tuning. It also improves generalization to new domains without catastrophic forgetting, highlighting the potential of text-only fine-tuning for low-resource domain adaptation of ASR.","authors":["Yangui Fang","Jing Peng","Xu Li","Yu Xi","Chengwei Zhang","Guohui Zhong","Kai Yu"],"pdf_url":"","comment":"This paper has been ACCEPTED for publication in ASRU"},{"id":"http://arxiv.org/abs/2512.19950v1","updated":"2025-12-23T00:41:48Z","published":"2025-12-23T00:41:48Z","title":"Bias Beneath the Tone: Empirical Characterisation of Tone Bias in LLM-Driven UX Systems","summary":"Large Language Models are increasingly used in conversational systems such as digital personal assistants, shaping how people interact with technology through language. While their responses often sound fluent and natural, they can also carry subtle tone biases such as sounding overly polite, cheerful, or cautious even when neutrality is expected. These tendencies can influence how users perceive trust, empathy, and fairness in dialogue. In this study, we explore tone bias as a hidden behavioral trait of large language models. The novelty of this research lies in the integration of controllable large language model based dialogue synthesis with tone classification models, enabling robust and ethical emotion recognition in personal assistant interactions. We created two synthetic dialogue datasets, one generated from neutral prompts and another explicitly guided to produce positive or negative tones. Surprisingly, even the neutral set showed consistent tonal skew, suggesting that bias may stem from the model's underlying conversational style. Using weak supervision through a pretrained DistilBERT model, we labeled tones and trained several classifiers to detect these patterns. Ensemble models achieved macro F1 scores up to 0.92, showing that tone bias is systematic, measurable, and relevant to designing fair and trustworthy conversational AI.","authors":["Heet Bodara","Md Masum Mushfiq","Isma Farah Siddiqui"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20848v1","updated":"2025-12-23T23:54:32Z","published":"2025-12-23T23:54:32Z","title":"Nemotron 3 Nano: Open, Efficient Mixture-of-Experts Hybrid Mamba-Transformer Model for Agentic Reasoning","summary":"We present Nemotron 3 Nano 30B-A3B, a Mixture-of-Experts hybrid Mamba-Transformer language model. Nemotron 3 Nano was pretrained on 25 trillion text tokens, including more than 3 trillion new unique tokens over Nemotron 2, followed by supervised fine tuning and large-scale RL on diverse environments. Nemotron 3 Nano achieves better accuracy than our previous generation Nemotron 2 Nano while activating less than half of the parameters per forward pass. It achieves up to 3.3x higher inference throughput than similarly-sized open models like GPT-OSS-20B and Qwen3-30B-A3B-Thinking-2507, while also being more accurate on popular benchmarks. Nemotron 3 Nano demonstrates enhanced agentic, reasoning, and chat abilities and supports context lengths up to 1M tokens. We release both our pretrained Nemotron 3 Nano 30B-A3B Base and post-trained Nemotron 3 Nano 30B-A3B checkpoints on Hugging Face.","authors":[" NVIDIA"," :","Aaron Blakeman","Aaron Grattafiori","Aarti Basant","Abhibha Gupta","Abhinav Khattar","Adi Renduchintala","Aditya Vavre","Akanksha Shukla","Akhiad Bercovich","Aleksander Ficek","Aleksandr Shaposhnikov","Alex Kondratenko","Alexander Bukharin","Alexandre Milesi","Ali Taghibakhshi","Alisa Liu","Amelia Barton","Ameya Sunil Mahabaleshwarkar","Amir Klein","Amit Zuker","Amnon Geifman","Amy Shen","Anahita Bhiwandiwalla","Andrew Tao","Ann Guan","Anubhav Mandarwal","Arham Mehta","Ashwath Aithal","Ashwin Poojary","Asif Ahamed","Asma Kuriparambil Thekkumpate","Ayush Dattagupta","Banghua Zhu","Bardiya Sadeghi","Barnaby Simkin","Ben Lanir","Benedikt Schifferer","Besmira Nushi","Bilal Kartal","Bita Darvish Rouhani","Boris Ginsburg","Brandon Norick","Brandon Soubasis","Branislav Kisacanin","Brian Yu","Bryan Catanzaro","Carlo del Mundo","Chantal Hwang","Charles Wang","Cheng-Ping Hsieh","Chenghao Zhang","Chenhan Yu","Chetan Mungekar","Chintan Patel","Chris Alexiuk","Christopher Parisien","Collin Neale","Damon Mosk-Aoyama","Dan Su","Dane Corneil","Daniel Afrimi","Daniel Rohrer","Daniel Serebrenik","Daria Gitman","Daria Levy","Darko Stosic","David Mosallanezhad","Deepak Narayanan","Dhruv Nathawani","Dima Rekesh","Dina Yared","Divyanshu Kakwani","Dong Ahn","Duncan Riach","Dusan Stosic","Edgar Minasyan","Edward Lin","Eileen Long","Eileen Peters Long","Elena Lantz","Ellie Evans","Elliott Ning","Eric Chung","Eric Harper","Eric Tramel","Erick Galinkin","Erik Pounds","Evan Briones","Evelina Bakhturina","Faisal Ladhak","Fay Wang","Fei Jia","Felipe Soares","Feng Chen","Ferenc Galko","Frankie Siino","Gal Hubara Agam","Ganesh Ajjanagadde","Gantavya Bhatt","Gargi Prasad","George Armstrong","Gerald Shen","Gorkem Batmaz","Grigor Nalbandyan","Haifeng Qian","Harsh Sharma","Hayley Ross","Helen Ngo","Herman Sahota","Hexin Wang","Himanshu Soni","Hiren Upadhyay","Huizi Mao","Huy C Nguyen","Huy Q Nguyen","Iain Cunningham","Ido Shahaf","Igor Gitman","Ilya Loshchilov","Ivan Moshkov","Izzy Putterman","Jan Kautz","Jane Polak Scowcroft","Jared Casper","Jatin Mitra","Jeffrey Glick","Jenny Chen","Jesse Oliver","Jian Zhang","Jiaqi Zeng","Jie Lou","Jimmy Zhang","Jining Huang","Joey Conway","Joey Guman","John Kamalu","Johnny Greco","Jonathan Cohen","Joseph Jennings","Joyjit Daw","Julien Veron Vialard","Junkeun Yi","Jupinder Parmar","Kai Xu","Kan Zhu","Kari Briski","Katherine Cheung","Katherine Luna","Keshav Santhanam","Kevin Shih","Kezhi Kong","Khushi Bhardwaj","Krishna C. Puvvada","Krzysztof Pawelec","Kumar Anik","Lawrence McAfee","Laya Sleiman","Leon Derczynski","Li Ding","Lucas Liebenwein","Luis Vega","Maanu Grover","Maarten Van Segbroeck","Maer Rodrigues de Melo","Makesh Narsimhan Sreedhar","Manoj Kilaru","Maor Ashkenazi","Marc Romeijn","Mark Cai","Markus Kliegl","Maryam Moosaei","Matvei Novikov","Mehrzad Samadi","Melissa Corpuz","Mengru Wang","Meredith Price","Michael Boone","Michael Evans","Miguel Martinez","Mike Chrzanowski","Mohammad Shoeybi","Mostofa Patwary","Nabin Mulepati","Natalie Hereth","Nave Assaf","Negar Habibi","Neta Zmora","Netanel Haber","Nicola Sessions","Nidhi Bhatia","Nikhil Jukar","Nikki Pope","Nikolai Ludwig","Nima Tajbakhsh","Nirmal Juluru","Oleksii Hrinchuk","Oleksii Kuchaiev","Olivier Delalleau","Oluwatobi Olabiyi","Omer Ullman Argov","Ouye Xie","Parth Chadha","Pasha Shamis","Pavlo Molchanov","Pawel Morkisz","Peter Dykas","Peter Jin","Pinky Xu","Piotr Januszewski","Pranav Prashant Thombre","Prasoon Varshney","Pritam Gundecha","Qing Miao","Rabeeh Karimi Mahabadi","Ran El-Yaniv","Ran Zilberstein","Rasoul Shafipour","Rich Harang","Rick Izzo","Rima Shahbazyan","Rishabh Garg","Ritika Borkar","Ritu Gala","Riyad Islam","Roger Waleffe","Rohit Watve","Roi Koren","Ruoxi Zhang","Russell J. Hewett","Ryan Prenger","Ryan Timbrook","Sadegh Mahdavi","Sahil Modi","Samuel Kriman","Sanjay Kariyappa","Sanjeev Satheesh","Saori Kaji","Satish Pasumarthi","Sean Narentharen","Sean Narenthiran","Seonmyeong Bak","Sergey Kashirsky","Seth Poulos","Shahar Mor","Shanmugam Ramasamy","Shantanu Acharya","Shaona Ghosh","Sharath Turuvekere Sreenivas","Shelby Thomas","Shiqing Fan","Shreya Gopal","Shrimai Prabhumoye","Shubham Pachori","Shubham Toshniwal","Shuoyang Ding","Siddharth Singh","Simeng Sun","Smita Ithape","Somshubra Majumdar","Soumye Singhal","Stefania Alborghetti","Stephen Ge","Sugam Dipak Devare","Sumeet Kumar Barua","Suseella Panguluri","Suyog Gupta","Sweta Priyadarshi","Syeda Nahida Akter","Tan Bui","Teodor-Dumitru Ene","Terry Kong","Thanh Do","Tijmen Blankevoort","Tom Balough","Tomer Asida","Tomer Bar Natan","Tugrul Konuk","Twinkle Vashishth","Udi Karpas","Ushnish De","Vahid Noorozi","Vahid Noroozi","Venkat Srinivasan","Venmugil Elango","Vijay Korthikanti","Vitaly Kurin","Vitaly Lavrukhin","Wanli Jiang","Wasi Uddin Ahmad","Wei Du","Wei Ping","Wenfei Zhou","Will Jennings","William Zhang","Wojciech Prazuch","Xiaowei Ren","Yashaswi Karnati","Yejin Choi","Yev Meyer","Yi-Fu Wu","Yian Zhang","Ying Lin","Yonatan Geifman","Yonggan Fu","Yoshi Subara","Yoshi Suhara","Yubo Gao","Zach Moshe","Zhen Dong","Zihan Liu","Zijia Chen","Zijie Yan"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2504.18839v3","updated":"2025-12-23T23:25:26Z","published":"2025-04-26T07:51:05Z","title":"Detect, Explain, Escalate: Sustainable Dialogue Breakdown Management for LLM Agents","summary":"Large Language Models (LLMs) have demonstrated substantial capabilities in conversational AI applications, yet their susceptibility to dialogue breakdowns poses significant challenges to deployment reliability and user trust. This paper introduces a \"Detect, Explain, Escalate\" framework to manage dialogue breakdowns in LLM-powered agents, emphasizing resource-efficient operation. Our approach integrates two key strategies: (1) We fine-tune a compact 8B-parameter model, augmented with teacher-generated reasoning traces, which serves as an efficient real-time breakdown detector and explainer. This model demonstrates robust classification and calibration on English and Japanese dialogues, and generalizes to the BETOLD dataset, improving accuracy by 7% over its baseline. (2) We systematically evaluate frontier LLMs using advanced prompting (few-shot, chain-of-thought, analogical reasoning) for high-fidelity breakdown assessment. These are integrated into an \"escalation\" architecture where our efficient detector defers to larger models only when necessary, substantially reducing operational costs and computational overhead. Our fine-tuned model and prompting strategies achieve state-of-the-art performance on DBDC5 and strong results on BETOLD, outperforming specialized classifiers on DBDC5 and narrowing the performance gap to larger proprietary models. The proposed monitor-escalate pipeline reduces inference costs by 54%, providing a cost-effective and interpretable solution for robust conversational AI in high-impact domains. Code and models will be publicly released.","authors":["Abdellah Ghassel","Xianzhi Li","Xiaodan Zhu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20822v1","updated":"2025-12-23T22:52:24Z","published":"2025-12-23T22:52:24Z","title":"MediEval: A Unified Medical Benchmark for Patient-Contextual and Knowledge-Grounded Reasoning in LLMs","summary":"Large Language Models (LLMs) are increasingly applied to medicine, yet their adoption is limited by concerns over reliability and safety. Existing evaluations either test factual medical knowledge in isolation or assess patient-level reasoning without verifying correctness, leaving a critical gap. We introduce MediEval, a benchmark that links MIMIC-IV electronic health records (EHRs) to a unified knowledge base built from UMLS and other biomedical vocabularies. MediEval generates diverse factual and counterfactual medical statements within real patient contexts, enabling systematic evaluation across a 4-quadrant framework that jointly considers knowledge grounding and contextual consistency. Using this framework, we identify critical failure modes, including hallucinated support and truth inversion, that current proprietary, open-source, and domain-specific LLMs frequently exhibit. To address these risks, we propose Counterfactual Risk-Aware Fine-tuning (CoRFu), a DPO-based method with an asymmetric penalty targeting unsafe confusions. CoRFu improves by +16.4 macro-F1 points over the base model and eliminates truth inversion errors, demonstrating both higher accuracy and substantially greater safety.","authors":["Zhan Qu","Michael Färber"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20817v1","updated":"2025-12-23T22:33:54Z","published":"2025-12-23T22:33:54Z","title":"EssayCBM: Rubric-Aligned Concept Bottleneck Models for Transparent Essay Grading","summary":"Understanding how automated grading systems evaluate essays remains a significant challenge for educators and students, especially when large language models function as black boxes. We introduce EssayCBM, a rubric-aligned framework that prioritizes interpretability in essay assessment. Instead of predicting grades directly from text, EssayCBM evaluates eight writing concepts, such as Thesis Clarity and Evidence Use, through dedicated prediction heads on an encoder. These concept scores form a transparent bottleneck, and a lightweight network computes the final grade using only concepts. Instructors can adjust concept predictions and instantly view the updated grade, enabling accountable human-in-the-loop evaluation. EssayCBM matches black-box performance while offering actionable, concept-level feedback through an intuitive web interface.","authors":["Kumar Satvik Chaudhary","Chengshuai Zhao","Fan Zhang","Yung Hin Tse","Garima Agrawal","Yuli Deng","Huan Liu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20812v1","updated":"2025-12-23T22:22:18Z","published":"2025-12-23T22:22:18Z","title":"Semantic Deception: When Reasoning Models Can't Compute an Addition","summary":"Large language models (LLMs) are increasingly used in situations where human values are at stake, such as decision-making tasks that involve reasoning when performed by humans. We investigate the so-called reasoning capabilities of LLMs over novel symbolic representations by introducing an experimental framework that tests their ability to process and manipulate unfamiliar symbols. We introduce semantic deceptions: situations in which symbols carry misleading semantic associations due to their form, such as being embedded in specific contexts, designed to probe whether LLMs can maintain symbolic abstraction or whether they default to exploiting learned semantic associations. We redefine standard digits and mathematical operators using novel symbols, and task LLMs with solving simple calculations expressed in this altered notation. The objective is: (1) to assess LLMs' capacity for abstraction and manipulation of arbitrary symbol systems; (2) to evaluate their ability to resist misleading semantic cues that conflict with the task's symbolic logic. Through experiments with four LLMs we show that semantic cues can significantly deteriorate reasoning models' performance on very simple tasks. They reveal limitations in current LLMs' ability for symbolic manipulations and highlight a tendency to over-rely on surface-level semantics, suggesting that chain-of-thoughts may amplify reliance on statistical correlations. Even in situations where LLMs seem to correctly follow instructions, semantic cues still impact basic capabilities. These limitations raise ethical and societal concerns, undermining the widespread and pernicious tendency to attribute reasoning abilities to LLMs and suggesting how LLMs might fail, in particular in decision-making contexts where robust symbolic reasoning is essential and should not be compromised by residual semantic associations inherited from the model's training.","authors":["Nathaniël de Leeuw","Marceau Nahon","Mathis Reymond","Raja Chatila","Mehdi Khamassi"],"pdf_url":"","comment":"22 pages, 5 figures"},{"id":"http://arxiv.org/abs/2512.20796v1","updated":"2025-12-23T21:44:20Z","published":"2025-12-23T21:44:20Z","title":"Measuring Mechanistic Independence: Can Bias Be Removed Without Erasing Demographics?","summary":"We investigate how independent demographic bias mechanisms are from general demographic recognition in language models. Using a multi-task evaluation setup where demographics are associated with names, professions, and education levels, we measure whether models can be debiased while preserving demographic detection capabilities. We compare attribution-based and correlation-based methods for locating bias features. We find that targeted sparse autoencoder feature ablations in Gemma-2-9B reduce bias without degrading recognition performance: attribution-based ablations mitigate race and gender profession stereotypes while preserving name recognition accuracy, whereas correlation-based ablations are more effective for education bias. Qualitative analysis further reveals that removing attribution features in education tasks induces ``prior collapse'', thus increasing overall bias. This highlights the need for dimension-specific interventions. Overall, our results show that demographic bias arises from task-specific mechanisms rather than absolute demographic markers, and that mechanistic inference-time interventions can enable surgical debiasing without compromising core model capabilities.","authors":["Zhengyang Shan","Aaron Mueller"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20794v1","updated":"2025-12-23T21:41:36Z","published":"2025-12-23T21:41:36Z","title":"Investigating Model Editing for Unlearning in Large Language Models","summary":"Machine unlearning aims to remove unwanted information from a model, but many methods are inefficient for LLMs with large numbers of parameters or fail to fully remove the intended information without degrading performance on knowledge that should be retained. Model editing algorithms solve a similar problem of changing information in models, but they focus on redirecting inputs to a new target rather than removing that information altogether. In this work, we explore the editing algorithms ROME, IKE, and WISE and design new editing targets for an unlearning setting. Through this investigation, we show that model editing approaches can exceed baseline unlearning methods in terms of quality of forgetting depending on the setting. Like traditional unlearning techniques, they struggle to encapsulate the scope of what is to be unlearned without damage to the overall model performance.","authors":["Shariqah Hossain","Lalana Kagal"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2509.22615v2","updated":"2025-12-23T21:29:42Z","published":"2025-09-26T17:41:57Z","title":"GaussianVision: Vision-Language Alignment from Compressed Image Representations using 2D Gaussian Splatting","summary":"Modern vision language pipelines are driven by RGB vision encoders trained on massive image text corpora. While these pipelines have enabled impressive zero-shot capabilities and strong transfer across tasks, they still inherit two structural inefficiencies from the pixel domain: (i) transmitting dense RGB images from edge devices to the cloud is energy-intensive and costly, and (ii) patch-based tokenization explodes sequence length, stressing attention budgets and context limits. We explore 2D Gaussian Splatting (2DGS) as an alternative visual substrate for alignment: a compact, spatially adaptive representation that parameterizes images by a set of colored anisotropic Gaussians. We develop a scalable 2DGS pipeline with structured initialization, luminance-aware pruning, and batched CUDA kernels, achieving over 90x faster fitting and about 97% GPU utilization compared to prior implementations. We further adapt contrastive language-image pre-training (CLIP) to 2DGS by reusing a frozen RGB-based transformer backbone with a lightweight splat-aware input stem and a perceiver resampler, training only 9.7% to 13.8% of the total parameters. On a 12.8M dataset from DataComp, GS encoders yield competitive zero-shot performance on 38 datasets from the CLIP benchmark while compressing inputs 3x to 23.5x relative to pixels. Our results establish 2DGS as a viable multimodal substrate, pinpoint architectural bottlenecks, and open a path toward representations that are both semantically powerful and transmission-efficient for edge-cloud learning.","authors":["Yasmine Omri","Connor Ding","Tsachy Weissman","Thierry Tambe"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20780v1","updated":"2025-12-23T21:29:09Z","published":"2025-12-23T21:29:09Z","title":"Large Language Models Approach Expert Pedagogical Quality in Math Tutoring but Differ in Instructional and Linguistic Profiles","summary":"Recent work has explored the use of large language models for generating tutoring responses in mathematics, yet it remains unclear how closely their instructional behavior aligns with expert human practice. We examine this question using a controlled, turn-level comparison in which expert human tutors, novice human tutors, and multiple large language models respond to the same set of math remediation conversation turns. We examine both instructional strategies and linguistic characteristics of tutoring responses, including restating and revoicing, pressing for accuracy, lexical diversity, readability, politeness, and agency. We find that large language models approach expert levels of perceived pedagogical quality on average but exhibit systematic differences in their instructional and linguistic profiles. In particular, large language models tend to underuse restating and revoicing strategies characteristic of expert human tutors, while producing longer, more lexically diverse, and more polite responses. Statistical analyses show that restating and revoicing, lexical diversity, and pressing for accuracy are positively associated with perceived pedagogical quality, whereas higher levels of agentic and polite language are negatively associated. Overall, recent large language models exhibit levels of perceived pedagogical quality comparable to expert human tutors, while relying on different instructional and linguistic strategies. These findings underscore the value of analyzing instructional strategies and linguistic characteristics when evaluating tutoring responses across human tutors and intelligent tutoring systems.","authors":["Ramatu Oiza Abdulsalam","Segun Aroyehun"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2506.06303v2","updated":"2025-12-23T21:21:10Z","published":"2025-05-21T16:15:01Z","title":"Reward Is Enough: LLMs Are In-Context Reinforcement Learners","summary":"Reinforcement learning (RL) is a framework for solving sequential decision-making problems. In this work, we demonstrate that, surprisingly, RL emerges during the inference time of large language models (LLMs), a phenomenon we term in-context RL (ICRL). To reveal this capability, we introduce a simple multi-round prompting framework, we call ICRL prompting, for inference-time self-improvement. The goal of ICRL prompting is to guide LLMs to perform reinforcement learning during inference for self-improvement on a given task. After each response, the model receives numerical scalar feedback, denoted as a reward. In the next round, we prompt the LLM again together with a context that concatenates all prior responses and their associated rewards. We consistently observe that response quality improves as the context grows. In other words, the LLM can optimize scalar reward signals during inference, exhibiting behavior analogous to reinforcement learning. We evaluate ICRL prompting on Game of 24, creative writing, ScienceWorld, and Olympiad-level math competitions (AIME and HMMT), demonstrating significant improvements over baselines such as Self-Refine and Reflexion. Notably, even when the reward signals are generated by the same LLM, ICRL prompting still improves performance, highlighting a promising new paradigm for test-time scaling.","authors":["Kefan Song","Amir Moeini","Peng Wang","Lei Gong","Rohan Chandra","Yanjun Qi","Shangtong Zhang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20773v1","updated":"2025-12-23T21:21:08Z","published":"2025-12-23T21:21:08Z","title":"Adversarial Training for Failure-Sensitive User Simulation in Mental Health Dialogue Optimization","summary":"Realistic user simulation is crucial for training and evaluating task-oriented dialogue (TOD) systems, yet creating simulators that accurately replicate human behavior remains challenging. A key property of effective simulators is their ability to expose failure modes of the systems they evaluate. We present an adversarial training framework that iteratively improves user simulator realism through a competitive dynamic between a generator (user simulator) and a discriminator. Applied to mental health support chatbots, our approach demonstrates that fine-tuned simulators dramatically outperform zero-shot base models at surfacing system issues, and adversarial training further enhances diversity, distributional alignment, and predictive validity. The resulting simulator achieves a strong correlation between simulated and real failure occurrence rates across diverse chatbot configurations while maintaining low distributional divergence of failure modes. Discriminator accuracy decreases drastically after three adversarial iterations, suggesting improved realism. These results provide evidence that adversarial training is a promising approach for creating realistic user simulators in mental health support TOD domains, enabling rapid, reliable, and cost-effective system evaluation before deployment.","authors":["Ziyi Zhu","Olivier Tieleman","Caitlin A. Stamatis","Luka Smyth","Thomas D. Hull","Daniel R. Cahn","Matteo Malgaroli"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20760v1","updated":"2025-12-23T20:45:31Z","published":"2025-12-23T20:45:31Z","title":"Generalization of RLVR Using Causal Reasoning as a Testbed","summary":"Reinforcement learning with verifiable rewards (RLVR) has emerged as a promising paradigm for post-training large language models (LLMs) on complex reasoning tasks. Yet, the conditions under which RLVR yields robust generalization remain poorly understood. This paper provides an empirical study of RLVR generalization in the setting of probabilistic inference over causal graphical models. This setting offers two natural axes along which to examine generalization: (i) the level of the probabilistic query -- associational, interventional, or counterfactual -- and (ii) the structural complexity of the query, measured by the size of its relevant subgraph. We construct datasets of causal graphs and queries spanning these difficulty axes and fine-tune Qwen-2.5-Instruct models using RLVR or supervised fine-tuning (SFT). We vary both the model scale (3B-32B) and the query level included in training. We find that RLVR yields stronger within-level and across-level generalization than SFT, but only for specific combinations of model size and training query level. Further analysis shows that RLVR's effectiveness depends on the model's initial reasoning competence. With sufficient initial competence, RLVR improves an LLM's marginalization strategy and reduces errors in intermediate probability calculations, producing substantial accuracy gains, particularly on more complex queries. These findings show that RLVR can improve specific causal reasoning subskills, with its benefits emerging only when the model has sufficient initial competence.","authors":["Brian Lu","Hongyu Zhao","Shuo Sun","Hao Peng","Rui Ding","Hongyuan Mei"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20757v1","updated":"2025-12-23T20:43:06Z","published":"2025-12-23T20:43:06Z","title":"TokSuite: Measuring the Impact of Tokenizer Choice on Language Model Behavior","summary":"Tokenizers provide the fundamental basis through which text is represented and processed by language models (LMs). Despite the importance of tokenization, its role in LM performance and behavior is poorly understood due to the challenge of measuring the impact of tokenization in isolation. To address this need, we present TokSuite, a collection of models and a benchmark that supports research into tokenization's influence on LMs. Specifically, we train fourteen models that use different tokenizers but are otherwise identical using the same architecture, dataset, training budget, and initialization. Additionally, we curate and release a new benchmark that specifically measures model performance subject to real-world perturbations that are likely to influence tokenization. Together, TokSuite allows robust decoupling of the influence of a model's tokenizer, supporting a series of novel findings that elucidate the respective benefits and shortcomings of a wide range of popular tokenizers.","authors":["Gül Sena Altıntaş","Malikeh Ehghaghi","Brian Lester","Fengyuan Liu","Wanru Zhao","Marco Ciccone","Colin Raffel"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2508.16753v3","updated":"2025-12-23T20:35:30Z","published":"2025-08-22T19:13:21Z","title":"GAICo: A Deployed and Extensible Framework for Evaluating Diverse and Multimodal Generative AI Outputs","summary":"The rapid proliferation of Generative AI (GenAI) into diverse, high-stakes domains necessitates robust and reproducible evaluation methods. However, practitioners often resort to ad-hoc, non-standardized scripts, as common metrics are often unsuitable for specialized, structured outputs (e.g., automated plans, time-series) or holistic comparison across modalities (e.g., text, audio, and image). This fragmentation hinders comparability and slows AI system development. To address this challenge, we present GAICo (Generative AI Comparator): a deployed, open-source Python library that streamlines and standardizes GenAI output comparison. GAICo provides a unified, extensible framework supporting a comprehensive suite of reference-based metrics for unstructured text, specialized structured data formats, and multimedia (images, audio). Its architecture features a high-level API for rapid, end-to-end analysis, from multi-model comparison to visualization and reporting, alongside direct metric access for granular control. We demonstrate GAICo's utility through a detailed case study evaluating and debugging complex, multi-modal AI Travel Assistant pipelines. GAICo empowers AI researchers and developers to efficiently assess system performance, make evaluation reproducible, improve development velocity, and ultimately build more trustworthy AI systems, aligning with the goal of moving faster and safer in AI deployment. Since its release on PyPI in Jun 2025, the tool has been downloaded over 13K times, across versions, by Aug 2025, demonstrating growing community interest.","authors":["Nitin Gupta","Pallav Koppisetti","Kausik Lakkaraju","Biplav Srivastava"],"pdf_url":"","comment":"11 pages, 7 figures; accepted at IAAI/AAAI 2026; extended version"},{"id":"http://arxiv.org/abs/2512.14856v2","updated":"2025-12-23T20:01:24Z","published":"2025-12-16T19:19:34Z","title":"T5Gemma 2: Seeing, Reading, and Understanding Longer","summary":"We introduce T5Gemma 2, the next generation of the T5Gemma family of lightweight open encoder-decoder models, featuring strong multilingual, multimodal and long-context capabilities. T5Gemma 2 follows the adaptation recipe (via UL2) in T5Gemma -- adapting a pretrained decoder-only model into an encoder-decoder model, and extends it from text-only regime to multimodal based on the Gemma 3 models. We further propose two methods to improve the efficiency: tied word embedding that shares all embeddings across encoder and decoder, and merged attention that unifies decoder self- and cross-attention into a single joint module. Experiments demonstrate the generality of the adaptation strategy over architectures and modalities as well as the unique strength of the encoder-decoder architecture on long context modeling. Similar to T5Gemma, T5Gemma 2 yields comparable or better pretraining performance and significantly improved post-training performance than its Gemma 3 counterpart. We release the pretrained models (270M-270M, 1B-1B and 4B-4B) to the community for future research.","authors":["Biao Zhang","Paul Suganthan","Gaël Liu","Ilya Philippov","Sahil Dua","Ben Hora","Kat Black","Gus Martins","Omar Sanseviero","Shreya Pathak","Cassidy Hardin","Francesco Visin","Jiageng Zhang","Kathleen Kenealy","Qin Yin","Xiaodan Song","Olivier Lacombe","Armand Joulin","Tris Warkentin","Adam Roberts"],"pdf_url":"","comment":"technical report"},{"id":"http://arxiv.org/abs/2505.14582v3","updated":"2025-12-23T19:59:27Z","published":"2025-05-20T16:38:32Z","title":"Can Pruning Improve Reasoning? Revisiting Long-CoT Compression with Capability in Mind for Better Reasoning","summary":"Long chain-of-thought (Long-CoT) reasoning improves accuracy in LLMs, yet its verbose, self-reflective style often hinders effective distillation into small language models (SLMs). We revisit Long-CoT compression through the lens of capability alignment and ask: Can pruning improve reasoning? We propose Prune-on-Logic, a structure-aware framework that transforms Long-CoT into logic graphs and selectively prunes low-utility reasoning steps under self-verification constraints. Through systematic analysis across three pruning strategies targeting entire chains, core reasoning, and verification, we find that verification pruning consistently improves accuracy while reducing token usage, whereas pruning reasoning steps or indiscriminate pruning degrades performance. Our study reveals that effective pruning aligns supervision with model capacity rather than merely shortening inputs. Gains hold across tasks, model scales, and CoT capability, with larger models benefiting more from pruning due to richer but more redundant reasoning. Our empirical findings highlight pruning as a structural optimization strategy for aligning CoT reasoning with SLM capacity.","authors":["Shangziqi Zhao","Jiahao Yuan","Jinyang Wu","Zhenglin Wang","Guisong Yang","Usman Naseem"],"pdf_url":"","comment":"19 pages,6 figures"},{"id":"http://arxiv.org/abs/2512.20745v1","updated":"2025-12-23T19:57:49Z","published":"2025-12-23T19:57:49Z","title":"AgentMath: Empowering Mathematical Reasoning for Large Language Models via Tool-Augmented Agent","summary":"Large Reasoning Models (LRMs) like o3 and DeepSeek-R1 have achieved remarkable progress in natural language reasoning with long chain-of-thought. However, they remain computationally inefficient and struggle with accuracy when solving problems requiring complex mathematical operations. In this work, we present AgentMath, an agent framework that seamlessly integrates language models' reasoning capabilities with code interpreters' computational precision to efficiently tackle complex mathematical problems. Our approach introduces three key innovations: (1) An automated method that converts natural language chain-of-thought into structured tool-augmented trajectories, generating high-quality supervised fine-tuning (SFT) data to alleviate data scarcity; (2) A novel agentic reinforcement learning (RL) paradigm that dynamically interleaves natural language generation with real-time code execution. This enables models to autonomously learn optimal tool-use strategies through multi-round interactive feedback, while fostering emergent capabilities in code refinement and error correction; (3) An efficient training system incorporating innovative techniques, including request-level asynchronous rollout scheduling, agentic partial rollout, and prefix-aware weighted load balancing, achieving 4-5x speedup and making efficient RL training feasible on ultra-long sequences with scenarios with massive tool calls.Extensive evaluations show that AgentMath achieves state-of-the-art performance on challenging mathematical competition benchmarks including AIME24, AIME25, and HMMT25. Specifically, AgentMath-30B-A3B attains 90.6%, 86.4%, and 73.8% accuracy respectively, achieving advanced capabilities.These results validate the effectiveness of our approach and pave the way for building more sophisticated and scalable mathematical reasoning agents.","authors":["Haipeng Luo","Huawen Feng","Qingfeng Sun","Can Xu","Kai Zheng","Yufei Wang","Tao Yang","Han Hu","Yansong Tang","Di Wang"],"pdf_url":"","comment":"LLM, Mathematical Reasoning"},{"id":"http://arxiv.org/abs/2512.20724v1","updated":"2025-12-23T19:35:02Z","published":"2025-12-23T19:35:02Z","title":"SA-DiffuSeq: Addressing Computational and Scalability Challenges in Long-Document Generation with Sparse Attention","summary":"Diffusion based approaches to long form text generation suffer from prohibitive computational cost and memory overhead as sequence length increases. We introduce SA-DiffuSeq, a diffusion framework that integrates sparse attention to fundamentally improve scalability for long document modeling. By selectively allocating attention within the diffusion process, SA-DiffuSeq significantly reduces computational complexity while maintaining semantic coherence and generation quality. A key component of our method is a soft absorbing state tailored to sparse attention dynamics, which stabilizes diffusion trajectories and accelerates sequence reconstruction. This design improves sampling efficiency and enhances precision in long range dependency modeling. Extensive experiments demonstrate that SA-DiffuSeq consistently surpasses state of the art diffusion baselines in both training efficiency and sampling speed, with especially strong gains on extended sequences. These properties make SA-DiffuSeq well suited for demanding long form applications such as scientific writing, large scale code generation, and multi turn long context dialogue. Overall, our results indicate that incorporating structured sparsity into diffusion models is a promising direction for efficient and expressive long text generation.","authors":["Alexandros Christoforos","Chadbourne Davis"],"pdf_url":"","comment":"Under submission"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2512.20619v1","updated":"2025-12-23T18:59:56Z","published":"2025-12-23T18:59:56Z","title":"SemanticGen: Video Generation in Semantic Space","summary":"State-of-the-art video generative models typically learn the distribution of video latents in the VAE space and map them to pixels using a VAE decoder. While this approach can generate high-quality videos, it suffers from slow convergence and is computationally expensive when generating long videos. In this paper, we introduce SemanticGen, a novel solution to address these limitations by generating videos in the semantic space. Our main insight is that, due to the inherent redundancy in videos, the generation process should begin in a compact, high-level semantic space for global planning, followed by the addition of high-frequency details, rather than directly modeling a vast set of low-level video tokens using bi-directional attention. SemanticGen adopts a two-stage generation process. In the first stage, a diffusion model generates compact semantic video features, which define the global layout of the video. In the second stage, another diffusion model generates VAE latents conditioned on these semantic features to produce the final output. We observe that generation in the semantic space leads to faster convergence compared to the VAE latent space. Our method is also effective and computationally efficient when extended to long video generation. Extensive experiments demonstrate that SemanticGen produces high-quality videos and outperforms state-of-the-art approaches and strong baselines.","authors":["Jianhong Bai","Xiaoshi Wu","Xintao Wang","Fu Xiao","Yuanxing Zhang","Qinghe Wang","Xiaoyu Shi","Menghan Xia","Zuozhu Liu","Haoji Hu","Pengfei Wan","Kun Gai"],"pdf_url":"","comment":"Project page: https://jianhongbai.github.io/SemanticGen/"},{"id":"http://arxiv.org/abs/2512.20618v1","updated":"2025-12-23T18:59:49Z","published":"2025-12-23T18:59:49Z","title":"LongVideoAgent: Multi-Agent Reasoning with Long Videos","summary":"Recent advances in multimodal LLMs and systems that use tools for long-video QA point to the promise of reasoning over hour-long episodes. However, many methods still compress content into lossy summaries or rely on limited toolsets, weakening temporal grounding and missing fine-grained cues. We propose a multi-agent framework in which a master LLM coordinates a grounding agent to localize question-relevant segments and a vision agent to extract targeted textual observations. The master agent plans with a step limit, and is trained with reinforcement learning to encourage concise, correct, and efficient multi-agent cooperation. This design helps the master agent focus on relevant clips via grounding, complements subtitles with visual detail, and yields interpretable trajectories. On our proposed LongTVQA and LongTVQA+ which are episode-level datasets aggregated from TVQA/TVQA+, our multi-agent system significantly outperforms strong non-agent baselines. Experiments also show reinforcement learning further strengthens reasoning and planning for the trained agent. Code and data will be shared at https://longvideoagent.github.io/.","authors":["Runtao Liu","Ziyi Liu","Jiaqi Tang","Yue Ma","Renjie Pi","Jipeng Zhang","Qifeng Chen"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20617v1","updated":"2025-12-23T18:59:46Z","published":"2025-12-23T18:59:46Z","title":"SpatialTree: How Spatial Abilities Branch Out in MLLMs","summary":"Cognitive science suggests that spatial ability develops progressively-from perception to reasoning and interaction. Yet in multimodal LLMs (MLLMs), this hierarchy remains poorly understood, as most studies focus on a narrow set of tasks. We introduce SpatialTree, a cognitive-science-inspired hierarchy that organizes spatial abilities into four levels: low-level perception (L1), mental mapping (L2), simulation (L3), and agentic competence (L4). Based on this taxonomy, we construct the first capability-centric hierarchical benchmark, thoroughly evaluating mainstream MLLMs across 27 sub-abilities. The evaluation results reveal a clear structure: L1 skills are largely orthogonal, whereas higher-level skills are strongly correlated, indicating increasing interdependency. Through targeted supervised fine-tuning, we uncover a surprising transfer dynamic-negative transfer within L1, but strong cross-level transfer from low- to high-level abilities with notable synergy. Finally, we explore how to improve the entire hierarchy. We find that naive RL that encourages extensive \"thinking\" is unreliable: it helps complex reasoning but hurts intuitive perception. We propose a simple auto-think strategy that suppresses unnecessary deliberation, enabling RL to consistently improve performance across all levels. By building SpatialTree, we provide a proof-of-concept framework for understanding and systematically scaling spatial abilities in MLLMs.","authors":["Yuxi Xiao","Longfei Li","Shen Yan","Xinhang Liu","Sida Peng","Yunchao Wei","Xiaowei Zhou","Bingyi Kang"],"pdf_url":"","comment":"webpage: https://spatialtree.github.io/"},{"id":"http://arxiv.org/abs/2512.20615v1","updated":"2025-12-23T18:59:16Z","published":"2025-12-23T18:59:16Z","title":"Active Intelligence in Video Avatars via Closed-loop World Modeling","summary":"Current video avatar generation methods excel at identity preservation and motion alignment but lack genuine agency, they cannot autonomously pursue long-term goals through adaptive environmental interaction. We address this by introducing L-IVA (Long-horizon Interactive Visual Avatar), a task and benchmark for evaluating goal-directed planning in stochastic generative environments, and ORCA (Online Reasoning and Cognitive Architecture), the first framework enabling active intelligence in video avatars. ORCA embodies Internal World Model (IWM) capabilities through two key innovations: (1) a closed-loop OTAR cycle (Observe-Think-Act-Reflect) that maintains robust state tracking under generative uncertainty by continuously verifying predicted outcomes against actual generations, and (2) a hierarchical dual-system architecture where System 2 performs strategic reasoning with state prediction while System 1 translates abstract plans into precise, model-specific action captions. By formulating avatar control as a POMDP and implementing continuous belief updating with outcome verification, ORCA enables autonomous multi-step task completion in open-domain scenarios. Extensive experiments demonstrate that ORCA significantly outperforms open-loop and non-reflective baselines in task success rate and behavioral coherence, validating our IWM-inspired design for advancing video avatar intelligence from passive animation to active, goal-oriented behavior.","authors":["Xuanhua He","Tianyu Yang","Ke Cao","Ruiqi Wu","Cheng Meng","Yong Zhang","Zhuoliang Kang","Xiaoming Wei","Qifeng Chen"],"pdf_url":"","comment":"Project Page: https://xuanhuahe.github.io/ORCA/"},{"id":"http://arxiv.org/abs/2512.20610v1","updated":"2025-12-23T18:57:53Z","published":"2025-12-23T18:57:53Z","title":"FedPOD: the deployable units of training for federated learning","summary":"This paper proposes FedPOD (Proportionally Orchestrated Derivative) for optimizing learning efficiency and communication cost in federated learning among multiple clients. Inspired by FedPIDAvg, we define a round-wise task for FedPOD to enhance training efficiency. FedPIDAvg achieved performance improvement by incorporating the training loss reduction for prediction entropy as weights using differential terms. Furthermore, by modeling data distribution with a Poisson distribution and using a PID controller, it reduced communication costs even in skewed data distribution. However, excluding participants classified as outliers based on the Poisson distribution can limit data utilization. Additionally, PID controller requires the same participants to be maintained throughout the federated learning process as it uses previous rounds' learning information in the current round. In our approach, FedPOD addresses these issues by including participants excluded as outliers, eliminating dependency on previous rounds' learning information, and applying a method for calculating validation loss at each round. In this challenge, FedPOD presents comparable performance to FedPIDAvg in metrics of Dice score, 0.78, 0.71 and 0.72 for WT, ET and TC in average, and projected convergence score, 0.74 in average. Furthermore, the concept of FedPOD draws inspiration from Kubernetes' smallest computing unit, POD, designed to be compatible with Kubernetes auto-scaling. Extending round-wise tasks of FedPOD to POD units allows flexible design by applying scale-out similar to Kubernetes' auto-scaling. This work demonstrated the potentials of FedPOD to enhance federated learning by improving efficiency, flexibility, and performance in metrics.","authors":["Daewoon Kim","Si Young Yie","Jae Sung Lee"],"pdf_url":"","comment":"12 pages, 12 figures, MICCAI"},{"id":"http://arxiv.org/abs/2512.20606v1","updated":"2025-12-23T18:54:10Z","published":"2025-12-23T18:54:10Z","title":"Repurposing Video Diffusion Transformers for Robust Point Tracking","summary":"Point tracking aims to localize corresponding points across video frames, serving as a fundamental task for 4D reconstruction, robotics, and video editing. Existing methods commonly rely on shallow convolutional backbones such as ResNet that process frames independently, lacking temporal coherence and producing unreliable matching costs under challenging conditions. Through systematic analysis, we find that video Diffusion Transformers (DiTs), pre-trained on large-scale real-world videos with spatio-temporal attention, inherently exhibit strong point tracking capability and robustly handle dynamic motions and frequent occlusions. We propose DiTracker, which adapts video DiTs through: (1) query-key attention matching, (2) lightweight LoRA tuning, and (3) cost fusion with a ResNet backbone. Despite training with 8 times smaller batch size, DiTracker achieves state-of-the-art performance on challenging ITTO benchmark and matches or outperforms state-of-the-art models on TAP-Vid benchmarks. Our work validates video DiT features as an effective and efficient foundation for point tracking.","authors":["Soowon Son","Honggyu An","Chaehyun Kim","Hyunah Ko","Jisu Nam","Dahyun Chung","Siyoon Jin","Jung Yi","Jaewon Min","Junhwa Hur","Seungryong Kim"],"pdf_url":"","comment":"Project Page: https://cvlab-kaist.github.io/DiTracker/"},{"id":"http://arxiv.org/abs/2505.08961v2","updated":"2025-12-23T18:50:46Z","published":"2025-05-13T21:01:53Z","title":"Learning Informative Attention Weights for Person Re-Identification","summary":"Attention mechanisms have been widely used in deep learning, and recent efforts have been devoted to incorporating attention modules into deep neural networks (DNNs) for person Re-Identification (Re-ID) to enhance their discriminative feature learning capabilities. Existing attention modules, including self-attention and channel attention, learn attention weights that quantify the importance of feature tokens or feature channels. However, existing attention methods do not explicitly ensure that the attention weights are informative for predicting the identity of the person in the input image, and may consequently introduce noisy information from the input image. To address this issue, we propose a novel method termed Reduction of Information Bottleneck loss (RIB), motivated by the principle of the Information Bottleneck (IB). A novel distribution-free and efficient variational upper bound for the IB loss (IBB), which can be optimized by standard SGD, is derived and incorporated into the training loss of the RIB models. RIB is applied to DNNs with self-attention modules through a novel Differentiable Channel Selection Attention module, or DCS-Attention, that selects the most informative channels for computing attention weights, leading to competitive models termed RIB-DCS. RIB is also incorporated into DNNs with existing channel attention modules to promote the learning of informative channel attention weights, leading to models termed RIB-CA. Both RIB-DCS and RIB-CA are applied to fixed neural network backbones and learnable backbones with Differentiable Neural Architecture Search (DNAS). Extensive experiments on multiple person Re-ID benchmarks show that RIB significantly enhances the prediction accuracy of DNNs for person Re-ID, even for the occluded person Re-ID.","authors":["Yancheng Wang","Nebojsa Jojic","Yingzhen Yang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20595v1","updated":"2025-12-23T18:43:05Z","published":"2025-12-23T18:43:05Z","title":"Cube Bench: A Benchmark for Spatial Visual Reasoning in MLLMs","summary":"We introduce Cube Bench, a Rubik's-cube benchmark for evaluating spatial and sequential reasoning in multimodal large language models (MLLMs). The benchmark decomposes performance into five skills: (i) reconstructing cube faces from images and text, (ii) choosing the optimal next move, (iii) predicting the outcome of a candidate move without applying it, (iv) executing multi-step plans while recovering from mistakes, and (v) detecting and revising one's own errors. Using a shared set of scrambled cube states, identical prompts and parsers, and a single distance-to-solved metric, we compare recent MLLMs side by side as a function of scramble depth. Across seven MLLMs, accuracy drops sharply with depth; once a trajectory stalls or diverges, models rarely recover, and high face-reconstruction accuracy does not guarantee competent action selection or multi-step execution. A pronounced closed- vs open-source gap emerges: the strongest closed model leads on both single-step perception tasks and multi-step control tasks, while open-weight models cluster near chance on the hardest settings; yet even the best MLLM degrades at higher cube complexity. A simple self-correction via reflective thinking yields modest gains but can also introduce overthinking. Cube Bench offers a compact, reproducible probe of sequential spatial reasoning in MLLMs.","authors":["Dhruv Anand","Ehsan Shareghi"],"pdf_url":"","comment":"27 pages, 5 figures, 9 tables. Cube available at https://github.com/dana-23/cube-bench"},{"id":"http://arxiv.org/abs/2512.20563v1","updated":"2025-12-23T18:07:43Z","published":"2025-12-23T18:07:43Z","title":"LEAD: Minimizing Learner-Expert Asymmetry in End-to-End Driving","summary":"Simulators can generate virtually unlimited driving data, yet imitation learning policies in simulation still struggle to achieve robust closed-loop performance. Motivated by this gap, we empirically study how misalignment between privileged expert demonstrations and sensor-based student observations can limit the effectiveness of imitation learning. More precisely, experts have significantly higher visibility (e.g., ignoring occlusions) and far lower uncertainty (e.g., knowing other vehicles' actions), making them difficult to imitate reliably. Furthermore, navigational intent (i.e., the route to follow) is under-specified in student models at test time via only a single target point. We demonstrate that these asymmetries can measurably limit driving performance in CARLA and offer practical interventions to address them. After careful modifications to narrow the gaps between expert and student, our TransFuser v6 (TFv6) student policy achieves a new state of the art on all major publicly available CARLA closed-loop benchmarks, reaching 95 DS on Bench2Drive and more than doubling prior performances on Longest6~v2 and Town13. Additionally, by integrating perception supervision from our dataset into a shared sim-to-real pipeline, we show consistent gains on the NAVSIM and Waymo Vision-Based End-to-End driving benchmarks. Our code, data, and models are publicly available at https://github.com/autonomousvision/lead.","authors":["Long Nguyen","Micha Fauth","Bernhard Jaeger","Daniel Dauner","Maximilian Igl","Andreas Geiger","Kashyap Chitta"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20561v1","updated":"2025-12-23T18:05:43Z","published":"2025-12-23T18:05:43Z","title":"FlashVLM: Text-Guided Visual Token Selection for Large Multimodal Models","summary":"Large vision-language models (VLMs) typically process hundreds or thousands of visual tokens per image or video frame, incurring quadratic attention cost and substantial redundancy. Existing token reduction methods often ignore the textual query or rely on deep attention maps, whose instability under aggressive pruning leads to degraded semantic alignment.\n  We propose FlashVLM, a text guided visual token selection framework that dynamically adapts visual inputs to the query. Instead of relying on noisy attention weights, FlashVLM computes an explicit cross modal similarity between projected image tokens and normalized text embeddings in the language model space. This extrinsic relevance is fused with intrinsic visual saliency using log domain weighting and temperature controlled sharpening. In addition, a diversity preserving partition retains a minimal yet representative set of background tokens to maintain global context.\n  Under identical token budgets and evaluation protocols, FlashVLM achieves beyond lossless compression, slightly surpassing the unpruned baseline while pruning up to 77.8 percent of visual tokens on LLaVA 1.5, and maintaining 92.8 percent accuracy even under 94.4 percent compression. Extensive experiments on 14 image and video benchmarks demonstrate that FlashVLM delivers state of the art efficiency performance trade offs while maintaining strong robustness and generalization across mainstream VLMs.","authors":["Kaitong Cai","Jusheng Zhang","Jing Yang","Yijia Fan","Pengtao Xie","Jian Wang","Keze Wang"],"pdf_url":"","comment":"Under submission"},{"id":"http://arxiv.org/abs/2512.20557v1","updated":"2025-12-23T17:56:36Z","published":"2025-12-23T17:56:36Z","title":"Learning to Reason in 4D: Dynamic Spatial Understanding for Vision Language Models","summary":"Vision-language models (VLM) excel at general understanding yet remain weak at dynamic spatial reasoning (DSR), i.e., reasoning about the evolvement of object geometry and relationship in 3D space over time, largely due to the scarcity of scalable 4D-aware training resources. To bridge this gap across aspects of dataset, benchmark and model, we introduce DSR Suite. First, we propose an automated pipeline that generates multiple-choice question-answer pairs from in-the-wild videos for DSR. By leveraging modern vision foundation models, the pipeline extracts rich geometric and motion information, including camera poses, local point clouds, object masks, orientations, and 3D trajectories. These geometric cues enable the construction of DSR-Train for learning and further human-refined DSR-Bench for evaluation. Compared with previous works, our data emphasize (i) in-the-wild video sources, (ii) object- and scene-level 3D requirements, (iii) viewpoint transformations, (iv) multi-object interactions, and (v) fine-grained, procedural answers. Beyond data, we propose a lightweight Geometry Selection Module (GSM) to seamlessly integrate geometric priors into VLMs, which condenses question semantics and extracts question-relevant knowledge from pretrained 4D reconstruction priors into a compact set of geometry tokens. This targeted extraction avoids overwhelming the model with irrelevant knowledge. Experiments show that integrating DSR-Train and GSM into Qwen2.5-VL-7B significantly enhances its dynamic spatial reasoning capability, while maintaining accuracy on general video understanding benchmarks.","authors":["Shengchao Zhou","Yuxin Chen","Yuying Ge","Wei Huang","Jiehong Lin","Ying Shan","Xiaojuan Qi"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20556v1","updated":"2025-12-23T17:55:35Z","published":"2025-12-23T17:55:35Z","title":"Multi-Grained Text-Guided Image Fusion for Multi-Exposure and Multi-Focus Scenarios","summary":"Image fusion aims to synthesize a single high-quality image from a pair of inputs captured under challenging conditions, such as differing exposure levels or focal depths. A core challenge lies in effectively handling disparities in dynamic range and focus depth between the inputs. With the advent of vision-language models, recent methods incorporate textual descriptions as auxiliary guidance to enhance fusion quality. However, simply incorporating coarse-grained descriptions hampers the understanding of fine-grained details and poses challenges for precise cross-modal alignment. To address these limitations, we propose Multi-grained Text-guided Image Fusion (MTIF), a novel fusion paradigm with three key designs. First, it introduces multi-grained textual descriptions that separately capture fine details, structural cues, and semantic content, guiding image fusion through a hierarchical cross-modal modulation module. Second, it involves supervision signals at each granularity to facilitate alignment between visual and textual features and enhance the utility of auxiliary text. Third, it adopts a saliency-driven enrichment module to augment training data with dense semantic content, further strengthening the cross-modal modulation and alignment. Extensive experiments show that MTIF consistently outperforms previous methods on both multi-exposure and multi-focus image fusion tasks.","authors":["Mingwei Tang","Jiahao Nie","Guang Yang","Ziqing Cui","Jie Li"],"pdf_url":"","comment":"Accepted to WACV 2026"},{"id":"http://arxiv.org/abs/2510.07191v2","updated":"2025-12-23T17:45:29Z","published":"2025-10-08T16:25:04Z","title":"Resolution scaling governs DINOv3 transfer performance in chest radiograph classification","summary":"Self-supervised learning (SSL) has advanced visual representation learning, but its value in chest radiography, a high-volume imaging modality with fine-grained findings, remains unclear. Meta's DINOv3 extends earlier SSL models through Gram-anchored self-distillation. Whether these design choices improve transfer learning for chest radiography has not been systematically tested. We benchmarked DINOv3 against DINOv2 and ImageNet initialization across seven datasets (n>814,000). Two representative backbones were evaluated: ViT-B/16 and ConvNeXt-B. Images were analyzed at 224x224, 512x512, and 1024x1024 pixels. We additionally assessed frozen features from a 7B model. The primary outcome was mean AUROC across labels. At 224x224, DINOv3 and DINOv2 achieved comparable performance on adult datasets. Increasing resolution to 512x512 yielded consistent improvements for DINOv3 over both DINOv2 and ImageNet. In contrast, results in pediatric cohort showed no differences across initializations. Across all settings, ConvNeXt-B outperformed ViT-B/16. Models using frozen DINOv3-7B features underperformed relative to fully finetuned 86-89M-parameter backbones, highlighting the importance of domain adaptation. Scaling to 1024x1024 did not further improve accuracy. Resolution-related gains were most evident for boundary-dependent and small focal abnormalities. In chest radiography, higher input resolution is critical for leveraging the benefits of modern self-supervised models. 512x512 pixels represent a practical upper limit where DINOv3-initialized ConvNeXt-B networks provide the strongest performance, while larger inputs offer minimal return on cost. Clinically, these findings support use of finetuned, mid-sized backbones at 512x512 for chest radiograph interpretation, with the greatest gains expected in detecting subtle or boundary-centered lesions relevant to emergency and critical care settings.","authors":["Soroosh Tayebi Arasteh","Mina Shaigan","Christiane Kuhl","Jakob Nikolas Kather","Sven Nebelung","Daniel Truhn"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.13507v3","updated":"2025-12-23T17:38:46Z","published":"2025-12-15T16:36:52Z","title":"Seedance 1.5 pro: A Native Audio-Visual Joint Generation Foundation Model","summary":"Recent strides in video generation have paved the way for unified audio-visual generation. In this work, we present Seedance 1.5 pro, a foundational model engineered specifically for native, joint audio-video generation. Leveraging a dual-branch Diffusion Transformer architecture, the model integrates a cross-modal joint module with a specialized multi-stage data pipeline, achieving exceptional audio-visual synchronization and superior generation quality. To ensure practical utility, we implement meticulous post-training optimizations, including Supervised Fine-Tuning (SFT) on high-quality datasets and Reinforcement Learning from Human Feedback (RLHF) with multi-dimensional reward models. Furthermore, we introduce an acceleration framework that boosts inference speed by over 10X. Seedance 1.5 pro distinguishes itself through precise multilingual and dialect lip-syncing, dynamic cinematic camera control, and enhanced narrative coherence, positioning it as a robust engine for professional-grade content creation. Seedance 1.5 pro is now accessible on Volcano Engine at https://console.volcengine.com/ark/region:ark+cn-beijing/experience/vision?type=GenVideo.","authors":["Team Seedance","Heyi Chen","Siyan Chen","Xin Chen","Yanfei Chen","Ying Chen","Zhuo Chen","Feng Cheng","Tianheng Cheng","Xinqi Cheng","Xuyan Chi","Jian Cong","Jing Cui","Qinpeng Cui","Qide Dong","Junliang Fan","Jing Fang","Zetao Fang","Chengjian Feng","Han Feng","Mingyuan Gao","Yu Gao","Dong Guo","Qiushan Guo","Boyang Hao","Qingkai Hao","Bibo He","Qian He","Tuyen Hoang","Ruoqing Hu","Xi Hu","Weilin Huang","Zhaoyang Huang","Zhongyi Huang","Donglei Ji","Siqi Jiang","Wei Jiang","Yunpu Jiang","Zhuo Jiang","Ashley Kim","Jianan Kong","Zhichao Lai","Shanshan Lao","Yichong Leng","Ai Li","Feiya Li","Gen Li","Huixia Li","JiaShi Li","Liang Li","Ming Li","Shanshan Li","Tao Li","Xian Li","Xiaojie Li","Xiaoyang Li","Xingxing Li","Yameng Li","Yifu Li","Yiying Li","Chao Liang","Han Liang","Jianzhong Liang","Ying Liang","Zhiqiang Liang","Wang Liao","Yalin Liao","Heng Lin","Kengyu Lin","Shanchuan Lin","Xi Lin","Zhijie Lin","Feng Ling","Fangfang Liu","Gaohong Liu","Jiawei Liu","Jie Liu","Jihao Liu","Shouda Liu","Shu Liu","Sichao Liu","Songwei Liu","Xin Liu","Xue Liu","Yibo Liu","Zikun Liu","Zuxi Liu","Junlin Lyu","Lecheng Lyu","Qian Lyu","Han Mu","Xiaonan Nie","Jingzhe Ning","Xitong Pan","Yanghua Peng","Lianke Qin","Xueqiong Qu","Yuxi Ren","Kai Shen","Guang Shi","Lei Shi","Yan Song","Yinglong Song","Fan Sun","Li Sun","Renfei Sun","Yan Sun","Zeyu Sun","Wenjing Tang","Yaxue Tang","Zirui Tao","Feng Wang","Furui Wang","Jinran Wang","Junkai Wang","Ke Wang","Kexin Wang","Qingyi Wang","Rui Wang","Sen Wang","Shuai Wang","Tingru Wang","Weichen Wang","Xin Wang","Yanhui Wang","Yue Wang","Yuping Wang","Yuxuan Wang","Ziyu Wang","Guoqiang Wei","Wanru Wei","Di Wu","Guohong Wu","Hanjie Wu","Jian Wu","Jie Wu","Ruolan Wu","Xinglong Wu","Yonghui Wu","Ruiqi Xia","Liang Xiang","Fei Xiao","XueFeng Xiao","Pan Xie","Shuangyi Xie","Shuang Xu","Jinlan Xue","Shen Yan","Bangbang Yang","Ceyuan Yang","Jiaqi Yang","Runkai Yang","Tao Yang","Yang Yang","Yihang Yang","ZhiXian Yang","Ziyan Yang","Songting Yao","Yifan Yao","Zilyu Ye","Bowen Yu","Jian Yu","Chujie Yuan","Linxiao Yuan","Sichun Zeng","Weihong Zeng","Xuejiao Zeng","Yan Zeng","Chuntao Zhang","Heng Zhang","Jingjie Zhang","Kuo Zhang","Liang Zhang","Liying Zhang","Manlin Zhang","Ting Zhang","Weida Zhang","Xiaohe Zhang","Xinyan Zhang","Yan Zhang","Yuan Zhang","Zixiang Zhang","Fengxuan Zhao","Huating Zhao","Yang Zhao","Hao Zheng","Jianbin Zheng","Xiaozheng Zheng","Yangyang Zheng","Yijie Zheng","Jiexin Zhou","Jiahui Zhu","Kuan Zhu","Shenhan Zhu","Wenjia Zhu","Benhui Zou","Feilong Zuo"],"pdf_url":"","comment":"Seedance 1.5 pro Technical Report"},{"id":"http://arxiv.org/abs/2512.20538v1","updated":"2025-12-23T17:29:08Z","published":"2025-12-23T17:29:08Z","title":"AlignPose: Generalizable 6D Pose Estimation via Multi-view Feature-metric Alignment","summary":"Single-view RGB model-based object pose estimation methods achieve strong generalization but are fundamentally limited by depth ambiguity, clutter, and occlusions. Multi-view pose estimation methods have the potential to solve these issues, but existing works rely on precise single-view pose estimates or lack generalization to unseen objects. We address these challenges via the following three contributions. First, we introduce AlignPose, a 6D object pose estimation method that aggregates information from multiple extrinsically calibrated RGB views and does not require any object-specific training or symmetry annotation. Second, the key component of this approach is a new multi-view feature-metric refinement specifically designed for object pose. It optimizes a single, consistent world-frame object pose minimizing the feature discrepancy between on-the-fly rendered object features and observed image features across all views simultaneously. Third, we report extensive experiments on four datasets (YCB-V, T-LESS, ITODD-MV, HouseCat6D) using the BOP benchmark evaluation and show that AlignPose outperforms other published methods, especially on challenging industrial datasets where multiple views are readily available in practice.","authors":["Anna Šárová Mikeštíková","Médéric Fourmy","Martin Cífka","Josef Sivic","Vladimir Petrik"],"pdf_url":"","comment":"18 pages, 9 figures"},{"id":"http://arxiv.org/abs/2512.20531v1","updated":"2025-12-23T17:23:21Z","published":"2025-12-23T17:23:21Z","title":"SirenPose: Dynamic Scene Reconstruction via Geometric Supervision","summary":"We introduce SirenPose, a geometry-aware loss formulation that integrates the periodic activation properties of sinusoidal representation networks with keypoint-based geometric supervision, enabling accurate and temporally consistent reconstruction of dynamic 3D scenes from monocular videos. Existing approaches often struggle with motion fidelity and spatiotemporal coherence in challenging settings involving fast motion, multi-object interaction, occlusion, and rapid scene changes. SirenPose incorporates physics inspired constraints to enforce coherent keypoint predictions across both spatial and temporal dimensions, while leveraging high frequency signal modeling to capture fine grained geometric details. We further expand the UniKPT dataset to 600,000 annotated instances and integrate graph neural networks to model keypoint relationships and structural correlations. Extensive experiments on benchmarks including Sintel, Bonn, and DAVIS demonstrate that SirenPose consistently outperforms state-of-the-art methods. On DAVIS, SirenPose achieves a 17.8 percent reduction in FVD, a 28.7 percent reduction in FID, and a 6.0 percent improvement in LPIPS compared to MoSCA. It also improves temporal consistency, geometric accuracy, user score, and motion smoothness. In pose estimation, SirenPose outperforms Monst3R with lower absolute trajectory error as well as reduced translational and rotational relative pose error, highlighting its effectiveness in handling rapid motion, complex dynamics, and physically plausible reconstruction.","authors":["Kaitong Cai","Jensen Zhang","Jing Yang","Keze Wang"],"pdf_url":"","comment":"Under submission"},{"id":"http://arxiv.org/abs/2512.18741v2","updated":"2025-12-23T16:47:46Z","published":"2025-12-21T14:02:53Z","title":"Memorize-and-Generate: Towards Long-Term Consistency in Real-Time Video Generation","summary":"Frame-level autoregressive (frame-AR) models have achieved significant progress, enabling real-time video generation comparable to bidirectional diffusion models and serving as a foundation for interactive world models and game engines. However, current approaches in long video generation typically rely on window attention, which naively discards historical context outside the window, leading to catastrophic forgetting and scene inconsistency; conversely, retaining full history incurs prohibitive memory costs. To address this trade-off, we propose Memorize-and-Generate (MAG), a framework that decouples memory compression and frame generation into distinct tasks. Specifically, we train a memory model to compress historical information into a compact KV cache, and a separate generator model to synthesize subsequent frames utilizing this compressed representation. Furthermore, we introduce MAG-Bench to strictly evaluate historical memory retention. Extensive experiments demonstrate that MAG achieves superior historical scene consistency while maintaining competitive performance on standard video generation benchmarks.","authors":["Tianrui Zhu","Shiyi Zhang","Zhirui Sun","Jingqi Tian","Yansong Tang"],"pdf_url":"","comment":"Code will be released at https://github.com/Xilluill/MAG"},{"id":"http://arxiv.org/abs/2512.20501v1","updated":"2025-12-23T16:46:58Z","published":"2025-12-23T16:46:58Z","title":"Bridging Modalities and Transferring Knowledge: Enhanced Multimodal Understanding and Recognition","summary":"This manuscript explores multimodal alignment, translation, fusion, and transference to enhance machine understanding of complex inputs. We organize the work into five chapters, each addressing unique challenges in multimodal machine learning.\n  Chapter 3 introduces Spatial-Reasoning Bert for translating text-based spatial relations into 2D arrangements between clip-arts. This enables effective decoding of spatial language into visual representations, paving the way for automated scene generation aligned with human spatial understanding.\n  Chapter 4 presents a method for translating medical texts into specific 3D locations within an anatomical atlas. We introduce a loss function leveraging spatial co-occurrences of medical terms to create interpretable mappings, significantly enhancing medical text navigability.\n  Chapter 5 tackles translating structured text into canonical facts within knowledge graphs. We develop a benchmark for linking natural language to entities and predicates, addressing ambiguities in text extraction to provide clearer, actionable insights.\n  Chapter 6 explores multimodal fusion methods for compositional action recognition. We propose a method fusing video frames and object detection representations, improving recognition robustness and accuracy.\n  Chapter 7 investigates multimodal knowledge transference for egocentric action recognition. We demonstrate how multimodal knowledge distillation enables RGB-only models to mimic multimodal fusion-based capabilities, reducing computational requirements while maintaining performance.\n  These contributions advance methodologies for spatial language understanding, medical text interpretation, knowledge graph enrichment, and action recognition, enhancing computational systems' ability to process complex, multimodal inputs across diverse applications.","authors":["Gorjan Radevski"],"pdf_url":"","comment":"Ph.D. manuscript; Supervisors/Mentors: Marie-Francine Moens and Tinne Tuytelaars"},{"id":"http://arxiv.org/abs/2509.09719v2","updated":"2025-12-23T16:32:05Z","published":"2025-09-09T22:16:24Z","title":"Spectral Bottleneck in Sinusoidal Representation Networks: Noise is All You Need","summary":"This work identifies and attempts to address a fundamental limitation of implicit neural representations with sinusoidal activation. The fitting error of SIRENs is highly sensitive to the target frequency content and to the choice of initialization. In extreme cases, this sensitivity leads to a spectral bottleneck that can result in a zero-valued output. This phenomenon is characterized by analyzing the evolution of activation spectra and the empirical neural tangent kernel (NTK) during the training process. An unfavorable distribution of energy across frequency modes was noted to give rise to this failure mode. Furthermore, the effect of Gaussian perturbations applied to the baseline uniformly initialized weights is examined, showing how these perturbations influence activation spectra and the NTK eigenbasis of SIREN. Overall, initialization emerges as a central factor governing the evolution of SIRENs, indicating the need for adaptive, target-aware strategies as the target length increases and fine-scale detail becomes essential. The proposed weight initialization scheme (WINNER) represents a simple ad hoc step in this direction and demonstrates that fitting accuracy can be significantly improved by modifying the spectral profile of network activations through a target-aware initialization. The approach achieves state-of-the-art performance on audio fitting tasks and yields notable improvements in image fitting tasks.","authors":["Hemanth Chandravamsi","Dhanush V. Shenoy","Itay Zinn","Ziv Chen","Shimon Pisnoy","Steven H. Frankel"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20487v1","updated":"2025-12-23T16:26:47Z","published":"2025-12-23T16:26:47Z","title":"Multi-temporal Adaptive Red-Green-Blue and Long-Wave Infrared Fusion for You Only Look Once-Based Landmine Detection from Unmanned Aerial Systems","summary":"Landmines remain a persistent humanitarian threat, with 110 million actively deployed mines across 60 countries, claiming 26,000 casualties annually. This research evaluates adaptive Red-Green-Blue (RGB) and Long-Wave Infrared (LWIR) fusion for Unmanned Aerial Systems (UAS)-based detection of surface-laid landmines, leveraging the thermal contrast between the ordnance and the surrounding soil to enhance feature extraction. Using You Only Look Once (YOLO) architectures (v8, v10, v11) across 114 test images, generating 35,640 model-condition evaluations, YOLOv11 achieved optimal performance (86.8% mAP), with 10 to 30% thermal fusion at 5 to 10m altitude identified as the optimal detection parameters. A complementary architectural comparison revealed that while RF-DETR achieved the highest accuracy (69.2% mAP), followed by Faster R-CNN (67.6%), YOLOv11 (64.2%), and RetinaNet (50.2%), YOLOv11 trained 17.7 times faster than the transformer-based RF-DETR (41 minutes versus 12 hours), presenting a critical accuracy-efficiency tradeoff for operational deployment. Aggregated multi-temporal training datasets outperformed season-specific approaches by 1.8 to 9.6%, suggesting that models benefit from exposure to diverse thermal conditions. Anti-Tank (AT) mines achieved 61.9% detection accuracy, compared with 19.2% for Anti-Personnel (AP) mines, reflecting both the size differential and thermal-mass differences between these ordnance classes. As this research examined surface-laid mines where thermal contrast is maximized, future research should quantify thermal contrast effects for mines buried at varying depths across heterogeneous soil types.","authors":["James E. Gallagher","Edward J. Oughton","Jana Kosecka"],"pdf_url":"","comment":"21 pages with 6 figures"},{"id":"http://arxiv.org/abs/2512.20479v1","updated":"2025-12-23T16:13:55Z","published":"2025-12-23T16:13:55Z","title":"UTDesign: A Unified Framework for Stylized Text Editing and Generation in Graphic Design Images","summary":"AI-assisted graphic design has emerged as a powerful tool for automating the creation and editing of design elements such as posters, banners, and advertisements. While diffusion-based text-to-image models have demonstrated strong capabilities in visual content generation, their text rendering performance, particularly for small-scale typography and non-Latin scripts, remains limited. In this paper, we propose UTDesign, a unified framework for high-precision stylized text editing and conditional text generation in design images, supporting both English and Chinese scripts. Our framework introduces a novel DiT-based text style transfer model trained from scratch on a synthetic dataset, capable of generating transparent RGBA text foregrounds that preserve the style of reference glyphs. We further extend this model into a conditional text generation framework by training a multi-modal condition encoder on a curated dataset with detailed text annotations, enabling accurate, style-consistent text synthesis conditioned on background images, prompts, and layout specifications. Finally, we integrate our approach into a fully automated text-to-design (T2D) pipeline by incorporating pre-trained text-to-image (T2I) models and an MLLM-based layout planner. Extensive experiments demonstrate that UTDesign achieves state-of-the-art performance among open-source methods in terms of stylistic consistency and text accuracy, and also exhibits unique advantages compared to proprietary commercial approaches. Code and data for this paper are available at https://github.com/ZYM-PKU/UTDesign.","authors":["Yiming Zhao","Yuanpeng Gao","Yuxuan Luo","Jiwei Duan","Shisong Lin","Longfei Xiong","Zhouhui Lian"],"pdf_url":"","comment":"22 pages, 25 figures, SIGGRAPH Asia 2025, Conference Paper"},{"id":"http://arxiv.org/abs/2512.20464v1","updated":"2025-12-23T15:57:08Z","published":"2025-12-23T15:57:08Z","title":"Snapshot 3D image projection using a diffractive decoder","summary":"3D image display is essential for next-generation volumetric imaging; however, dense depth multiplexing for 3D image projection remains challenging because diffraction-induced cross-talk rapidly increases as the axial image planes get closer. Here, we introduce a 3D display system comprising a digital encoder and a diffractive optical decoder, which simultaneously projects different images onto multiple target axial planes with high axial resolution. By leveraging multi-layer diffractive wavefront decoding and deep learning-based end-to-end optimization, the system achieves high-fidelity depth-resolved 3D image projection in a snapshot, enabling axial plane separations on the order of a wavelength. The digital encoder leverages a Fourier encoder network to capture multi-scale spatial and frequency-domain features from input images, integrates axial position encoding, and generates a unified phase representation that simultaneously encodes all images to be axially projected in a single snapshot through a jointly-optimized diffractive decoder. We characterized the impact of diffractive decoder depth, output diffraction efficiency, spatial light modulator resolution, and axial encoding density, revealing trade-offs that govern axial separation and 3D image projection quality. We further demonstrated the capability to display volumetric images containing 28 axial slices, as well as the ability to dynamically reconfigure the axial locations of the image planes, performed on demand. Finally, we experimentally validated the presented approach, demonstrating close agreement between the measured results and the target images. These results establish the diffractive 3D display system as a compact and scalable framework for depth-resolved snapshot 3D image projection, with potential applications in holographic displays, AR/VR interfaces, and volumetric optical computing.","authors":["Cagatay Isil","Alexander Chen","Yuhang Li","F. Onuralp Ardic","Shiqi Chen","Che-Yung Shen","Aydogan Ozcan"],"pdf_url":"","comment":"22 Pages, 8 Figures"},{"id":"http://arxiv.org/abs/2512.20451v1","updated":"2025-12-23T15:43:48Z","published":"2025-12-23T15:43:48Z","title":"Beyond Motion Pattern: An Empirical Study of Physical Forces for Human Motion Understanding","summary":"Human motion understanding has advanced rapidly through vision-based progress in recognition, tracking, and captioning. However, most existing methods overlook physical cues such as joint actuation forces that are fundamental in biomechanics. This gap motivates our study: if and when do physically inferred forces enhance motion understanding? By incorporating forces into established motion understanding pipelines, we systematically evaluate their impact across baseline models on 3 major tasks: gait recognition, action recognition, and fine-grained video captioning. Across 8 benchmarks, incorporating forces yields consistent performance gains; for example, on CASIA-B, Rank-1 gait recognition accuracy improved from 89.52% to 90.39% (+0.87), with larger gain observed under challenging conditions: +2.7% when wearing a coat and +3.0% at the side view. On Gait3D, performance also increases from 46.0% to 47.3% (+1.3). In action recognition, CTR-GCN achieved +2.00% on Penn Action, while high-exertion classes like punching/slapping improved by +6.96%. Even in video captioning, Qwen2.5-VL's ROUGE-L score rose from 0.310 to 0.339 (+0.029), indicating that physics-inferred forces enhance temporal grounding and semantic richness. These results demonstrate that force cues can substantially complement visual and kinematic features under dynamic, occluded, or appearance-varying conditions.","authors":["Anh Dao","Manh Tran","Yufei Zhang","Xiaoming Liu","Zijun Cui"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2506.12460v3","updated":"2025-12-23T15:42:00Z","published":"2025-06-14T11:56:44Z","title":"Binarization-Aware Adjuster for Discrete Decision Learning with an Application to Edge Detection","summary":"Discrete decision tasks in machine learning exhibit a fundamental misalignment between training and inference: models are optimized with continuous-valued outputs but evaluated using discrete predictions. This misalignment arises from the discontinuity of discretization operations, which prevents decision behavior from being directly incorporated into gradient-based optimization. To address this issue, we propose a theoretically grounded framework termed the Binarization-Aware Adjuster (BAA), which embeds binarization characteristics into continuous optimization. The framework is built upon the Distance Weight Function (DWF), which modulates loss contributions according to prediction correctness and proximity to the decision threshold, thereby aligning optimization emphasis with decision-critical regions while remaining compatible with standard learning pipelines. We apply the proposed BAA framework to the edge detection (ED) task, a representative binary decision problem. Experimental results on representative models and datasets show that incorporating BAA into optimization leads to consistent performance improvements, supporting its effectiveness. Overall, this work establishes a principled approach for aligning continuous optimization with discrete decision behavior, with its effectiveness demonstrated in a concrete application setting.","authors":["Hao Shu"],"pdf_url":"","comment":"28 pages"},{"id":"http://arxiv.org/abs/2512.20436v1","updated":"2025-12-23T15:24:31Z","published":"2025-12-23T15:24:31Z","title":"Dual-Encoder Transformer-Based Multimodal Learning for Ischemic Stroke Lesion Segmentation Using Diffusion MRI","summary":"Accurate segmentation of ischemic stroke lesions from diffusion magnetic resonance imaging (MRI) is essential for clinical decision-making and outcome assessment. Diffusion-Weighted Imaging (DWI) and Apparent Diffusion Coefficient (ADC) scans provide complementary information on acute and sub-acute ischemic changes; however, automated lesion delineation remains challenging due to variability in lesion appearance.\n  In this work, we study ischemic stroke lesion segmentation using multimodal diffusion MRI from the ISLES 2022 dataset. Several state-of-the-art convolutional and transformer-based architectures, including U-Net variants, Swin-UNet, and TransUNet, are benchmarked. Based on performance, a dual-encoder TransUNet architecture is proposed to learn modality-specific representations from DWI and ADC inputs. To incorporate spatial context, adjacent slice information is integrated using a three-slice input configuration.\n  All models are trained under a unified framework and evaluated using the Dice Similarity Coefficient (DSC). Results show that transformer-based models outperform convolutional baselines, and the proposed dual-encoder TransUNet achieves the best performance, reaching a Dice score of 85.4% on the test set. The proposed framework offers a robust solution for automated ischemic stroke lesion segmentation from diffusion MRI.","authors":["Muhammad Usman","Azka Rehman","Muhammad Mutti Ur Rehman","Abd Ur Rehman","Muhammad Umar Farooq"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20432v1","updated":"2025-12-23T15:21:18Z","published":"2025-12-23T15:21:18Z","title":"High Dimensional Data Decomposition for Anomaly Detection of Textured Images","summary":"In the realm of diverse high-dimensional data, images play a significant role across various processes of manufacturing systems where efficient image anomaly detection has emerged as a core technology of utmost importance. However, when applied to textured defect images, conventional anomaly detection methods have limitations including non-negligible misidentification, low robustness, and excessive reliance on large-scale and structured datasets. This paper proposes a texture basis integrated smooth decomposition (TBSD) approach, which is targeted at efficient anomaly detection in textured images with smooth backgrounds and sparse anomalies. Mathematical formulation of quasi-periodicity and its theoretical properties are investigated for image texture estimation. TBSD method consists of two principal processes: the first process learns the texture basis functions to effectively extract quasi-periodic texture patterns; the subsequent anomaly detection process utilizes that texture basis as prior knowledge to prevent texture misidentification and capture potential anomalies with high accuracy.The proposed method surpasses benchmarks with less misidentification, smaller training dataset requirement, and superior anomaly detection performance on both simulation and real-world datasets.","authors":["Ji Song","Xing Wang","Jianguo Wu","Xiaowei Yue"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20431v1","updated":"2025-12-23T15:20:47Z","published":"2025-12-23T15:20:47Z","title":"Skin Lesion Classification Using a Soft Voting Ensemble of Convolutional Neural Networks","summary":"Skin cancer can be identified by dermoscopic examination and ocular inspection, but early detection significantly increases survival chances. Artificial intelligence (AI), using annotated skin images and Convolutional Neural Networks (CNNs), improves diagnostic accuracy. This paper presents an early skin cancer classification method using a soft voting ensemble of CNNs. In this investigation, three benchmark datasets, namely HAM10000, ISIC 2016, and ISIC 2019, were used. The process involved rebalancing, image augmentation, and filtering techniques, followed by a hybrid dual encoder for segmentation via transfer learning. Accurate segmentation focused classification models on clinically significant features, reducing background artifacts and improving accuracy. Classification was performed through an ensemble of MobileNetV2, VGG19, and InceptionV3, balancing accuracy and speed for real-world deployment. The method achieved lesion recognition accuracies of 96.32\\%, 90.86\\%, and 93.92\\% for the three datasets. The system performance was evaluated using established skin lesion detection metrics, yielding impressive results.","authors":["Abdullah Al Shafi","Abdul Muntakim","Pintu Chandra Shill","Rowzatul Zannat","Abdullah Al-Amin"],"pdf_url":"","comment":"Authors' version of the paper published in proceedings of ECCE, DOI: https://doi.org/10.1109/ECCE64574.2025.11013422"},{"id":"http://arxiv.org/abs/2511.21541v2","updated":"2025-12-23T15:17:06Z","published":"2025-11-26T16:14:18Z","title":"Video Generation Models Are Good Latent Reward Models","summary":"Reward feedback learning (ReFL) has proven effective for aligning image generation with human preferences. However, its extension to video generation faces significant challenges. Existing video reward models rely on vision-language models designed for pixel-space inputs, confining ReFL optimization to near-complete denoising steps after computationally expensive VAE decoding. This pixel-space approach incurs substantial memory overhead and increased training time, and its late-stage optimization lacks early-stage supervision, refining only visual quality rather than fundamental motion dynamics and structural coherence. In this work, we show that pre-trained video generation models are naturally suited for reward modeling in the noisy latent space, as they are explicitly designed to process noisy latent representations at arbitrary timesteps and inherently preserve temporal information through their sequential modeling capabilities. Accordingly, we propose Process Reward Feedback Learning~(PRFL), a framework that conducts preference optimization entirely in latent space, enabling efficient gradient backpropagation throughout the full denoising chain without VAE decoding. Extensive experiments demonstrate that PRFL significantly improves alignment with human preferences, while achieving substantial reductions in memory consumption and training time compared to RGB ReFL.","authors":["Xiaoyue Mi","Wenqing Yu","Jiesong Lian","Shibo Jie","Ruizhe Zhong","Zijun Liu","Guozhen Zhang","Zixiang Zhou","Zhiyong Xu","Yuan Zhou","Qinglin Lu","Fan Tang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2505.02824v2","updated":"2025-12-23T15:15:42Z","published":"2025-05-05T17:51:55Z","title":"Towards Dataset Copyright Evasion Attack against Personalized Text-to-Image Diffusion Models","summary":"Text-to-image (T2I) diffusion models enable high-quality image generation conditioned on textual prompts. However, fine-tuning these pre-trained models for personalization raises concerns about unauthorized dataset usage. To address this issue, dataset ownership verification (DOV) has recently been proposed, which embeds watermarks into fine-tuning datasets via backdoor techniques. These watermarks remain dormant on benign samples but produce owner-specified outputs when triggered. Despite its promise, the robustness of DOV against copyright evasion attacks (CEA) remains unexplored. In this paper, we investigate how adversaries can circumvent these mechanisms, enabling models trained on watermarked datasets to bypass ownership verification. We begin by analyzing the limitations of potential attacks achieved by backdoor removal, including TPD and T2IShield. In practice, TPD suffers from inconsistent effectiveness due to randomness, while T2IShield fails when watermarks are embedded as local image patches. To this end, we introduce CEAT2I, the first CEA specifically targeting DOV in T2I diffusion models. CEAT2I consists of three stages: (1) motivated by the observation that T2I models converge faster on watermarked samples with respect to intermediate features rather than training loss, we reliably detect watermarked samples; (2) we iteratively ablate tokens from the prompts of detected samples and monitor feature shifts to identify trigger tokens; and (3) we apply a closed-form concept erasure method to remove the injected watermarks. Extensive experiments demonstrate that CEAT2I effectively evades state-of-the-art DOV mechanisms while preserving model performance. The code is available at https://github.com/csyufei/CEAT2I.","authors":["Kuofeng Gao","Yufei Zhu","Yiming Li","Jiawang Bai","Yong Yang","Zhifeng Li","Shu-Tao Xia"],"pdf_url":"","comment":"Accepted by IEEE Transactions on Information Forensics and Security"},{"id":"http://arxiv.org/abs/2512.20420v1","updated":"2025-12-23T15:02:12Z","published":"2025-12-23T15:02:12Z","title":"Simplifying Multi-Task Architectures Through Task-Specific Normalization","summary":"Multi-task learning (MTL) aims to leverage shared knowledge across tasks to improve generalization and parameter efficiency, yet balancing resources and mitigating interference remain open challenges. Architectural solutions often introduce elaborate task-specific modules or routing schemes, increasing complexity and overhead. In this work, we show that normalization layers alone are sufficient to address many of these challenges. Simply replacing shared normalization with task-specific variants already yields competitive performance, questioning the need for complex designs. Building on this insight, we propose Task-Specific Sigmoid Batch Normalization (TS$σ$BN), a lightweight mechanism that enables tasks to softly allocate network capacity while fully sharing feature extractors. TS$σ$BN improves stability across CNNs and Transformers, matching or exceeding performance on NYUv2, Cityscapes, CelebA, and PascalContext, while remaining highly parameter-efficient. Moreover, its learned gates provide a natural framework for analyzing MTL dynamics, offering interpretable insights into capacity allocation, filter specialization, and task relationships. Our findings suggest that complex MTL architectures may be unnecessary and that task-specific normalization offers a simple, interpretable, and efficient alternative.","authors":["Mihai Suteu","Ovidiu Serban"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2501.12289v2","updated":"2025-12-23T15:02:02Z","published":"2025-01-21T16:59:13Z","title":"Regressor-Guided Generative Image Editing Balances User Emotions to Reduce Time Spent Online","summary":"Internet overuse is a widespread phenomenon in today's digital society. Existing interventions, such as time limits or grayscaling, often rely on restrictive controls that provoke psychological reactance and are frequently circumvented. Building on prior work showing that emotional responses mediate the relationship between content consumption and online engagement, we investigate whether regulating the emotional impact of images can reduce online use in a non-coercive manner. We introduce and systematically analyze three regressor-guided image-editing approaches: (i) global optimization of emotion-related image attributes, (ii) optimization in a style latent space, and (iii) a diffusion-based method using classifier and classifier-free guidance. While the first two approaches modify low-level visual features (e.g., contrast, color), the diffusion-based method enables higher-level changes (e.g., adjusting clothing, facial features). Results from a controlled image-rating study and a social media experiment show that diffusion-based edits balance emotional responses and are associated with lower usage duration while preserving visual quality.","authors":["Christoph Gebhardt","Robin Willardt","Seyedmorteza Sadat","Chih-Wei Ning","Andreas Brombach","Jie Song","Otmar Hilliges","Christian Holz"],"pdf_url":"","comment":"44 pages, 22 figures"},{"id":"http://arxiv.org/abs/2512.20417v1","updated":"2025-12-23T15:01:05Z","published":"2025-12-23T15:01:05Z","title":"Chain-of-Anomaly Thoughts with Large Vision-Language Models","summary":"Automated video surveillance with Large Vision-Language Models is limited by their inherent bias towards normality, often failing to detect crimes. While Chain-of-Thought reasoning strategies show significant potential for improving performance in language tasks, the lack of inductive anomaly biases in their reasoning further steers the models towards normal interpretations. To address this, we propose Chain-of-Anomaly-Thoughts (CoAT), a multi-agent reasoning framework that introduces inductive criminal bias in the reasoning process through a final, anomaly-focused classification layer. Our method significantly improves Anomaly Detection, boosting F1-score by 11.8 p.p. on challenging low-resolution footage and Anomaly Classification by 3.78 p.p. in high-resolution videos.","authors":["Pedro Domingos","João Pereira","Vasco Lopes","João Neves","David Semedo"],"pdf_url":"","comment":"2 pages, 3 figures, 1 table. Accepted for RECPAD 2025"},{"id":"http://arxiv.org/abs/2512.20409v1","updated":"2025-12-23T14:55:53Z","published":"2025-12-23T14:55:53Z","title":"DETACH : Decomposed Spatio-Temporal Alignment for Exocentric Video and Ambient Sensors with Staged Learning","summary":"Aligning egocentric video with wearable sensors have shown promise for human action recognition, but face practical limitations in user discomfort, privacy concerns, and scalability. We explore exocentric video with ambient sensors as a non-intrusive, scalable alternative. While prior egocentric-wearable works predominantly adopt Global Alignment by encoding entire sequences into unified representations, this approach fails in exocentric-ambient settings due to two problems: (P1) inability to capture local details such as subtle motions, and (P2) over-reliance on modality-invariant temporal patterns, causing misalignment between actions sharing similar temporal patterns with different spatio-semantic contexts. To resolve these problems, we propose DETACH, a decomposed spatio-temporal framework. This explicit decomposition preserves local details, while our novel sensor-spatial features discovered via online clustering provide semantic grounding for context-aware alignment. To align the decomposed features, our two-stage approach establishes spatial correspondence through mutual supervision, then performs temporal alignment via a spatial-temporal weighted contrastive loss that adaptively handles easy negatives, hard negatives, and false negatives. Comprehensive experiments with downstream tasks on Opportunity++ and HWU-USP datasets demonstrate substantial improvements over adapted egocentric-wearable baselines.","authors":["Junho Yoon","Jaemo Jung","Hyunju Kim","Dongman Lee"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2411.07576v2","updated":"2025-12-23T14:44:20Z","published":"2024-11-12T06:29:18Z","title":"Multiscale Corrections by Continuous Super-Resolution","summary":"Finite element methods typically require a high resolution to satisfactorily approximate micro and even macro patterns of an underlying physical model. This issue can be circumvented by appropriate multiscale strategies that are able to obtain reasonable approximations on under-resolved scales. In this paper, we study the implicit neural representation and propose a continuous super-resolution network as a correction strategy for multiscale effects. It can take coarse finite element data to learn both in-distribution and out-of-distribution high-resolution finite element predictions. Our highlight is the design of a local implicit transformer, which is able to learn multiscale features. We also propose Gabor wavelet-based coordinate encodings, which can overcome the bias of neural networks learning low-frequency features. Finally, perception is often preferred over distortion, so scientists can recognize the visual pattern for further investigation. However, implicit neural representation is known for its lack of local pattern supervision. We propose to use stochastic cosine similarities to compare the local feature differences between prediction and ground truth. It shows better performance on structural alignments. Our experiments show that our proposed strategy achieves superior performance as an in-distribution and out-of-distribution super-resolution strategy.","authors":["Zhi-Song Liu","Roland Maier","Andreas Rupp"],"pdf_url":"","comment":"15 pages, 11 figures"},{"id":"http://arxiv.org/abs/2508.00477v2","updated":"2025-12-23T14:27:42Z","published":"2025-08-01T09:51:54Z","title":"LAMIC: Layout-Aware Multi-Image Composition via Scalability of Multimodal Diffusion Transformer","summary":"In controllable image synthesis, generating coherent and consistent images from multiple references with spatial layout awareness remains an open challenge. We present LAMIC, a Layout-Aware Multi-Image Composition framework that, for the first time, extends single-reference diffusion models to multi-reference scenarios in a training-free manner. Built upon the MMDiT model, LAMIC introduces two plug-and-play attention mechanisms: 1) Group Isolation Attention (GIA) to enhance entity disentanglement; and 2) Region-Modulated Attention (RMA) to enable layout-aware generation. To comprehensively evaluate model capabilities, we further introduce three metrics: 1) Inclusion Ratio (IN-R) and Fill Ratio (FI-R) for assessing layout control; and 2) Background Similarity (BG-S) for measuring background consistency. Extensive experiments show that LAMIC achieves state-of-the-art performance across most major metrics: it consistently outperforms existing multi-reference baselines in ID-S, BG-S, IN-R and AVG scores across all settings, and achieves the best DPG in complex composition tasks. These results demonstrate LAMIC's superior abilities in identity keeping, background preservation, layout control, and prompt-following, all achieved without any training or fine-tuning, showcasing strong zero-shot generalization ability. By inheriting the strengths of advanced single-reference models and enabling seamless extension to multi-image scenarios, LAMIC establishes a new training-free paradigm for controllable multi-image composition. As foundation models continue to evolve, LAMIC's performance is expected to scale accordingly. Our implementation is available at: https://github.com/Suchenl/LAMIC.","authors":["Yuzhuo Chen","Zehua Ma","Jianhua Wang","Kai Kang","Shunyu Yao","Weiming Zhang"],"pdf_url":"","comment":"8 pages, 5 figures, 3 tables"},{"id":"http://arxiv.org/abs/2512.20387v1","updated":"2025-12-23T14:22:26Z","published":"2025-12-23T14:22:26Z","title":"Generative Digital Twins: Vision-Language Simulation Models for Executable Industrial Systems","summary":"We propose a Vision-Language Simulation Model (VLSM) that unifies visual and textual understanding to synthesize executable FlexScript from layout sketches and natural-language prompts, enabling cross-modal reasoning for industrial simulation systems. To support this new paradigm, the study constructs the first large-scale dataset for generative digital twins, comprising over 120,000 prompt-sketch-code triplets that enable multimodal learning between textual descriptions, spatial structures, and simulation logic. In parallel, three novel evaluation metrics, Structural Validity Rate (SVR), Parameter Match Rate (PMR), and Execution Success Rate (ESR), are proposed specifically for this task to comprehensively evaluate structural integrity, parameter fidelity, and simulator executability. Through systematic ablation across vision encoders, connectors, and code-pretrained language backbones, the proposed models achieve near-perfect structural accuracy and high execution robustness. This work establishes a foundation for generative digital twins that integrate visual reasoning and language understanding into executable industrial simulation systems.","authors":["YuChe Hsu","AnJui Wang","TsaiChing Ni","YuanFu Yang"],"pdf_url":"","comment":"10 pages, 9 figures"},{"id":"http://arxiv.org/abs/2508.08189v3","updated":"2025-12-23T14:03:44Z","published":"2025-08-11T17:08:55Z","title":"Reinforcement Learning for Large Model: A Survey","summary":"Recent advances at the intersection of reinforcement learning (RL) and visual intelligence have enabled agents that not only perceive complex visual scenes but also reason, generate, and act within them. This survey offers a critical and up-to-date synthesis of the field. We first formalize visual RL problems and trace the evolution of policy-optimization strategies from RLHF to verifiable reward paradigms, and from Proximal Policy Optimization to Group Relative Policy Optimization. We then organize more than 200 representative works into four thematic pillars: multi-modal large language models, visual generation, unified model frameworks, and vision-language-action models. For each pillar we examine algorithmic design, reward engineering, benchmark progress, and we distill trends such as curriculum-driven training, preference-aligned diffusion, and unified reward modeling. Finally, we review evaluation protocols spanning set-level fidelity, sample-level preference, and state-level stability, and we identify open challenges that include sample efficiency, generalization, and safe deployment. Our goal is to provide researchers and practitioners with a coherent map of the rapidly expanding landscape of visual RL and to highlight promising directions for future inquiry. Resources are available at: https://github.com/weijiawu/Awesome-Visual-Reinforcement-Learning.","authors":["Weijia Wu","Chen Gao","Joya Chen","Kevin Qinghong Lin","Qingwei Meng","Yiming Zhang","Yuke Qiu","Hong Zhou","Mike Zheng Shou"],"pdf_url":"","comment":"22 pages"},{"id":"http://arxiv.org/abs/2512.20377v1","updated":"2025-12-23T14:00:55Z","published":"2025-12-23T14:00:55Z","title":"SmartSplat: Feature-Smart Gaussians for Scalable Compression of Ultra-High-Resolution Images","summary":"Recent advances in generative AI have accelerated the production of ultra-high-resolution visual content, posing significant challenges for efficient compression and real-time decoding on end-user devices. Inspired by 3D Gaussian Splatting, recent 2D Gaussian image models improve representation efficiency, yet existing methods struggle to balance compression ratio and reconstruction fidelity in ultra-high-resolution scenarios. To address this issue, we propose SmartSplat, a highly adaptive and feature-aware GS-based image compression framework that supports arbitrary image resolutions and compression ratios. SmartSplat leverages image-aware features such as gradients and color variances, introducing a Gradient-Color Guided Variational Sampling strategy together with an Exclusion-based Uniform Sampling scheme to improve the non-overlapping coverage of Gaussian primitives in pixel space. In addition, we propose a Scale-Adaptive Gaussian Color Sampling method to enhance color initialization across scales. Through joint optimization of spatial layout, scale, and color initialization, SmartSplat efficiently captures both local structures and global textures using a limited number of Gaussians, achieving high reconstruction quality under strong compression. Extensive experiments on DIV8K and a newly constructed 16K dataset demonstrate that SmartSplat consistently outperforms state-of-the-art methods at comparable compression ratios and exceeds their compression limits, showing strong scalability and practical applicability. The code is publicly available at https://github.com/lif314/SmartSplat.","authors":["Linfei Li","Lin Zhang","Zhong Wang","Ying Shen"],"pdf_url":"","comment":"Accepted by AAAI 2026"},{"id":"http://arxiv.org/abs/2512.20376v1","updated":"2025-12-23T14:00:34Z","published":"2025-12-23T14:00:34Z","title":"Linking Faces and Voices Across Languages: Insights from the FAME 2026 Challenge","summary":"Over half of the world's population is bilingual and people often communicate under multilingual scenarios. The Face-Voice Association in Multilingual Environments (FAME) 2026 Challenge, held at ICASSP 2026, focuses on developing methods for face-voice association that are effective when the language at test-time is different than the training one. This report provides a brief summary of the challenge.","authors":["Marta Moscati","Ahmed Abdullah","Muhammad Saad Saeed","Shah Nawaz","Rohan Kumar Das","Muhammad Zaigham Zaheer","Junaid Mir","Muhammad Haroon Yousaf","Khalid Mahmood Malik","Markus Schedl"],"pdf_url":"","comment":"Accepted at ICASSP 2026"},{"id":"http://arxiv.org/abs/2512.20374v1","updated":"2025-12-23T13:58:12Z","published":"2025-12-23T13:58:12Z","title":"CLIP Based Region-Aware Feature Fusion for Automated BBPS Scoring in Colonoscopy Images","summary":"Accurate assessment of bowel cleanliness is essential for effective colonoscopy procedures. The Boston Bowel Preparation Scale (BBPS) offers a standardized scoring system but suffers from subjectivity and inter-observer variability when performed manually. In this paper, to support robust training and evaluation, we construct a high-quality colonoscopy dataset comprising 2,240 images from 517 subjects, annotated with expert-agreed BBPS scores. We propose a novel automated BBPS scoring framework that leverages the CLIP model with adapter-based transfer learning and a dedicated fecal-feature extraction branch. Our method fuses global visual features with stool-related textual priors to improve the accuracy of bowel cleanliness evaluation without requiring explicit segmentation. Extensive experiments on both our dataset and the public NERTHU dataset demonstrate the superiority of our approach over existing baselines, highlighting its potential for clinical deployment in computer-aided colonoscopy analysis.","authors":["Yujia Fu","Zhiyu Dong","Tianwen Qian","Chenye Zheng","Danian Ji","Linhai Zhuo"],"pdf_url":"","comment":"12 pages, 9 figures, BMVC 2025 submission"},{"id":"http://arxiv.org/abs/2512.20362v1","updated":"2025-12-23T13:44:41Z","published":"2025-12-23T13:44:41Z","title":"CRAFT: Continuous Reasoning and Agentic Feedback Tuning for Multimodal Text-to-Image Generation","summary":"Recent work has shown that inference-time reasoning and reflection can improve text-to-image generation without retraining. However, existing approaches often rely on implicit, holistic critiques or unconstrained prompt rewrites, making their behavior difficult to interpret, control, or stop reliably. In contrast, large language models have benefited from explicit, structured forms of **thinking** based on verification, targeted correction, and early stopping.\n  We introduce CRAFT (Continuous Reasoning and Agentic Feedback Tuning), a training-free, model-agnostic framework that brings this structured reasoning paradigm to multimodal image generation. CRAFT decomposes a prompt into dependency-structured visual questions, veries generated images using a vision-language model, and applies targeted prompt edits through an LLM agent only where constraints fail. The process iterates with an explicit stopping criterion once all constraints are satised, yielding an interpretable and controllable inference-time renement loop.\n  Across multiple model families and challenging benchmarks, CRAFT consistently improves compositional accuracy, text rendering, and preference-based evaluations, with particularly strong gains for lightweight generators. Importantly, these improvements incur only a negligible inference-time overhead, allowing smaller or cheaper models to approach the quality of substantially more expensive systems. Our results suggest that explicitly structured, constraint-driven inference-time reasoning is a key ingredient for improving the reliability of multimodal generative models.","authors":["V. Kovalev","A. Kuvshinov","A. Buzovkin","D. Pokidov","D. Timonin"],"pdf_url":"","comment":"37 pages, 42 figures"},{"id":"http://arxiv.org/abs/2512.20350v1","updated":"2025-12-23T13:31:21Z","published":"2025-12-23T13:31:21Z","title":"Field-Space Attention for Structure-Preserving Earth System Transformers","summary":"Accurate and physically consistent modeling of Earth system dynamics requires machine-learning architectures that operate directly on continuous geophysical fields and preserve their underlying geometric structure. Here we introduce Field-Space attention, a mechanism for Earth system Transformers that computes attention in the physical domain rather than in a learned latent space. By maintaining all intermediate representations as continuous fields on the sphere, the architecture enables interpretable internal states and facilitates the enforcement of scientific constraints. The model employs a fixed, non-learned multiscale decomposition and learns structure-preserving deformations of the input field, allowing coherent integration of coarse and fine-scale information while avoiding the optimization instabilities characteristic of standard single-scale Vision Transformers. Applied to global temperature super-resolution on a HEALPix grid, Field-Space Transformers converge more rapidly and stably than conventional Vision Transformers and U-Net baselines, while requiring substantially fewer parameters. The explicit preservation of field structure throughout the network allows physical and statistical priors to be embedded directly into the architecture, yielding improved fidelity and reliability in data-driven Earth system modeling. These results position Field-Space Attention as a compact, interpretable, and physically grounded building block for next-generation Earth system prediction and generative modeling frameworks.","authors":["Maximilian Witte","Johannes Meuer","Étienne Plésiat","Christopher Kadow"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20340v1","updated":"2025-12-23T13:15:31Z","published":"2025-12-23T13:15:31Z","title":"The devil is in the details: Enhancing Video Virtual Try-On via Keyframe-Driven Details Injection","summary":"Although diffusion transformer (DiT)-based video virtual try-on (VVT) has made significant progress in synthesizing realistic videos, existing methods still struggle to capture fine-grained garment dynamics and preserve background integrity across video frames. They also incur high computational costs due to additional interaction modules introduced into DiTs, while the limited scale and quality of existing public datasets also restrict model generalization and effective training. To address these challenges, we propose a novel framework, KeyTailor, along with a large-scale, high-definition dataset, ViT-HD. The core idea of KeyTailor is a keyframe-driven details injection strategy, motivated by the fact that keyframes inherently contain both foreground dynamics and background consistency. Specifically, KeyTailor adopts an instruction-guided keyframe sampling strategy to filter informative frames from the input video. Subsequently,two tailored keyframe-driven modules, the garment details enhancement module and the collaborative background optimization module, are employed to distill garment dynamics into garment-related latents and to optimize the integrity of background latents, both guided by keyframes.These enriched details are then injected into standard DiT blocks together with pose, mask, and noise latents, enabling efficient and realistic try-on video synthesis. This design ensures consistency without explicitly modifying the DiT architecture, while simultaneously avoiding additional complexity. In addition, our dataset ViT-HD comprises 15, 070 high-quality video samples at a resolution of 810*1080, covering diverse garments. Extensive experiments demonstrate that KeyTailor outperforms state-of-the-art baselines in terms of garment fidelity and background integrity across both dynamic and static scenarios.","authors":["Qingdong He","Xueqin Chen","Yanjie Pan","Peng Tang","Pengcheng Xu","Zhenye Gan","Chengjie Wang","Xiaobin Hu","Jiangning Zhang","Yabiao Wang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19253v2","updated":"2025-12-23T13:00:45Z","published":"2025-12-22T10:40:03Z","title":"Machine Unlearning in the Era of Quantum Machine Learning: An Empirical Study","summary":"We present the first comprehensive empirical study of machine unlearning (MU) in hybrid quantum-classical neural networks. While MU has been extensively explored in classical deep learning, its behavior within variational quantum circuits (VQCs) and quantum-augmented architectures remains largely unexplored. First, we adapt a broad suite of unlearning methods to quantum settings, including gradient-based, distillation-based, regularization-based and certified techniques. Second, we introduce two new unlearning strategies tailored to hybrid models. Experiments across Iris, MNIST, and Fashion-MNIST, under both subset removal and full-class deletion, reveal that quantum models can support effective unlearning, but outcomes depend strongly on circuit depth, entanglement structure, and task complexity. Shallow VQCs display high intrinsic stability with minimal memorization, whereas deeper hybrid models exhibit stronger trade-offs between utility, forgetting strength, and alignment with retrain oracle. We find that certain methods, e.g. EU-k, LCA, and Certified Unlearning, consistently provide the best balance across metrics. These findings establish baseline empirical insights into quantum machine unlearning and highlight the need for quantum-aware algorithms and theoretical guarantees, as quantum machine learning systems continue to expand in scale and capability. We publicly release our code at: https://github.com/CrivoiCarla/HQML.","authors":["Carla Crivoi","Radu Tudor Ionescu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2409.03945v3","updated":"2025-12-23T12:47:08Z","published":"2024-09-05T23:54:32Z","title":"TropNNC: Structured Neural Network Compression Using Tropical Geometry","summary":"We present TropNNC, a framework for compressing neural networks with linear and convolutional layers and ReLU activations using tropical geometry. By representing a network's output as a tropical rational function, TropNNC enables structured compression via reduction of the corresponding tropical polynomials. Our method refines the geometric approximation of previous work by adaptively selecting the weights of retained neurons. Key contributions include the first application of tropical geometry to convolutional layers and the tightest known theoretical compression bound. TropNNC requires only access to network weights - no training data - and achieves competitive performance on MNIST, CIFAR, and ImageNet, matching strong baselines such as ThiNet and CUP.","authors":["Konstantinos Fotopoulos","Petros Maragos","Panagiotis Misiakos"],"pdf_url":"","comment":"v3: restructured the paper, formalized some heuristic improvements to the algorithm, and added acknowledgments"},{"id":"http://arxiv.org/abs/2511.18417v2","updated":"2025-12-23T12:33:25Z","published":"2025-11-23T12:07:45Z","title":"Categorical Equivariant Deep Learning: Category-Equivariant Neural Networks and Universal Approximation Theorems","summary":"We develop a theory of category-equivariant neural networks (CENNs) that unifies group/groupoid-equivariant networks, poset/lattice-equivariant networks, graph and sheaf neural networks. Equivariance is formulated as naturality in a topological category with Radon measures. Formulating linear and nonlinear layers in the categorical setup, we prove the equivariant universal approximation theorem in the general setting: the class of finite-depth CENNs is dense in the space of continuous equivariant transformations. We instantiate the framework for groups/groupoids, posets/lattices, graphs and cellular sheaves, deriving universal approximation theorems for them in a systematic manner. Categorical equivariant deep learning thus allows us to expand the horizons of equivariant deep learning beyond group actions, encompassing not only geometric symmetries but also contextual and compositional symmetries.","authors":["Yoshihiro Maruyama"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.10652v2","updated":"2025-12-23T12:14:27Z","published":"2025-12-11T14:01:01Z","title":"TriDF: Evaluating Perception, Detection, and Hallucination for Interpretable DeepFake Detection","summary":"Advances in generative modeling have made it increasingly easy to fabricate realistic portrayals of individuals, creating serious risks for security, communication, and public trust. Detecting such person-driven manipulations requires systems that not only distinguish altered content from authentic media but also provide clear and reliable reasoning. In this paper, we introduce TriDF, a comprehensive benchmark for interpretable DeepFake detection. TriDF contains high-quality forgeries from advanced synthesis models, covering 16 DeepFake types across image, video, and audio modalities. The benchmark evaluates three key aspects: Perception, which measures the ability of a model to identify fine-grained manipulation artifacts using human-annotated evidence; Detection, which assesses classification performance across diverse forgery families and generators; and Hallucination, which quantifies the reliability of model-generated explanations. Experiments on state-of-the-art multimodal large language models show that accurate perception is essential for reliable detection, but hallucination can severely disrupt decision-making, revealing the interdependence of these three aspects. TriDF provides a unified framework for understanding the interaction between detection accuracy, evidence identification, and explanation reliability, offering a foundation for building trustworthy systems that address real-world synthetic media threats.","authors":["Jian-Yu Jiang-Lin","Kang-Yang Huang","Ling Zou","Ling Lo","Sheng-Ping Yang","Yu-Wen Tseng","Kun-Hsiang Lin","Chia-Ling Chen","Yu-Ting Ta","Yan-Tsung Wang","Po-Ching Chen","Hongxia Xie","Hong-Han Shuai","Wen-Huang Cheng"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20299v1","updated":"2025-12-23T12:08:00Z","published":"2025-12-23T12:08:00Z","title":"KnowVal: A Knowledge-Augmented and Value-Guided Autonomous Driving System","summary":"Visual-language reasoning, driving knowledge, and value alignment are essential for advanced autonomous driving systems. However, existing approaches largely rely on data-driven learning, making it difficult to capture the complex logic underlying decision-making through imitation or limited reinforcement rewards. To address this, we propose KnowVal, a new autonomous driving system that enables visual-language reasoning through the synergistic integration of open-world perception and knowledge retrieval. Specifically, we construct a comprehensive driving knowledge graph that encodes traffic laws, defensive driving principles, and ethical norms, complemented by an efficient LLM-based retrieval mechanism tailored for driving scenarios. Furthermore, we develop a human-preference dataset and train a Value Model to guide interpretable, value-aligned trajectory assessment. Experimental results show that our method substantially improves planning performance while remaining compatible with existing architectures. Notably, KnowVal achieves the lowest collision rate on nuScenes and state-of-the-art results on Bench2Drive.","authors":["Zhongyu Xia","Wenhao Chen","Yongtao Wang","Ming-Hsuan Yang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20296v1","updated":"2025-12-23T12:04:23Z","published":"2025-12-23T12:04:23Z","title":"TAVID: Text-Driven Audio-Visual Interactive Dialogue Generation","summary":"The objective of this paper is to jointly synthesize interactive videos and conversational speech from text and reference images. With the ultimate goal of building human-like conversational systems, recent studies have explored talking or listening head generation as well as conversational speech generation. However, these works are typically studied in isolation, overlooking the multimodal nature of human conversation, which involves tightly coupled audio-visual interactions. In this paper, we introduce TAVID, a unified framework that generates both interactive faces and conversational speech in a synchronized manner. TAVID integrates face and speech generation pipelines through two cross-modal mappers (i.e., a motion mapper and a speaker mapper), which enable bidirectional exchange of complementary information between the audio and visual modalities. We evaluate our system across four dimensions: talking face realism, listening head responsiveness, dyadic interaction fluency, and speech quality. Extensive experiments demonstrate the effectiveness of our approach across all these aspects.","authors":["Ji-Hoon Kim","Junseok Ahn","Doyeop Kwak","Joon Son Chung","Shinji Watanabe"],"pdf_url":"","comment":"Project page: https://mm.kaist.ac.kr/projects/TAVID"},{"id":"http://arxiv.org/abs/2512.20288v1","updated":"2025-12-23T11:57:34Z","published":"2025-12-23T11:57:34Z","title":"UbiQVision: Quantifying Uncertainty in XAI for Image Recognition","summary":"Recent advances in deep learning have led to its widespread adoption across diverse domains, including medical imaging. This progress is driven by increasingly sophisticated model architectures, such as ResNets, Vision Transformers, and Hybrid Convolutional Neural Networks, that offer enhanced performance at the cost of greater complexity. This complexity often compromises model explainability and interpretability. SHAP has emerged as a prominent method for providing interpretable visualizations that aid domain experts in understanding model predictions. However, SHAP explanations can be unstable and unreliable in the presence of epistemic and aleatoric uncertainty. In this study, we address this challenge by using Dirichlet posterior sampling and Dempster-Shafer theory to quantify the uncertainty that arises from these unstable explanations in medical imaging applications. The framework uses a belief, plausible, and fusion map approach alongside statistical quantitative analysis to produce quantification of uncertainty in SHAP. Furthermore, we evaluated our framework on three medical imaging datasets with varying class distributions, image qualities, and modality types which introduces noise due to varying image resolutions and modality-specific aspect covering the examples from pathology, ophthalmology, and radiology, introducing significant epistemic uncertainty.","authors":["Akshat Dubey","Aleksandar Anžel","Bahar İlgen","Georges Hattab"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20260v1","updated":"2025-12-23T11:16:16Z","published":"2025-12-23T11:16:16Z","title":"${D}^{3}${ETOR}: ${D}$ebate-Enhanced Pseudo Labeling and Frequency-Aware Progressive ${D}$ebiasing for Weakly-Supervised Camouflaged Object ${D}$etection with Scribble Annotations","summary":"Weakly-Supervised Camouflaged Object Detection (WSCOD) aims to locate and segment objects that are visually concealed within their surrounding scenes, relying solely on sparse supervision such as scribble annotations. Despite recent progress, existing WSCOD methods still lag far behind fully supervised ones due to two major limitations: (1) the pseudo masks generated by general-purpose segmentation models (e.g., SAM) and filtered via rules are often unreliable, as these models lack the task-specific semantic understanding required for effective pseudo labeling in COD; and (2) the neglect of inherent annotation bias in scribbles, which hinders the model from capturing the global structure of camouflaged objects. To overcome these challenges, we propose ${D}^{3}$ETOR, a two-stage WSCOD framework consisting of Debate-Enhanced Pseudo Labeling and Frequency-Aware Progressive Debiasing. In the first stage, we introduce an adaptive entropy-driven point sampling method and a multi-agent debate mechanism to enhance the capability of SAM for COD, improving the interpretability and precision of pseudo masks. In the second stage, we design FADeNet, which progressively fuses multi-level frequency-aware features to balance global semantic understanding with local detail modeling, while dynamically reweighting supervision strength across regions to alleviate scribble bias. By jointly exploiting the supervision signals from both the pseudo masks and scribble semantics, ${D}^{3}$ETOR significantly narrows the gap between weakly and fully supervised COD, achieving state-of-the-art performance on multiple benchmarks.","authors":["Jiawei Ge","Jiuxin Cao","Xinyi Li","Xuelin Zhu","Chang Liu","Bo Liu","Chen Feng","Ioannis Patras"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20257v1","updated":"2025-12-23T11:14:58Z","published":"2025-12-23T11:14:58Z","title":"LADLE-MM: Limited Annotation based Detector with Learned Ensembles for Multimodal Misinformation","summary":"With the rise of easily accessible tools for generating and manipulating multimedia content, realistic synthetic alterations to digital media have become a widespread threat, often involving manipulations across multiple modalities simultaneously. Recently, such techniques have been increasingly employed to distort narratives of important events and to spread misinformation on social media, prompting the development of misinformation detectors. In the context of misinformation conveyed through image-text pairs, several detection methods have been proposed. However, these approaches typically rely on computationally intensive architectures or require large amounts of annotated data. In this work we introduce LADLE-MM: Limited Annotation based Detector with Learned Ensembles for Multimodal Misinformation, a model-soup initialized multimodal misinformation detector designed to operate under a limited annotation setup and constrained training resources. LADLE-MM is composed of two unimodal branches and a third multimodal one that enhances image and text representations with additional multimodal embeddings extracted from BLIP, serving as fixed reference space. Despite using 60.3% fewer trainable parameters than previous state-of-the-art models, LADLE-MM achieves competitive performance on both binary and multi-label classification tasks on the DGM4 benchmark, outperforming existing methods when trained without grounding annotations. Moreover, when evaluated on the VERITE dataset, LADLE-MM outperforms current state-of-the-art approaches that utilize more complex architectures involving Large Vision-Language-Models, demonstrating the effective generalization ability in an open-set setting and strong robustness to unimodal bias.","authors":["Daniele Cardullo","Simone Teglia","Irene Amerini"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20255v1","updated":"2025-12-23T11:13:01Z","published":"2025-12-23T11:13:01Z","title":"BiCoR-Seg: Bidirectional Co-Refinement Framework for High-Resolution Remote Sensing Image Segmentation","summary":"High-resolution remote sensing image semantic segmentation (HRSS) is a fundamental yet critical task in the field of Earth observation. However, it has long faced the challenges of high inter-class similarity and large intra-class variability. Existing approaches often struggle to effectively inject abstract yet strongly discriminative semantic knowledge into pixel-level feature learning, leading to blurred boundaries and class confusion in complex scenes. To address these challenges, we propose Bidirectional Co-Refinement Framework for HRSS (BiCoR-Seg). Specifically, we design a Heatmap-driven Bidirectional Information Synergy Module (HBIS), which establishes a bidirectional information flow between feature maps and class embeddings by generating class-level heatmaps. Based on HBIS, we further introduce a hierarchical supervision strategy, where the interpretable heatmaps generated by each HBIS module are directly utilized as low-resolution segmentation predictions for supervision, thereby enhancing the discriminative capacity of shallow features. In addition, to further improve the discriminability of the embedding representations, we propose a cross-layer class embedding Fisher Discriminative Loss to enforce intra-class compactness and enlarge inter-class separability. Extensive experiments on the LoveDA, Vaihingen, and Potsdam datasets demonstrate that BiCoR-Seg achieves outstanding segmentation performance while offering stronger interpretability. The released code is available at https://github.com/ShiJinghao566/BiCoR-Seg.","authors":["Jinghao Shi","Jianing Song"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20251v1","updated":"2025-12-23T11:05:36Z","published":"2025-12-23T11:05:36Z","title":"Degradation-Aware Metric Prompting for Hyperspectral Image Restoration","summary":"Unified hyperspectral image (HSI) restoration aims to recover various degraded HSIs using a single model, offering great practical value. However, existing methods often depend on explicit degradation priors (e.g., degradation labels) as prompts to guide restoration, which are difficult to obtain due to complex and mixed degradations in real-world scenarios. To address this challenge, we propose a Degradation-Aware Metric Prompting (DAMP) framework. Instead of relying on predefined degradation priors, we design spatial-spectral degradation metrics to continuously quantify multi-dimensional degradations, serving as Degradation Prompts (DP). These DP enable the model to capture cross-task similarities in degradation distributions and enhance shared feature learning. Furthermore, we introduce a Spatial-Spectral Adaptive Module (SSAM) that dynamically modulates spatial and spectral feature extraction through learnable parameters. By integrating SSAM as experts within a Mixture-of-Experts architecture, and using DP as the gating router, the framework enables adaptive, efficient, and robust restoration under diverse, mixed, or unseen degradations. Extensive experiments on natural and remote sensing HSI datasets show that DAMP achieves state-of-the-art performance and demonstrates exceptional generalization capability. Code is publicly available at https://github.com/MiliLab/DAMP.","authors":["Binfeng Wang","Di Wang","Haonan Guo","Ying Fu","Jing Zhang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20249v1","updated":"2025-12-23T11:04:34Z","published":"2025-12-23T11:04:34Z","title":"Unified Multimodal Brain Decoding via Cross-Subject Soft-ROI Fusion","summary":"Multimodal brain decoding aims to reconstruct semantic information that is consistent with visual stimuli from brain activity signals such as fMRI, and then generate readable natural language descriptions. However, multimodal brain decoding still faces key challenges in cross-subject generalization and interpretability. We propose a BrainROI model and achieve leading-level results in brain-captioning evaluation on the NSD dataset. Under the cross-subject setting, compared with recent state-of-the-art methods and representative baselines, metrics such as BLEU-4 and CIDEr show clear improvements. Firstly, to address the heterogeneity of functional brain topology across subjects, we design a new fMRI encoder. We use multi-atlas soft functional parcellations (soft-ROI) as a shared space. We extend the discrete ROI Concatenation strategy in MINDLLM to a voxel-wise gated fusion mechanism (Voxel-gate). We also ensure consistent ROI mapping through global label alignment, which enhances cross-subject transferability. Secondly, to overcome the limitations of manual and black-box prompting methods in stability and transparency, we introduce an interpretable prompt optimization process. In a small-sample closed loop, we use a locally deployed Qwen model to iteratively generate and select human-readable prompts. This process improves the stability of prompt design and preserves an auditable optimization trajectory. Finally, we impose parameterized decoding constraints during inference to further improve the stability and quality of the generated descriptions.","authors":["Xuanyu Hu"],"pdf_url":"","comment":"15 pages, 2 figures, 4 tables. Submitted to ICPR 2026"},{"id":"http://arxiv.org/abs/2512.20236v1","updated":"2025-12-23T10:49:37Z","published":"2025-12-23T10:49:37Z","title":"IndicDLP: A Foundational Dataset for Multi-Lingual and Multi-Domain Document Layout Parsing","summary":"Document layout analysis is essential for downstream tasks such as information retrieval, extraction, OCR, and digitization. However, existing large-scale datasets like PubLayNet and DocBank lack fine-grained region labels and multilingual diversity, making them insufficient for representing complex document layouts. In contrast, human-annotated datasets such as M6Doc and D4LA offer richer labels and greater domain diversity, but are too small to train robust models and lack adequate multilingual coverage. This gap is especially pronounced for Indic documents, which encompass diverse scripts yet remain underrepresented in current datasets, further limiting progress in this space. To address these shortcomings, we introduce IndicDLP, a large-scale foundational document layout dataset spanning 11 representative Indic languages alongside English and 12 common document domains. Additionally, we curate UED-mini, a dataset derived from DocLayNet and M6Doc, to enhance pretraining and provide a solid foundation for Indic layout models. Our experiments demonstrate that fine-tuning existing English models on IndicDLP significantly boosts performance, validating its effectiveness. Moreover, models trained on IndicDLP generalize well beyond Indic layouts, making it a valuable resource for document digitization. This work bridges gaps in scale, diversity, and annotation granularity, driving inclusive and efficient document understanding.","authors":["Oikantik Nath","Sahithi Kukkala","Mitesh Khapra","Ravi Kiran Sarvadevabhatla"],"pdf_url":"","comment":"Accepted in ICDAR 2025 (Oral Presentation) - Best Student Paper Runner-Up Award"},{"id":"http://arxiv.org/abs/2512.20233v1","updated":"2025-12-23T10:46:48Z","published":"2025-12-23T10:46:48Z","title":"How I Met Your Bias: Investigating Bias Amplification in Diffusion Models","summary":"Diffusion-based generative models demonstrate state-of-the-art performance across various image synthesis tasks, yet their tendency to replicate and amplify dataset biases remains poorly understood. Although previous research has viewed bias amplification as an inherent characteristic of diffusion models, this work provides the first analysis of how sampling algorithms and their hyperparameters influence bias amplification. We empirically demonstrate that samplers for diffusion models -- commonly optimized for sample quality and speed -- have a significant and measurable effect on bias amplification. Through controlled studies with models trained on Biased MNIST, Multi-Color MNIST and BFFHQ, and with Stable Diffusion, we show that sampling hyperparameters can induce both bias reduction and amplification, even when the trained model is fixed. Source code is available at https://github.com/How-I-met-your-bias/how_i_met_your_bias.","authors":["Nathan Roos","Ekaterina Iakovleva","Ani Gjergji","Vito Paolo Pastore","Enzo Tartaglione"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2504.10201v2","updated":"2025-12-23T10:39:46Z","published":"2025-04-14T13:09:49Z","title":"VibrantLeaves: A principled parametric image generator for training deep restoration models","summary":"Even though Deep Neural Networks are extremely powerful for image restoration tasks, they have several limitations. They are poorly understood and suffer from strong biases inherited from the training sets. One way to address these shortcomings is to have a better control over the training sets, in particular by using synthetic sets. In this paper, we propose a synthetic image generator relying on a few simple principles. In particular, we focus on geometric modeling, textures, and a simple modeling of image acquisition. These properties, integrated in a classical Dead Leaves model, enable the creation of efficient training sets. Standard image denoising and super-resolution networks can be trained on such datasets, reaching performance almost on par with training on natural image datasets. As a first step towards explainability, we provide a careful analysis of the considered principles, identifying which image properties are necessary to obtain good performances. Besides, such training also yields better robustness to various geometric and radiometric perturbations of the test sets.","authors":["Raphael Achddou","Yann Gousseau","Saïd Ladjal","Sabine Süsstrunk"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2511.14396v5","updated":"2025-12-23T10:17:59Z","published":"2025-11-18T12:01:06Z","title":"Continuous Vision-Language-Action Co-Learning with Semantic-Physical Alignment for Behavioral Cloning","summary":"Language-conditioned manipulation facilitates human-robot interaction via behavioral cloning (BC), which learns control policies from human demonstrations and serves as a cornerstone of embodied AI. Overcoming compounding errors in sequential action decisions remains a central challenge to improving BC performance. Existing approaches mitigate compounding errors through data augmentation, expressive representation, or temporal abstraction. However, they suffer from physical discontinuities and semantic-physical misalignment, leading to inaccurate action cloning and intermittent execution. In this paper, we present Continuous vision-language-action Co-Learning with Semantic-Physical Alignment (CCoL), a novel BC framework that ensures temporally consistent execution and fine-grained semantic grounding. It generates robust and smooth action execution trajectories through continuous co-learning across vision, language, and proprioceptive inputs (e.g., robot internal states). Meanwhile, we anchor language semantics to visuomotor representations by a bidirectional cross-attention to learn contextual information for action generation, successfully overcoming the problem of semantic-physical misalignment. Extensive experiments show that CCoL achieves an average 8.0% relative improvement across three simulation suites, with up to 19.2% relative gain in human-demonstrated bimanual insertion tasks. Real-world tests on a 7-DoF robot further confirm CCoL's generalization under unseen and noisy object states.","authors":["Xiuxiu Qi","Yu Yang","Jiannong Cao","Luyao Bai","Chongshan Fan","Chengtai Cao","Hongpeng Wang"],"pdf_url":"","comment":"Accepted at AAAI 2026, the Project website is available at https://qhemu.github.io/CCoL/"},{"id":"http://arxiv.org/abs/2512.20217v1","updated":"2025-12-23T10:16:33Z","published":"2025-12-23T10:16:33Z","title":"LiteFusion: Taming 3D Object Detectors from Vision-Based to Multi-Modal with Minimal Adaptation","summary":"3D object detection is fundamental for safe and robust intelligent transportation systems. Current multi-modal 3D object detectors often rely on complex architectures and training strategies to achieve higher detection accuracy. However, these methods heavily rely on the LiDAR sensor so that they suffer from large performance drops when LiDAR is absent, which compromises the robustness and safety of autonomous systems in practical scenarios. Moreover, existing multi-modal detectors face difficulties in deployment on diverse hardware platforms, such as NPUs and FPGAs, due to their reliance on 3D sparse convolution operators, which are primarily optimized for NVIDIA GPUs. To address these challenges, we reconsider the role of LiDAR in the camera-LiDAR fusion paradigm and introduce a novel multi-modal 3D detector, LiteFusion. Instead of treating LiDAR point clouds as an independent modality with a separate feature extraction backbone, LiteFusion utilizes LiDAR data as a complementary source of geometric information to enhance camera-based detection. This straightforward approach completely eliminates the reliance on a 3D backbone, making the method highly deployment-friendly. Specifically, LiteFusion integrates complementary features from LiDAR points into image features within a quaternion space, where the orthogonal constraints are well-preserved during network training. This helps model domain-specific relations across modalities, yielding a compact cross-modal embedding. Experiments on the nuScenes dataset show that LiteFusion improves the baseline vision-based detector by +20.4% mAP and +19.7% NDS with a minimal increase in parameters (1.1%) without using dedicated LiDAR encoders. Notably, even in the absence of LiDAR input, LiteFusion maintains strong results , highlighting its favorable robustness and effectiveness across diverse fusion paradigms and deployment scenarios.","authors":["Xiangxuan Ren","Zhongdao Wang","Pin Tang","Guoqing Wang","Jilai Zheng","Chao Ma"],"pdf_url":"","comment":"13 pages, 9 figures, 8 tables"},{"id":"http://arxiv.org/abs/2512.17601v2","updated":"2025-12-23T10:13:38Z","published":"2025-12-19T14:07:34Z","title":"HeadHunt-VAD: Hunting Robust Anomaly-Sensitive Heads in MLLM for Tuning-Free Video Anomaly Detection","summary":"Video Anomaly Detection (VAD) aims to locate events that deviate from normal patterns in videos. Traditional approaches often rely on extensive labeled data and incur high computational costs. Recent tuning-free methods based on Multimodal Large Language Models (MLLMs) offer a promising alternative by leveraging their rich world knowledge. However, these methods typically rely on textual outputs, which introduces information loss, exhibits normalcy bias, and suffers from prompt sensitivity, making them insufficient for capturing subtle anomalous cues. To address these constraints, we propose HeadHunt-VAD, a novel tuning-free VAD paradigm that bypasses textual generation by directly hunting robust anomaly-sensitive internal attention heads within the frozen MLLM. Central to our method is a Robust Head Identification module that systematically evaluates all attention heads using a multi-criteria analysis of saliency and stability, identifying a sparse subset of heads that are consistently discriminative across diverse prompts. Features from these expert heads are then fed into a lightweight anomaly scorer and a temporal locator, enabling efficient and accurate anomaly detection with interpretable outputs. Extensive experiments show that HeadHunt-VAD achieves state-of-the-art performance among tuning-free methods on two major VAD benchmarks while maintaining high efficiency, validating head-level probing in MLLMs as a powerful and practical solution for real-world anomaly detection.","authors":["Zhaolin Cai","Fan Li","Ziwei Zheng","Haixia Bi","Lijun He"],"pdf_url":"","comment":"AAAI 2026 Oral"},{"id":"http://arxiv.org/abs/2512.20213v1","updated":"2025-12-23T10:12:35Z","published":"2025-12-23T10:12:35Z","title":"JDPNet: A Network Based on Joint Degradation Processing for Underwater Image Enhancement","summary":"Given the complexity of underwater environments and the variability of water as a medium, underwater images are inevitably subject to various types of degradation. The degradations present nonlinear coupling rather than simple superposition, which renders the effective processing of such coupled degradations particularly challenging. Most existing methods focus on designing specific branches, modules, or strategies for specific degradations, with little attention paid to the potential information embedded in their coupling. Consequently, they struggle to effectively capture and process the nonlinear interactions of multiple degradations from a bottom-up perspective. To address this issue, we propose JDPNet, a joint degradation processing network, that mines and unifies the potential information inherent in coupled degradations within a unified framework. Specifically, we introduce a joint feature-mining module, along with a probabilistic bootstrap distribution strategy, to facilitate effective mining and unified adjustment of coupled degradation features. Furthermore, to balance color, clarity, and contrast, we design a novel AquaBalanceLoss to guide the network in learning from multiple coupled degradation losses. Experiments on six publicly available underwater datasets, as well as two new datasets constructed in this study, show that JDPNet exhibits state-of-the-art performance while offering a better tradeoff between performance, parameter size, and computational cost.","authors":["Tao Ye","Hongbin Ren","Chongbing Zhang","Haoran Chen","Xiaosong Li"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2411.09484v4","updated":"2025-12-23T09:36:15Z","published":"2024-11-14T14:37:50Z","title":"Image Matching Filtering and Refinement by Planes and Beyond","summary":"This paper introduces a modular, non-deep learning method for filtering and refining sparse correspondences in image matching. Assuming that motion flow within the scene can be approximated by local homography transformations, matches are aggregated into overlapping clusters corresponding to virtual planes using an iterative RANSAC-based approach discarding incompatible correspondences. Moreover, the underlying planar structural design provides an explicit map between local patches associated with the matches, by which optionally refine the keypoint positions through cross-correlation template matching after the patch reprojection. Finally, to enhance robustness and fault-tolerance against violations of the piece-wise planar approximation assumption, a further strategy is designed in order to minimize the relative patch distortion in the plane reprojection by introducing an intermediate homography that projects both patches into a common plane. The proposed method is extensively evaluated on standard datasets and image matching pipelines, and compared with state-of-the-art approaches. Unlike other current comparisons, the proposed benchmark also takes into account the more general, real, and practical cases where camera intrinsics are unavailable. Experimental results demonstrate that our proposed non-deep learning, geometry-based filter is effective in presence of outliers and the optional cross-correlation refinement step is valid in the case of corner-like keypoints. Finally, this study suggests that there is still significant development potential in practical image matching solutions in the considered research direction, which could be in the future incorporated in novel deep image matching architectures.","authors":["Fabio Bellavia","Zhenjun Zhao","Luca Morelli","Fabio Remondino"],"pdf_url":"","comment":"project page: https://github.com/fb82/MiHo"},{"id":"http://arxiv.org/abs/2507.13387v3","updated":"2025-12-23T09:36:05Z","published":"2025-07-16T01:57:16Z","title":"From Binary to Semantic: Utilizing Large-Scale Binary Occupancy Data for 3D Semantic Occupancy Prediction","summary":"Accurate perception of the surrounding environment is essential for safe autonomous driving. 3D occupancy prediction, which estimates detailed 3D structures of roads, buildings, and other objects, is particularly important for vision-centric autonomous driving systems that do not rely on LiDAR sensors. However, in 3D semantic occupancy prediction -- where each voxel is assigned a semantic label -- annotated LiDAR point clouds are required, making data acquisition costly. In contrast, large-scale binary occupancy data, which only indicate occupied or free space without semantic labels, can be collected at a lower cost. Despite their availability, the potential of leveraging such data remains unexplored. In this study, we investigate the utilization of large-scale binary occupancy data from two perspectives: (1) pre-training and (2) learning-based auto-labeling. We propose a novel binary occupancy-based framework that decomposes the prediction process into binary and semantic occupancy modules, enabling effective use of binary occupancy data. Our experimental results demonstrate that the proposed framework outperforms existing methods in both pre-training and auto-labeling tasks, highlighting its effectiveness in enhancing 3D semantic occupancy prediction. The code will be available at https://github.com/ToyotaInfoTech/b2s-occupancy","authors":["Chihiro Noguchi","Takaki Yamamoto"],"pdf_url":"","comment":"Accepted to ICCV Workshop 2025"},{"id":"http://arxiv.org/abs/2512.20194v1","updated":"2025-12-23T09:35:40Z","published":"2025-12-23T09:35:40Z","title":"Generative Latent Coding for Ultra-Low Bitrate Image Compression","summary":"Most existing image compression approaches perform transform coding in the pixel space to reduce its spatial redundancy. However, they encounter difficulties in achieving both high-realism and high-fidelity at low bitrate, as the pixel-space distortion may not align with human perception. To address this issue, we introduce a Generative Latent Coding (GLC) architecture, which performs transform coding in the latent space of a generative vector-quantized variational auto-encoder (VQ-VAE), instead of in the pixel space. The generative latent space is characterized by greater sparsity, richer semantic and better alignment with human perception, rendering it advantageous for achieving high-realism and high-fidelity compression. Additionally, we introduce a categorical hyper module to reduce the bit cost of hyper-information, and a code-prediction-based supervision to enhance the semantic consistency. Experiments demonstrate that our GLC maintains high visual quality with less than 0.04 bpp on natural images and less than 0.01 bpp on facial images. On the CLIC2020 test set, we achieve the same FID as MS-ILLM with 45% fewer bits. Furthermore, the powerful generative latent space enables various applications built on our GLC pipeline, such as image restoration and style transfer. The code is available at https://github.com/jzyustc/GLC.","authors":["Zhaoyang Jia","Jiahao Li","Bin Li","Houqiang Li","Yan Lu"],"pdf_url":"","comment":"Accepted at CVPR 2024"},{"id":"http://arxiv.org/abs/2512.19316v2","updated":"2025-12-23T09:25:34Z","published":"2025-12-22T12:07:05Z","title":"Neural Implicit Heart Coordinates: 3D cardiac shape reconstruction from sparse segmentations","summary":"Accurate reconstruction of cardiac anatomy from sparse clinical images remains a major challenge in patient-specific modeling. While neural implicit functions have previously been applied to this task, their application to mapping anatomical consistency across subjects has been limited. In this work, we introduce Neural Implicit Heart Coordinates (NIHCs), a standardized implicit coordinate system, based on universal ventricular coordinates, that provides a common anatomical reference frame for the human heart. Our method predicts NIHCs directly from a limited number of 2D segmentations (sparse acquisition) and subsequently decodes them into dense 3D segmentations and high-resolution meshes at arbitrary output resolution. Trained on a large dataset of 5,000 cardiac meshes, the model achieves high reconstruction accuracy on clinical contours, with mean Euclidean surface errors of 2.51$\\pm$0.33 mm in a diseased cohort (n=4549) and 2.3$\\pm$0.36 mm in a healthy cohort (n=5576). The NIHC representation enables anatomically coherent reconstruction even under severe slice sparsity and segmentation noise, faithfully recovering complex structures such as the valve planes. Compared with traditional pipelines, inference time is reduced from over 60 s to 5-15 s. These results demonstrate that NIHCs constitute a robust and efficient anatomical representation for patient-specific 3D cardiac reconstruction from minimal input data.","authors":["Marica Muffoletto","Uxio Hermida","Charlène Mauger","Avan Suinesiaputra","Yiyang Xu","Richard Burns","Lisa Pankewitz","Andrew D McCulloch","Steffen E Petersen","Daniel Rueckert","Alistair A Young"],"pdf_url":"","comment":"42 pages, 8 figures"},{"id":"http://arxiv.org/abs/2508.12711v4","updated":"2025-12-23T09:17:31Z","published":"2025-08-18T08:19:43Z","title":"Drifting Away from Truth: GenAI-Driven News Diversity Challenges LVLM-Based Misinformation Detection","summary":"The proliferation of multimodal misinformation poses growing threats to public discourse and societal trust. While Large Vision-Language Models (LVLMs) have enabled recent progress in multimodal misinformation detection (MMD), the rise of generative AI (GenAI) tools introduces a new challenge: GenAI-driven news diversity, characterized by highly varied and complex content. We show that this diversity induces multi-level drift, comprising (1) model-level misperception drift, where stylistic variations disrupt a model's internal reasoning, and (2) evidence-level drift, where expression diversity degrades the quality or relevance of retrieved external evidence. These drifts significantly degrade the robustness of current LVLM-based MMD systems. To systematically study this problem, we introduce DriftBench, a large-scale benchmark comprising 16,000 news instances across six categories of diversification. We design three evaluation tasks: (1) robustness of truth verification under multi-level drift; (2) susceptibility to adversarial evidence contamination generated by GenAI; and (3) analysis of reasoning consistency across diverse inputs. Experiments with six state-of-the-art LVLM-based detectors show substantial performance drops (average F1 -14.8%) and increasingly unstable reasoning traces, with even more severe failures under adversarial evidence injection. Our findings uncover fundamental vulnerabilities in existing MMD systems and suggest an urgent need for more resilient approaches in the GenAI era.","authors":["Fanxiao Li","Jiaying Wu","Tingchao Fu","Yunyun Dong","Bingbing Song","Wei Zhou"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20174v1","updated":"2025-12-23T09:14:16Z","published":"2025-12-23T09:14:16Z","title":"Towards Natural Language-Based Document Image Retrieval: New Dataset and Benchmark","summary":"Document image retrieval (DIR) aims to retrieve document images from a gallery according to a given query. Existing DIR methods are primarily based on image queries that retrieve documents within the same coarse semantic category, e.g., newspapers or receipts. However, these methods struggle to effectively retrieve document images in real-world scenarios where textual queries with fine-grained semantics are usually provided. To bridge this gap, we introduce a new Natural Language-based Document Image Retrieval (NL-DIR) benchmark with corresponding evaluation metrics. In this work, natural language descriptions serve as semantically rich queries for the DIR task. The NL-DIR dataset contains 41K authentic document images, each paired with five high-quality, fine-grained semantic queries generated and evaluated through large language models in conjunction with manual verification. We perform zero-shot and fine-tuning evaluations of existing mainstream contrastive vision-language models and OCR-free visual document understanding (VDU) models. A two-stage retrieval method is further investigated for performance improvement while achieving both time and space efficiency. We hope the proposed NL-DIR benchmark can bring new opportunities and facilitate research for the VDU community. Datasets and codes will be publicly available at huggingface.co/datasets/nianbing/NL-DIR.","authors":["Hao Guo","Xugong Qin","Jun Jie Ou Yang","Peng Zhang","Gangyan Zeng","Yubo Li","Hailun Lin"],"pdf_url":"","comment":"CVPR 2025"},{"id":"http://arxiv.org/abs/2511.17004v3","updated":"2025-12-23T08:46:41Z","published":"2025-11-21T07:14:46Z","title":"Vision Language Models are Confused Tourists","summary":"Although the cultural dimension has been one of the key aspects in evaluating Vision-Language Models (VLMs), their ability to remain stable across diverse cultural inputs remains largely untested, despite being crucial to support diversity and multicultural societies. Existing evaluations often rely on benchmarks featuring only a singular cultural concept per image, overlooking scenarios where multiple, potentially unrelated cultural cues coexist. To address this gap, we introduce ConfusedTourist, a novel cultural adversarial robustness suite designed to assess VLMs' stability against perturbed geographical cues. Our experiments reveal a critical vulnerability, where accuracy drops heavily under simple image-stacking perturbations and even worsens with its image-generation-based variant. Interpretability analyses further show that these failures stem from systematic attention shifts toward distracting cues, diverting the model from its intended focus. These findings highlight a critical challenge: visual cultural concept mixing can substantially impair even state-of-the-art VLMs, underscoring the urgent need for more culturally robust multimodal understanding.","authors":["Patrick Amadeus Irawan","Ikhlasul Akmal Hanif","Muhammad Dehan Al Kautsar","Genta Indra Winata","Fajri Koto","Alham Fikri Aji"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20157v1","updated":"2025-12-23T08:37:11Z","published":"2025-12-23T08:37:11Z","title":"AMoE: Agglomerative Mixture-of-Experts Vision Foundation Model","summary":"Vision foundation models trained via multi-teacher distillation offer a promising path toward unified visual representations, yet the learning dynamics and data efficiency of such approaches remain underexplored. In this paper, we systematically study multi-teacher distillation for vision foundation models and identify key factors that enable training at lower computational cost. We introduce Agglomerative Mixture-of-Experts Vision Foundation Models (AMoE), which distill knowledge from SigLIP2 and DINOv3 simultaneously into a Mixture-of-Experts student. We show that (1) our Asymmetric Relation-Knowledge Distillation loss preserves the geometric properties of each teacher while enabling effective knowledge transfer, (2) token-balanced batching that packs varying-resolution images into sequences with uniform token budgets stabilizes representation learning across resolutions without sacrificing performance, and (3) hierarchical clustering and sampling of training data--typically reserved for self-supervised learning--substantially improves sample efficiency over random sampling for multi-teacher distillation. By combining these findings, we curate OpenLVD200M, a 200M-image corpus that demonstrates superior efficiency for multi-teacher distillation. Instantiated in a Mixture-of-Experts. We release OpenLVD200M and distilled models.","authors":["Sofian Chaybouti","Sanath Narayan","Yasser Dahou","Phúc H. Lê Khac","Ankit Singh","Ngoc Dung Huynh","Wamiq Reyaz Para","Hilde Kuehne","Hakim Hacid"],"pdf_url":"","comment":"17 pages, 8 figures, 11 tables"},{"id":"http://arxiv.org/abs/2512.16922v2","updated":"2025-12-23T08:36:48Z","published":"2025-12-18T18:59:58Z","title":"Next-Embedding Prediction Makes Strong Vision Learners","summary":"Inspired by the success of generative pretraining in natural language, we ask whether the same principles can yield strong self-supervised visual learners. Instead of training models to output features for downstream use, we train them to generate embeddings to perform predictive tasks directly. This work explores such a shift from learning representations to learning models. Specifically, models learn to predict future patch embeddings conditioned on past ones, using causal masking and stop gradient, which we refer to as Next-Embedding Predictive Autoregression (NEPA). We demonstrate that a simple Transformer pretrained on ImageNet-1k with next embedding prediction as its sole learning objective is effective - no pixel reconstruction, discrete tokens, contrastive loss, or task-specific heads. This formulation retains architectural simplicity and scalability, without requiring additional design complexity. NEPA achieves strong results across tasks, attaining 83.8% and 85.3% top-1 accuracy on ImageNet-1K with ViT-B and ViT-L backbones after fine-tuning, and transferring effectively to semantic segmentation on ADE20K. We believe generative pretraining from embeddings provides a simple, scalable, and potentially modality-agnostic alternative to visual self-supervised learning.","authors":["Sihan Xu","Ziqiao Ma","Wenhao Chai","Xuweiyi Chen","Weiyang Jin","Joyce Chai","Saining Xie","Stella X. Yu"],"pdf_url":"","comment":"Project Page: https://sihanxu.me/nepa"},{"id":"http://arxiv.org/abs/2512.20153v1","updated":"2025-12-23T08:31:36Z","published":"2025-12-23T08:31:36Z","title":"CoDi -- an exemplar-conditioned diffusion model for low-shot counting","summary":"Low-shot object counting addresses estimating the number of previously unobserved objects in an image using only few or no annotated test-time exemplars. A considerable challenge for modern low-shot counters are dense regions with small objects. While total counts in such situations are typically well addressed by density-based counters, their usefulness is limited by poor localization capabilities. This is better addressed by point-detection-based counters, which are based on query-based detectors. However, due to limited number of pre-trained queries, they underperform on images with very large numbers of objects, and resort to ad-hoc techniques like upsampling and tiling. We propose CoDi, the first latent diffusion-based low-shot counter that produces high-quality density maps on which object locations can be determined by non-maxima suppression. Our core contribution is the new exemplar-based conditioning module that extracts and adjusts the object prototypes to the intermediate layers of the denoising network, leading to accurate object location estimation. On FSC benchmark, CoDi outperforms state-of-the-art by 15% MAE, 13% MAE and 10% MAE in the few-shot, one-shot, and reference-less scenarios, respectively, and sets a new state-of-the-art on MCAC benchmark by outperforming the top method by 44% MAE. The code is available at https://github.com/gsustar/CoDi.","authors":["Grega Šuštar","Jer Pelhan","Alan Lukežič","Matej Kristan"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2507.19280v3","updated":"2025-12-23T08:24:14Z","published":"2025-07-25T13:58:11Z","title":"RemoteReasoner: Towards Unifying Geospatial Reasoning Workflow","summary":"Remote sensing imagery presents vast, inherently unstructured spatial data, necessitating sophisticated reasoning to interpret complex user intents and contextual relationships beyond simple recognition tasks. In this paper, we aim to construct an Earth observation workflow to handle complex queries by reasoning about spatial context and user intent. As a reasoning workflow, it should autonomously explore and construct its own inference paths, rather than being confined to predefined ground-truth sequences. Ideally, its architecture ought to be unified yet generalized, possessing capabilities to perform diverse reasoning tasks through one model without requiring additional fine-tuning. Existing remote sensing approaches rely on supervised fine-tuning paradigms and task-specific heads, limiting both autonomous reasoning and unified generalization. To this end, we propose RemoteReasoner, a unified workflow for geospatial reasoning. The design of RemoteReasoner integrates a multi-modal large language model (MLLM) for interpreting user instructions and localizing targets, together with task transformation strategies that enable multi-granularity tasks, including object-, region-, and pixel-level. In contrast to existing methods, our framework is trained with reinforcement learning (RL) to endow the MLLM sufficient reasoning autonomy. At the inference stage, our transformation strategies enable diverse task output formats without requiring task-specific decoders or further fine-tuning. Experiments demonstrated that RemoteReasoner achieves state-of-the-art (SOTA) performance across multi-granularity reasoning tasks. Furthermore, it retains the MLLM's inherent generalization capability, demonstrating robust performance on unseen tasks and out-of-distribution categories.","authors":["Liang Yao","Fan Liu","Hongbo Lu","Chuanyi Zhang","Rui Min","Shengxiang Xu","Shimin Di","Pai Peng"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20148v1","updated":"2025-12-23T08:19:55Z","published":"2025-12-23T08:19:55Z","title":"Enhancing annotations for 5D apple pose estimation through 3D Gaussian Splatting (3DGS)","summary":"Automating tasks in orchards is challenging because of the large amount of variation in the environment and occlusions. One of the challenges is apple pose estimation, where key points, such as the calyx, are often occluded. Recently developed pose estimation methods no longer rely on these key points, but still require them for annotations, making annotating challenging and time-consuming. Due to the abovementioned occlusions, there can be conflicting and missing annotations of the same fruit between different images. Novel 3D reconstruction methods can be used to simplify annotating and enlarge datasets. We propose a novel pipeline consisting of 3D Gaussian Splatting to reconstruct an orchard scene, simplified annotations, automated projection of the annotations to images, and the training and evaluation of a pose estimation method. Using our pipeline, 105 manual annotations were required to obtain 28,191 training labels, a reduction of 99.6%. Experimental results indicated that training with labels of fruits that are $\\leq95\\%$ occluded resulted in the best performance, with a neutral F1 score of 0.927 on the original images and 0.970 on the rendered images. Adjusting the size of the training dataset had small effects on the model performance in terms of F1 score and pose estimation accuracy. It was found that the least occluded fruits had the best position estimation, which worsened as the fruits became more occluded. It was also found that the tested pose estimation method was unable to correctly learn the orientation estimation of apples.","authors":["Robert van de Ven","Trim Bresilla","Bram Nelissen","Ard Nieuwenhuizen","Eldert J. van Henten","Gert Kootstra"],"pdf_url":"","comment":"33 pages, excluding appendices. 17 figures"},{"id":"http://arxiv.org/abs/2512.20145v1","updated":"2025-12-23T08:15:34Z","published":"2025-12-23T08:15:34Z","title":"Retrieval-augmented Prompt Learning for Pre-trained Foundation Models","summary":"The pre-trained foundation models (PFMs) have become essential for facilitating large-scale multimodal learning. Researchers have effectively employed the ``pre-train, prompt, and predict'' paradigm through prompt learning to induce improved few-shot performance. However, prompt learning approaches for PFMs still follow a parametric learning paradigm. As such, the stability of generalization in memorization and rote learning can be compromised. More specifically, conventional prompt learning might face difficulties in fully utilizing atypical instances and avoiding overfitting to shallow patterns with limited data during the process of fully-supervised training. To overcome these constraints, we present our approach, named RetroPrompt, which aims to achieve a balance between memorization and generalization by decoupling knowledge from mere memorization. Unlike traditional prompting methods, RetroPrompt leverages a publicly accessible knowledge base generated from the training data and incorporates a retrieval mechanism throughout the input, training, and inference stages. This enables the model to actively retrieve relevant contextual information from the corpus, thereby enhancing the available cues. We conduct comprehensive experiments on a variety of datasets across natural language processing and computer vision tasks to demonstrate the superior performance of our proposed approach, RetroPrompt, in both zero-shot and few-shot scenarios. Through detailed analysis of memorization patterns, we observe that RetroPrompt effectively reduces the reliance on rote memorization, leading to enhanced generalization.","authors":["Xiang Chen","Yixin Ou","Quan Feng","Lei Li","Piji Li","Haibo Ye","Sheng-Jun Huang","Shuofei Qiao","Shumin Deng","Huajun Chen","Ningyu Zhang"],"pdf_url":"","comment":"IEEE/ACM Transactions on Audio, Speech and Language Processing"},{"id":"http://arxiv.org/abs/2512.17781v2","updated":"2025-12-23T07:59:10Z","published":"2025-12-19T16:50:52Z","title":"LiteGE: Lightweight Geodesic Embedding for Efficient Geodesics Computation and Non-Isometric Shape Correspondence","summary":"Computing geodesic distances on 3D surfaces is fundamental to many tasks in 3D vision and geometry processing, with deep connections to tasks such as shape correspondence. Recent learning-based methods achieve strong performance but rely on large 3D backbones, leading to high memory usage and latency, which limit their use in interactive or resource-constrained settings. We introduce LiteGE, a lightweight approach that constructs compact, category-aware shape descriptors by applying Principal Component Analysis (PCA) to unsigned distance field (UDFs) samples at informative voxels. This descriptor is efficient to compute and removes the need for high-capacity networks. LiteGE remains robust on sparse point clouds, supporting inputs with as few as 300 points, where prior methods fail. Extensive experiments show that LiteGE reduces memory usage and inference time by up to 300$\\times$ compared to existing neural approaches. In addition, by exploiting the intrinsic relationship between geodesic distance and shape correspondence, LiteGE enables fast and accurate shape matching. Our method achieves up to 1000$\\times$ speedup over state-of-the-art mesh-based approaches while maintaining comparable accuracy on non-isometric shape pairs, including evaluations on point-cloud inputs.","authors":["Yohanes Yudhi Adikusuma","Qixing Huang","Ying He"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2504.21850v2","updated":"2025-12-23T07:49:12Z","published":"2025-04-30T17:57:22Z","title":"COMPACT: COMPositional Atomic-to-Complex Visual Capability Tuning","summary":"Visual instruction tuning (VIT) datasets are constructed from randomly sampled image-question pairs, without regard to the informativeness of each pair. Recent dataset selection methods have shown that a small fraction of such datasets enriched with informative samples can lead to efficient finetuning of Multimodal Large Language Models. In this work, we explore the impact of sample complexity on informative data curation and introduce COMPACT (COMPositional Atomic-to-complex Visual Capability Tuning), a VIT data recipe that scales training sample complexity by combining multiple atomic visual capabilities in a single training example. Concretely, we synthesize rich and informative text questions for each image, allowing us to significantly reduce the number of training examples required for effective visual instruction tuning. COMPACT demonstrates superior data efficiency compared to existing data reduction methods. When applied to the LLAVA-665K VIT dataset, COMPACT reduces the data budget by 90% while still achieving 100.2% of the full VIT performance (compared to only 97.5% by the state-of-the-art method) across eight multimodal benchmarks. Further, training on the COMPACT data outperforms training on the full-scale data on particularly complex benchmarks such as MM-Vet (+8.6%) and MMStar (+2.9%). COMPACT offers a scalable and efficient synthetic data generation recipe to improve on visual language tasks.","authors":["Xindi Wu","Hee Seung Hwang","Polina Kirichenko","Esin Tureci","Olga Russakovsky"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20129v1","updated":"2025-12-23T07:43:53Z","published":"2025-12-23T07:43:53Z","title":"Dreamcrafter: Immersive Editing of 3D Radiance Fields Through Flexible, Generative Inputs and Outputs","summary":"Authoring 3D scenes is a central task for spatial computing applications. Competing visions for lowering existing barriers are (1) focus on immersive, direct manipulation of 3D content or (2) leverage AI techniques that capture real scenes (3D Radiance Fields such as, NeRFs, 3D Gaussian Splatting) and modify them at a higher level of abstraction, at the cost of high latency. We unify the complementary strengths of these approaches and investigate how to integrate generative AI advances into real-time, immersive 3D Radiance Field editing. We introduce Dreamcrafter, a VR-based 3D scene editing system that: (1) provides a modular architecture to integrate generative AI algorithms; (2) combines different levels of control for creating objects, including natural language and direct manipulation; and (3) introduces proxy representations that support interaction during high-latency operations. We contribute empirical findings on control preferences and discuss how generative AI interfaces beyond text input enhance creativity in scene editing and world building.","authors":["Cyrus Vachha","Yixiao Kang","Zach Dive","Ashwat Chidambaram","Anik Gupta","Eunice Jun","Bjoern Hartmann"],"pdf_url":"","comment":"CHI 2025, Project page: https://dream-crafter.github.io/"},{"id":"http://arxiv.org/abs/2512.20128v1","updated":"2025-12-23T07:40:25Z","published":"2025-12-23T07:40:25Z","title":"milliMamba: Specular-Aware Human Pose Estimation via Dual mmWave Radar with Multi-Frame Mamba Fusion","summary":"Millimeter-wave radar offers a privacy-preserving and lighting-invariant alternative to RGB sensors for Human Pose Estimation (HPE) task. However, the radar signals are often sparse due to specular reflection, making the extraction of robust features from radar signals highly challenging. To address this, we present milliMamba, a radar-based 2D human pose estimation framework that jointly models spatio-temporal dependencies across both the feature extraction and decoding stages. Specifically, given the high dimensionality of radar inputs, we adopt a Cross-View Fusion Mamba encoder to efficiently extract spatio-temporal features from longer sequences with linear complexity. A Spatio-Temporal-Cross Attention decoder then predicts joint coordinates across multiple frames. Together, this spatio-temporal modeling pipeline enables the model to leverage contextual cues from neighboring frames and joints to infer missing joints caused by specular reflections. To reinforce motion smoothness, we incorporate a velocity loss alongside the standard keypoint loss during training. Experiments on the TransHuPR and HuPR datasets demonstrate that our method achieves significant performance improvements, exceeding the baselines by 11.0 AP and 14.6 AP, respectively, while maintaining reasonable complexity. Code: https://github.com/NYCU-MAPL/milliMamba","authors":["Niraj Prakash Kini","Shiau-Rung Tsai","Guan-Hsun Lin","Wen-Hsiao Peng","Ching-Wen Ma","Jenq-Neng Hwang"],"pdf_url":"","comment":"Accepted at WACV 2026"},{"id":"http://arxiv.org/abs/2512.20120v1","updated":"2025-12-23T07:23:16Z","published":"2025-12-23T07:23:16Z","title":"HEART-VIT: Hessian-Guided Efficient Dynamic Attention and Token Pruning in Vision Transformer","summary":"Vision Transformers (ViTs) deliver state-of-the-art accuracy but their quadratic attention cost and redundant computations severely hinder deployment on latency and resource-constrained platforms. Existing pruning approaches treat either tokens or heads in isolation, relying on heuristics or first-order signals, which often sacrifice accuracy or fail to generalize across inputs. We introduce HEART-ViT, a Hessian-guided efficient dynamic attention and token pruning framework for vision transformers, which to the best of our knowledge is the first unified, second-order, input-adaptive framework for ViT optimization. HEART-ViT estimates curvature-weighted sensitivities of both tokens and attention heads using efficient Hessian-vector products, enabling principled pruning decisions under explicit loss budgets.This dual-view sensitivity reveals an important structural insight: token pruning dominates computational savings, while head pruning provides fine-grained redundancy removal, and their combination achieves a superior trade-off. On ImageNet-100 and ImageNet-1K with ViT-B/16 and DeiT-B/16, HEART-ViT achieves up to 49.4 percent FLOPs reduction, 36 percent lower latency, and 46 percent higher throughput, while consistently matching or even surpassing baseline accuracy after fine-tuning, for example 4.7 percent recovery at 40 percent token pruning. Beyond theoretical benchmarks, we deploy HEART-ViT on different edge devices such as AGX Orin, demonstrating that our reductions in FLOPs and latency translate directly into real-world gains in inference speed and energy efficiency. HEART-ViT bridges the gap between theory and practice, delivering the first unified, curvature-driven pruning framework that is both accuracy-preserving and edge-efficient.","authors":["Mohammad Helal Uddin","Liam Seymour","Sabur Baidya"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.18279v2","updated":"2025-12-23T07:22:18Z","published":"2025-12-20T09:01:18Z","title":"UniMPR: A Unified Framework for Multimodal Place Recognition with Heterogeneous Sensor Configurations","summary":"Place recognition is a critical component of autonomous vehicles and robotics, enabling global localization in GPS-denied environments. Recent advances have spurred significant interest in multimodal place recognition (MPR), which leverages complementary strengths of multiple modalities. Despite its potential, most existing MPR methods still face three key challenges: (1) dynamically adapting to various modality inputs within a unified framework, (2) maintaining robustness with missing or degraded modalities, and (3) generalizing across diverse sensor configurations and setups. In this paper, we propose UniMPR, a unified framework for multimodal place recognition. Using only one trained model, it can seamlessly adapt to any combination of common perceptual modalities (e.g., camera, LiDAR, radar). To tackle the data heterogeneity, we unify all inputs within a polar BEV feature space. Subsequently, the polar BEVs are fed into a multi-branch network to exploit discriminative intra-model and inter-modal features from any modality combinations. To fully exploit the network's generalization capability and robustness, we construct a large-scale training set from multiple datasets and introduce an adaptive label assignment strategy for extensive pre-training. Experiments on seven datasets demonstrate that UniMPR achieves state-of-the-art performance under varying sensor configurations, modality combinations, and environmental conditions. Our code will be released at https://github.com/QiZS-BIT/UniMPR.","authors":["Zhangshuo Qi","Jingyi Xu","Luqi Cheng","Shichen Wen","Yiming Ma","Guangming Xiong"],"pdf_url":"","comment":"14 pages, 9 figures"},{"id":"http://arxiv.org/abs/2512.20117v1","updated":"2025-12-23T07:21:21Z","published":"2025-12-23T07:21:21Z","title":"DDAVS: Disentangled Audio Semantics and Delayed Bidirectional Alignment for Audio-Visual Segmentation","summary":"Audio-Visual Segmentation (AVS) aims to localize sound-producing objects at the pixel level by jointly leveraging auditory and visual information. However, existing methods often suffer from multi-source entanglement and audio-visual misalignment, which lead to biases toward louder or larger objects while overlooking weaker, smaller, or co-occurring sources. To address these challenges, we propose DDAVS, a Disentangled Audio Semantics and Delayed Bidirectional Alignment framework. To mitigate multi-source entanglement, DDAVS employs learnable queries to extract audio semantics and anchor them within a structured semantic space derived from an audio prototype memory bank. This is further optimized through contrastive learning to enhance discriminability and robustness. To alleviate audio-visual misalignment, DDAVS introduces dual cross-attention with delayed modality interaction, improving the robustness of multimodal alignment. Extensive experiments on the AVS-Objects and VPO benchmarks demonstrate that DDAVS consistently outperforms existing approaches, exhibiting strong performance across single-source, multi-source, and multi-instance scenarios. These results validate the effectiveness and generalization ability of our framework under challenging real-world audio-visual segmentation conditions. Project page: https://trilarflagz.github.io/DDAVS-page/","authors":["Jingqi Tian","Yiheng Du","Haoji Zhang","Yuji Wang","Isaac Ning Lee","Xulong Bai","Tianrui Zhu","Jingxuan Niu","Yansong Tang"],"pdf_url":"","comment":"https://trilarflagz.github.io/DDAVS-page/"},{"id":"http://arxiv.org/abs/2512.20113v1","updated":"2025-12-23T07:16:18Z","published":"2025-12-23T07:16:18Z","title":"Multi Modal Attention Networks with Uncertainty Quantification for Automated Concrete Bridge Deck Delamination Detection","summary":"Deteriorating civil infrastructure requires automated inspection techniques overcoming limitations of visual assessment. While Ground Penetrating Radar and Infrared Thermography enable subsurface defect detection, single modal approaches face complementary constraints radar struggles with moisture and shallow defects, while thermography exhibits weather dependency and limited depth. This paper presents a multi modal attention network fusing radar temporal patterns with thermal spatial signatures for bridge deck delamination detection. Our architecture introduces temporal attention for radar processing, spatial attention for thermal features, and cross modal fusion with learnable embeddings discovering complementary defect patterns invisible to individual sensors. We incorporate uncertainty quantification through Monte Carlo dropout and learned variance estimation, decomposing uncertainty into epistemic and aleatoric components for safety critical decisions. Experiments on five bridge datasets reveal that on balanced to moderately imbalanced data, our approach substantially outperforms baselines in accuracy and AUC representing meaningful improvements over single modal and concatenation based fusion. Ablation studies demonstrate cross modal attention provides critical gains beyond within modality attention, while multi head mechanisms achieve improved calibration. Uncertainty quantification reduces calibration error, enabling selective prediction by rejecting uncertain cases. However, under extreme class imbalance, attention mechanisms show vulnerability to majority class collapse. These findings provide actionable guidance: attention based architecture performs well across typical scenarios, while extreme imbalance requires specialized techniques. Our system maintains deployment efficiency, enabling real time inspection with characterized capabilities and limitations.","authors":["Alireza Moayedikia","Sattar Dorafshan"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20107v1","updated":"2025-12-23T07:08:00Z","published":"2025-12-23T07:08:00Z","title":"UMAMI: Unifying Masked Autoregressive Models and Deterministic Rendering for View Synthesis","summary":"Novel view synthesis (NVS) seeks to render photorealistic, 3D-consistent images of a scene from unseen camera poses given only a sparse set of posed views. Existing deterministic networks render observed regions quickly but blur unobserved areas, whereas stochastic diffusion-based methods hallucinate plausible content yet incur heavy training- and inference-time costs. In this paper, we propose a hybrid framework that unifies the strengths of both paradigms. A bidirectional transformer encodes multi-view image tokens and Plucker-ray embeddings, producing a shared latent representation. Two lightweight heads then act on this representation: (i) a feed-forward regression head that renders pixels where geometry is well constrained, and (ii) a masked autoregressive diffusion head that completes occluded or unseen regions. The entire model is trained end-to-end with joint photometric and diffusion losses, without handcrafted 3D inductive biases, enabling scalability across diverse scenes. Experiments demonstrate that our method attains state-of-the-art image quality while reducing rendering time by an order of magnitude compared with fully generative baselines.","authors":["Thanh-Tung Le","Tuan Pham","Tung Nguyen","Deying Kong","Xiaohui Xie","Stephan Mandt"],"pdf_url":"","comment":"Accepted to NeurIPS 2025. The first two authors contributed equally"},{"id":"http://arxiv.org/abs/2512.20105v1","updated":"2025-12-23T07:03:31Z","published":"2025-12-23T07:03:31Z","title":"LiDARDraft: Generating LiDAR Point Cloud from Versatile Inputs","summary":"Generating realistic and diverse LiDAR point clouds is crucial for autonomous driving simulation. Although previous methods achieve LiDAR point cloud generation from user inputs, they struggle to attain high-quality results while enabling versatile controllability, due to the imbalance between the complex distribution of LiDAR point clouds and the simple control signals. To address the limitation, we propose LiDARDraft, which utilizes the 3D layout to build a bridge between versatile conditional signals and LiDAR point clouds. The 3D layout can be trivially generated from various user inputs such as textual descriptions and images. Specifically, we represent text, images, and point clouds as unified 3D layouts, which are further transformed into semantic and depth control signals. Then, we employ a rangemap-based ControlNet to guide LiDAR point cloud generation. This pixel-level alignment approach demonstrates excellent performance in controllable LiDAR point clouds generation, enabling \"simulation from scratch\", allowing self-driving environments to be created from arbitrary textual descriptions, images and sketches.","authors":["Haiyun Wei","Fan Lu","Yunwei Zhu","Zehan Zheng","Weiyi Xue","Lin Shao","Xudong Zhang","Ya Wu","Rong Fu","Guang Chen"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2508.20615v2","updated":"2025-12-23T07:03:05Z","published":"2025-08-28T10:02:06Z","title":"EmoCAST: Emotional Talking Portrait via Emotive Text Description","summary":"Emotional talking head synthesis aims to generate talking portrait videos with vivid expressions. Existing methods still exhibit limitations in control flexibility, motion naturalness, and expression quality. Moreover, currently available datasets are mainly collected in lab settings, further exacerbating these shortcomings and hindering real-world deployment. To address these challenges, we propose EmoCAST, a diffusion-based talking head framework for precise, text-driven emotional synthesis. Its contributions are threefold: (1) architectural modules that enable effective text control; (2) an emotional talking-head dataset that expands the framework's ability; and (3) training strategies that further improve performance. Specifically, for appearance modeling, emotional prompts are integrated through a text-guided emotive attention module, enhancing spatial knowledge to improve emotion understanding. To strengthen audio-emotion alignment, we introduce an emotive audio attention module to capture the interplay between controlled emotion and driving audio, generating emotion-aware features to guide precise facial motion synthesis. Additionally, we construct a large-scale, in-the-wild emotional talking head dataset with emotive text descriptions to optimize the framework's performance. Based on this dataset, we propose an emotion-aware sampling strategy and a progressive functional training strategy that improve the model's ability to capture nuanced expressive features and achieve accurate lip-sync. Overall, EmoCAST achieves state-of-the-art performance in generating realistic, emotionally expressive, and audio-synchronized talking-head videos. Project Page: https://github.com/GVCLab/EmoCAST","authors":["Yiguo Jiang","Xiaodong Cun","Yong Zhang","Yudian Zheng","Fan Tang","Chi-Man Pun"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20104v1","updated":"2025-12-23T07:01:45Z","published":"2025-12-23T07:01:45Z","title":"Effect of Activation Function and Model Optimizer on the Performance of Human Activity Recognition System Using Various Deep Learning Models","summary":"Human Activity Recognition (HAR) plays a vital role in healthcare, surveillance, and innovative environments, where reliable action recognition supports timely decision-making and automation. Although deep learning-based HAR systems are widely adopted, the impact of Activation Functions (AFs) and Model Optimizers (MOs) on performance has not been sufficiently analyzed, particularly regarding how their combinations influence model behavior in practical scenarios. Most existing studies focus on architecture design, while the interaction between AF and MO choices remains relatively unexplored. In this work, we investigate the effect of three commonly used activation functions (ReLU, Sigmoid, and Tanh) combined with four optimization algorithms (SGD, Adam, RMSprop, and Adagrad) using two recurrent deep learning architectures, namely BiLSTM and ConvLSTM. Experiments are conducted on six medically relevant activity classes selected from the HMDB51 and UCF101 datasets, considering their suitability for healthcare-oriented HAR applications. Our experimental results show that ConvLSTM consistently outperforms BiLSTM across both datasets. ConvLSTM, combined with Adam or RMSprop, achieves an accuracy of up to 99.00%, demonstrating strong spatio-temporal learning capabilities and stable performance. While BiLSTM performs reasonably well on UCF101, with accuracy approaching 98.00%, its performance drops to approximately 60.00% on HMDB51, indicating limited robustness across datasets and weaker sensitivity to AF and MO variations. This study provides practical insights for optimizing HAR systems, particularly for real-world healthcare environments where fast and precise activity detection is critical.","authors":["Subrata Kumer Paula","Dewan Nafiul Islam Noora","Rakhi Rani Paula","Md. Ekramul Hamidb","Fahmid Al Faridc","Hezerul Abdul Karimd","Md. Maruf Al Hossain Princee","Abu Saleh Musa Miahb"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20088v1","updated":"2025-12-23T06:30:33Z","published":"2025-12-23T06:30:33Z","title":"Item Region-based Style Classification Network (IRSN): A Fashion Style Classifier Based on Domain Knowledge of Fashion Experts","summary":"Fashion style classification is a challenging task because of the large visual variation within the same style and the existence of visually similar styles.\n  Styles are expressed not only by the global appearance, but also by the attributes of individual items and their combinations.\n  In this study, we propose an item region-based fashion style classification network (IRSN) to effectively classify fashion styles by analyzing item-specific features and their combinations in addition to global features.\n  IRSN extracts features of each item region using item region pooling (IRP), analyzes them separately, and combines them using gated feature fusion (GFF).\n  In addition, we improve the feature extractor by applying a dual-backbone architecture that combines a domain-specific feature extractor and a general feature extractor pre-trained with a large-scale image-text dataset.\n  In experiments, applying IRSN to six widely-used backbones, including EfficientNet, ConvNeXt, and Swin Transformer, improved style classification accuracy by an average of 6.9% and a maximum of 14.5% on the FashionStyle14 dataset and by an average of 7.6% and a maximum of 15.1% on the ShowniqV3 dataset. Visualization analysis also supports that the IRSN models are better than the baseline models at capturing differences between similar style classes.","authors":["Jinyoung Choi","Youngchae Kwon","Injung Kim"],"pdf_url":"","comment":"This is a pre-print of an article published in Applied Intelligence. The final authenticated version is available online at: https://doi.org/10.1007/s10489-024-05683-9"},{"id":"http://arxiv.org/abs/2505.08528v2","updated":"2025-12-23T06:18:57Z","published":"2025-05-13T13:01:38Z","title":"GradMix: Gradient-based Selective Mixup for Robust Data Augmentation in Class-Incremental Learning","summary":"In the context of continual learning, acquiring new knowledge while maintaining previous knowledge presents a significant challenge. Existing methods often use experience replay techniques that store a small portion of previous task data for training. In experience replay approaches, data augmentation has emerged as a promising strategy to further improve the model performance by mixing limited previous task data with sufficient current task data. However, we theoretically and empirically analyze that training with mixed samples from random sample pairs may harm the knowledge of previous tasks and cause greater catastrophic forgetting. We then propose GradMix, a robust data augmentation method specifically designed for mitigating catastrophic forgetting in class-incremental learning. GradMix performs gradient-based selective mixup using a class-based criterion that mixes only samples from helpful class pairs and not from detrimental class pairs for reducing catastrophic forgetting. Our experiments on various real datasets show that GradMix outperforms data augmentation baselines in accuracy by minimizing the forgetting of previous knowledge.","authors":["Minsu Kim","Seong-Hyeon Hwang","Steven Euijong Whang"],"pdf_url":"","comment":"Accepted to KDD 2026"},{"id":"http://arxiv.org/abs/2501.16222v3","updated":"2025-12-23T06:10:31Z","published":"2025-01-27T17:13:03Z","title":"SPECIAL: Zero-shot Hyperspectral Image Classification With CLIP","summary":"Hyperspectral image (HSI) classification aims to categorize each pixel in an HSI into a specific land cover class, which is crucial for applications such as remote sensing, environmental monitoring, and agriculture. Although deep learning-based HSI classification methods have achieved significant advancements, existing methods still rely on manually labeled data for training, which is both time-consuming and labor-intensive. To address this limitation, we introduce a novel zero-shot hyperspectral image classification framework based on CLIP (SPECIAL), aiming to eliminate the need for manual annotations. The SPECIAL framework consists of two main stages: (1) CLIP-based pseudo-label generation, and (2) noisy label learning. In the first stage, HSI is spectrally interpolated to produce RGB bands. These bands are subsequently classified using CLIP, resulting in noisy pseudo-labels that are accompanied by confidence scores. To improve the quality of these labels, we propose a scaling strategy that fuses predictions from multiple spatial scales. In the second stage, spectral information and a label refinement technique are incorporated to mitigate label noise and further enhance classification accuracy. Experimental results on three benchmark datasets demonstrate that our SPECIAL outperforms existing methods in zero-shot HSI classification, showing its potential for more practical applications. The code is available at https://github.com/LiPang/SPECIAL.","authors":["Li Pang","Jing Yao","Kaiyu Li","Jun Zhou","Deyu Meng","Xiangyong Cao"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2508.01741v2","updated":"2025-12-23T05:52:33Z","published":"2025-08-03T12:51:47Z","title":"Simulated Ensemble Attack: Transferring Jailbreaks Across Fine-tuned Vision-Language Models","summary":"Fine-tuning open-source Vision-Language Models (VLMs) creates a critical yet underexplored attack surface: vulnerabilities in the base VLM could be retained in fine-tuned variants, rendering them susceptible to transferable jailbreak attacks. To demonstrate this risk, we introduce the Simulated Ensemble Attack (SEA), a novel grey-box jailbreak method in which the adversary has full access to the base VLM but no knowledge of the fine-tuned target's weights or training configuration. To improve jailbreak transferability across fine-tuned VLMs, SEA combines two key techniques: Fine-tuning Trajectory Simulation (FTS) and Targeted Prompt Guidance (TPG). FTS generates transferable adversarial images by simulating the vision encoder's parameter shifts, while TPG is a textual strategy that steers the language decoder toward adversarially optimized outputs. Experiments on the Qwen2-VL family (2B and 7B) demonstrate that SEA achieves high transfer attack success rates exceeding 86.5% and toxicity rates near 49.5% across diverse fine-tuned variants, even those specifically fine-tuned to improve safety behaviors. Notably, while direct PGD-based image jailbreaks rarely transfer across fine-tuned VLMs, SEA reliably exploits inherited vulnerabilities from the base model, significantly enhancing transferability. These findings highlight an urgent need to safeguard fine-tuned proprietary VLMs against transferable vulnerabilities inherited from open-source foundations, motivating the development of holistic defenses across the entire model lifecycle.","authors":["Ruofan Wang","Xin Wang","Yang Yao","Xuan Tong","Xingjun Ma"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2510.04944v2","updated":"2025-12-23T05:51:37Z","published":"2025-10-06T15:46:50Z","title":"On Structured State-Space Duality","summary":"Structured State-Space Duality (SSD) [Dao & Gu, ICML 2024] is an equivalence between a simple Structured State-Space Model (SSM) and a masked attention mechanism. In particular, a state-space model with a scalar-times-identity state matrix is equivalent to a masked self-attention with a $1$-semiseparable causal mask. Consequently, the same sequence transformation (model) has two algorithmic realizations: as a linear-time $O(T)$ recurrence or as a quadratic-time $O(T^2)$ attention. In this note, we formalize and generalize this duality: (i) we extend SSD from the scalar-identity case to general diagonal SSMs (diagonal state matrices); (ii) we show that these diagonal SSMs match the scalar case's training complexity lower bounds while supporting richer dynamics; (iii) we establish a necessary and sufficient condition under which an SSM is equivalent to $1$-semiseparable masked attention; and (iv) we show that such duality fails to extend to standard softmax attention due to rank explosion. Together, these results tighten bridge between recurrent SSMs and Transformers, and widen the design space for expressive yet efficient sequence models.","authors":["Jerry Yao-Chieh Hu","Xiwen Zhang","Ali ElSheikh","Weimin Wu","Han Liu"],"pdf_url":"","comment":"v2 fixed typos and added numerical results (Appendix B)"},{"id":"http://arxiv.org/abs/2512.20070v1","updated":"2025-12-23T05:45:38Z","published":"2025-12-23T05:45:38Z","title":"Progressive Learned Image Compression for Machine Perception","summary":"Recent advances in learned image codecs have been extended from human perception toward machine perception. However, progressive image compression with fine granular scalability (FGS)-which enables decoding a single bitstream at multiple quality levels-remains unexplored for machine-oriented codecs. In this work, we propose a novel progressive learned image compression codec for machine perception, PICM-Net, based on trit-plane coding. By analyzing the difference between human- and machine-oriented rate-distortion priorities, we systematically examine the latent prioritization strategies in terms of machine-oriented codecs. To further enhance real-world adaptability, we design an adaptive decoding controller, which dynamically determines the necessary decoding level during inference time to maintain the desired confidence of downstream machine prediction. Extensive experiments demonstrate that our approach enables efficient and adaptive progressive transmission while maintaining high performance in the downstream classification task, establishing a new paradigm for machine-aware progressive image compression.","authors":["Jungwoo Kim","Jun-Hyuk Kim","Jong-Seok Lee"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19629v2","updated":"2025-12-23T05:37:16Z","published":"2025-12-22T18:03:08Z","title":"LoGoPlanner: Localization Grounded Navigation Policy with Metric-aware Visual Geometry","summary":"Trajectory planning in unstructured environments is a fundamental and challenging capability for mobile robots. Traditional modular pipelines suffer from latency and cascading errors across perception, localization, mapping, and planning modules. Recent end-to-end learning methods map raw visual observations directly to control signals or trajectories, promising greater performance and efficiency in open-world settings. However, most prior end-to-end approaches still rely on separate localization modules that depend on accurate sensor extrinsic calibration for self-state estimation, thereby limiting generalization across embodiments and environments. We introduce LoGoPlanner, a localization-grounded, end-to-end navigation framework that addresses these limitations by: (1) finetuning a long-horizon visual-geometry backbone to ground predictions with absolute metric scale, thereby providing implicit state estimation for accurate localization; (2) reconstructing surrounding scene geometry from historical observations to supply dense, fine-grained environmental awareness for reliable obstacle avoidance; and (3) conditioning the policy on implicit geometry bootstrapped by the aforementioned auxiliary tasks, thereby reducing error propagation. We evaluate LoGoPlanner in both simulation and real-world settings, where its fully end-to-end design reduces cumulative error while metric-aware geometry memory enhances planning consistency and obstacle avoidance, leading to more than a 27.3\\% improvement over oracle-localization baselines and strong generalization across embodiments and environments. The code and models have been made publicly available on the https://steinate.github.io/logoplanner.github.io.","authors":["Jiaqi Peng","Wenzhe Cai","Yuqiang Yang","Tai Wang","Yuan Shen","Jiangmiao Pang"],"pdf_url":"","comment":"Project page:https://steinate.github.io/logoplanner.github.io/"},{"id":"http://arxiv.org/abs/2512.20056v1","updated":"2025-12-23T05:14:01Z","published":"2025-12-23T05:14:01Z","title":"Towards Generative Location Awareness for Disaster Response: A Probabilistic Cross-view Geolocalization Approach","summary":"As Earth's climate changes, it is impacting disasters and extreme weather events across the planet. Record-breaking heat waves, drenching rainfalls, extreme wildfires, and widespread flooding during hurricanes are all becoming more frequent and more intense. Rapid and efficient response to disaster events is essential for climate resilience and sustainability. A key challenge in disaster response is to accurately and quickly identify disaster locations to support decision-making and resources allocation. In this paper, we propose a Probabilistic Cross-view Geolocalization approach, called ProbGLC, exploring new pathways towards generative location awareness for rapid disaster response. Herein, we combine probabilistic and deterministic geolocalization models into a unified framework to simultaneously enhance model explainability (via uncertainty quantification) and achieve state-of-the-art geolocalization performance. Designed for rapid diaster response, the ProbGLC is able to address cross-view geolocalization across multiple disaster events as well as to offer unique features of probabilistic distribution and localizability score. To evaluate the ProbGLC, we conduct extensive experiments on two cross-view disaster datasets (i.e., MultiIAN and SAGAINDisaster), consisting diverse cross-view imagery pairs of multiple disaster types (e.g., hurricanes, wildfires, floods, to tornadoes). Preliminary results confirms the superior geolocalization accuracy (i.e., 0.86 in Acc@1km and 0.97 in Acc@25km) and model explainability (i.e., via probabilistic distributions and localizability scores) of the proposed ProbGLC approach, highlighting the great potential of leveraging generative cross-view approach to facilitate location awareness for better and faster disaster response. The data and code is publicly available at https://github.com/bobleegogogo/ProbGLC","authors":["Hao Li","Fabian Deuser","Wenping Yin","Steffen Knoblauch","Wufan Zhao","Filip Biljecki","Yong Xue","Wei Huang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.18200v2","updated":"2025-12-23T04:54:13Z","published":"2025-12-20T03:48:05Z","title":"SLIM: Semantic-based Low-bitrate Image compression for Machines by leveraging diffusion","summary":"In recent years, the demand of image compression models for machine vision has increased dramatically. However, the training frameworks of image compression still focus on the vision of human, maintaining the excessive perceptual details, thus have limitations in optimally reducing the bits per pixel in the case of performing machine vision tasks. In this paper, we propose Semantic-based Low-bitrate Image compression for Machines by leveraging diffusion, termed SLIM. This is a new effective training framework of image compression for machine vision, using a pretrained latent diffusion model.The compressor model of our method focuses only on the Region-of-Interest (RoI) areas for machine vision in the image latent, to compress it compactly. Then the pretrained Unet model enhances the decompressed latent, utilizing a RoI-focused text caption which containing semantic information of the image. Therefore, SLIM is able to focus on RoI areas of the image without any guide mask at the inference stage, achieving low bitrate when compressing. And SLIM is also able to enhance a decompressed latent by denoising steps, so the final reconstructed image from the enhanced latent can be optimized for the machine vision task while still containing perceptual details for human vision. Experimental results show that SLIM achieves a higher classification accuracy in the same bits per pixel condition, compared to conventional image compression models for machines.","authors":["Hyeonjin Lee","Jun-Hyuk Kim","Jong-Seok Lee"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20042v1","updated":"2025-12-23T04:21:15Z","published":"2025-12-23T04:21:15Z","title":"Beyond Vision: Contextually Enriched Image Captioning with Multi-Modal Retrieva","summary":"Real-world image captions often lack contextual depth, omitting crucial details such as event background, temporal cues, outcomes, and named entities that are not visually discernible. This gap limits the effectiveness of image understanding in domains like journalism, education, and digital archives, where richer, more informative descriptions are essential. To address this, we propose a multimodal pipeline that augments visual input with external textual knowledge. Our system retrieves semantically similar images using BEIT-3 (Flickr30k-384 and COCO-384) and SigLIP So-384, reranks them using ORB and SIFT for geometric alignment, and extracts contextual information from related articles via semantic search. A fine-tuned Qwen3 model with QLoRA then integrates this context with base captions generated by Instruct BLIP (Vicuna-7B) to produce event-enriched, context-aware descriptions. Evaluated on the OpenEvents v1 dataset, our approach generates significantly more informative captions compared to traditional methods, showing strong potential for real-world applications requiring deeper visual-textual understanding","authors":["Nguyen Lam Phu Quy","Pham Phu Hoa","Tran Chi Nguyen","Dao Sy Duy Minh","Nguyen Hoang Minh Ngoc","Huynh Trung Kiet"],"pdf_url":"","comment":"7 pages, 5 figures. System description for the EVENTA Grand Challenge (Track 1) at ACM MM'25"},{"id":"http://arxiv.org/abs/2504.09472v2","updated":"2025-12-23T04:09:07Z","published":"2025-04-13T08:04:11Z","title":"I Want It That Way! Specifying Nuanced Camera Motions in Video Editing","summary":"Specifying nuanced and compelling camera motion remains a major hurdle for non-expert creators using generative tools, creating an ``expressive gap\" where generic text prompts fail to capture cinematic vision. To address this, we present a novel zero-shot diffusion-based system that enables personalized camera motion transfer from a single reference video onto a user-provided static image. Our technical contribution introduces an intuitive interaction paradigm that bypasses the need for 3D data, predefined trajectories, or complex graphical interfaces. The core pipeline leverages a text-to-video diffusion model, employing a two-phase strategy: 1) a multi-concept learning method using LoRA layers and an orthogonality loss to distinctly capture spatial-temporal characteristics and scene features, and 2) a homography-based refinement strategy to enhance temporal and spatial alignment of the generated video. Extensive evaluation demonstrates the efficacy of our method. In a comparative study with 72 participants, our system was significantly preferred over prior work for both motion accuracy (90.45\\%) and scene preservation (70.31\\%). A second study confirmed our interface significantly improves usability and creative control for video direction. Our work contributes a robust technical solution and a novel human-centered design, significantly expanding cinematic video editing for diverse users.","authors":["Pooja Guhan","Divya Kothandaraman","Geonsun Lee","Tsung-Wei Huang","Guan-Ming Su","Dinesh Manocha"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20033v1","updated":"2025-12-23T03:54:48Z","published":"2025-12-23T03:54:48Z","title":"FlashLips: 100-FPS Mask-Free Latent Lip-Sync using Reconstruction Instead of Diffusion or GANs","summary":"We present FlashLips, a two-stage, mask-free lip-sync system that decouples lips control from rendering and achieves real-time performance running at over 100 FPS on a single GPU, while matching the visual quality of larger state-of-the-art models. Stage 1 is a compact, one-step latent-space editor that reconstructs an image using a reference identity, a masked target frame, and a low-dimensional lips-pose vector, trained purely with reconstruction losses - no GANs or diffusion. To remove explicit masks at inference, we use self-supervision: we generate mouth-altered variants of the target image, that serve as pseudo ground truth for fine-tuning, teaching the network to localize edits to the lips while preserving the rest. Stage 2 is an audio-to-pose transformer trained with a flow-matching objective to predict lips-poses vectors from speech. Together, these stages form a simple and stable pipeline that combines deterministic reconstruction with robust audio control, delivering high perceptual quality and faster-than-real-time speed.","authors":["Andreas Zinonos","Michał Stypułkowski","Antoni Bigata","Stavros Petridis","Maja Pantic","Nikita Drobyshev"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20032v1","updated":"2025-12-23T03:52:29Z","published":"2025-12-23T03:52:29Z","title":"VALLR-Pin: Dual-Decoding Visual Speech Recognition for Mandarin with Pinyin-Guided LLM Refinement","summary":"Visual Speech Recognition aims to transcribe spoken words from silent lip-motion videos. This task is particularly challenging for Mandarin, as visemes are highly ambiguous and homophones are prevalent. We propose VALLR-Pin, a novel two-stage framework that extends the recent VALLR architecture from English to Mandarin. First, a shared video encoder feeds into dual decoders, which jointly predict both Chinese character sequences and their standard Pinyin romanization. The multi-task learning of character and phonetic outputs fosters robust visual-semantic representations. During inference, the text decoder generates multiple candidate transcripts. We construct a prompt by concatenating the Pinyin output with these candidate Chinese sequences and feed it to a large language model to resolve ambiguities and refine the transcription. This provides the LLM with explicit phonetic context to correct homophone-induced errors. Finally, we fine-tune the LLM on synthetic noisy examples: we generate imperfect Pinyin-text pairs from intermediate VALLR-Pin checkpoints using the training data, creating instruction-response pairs for error correction. This endows the LLM with awareness of our model's specific error patterns. In summary, VALLR-Pin synergizes visual features with phonetic and linguistic context to improve Mandarin lip-reading performance.","authors":["Chang Sun","Dongliang Xie","Bo Qin","Hong Yang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20029v1","updated":"2025-12-23T03:46:04Z","published":"2025-12-23T03:46:04Z","title":"$\\text{H}^2$em: Learning Hierarchical Hyperbolic Embeddings for Compositional Zero-Shot Learning","summary":"Compositional zero-shot learning (CZSL) aims to recognize unseen state-object compositions by generalizing from a training set of their primitives (state and object). Current methods often overlook the rich hierarchical structures, such as the semantic hierarchy of primitives (e.g., apple fruit) and the conceptual hierarchy between primitives and compositions (e.g, sliced apple apple). A few recent efforts have shown effectiveness in modeling these hierarchies through loss regularization within Euclidean space. In this paper, we argue that they fail to scale to the large-scale taxonomies required for real-world CZSL: the space's polynomial volume growth in flat geometry cannot match the exponential structure, impairing generalization capacity. To this end, we propose H2em, a new framework that learns Hierarchical Hyperbolic EMbeddings for CZSL. H2em leverages the unique properties of hyperbolic geometry, a space naturally suited for embedding tree-like structures with low distortion. However, a naive hyperbolic mapping may suffer from hierarchical collapse and poor fine-grained discrimination. We further design two learning objectives to structure this space: a Dual-Hierarchical Entailment Loss that uses hyperbolic entailment cones to enforce the predefined hierarchies, and a Discriminative Alignment Loss with hard negative mining to establish a large geodesic distance between semantically similar compositions. Furthermore, we devise Hyperbolic Cross-Modal Attention to realize instance-aware cross-modal infusion within hyperbolic geometry. Extensive ablations on three benchmarks demonstrate that H2em establishes a new state-of-the-art in both closed-world and open-world scenarios. Our codes will be released.","authors":["Lin Li","Jiahui Li","Jiaming Lei","Jun Xiao","Feifei Shao","Long Chen"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20026v1","updated":"2025-12-23T03:38:57Z","published":"2025-12-23T03:38:57Z","title":"MAPI-GNN: Multi-Activation Plane Interaction Graph Neural Network for Multimodal Medical Diagnosis","summary":"Graph neural networks are increasingly applied to multimodal medical diagnosis for their inherent relational modeling capabilities. However, their efficacy is often compromised by the prevailing reliance on a single, static graph built from indiscriminate features, hindering the ability to model patient-specific pathological relationships. To this end, the proposed Multi-Activation Plane Interaction Graph Neural Network (MAPI-GNN) reconstructs this single-graph paradigm by learning a multifaceted graph profile from semantically disentangled feature subspaces. The framework first uncovers latent graph-aware patterns via a multi-dimensional discriminator; these patterns then guide the dynamic construction of a stack of activation graphs; and this multifaceted profile is finally aggregated and contextualized by a relational fusion engine for a robust diagnosis. Extensive experiments on two diverse tasks, comprising over 1300 patient samples, demonstrate that MAPI-GNN significantly outperforms state-of-the-art methods.","authors":["Ziwei Qin","Xuhui Song","Deqing Huang","Na Qin","Jun Li"],"pdf_url":"","comment":"Accepted by Proceedings of the AAAI Conference on Artificial Intelligence 40 (AAAI-26)"},{"id":"http://arxiv.org/abs/2512.20025v1","updated":"2025-12-23T03:36:26Z","published":"2025-12-23T03:36:26Z","title":"A Contextual Analysis of Driver-Facing and Dual-View Video Inputs for Distraction Detection in Naturalistic Driving Environments","summary":"Despite increasing interest in computer vision-based distracted driving detection, most existing models rely exclusively on driver-facing views and overlook crucial environmental context that influences driving behavior. This study investigates whether incorporating road-facing views alongside driver-facing footage improves distraction detection accuracy in naturalistic driving conditions. Using synchronized dual-camera recordings from real-world driving, we benchmark three leading spatiotemporal action recognition architectures: SlowFast-R50, X3D-M, and SlowOnly-R50. Each model is evaluated under two input configurations: driver-only and stacked dual-view. Results show that while contextual inputs can improve detection in certain models, performance gains depend strongly on the underlying architecture. The single-pathway SlowOnly model achieved a 9.8 percent improvement with dual-view inputs, while the dual-pathway SlowFast model experienced a 7.2 percent drop in accuracy due to representational conflicts. These findings suggest that simply adding visual context is not sufficient and may lead to interference unless the architecture is specifically designed to support multi-view integration. This study presents one of the first systematic comparisons of single- and dual-view distraction detection models using naturalistic driving data and underscores the importance of fusion-aware design for future multimodal driver monitoring systems.","authors":["Anthony Dontoh","Stephanie Ivey","Armstrong Aboah"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2503.11006v2","updated":"2025-12-23T03:28:48Z","published":"2025-03-14T02:05:16Z","title":"Fine-Grained Instruction-Guided Graph Reasoning for Vision-and-Language Navigation","summary":"Vision-and-Language Navigation (VLN) requires an embodied agent to traverse complex environments by following natural language instructions, demanding accurate alignment between visual observations and linguistic guidance. Despite recent progress, existing methods typically encode visual and directional cues in a coupled manner, and process instructions without explicitly extracting navigation-critical semantics, which often leads to imprecise spatial reasoning and suboptimal cross-modal alignment. To address these challenges, we propose a fine-grained instruction-guided graph reasoning framework (OIKG) that enhances both spatial representation and instruction understanding during navigation. Specifically, an observation-graph interaction mechanism is introduced to disentangle angular and visual cues while strengthening directed edge representations through geometric embedding, enabling more reliable spatial reasoning within the navigation graph. In addition, a fine-grained instruction guidance module is designed to explicitly extract and leverage location-specific and object-centric information from language instructions, facilitating more precise cross-modal alignment between linguistic semantics and navigable trajectories. By jointly integrating structured graph reasoning with instruction-critical semantic cues, the proposed approach significantly improves the agent's ability to follow complex navigation instructions. Extensive experiments on the R2R and RxR benchmarks demonstrate that our method consistently achieves state-of-the-art performance across multiple evaluation metrics, validating the effectiveness of fine-grained instruction-guided graph reasoning for vision-and-language navigation.","authors":["Yaohua Liu","Xinyuan Song","Yunfu Deng","Yifan Xie","Binkai Ou","Yan Zhong"],"pdf_url":"","comment":"10 pages, 4 figures"},{"id":"http://arxiv.org/abs/2512.20013v1","updated":"2025-12-23T03:10:17Z","published":"2025-12-23T03:10:17Z","title":"SegEarth-R2: Towards Comprehensive Language-guided Segmentation for Remote Sensing Images","summary":"Effectively grounding complex language to pixels in remote sensing (RS) images is a critical challenge for applications like disaster response and environmental monitoring. Current models can parse simple, single-target commands but fail when presented with complex geospatial scenarios, e.g., segmenting objects at various granularities, executing multi-target instructions, and interpreting implicit user intent. To drive progress against these failures, we present LaSeRS, the first large-scale dataset built for comprehensive training and evaluation across four critical dimensions of language-guided segmentation: hierarchical granularity, target multiplicity, reasoning requirements, and linguistic variability. By capturing these dimensions, LaSeRS moves beyond simple commands, providing a benchmark for complex geospatial reasoning. This addresses a critical gap: existing datasets oversimplify, leading to sensitivity-prone real-world models. We also propose SegEarth-R2, an MLLM architecture designed for comprehensive language-guided segmentation in RS, which directly confronts these challenges. The model's effectiveness stems from two key improvements: (1) a spatial attention supervision mechanism specifically handles the localization of small objects and their components, and (2) a flexible and efficient segmentation query mechanism that handles both single-target and multi-target scenarios. Experimental results demonstrate that our SegEarth-R2 achieves outstanding performance on LaSeRS and other benchmarks, establishing a powerful baseline for the next generation of geospatial segmentation. All data and code will be released at https://github.com/earth-insights/SegEarth-R2.","authors":["Zepeng Xin","Kaiyu Li","Luodi Chen","Wanchen Li","Yuchen Xiao","Hui Qiao","Weizhan Zhang","Deyu Meng","Xiangyong Cao"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20011v1","updated":"2025-12-23T03:09:49Z","published":"2025-12-23T03:09:49Z","title":"PaveSync: A Unified and Comprehensive Dataset for Pavement Distress Analysis and Classification","summary":"Automated pavement defect detection often struggles to generalize across diverse real-world conditions due to the lack of standardized datasets. Existing datasets differ in annotation styles, distress type definitions, and formats, limiting their integration for unified training. To address this gap, we introduce a comprehensive benchmark dataset that consolidates multiple publicly available sources into a standardized collection of 52747 images from seven countries, with 135277 bounding box annotations covering 13 distinct distress types. The dataset captures broad real-world variation in image quality, resolution, viewing angles, and weather conditions, offering a unique resource for consistent training and evaluation. Its effectiveness was demonstrated through benchmarking with state-of-the-art object detection models including YOLOv8-YOLOv12, Faster R-CNN, and DETR, which achieved competitive performance across diverse scenarios. By standardizing class definitions and annotation formats, this dataset provides the first globally representative benchmark for pavement defect detection and enables fair comparison of models, including zero-shot transfer to new environments.","authors":["Blessing Agyei Kyem","Joshua Kofi Asamoah","Anthony Dontoh","Andrews Danyo","Eugene Denteh","Armstrong Aboah"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.15649v2","updated":"2025-12-23T03:03:58Z","published":"2025-12-17T17:58:35Z","title":"VTCBench: Can Vision-Language Models Understand Long Context with Vision-Text Compression?","summary":"The computational and memory overheads associated with expanding the context window of LLMs severely limit their scalability. A noteworthy solution is vision-text compression (VTC), exemplified by frameworks like DeepSeek-OCR and Glyph, which convert long texts into dense 2D visual representations, thereby achieving token compression ratios of 3x-20x. However, the impact of this high information density on the core long-context capabilities of vision-language models (VLMs) remains under-investigated. To address this gap, we introduce the first benchmark for VTC and systematically assess the performance of VLMs across three long-context understanding settings: VTC-Retrieval, which evaluates the model's ability to retrieve and aggregate information; VTC-Reasoning, which requires models to infer latent associations to locate facts with minimal lexical overlap; and VTC-Memory, which measures comprehensive question answering within long-term dialogue memory. Furthermore, we establish the VTCBench-Wild to simulate diverse input scenarios.We comprehensively evaluate leading open-source and proprietary models on our benchmarks. The results indicate that, despite being able to decode textual information (e.g., OCR) well, most VLMs exhibit a surprisingly poor long-context understanding ability with VTC-processed information, failing to capture long associations or dependencies in the context.This study provides a deep understanding of VTC and serves as a foundation for designing more efficient and scalable VLMs.","authors":["Hongbo Zhao","Meng Wang","Fei Zhu","Wenzhuo Liu","Bolin Ni","Fanhu Zeng","Gaofeng Meng","Zhaoxiang Zhang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2501.11340v2","updated":"2025-12-23T02:57:06Z","published":"2025-01-20T08:58:56Z","title":"GenVidBench: A 6-Million Benchmark for AI-Generated Video Detection","summary":"The rapid advancement of video generation models has made it increasingly challenging to distinguish AI-generated videos from real ones. This issue underscores the urgent need for effective AI-generated video detectors to prevent the dissemination of false information via such videos. However, the development of high-performance AI-generated video detectors is currently impeded by the lack of large-scale, high-quality datasets specifically designed for generative video detection. To this end, we introduce GenVidBench, a challenging AI-generated video detection dataset with several key advantages: 1) Large-scale video collection: The dataset contains 6.78 million videos and is currently the largest dataset for AI-generated video detection. 2) Cross-Source and Cross-Generator: The cross-source generation reduces the interference of video content on the detection. The cross-generator ensures diversity in video attributes between the training and test sets, preventing them from being overly similar. 3) State-of-the-Art Video Generators: The dataset includes videos from 11 state-of-the-art AI video generators, ensuring that it covers the latest advancements in the field of video generation. These generators ensure that the datasets are not only large in scale but also diverse, aiding in the development of generalized and effective detection models. Additionally, we present extensive experimental results with advanced video classification models. With GenVidBench, researchers can efficiently develop and evaluate AI-generated video detection models.. Datasets and code are available at https://genvidbench.github.io.","authors":["Zhenliang Ni","Qiangyu Yan","Mouxiao Huang","Tianning Yuan","Yehui Tang","Hailin Hu","Xinghao Chen","Yunhe Wang"],"pdf_url":"","comment":"AAAI 2026"},{"id":"http://arxiv.org/abs/2507.08268v2","updated":"2025-12-23T02:53:23Z","published":"2025-07-11T02:29:26Z","title":"Portable Biomechanics Laboratory: Clinically Accessible Movement Analysis from a Handheld Smartphone","summary":"Movement directly reflects neurological and musculoskeletal health, yet objective biomechanical assessment is rarely available in routine care. We introduce Portable Biomechanics Laboratory (PBL), a secure platform for fitting biomechanical models to video collected with a handheld, moving, smartphone. We validate this approach on over 15 hours of data synchronized to ground truth motion capture, finding mean joint-angle errors < 3$°$ and pelvis-translation errors of a few centimeters across patients with neurological-injury, lower-limb prosthesis users, pediatric in-patients, and controls. In > 5 hours of prospective deployments to neurosurgery and sports-medicine clinics, PBL was easy to setup, yielded highly reliable gait metrics (ICC > 0.9), and detected clinically relevant differences. For cervical-myelopathy patients, its measurement of gait quality correlated with modified Japanese Orthopedic Association (mJOA) scores and were responsive to clinical intervention. Handheld smartphone video can therefore deliver accurate, scalable, and low-burden biomechanical measurement, enabling greatly increased monitoring of movement impairments. We release the first clinically-validated method for measuring whole-body kinematics from handheld smartphone video at https://IntelligentSensingAndRehabilitation.github.io/MonocularBiomechanics/.","authors":["J. D. Peiffer","Kunal Shah","Irina Djuraskovic","Shawana Anarwala","Kayan Abdou","Rujvee Patel","Prakash Jayabalan","Brenton Pennicooke","R. James Cotton"],"pdf_url":"","comment":"24 pages, 10 figures"},{"id":"http://arxiv.org/abs/2512.20000v1","updated":"2025-12-23T02:52:18Z","published":"2025-12-23T02:52:18Z","title":"Few-Shot-Based Modular Image-to-Video Adapter for Diffusion Models","summary":"Diffusion models (DMs) have recently achieved impressive photorealism in image and video generation. However, their application to image animation remains limited, even when trained on large-scale datasets. Two primary challenges contribute to this: the high dimensionality of video signals leads to a scarcity of training data, causing DMs to favor memorization over prompt compliance when generating motion; moreover, DMs struggle to generalize to novel motion patterns not present in the training set, and fine-tuning them to learn such patterns, especially using limited training data, is still under-explored. To address these limitations, we propose Modular Image-to-Video Adapter (MIVA), a lightweight sub-network attachable to a pre-trained DM, each designed to capture a single motion pattern and scalable via parallelization. MIVAs can be efficiently trained on approximately ten samples using a single consumer-grade GPU. At inference time, users can specify motion by selecting one or multiple MIVAs, eliminating the need for prompt engineering. Extensive experiments demonstrate that MIVA enables more precise motion control while maintaining, or even surpassing, the generation quality of models trained on significantly larger datasets.","authors":["Zhenhao Li","Shaohan Yi","Zheng Liu","Leonartinus Gao","Minh Ngoc Le","Ambrose Ling","Zhuoran Wang","Md Amirul Islam","Zhixiang Chi","Yuanhao Yu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2412.06868v2","updated":"2025-12-23T02:50:21Z","published":"2024-12-09T09:55:54Z","title":"Compression for Better: A General and Stable Lossless Compression Framework","summary":"This work focus on how to stabilize and lossless model compression, aiming to reduce model complexity and enhance efficiency without sacrificing performance due to compression errors. A key challenge is effectively leveraging compression errors and defining the boundaries for lossless compression to minimize model loss. i.e., compression for better. Currently, there is no systematic approach to determining this error boundary or understanding its specific impact on model performance. We propose a general \\textbf{L}oss\\textbf{L}ess \\textbf{C}ompression theoretical framework (\\textbf{LLC}), which further delineates the compression neighborhood and higher-order analysis boundaries through the total differential, thereby specifying the error range within which a model can be compressed without loss. To verify the effectiveness of LLC, we apply various compression techniques, including quantization and decomposition. Specifically, for quantization, we reformulate the classic quantization search problem as a grouped knapsack problem within the lossless neighborhood, achieving lossless quantization while improving computational efficiency. For decomposition, LLC addresses the approximation problem under low-rank constraints, automatically determining the rank for each layer and producing lossless low-rank models. We conduct extensive experiments on multiple neural network architectures on different datasets. The results show that without fancy tricks, LLC can effectively achieve lossless model compression. Our code will be made publicly.","authors":["Boyang Zhang","Daning Cheng","Yunquan Zhang","Fangming Liu","Wenguang Chen"],"pdf_url":"","comment":"Under Review"},{"id":"http://arxiv.org/abs/2512.19990v1","updated":"2025-12-23T02:32:02Z","published":"2025-12-23T02:32:02Z","title":"A Dual-Branch Local-Global Framework for Cross-Resolution Land Cover Mapping","summary":"Cross-resolution land cover mapping aims to produce high-resolution semantic predictions from coarse or low-resolution supervision, yet the severe resolution mismatch makes effective learning highly challenging. Existing weakly supervised approaches often struggle to align fine-grained spatial structures with coarse labels, leading to noisy supervision and degraded mapping accuracy. To tackle this problem, we propose DDTM, a dual-branch weakly supervised framework that explicitly decouples local semantic refinement from global contextual reasoning. Specifically, DDTM introduces a diffusion-based branch to progressively refine fine-scale local semantics under coarse supervision, while a transformer-based branch enforces long-range contextual consistency across large spatial extents. In addition, we design a pseudo-label confidence evaluation module to mitigate noise induced by cross-resolution inconsistencies and to selectively exploit reliable supervisory signals. Extensive experiments demonstrate that DDTM establishes a new state-of-the-art on the Chesapeake Bay benchmark, achieving 66.52\\% mIoU and substantially outperforming prior weakly supervised methods. The code is available at https://github.com/gpgpgp123/DDTM.","authors":["Peng Gao","Ke Li","Di Wang","Yongshan Zhu","Yiming Zhang","Xuemei Luo","Yifeng Wang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19989v1","updated":"2025-12-23T02:30:50Z","published":"2025-12-23T02:30:50Z","title":"A Novel CNN Gradient Boosting Ensemble for Guava Disease Detection","summary":"As a significant agricultural country, Bangladesh utilizes its fertile land for guava cultivation and dedicated labor to boost its economic development. In a nation like Bangladesh, enhancing guava production and agricultural practices plays a crucial role in its economy. Anthracnose and fruit fly infection can lower the quality and productivity of guava, a crucial tropical fruit. Expert systems that detect diseases early can reduce losses and safeguard the harvest. Images of guava fruits classified into the Healthy, Fruit Flies, and Anthracnose classes are included in the Guava Fruit Disease Dataset 2024 (GFDD24), which comes from plantations in Rajshahi and Pabna, Bangladesh. This study aims to create models using CNN alongside traditional machine learning techniques that can effectively identify guava diseases in locally cultivated varieties in Bangladesh. In order to achieve the highest classification accuracy of approximately 99.99% for the guava dataset, we propose utilizing ensemble models that combine CNNML with Gradient Boosting Machine. In general, the CNN-ML cascade framework exhibits strong, high-accuracy guava disease detection that is appropriate for real-time agricultural monitoring systems.","authors":["Tamim Ahasan Rijon","Yeasin Arafath"],"pdf_url":"","comment":"Accepted at IEEE ICCIT 2025. This is the author accepted manuscript"},{"id":"http://arxiv.org/abs/2512.19982v1","updated":"2025-12-23T02:10:24Z","published":"2025-12-23T02:10:24Z","title":"WSD-MIL: Window Scale Decay Multiple Instance Learning for Whole Slide Image Classification","summary":"In recent years, the integration of pre-trained foundational models with multiple instance learning (MIL) has improved diagnostic accuracy in computational pathology. However, existing MIL methods focus on optimizing feature extractors and aggregation strategies while overlooking the complex semantic relationships among instances within whole slide image (WSI). Although Transformer-based MIL approaches aiming to model instance dependencies, the quadratic computational complexity limits their scalability to large-scale WSIs. Moreover, due to the pronounced variations in tumor region scales across different WSIs, existing Transformer-based methods employing fixed-scale attention mechanisms face significant challenges in precisely capturing local instance correlations and fail to account for the distance-based decay effect of patch relevance. To address these challenges, we propose window scale decay MIL (WSD-MIL), designed to enhance the capacity to model tumor regions of varying scales while improving computational efficiency. WSD-MIL comprises: 1) a window scale decay based attention module, which employs a cluster-based sampling strategy to reduce computational costs while progressively decaying attention window-scale to capture local instance relationships at varying scales; and 2) a squeeze-and-excitation based region gate module, which dynamically adjusts window weights to enhance global information modeling. Experimental results demonstrate that WSD-MIL achieves state-of-the-art performance on the CAMELYON16 and TCGA-BRCA datasets while reducing 62% of the computational memory. The code will be publicly available.","authors":["Le Feng","Li Xiao"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2504.11705v4","updated":"2025-12-23T01:57:40Z","published":"2025-04-16T02:05:47Z","title":"FiGO: Fine-Grained Object Counting without Annotations","summary":"Class-agnostic counting (CAC) methods reduce annotation costs by letting users define what to count at test-time through text or visual exemplars. However, current open-vocabulary approaches work well for broad categories but fail when fine-grained category distinctions are needed, such as telling apart waterfowl species or pepper cultivars. We present FiGO, a new annotation-free method that adapts existing counting models to fine-grained categories using only the category name. Our approach uses a text-to-image diffusion model to create synthetic examples and a joint positive/hard-negative loss to learn a compact concept embedding that conditions a specialization module to convert outputs from any frozen counter into accurate, fine-grained estimates. To evaluate fine-grained counting, we introduce LOOKALIKES, a dataset of 37 subcategories across 14 parent categories with many visually similar objects per image. Our method substantially outperforms strong open-vocabulary baselines, moving counting systems from \"count all the peppers\" to \"count only the habaneros.\"","authors":["Adriano D'Alessandro","Ali Mahdavi-Amiri","Ghassan Hamarneh"],"pdf_url":"","comment":"data - https://dalessandro.dev/datasets/lookalikes/"},{"id":"http://arxiv.org/abs/2512.19954v1","updated":"2025-12-23T00:58:27Z","published":"2025-12-23T00:58:27Z","title":"HistoWAS: A Pathomics Framework for Large-Scale Feature-Wide Association Studies of Tissue Topology and Patient Outcomes","summary":"High-throughput \"pathomic\" analysis of Whole Slide Images (WSIs) offers new opportunities to study tissue characteristics and for biomarker discovery. However, the clinical relevance of the tissue characteristics at the micro- and macro-environment level is limited by the lack of tools that facilitate the measurement of the spatial interaction of individual structure characteristics and their association with clinical parameters. To address these challenges, we introduce HistoWAS (Histology-Wide Association Study), a computational framework designed to link tissue spatial organization to clinical outcomes. Specifically, HistoWAS implements (1) a feature space that augments conventional metrics with 30 topological and spatial features, adapted from Geographic Information Systems (GIS) point pattern analysis, to quantify tissue micro-architecture; and (2) an association study engine, inspired by Phenome-Wide Association Studies (PheWAS), that performs mass univariate regression for each feature with statistical correction. As a proof of concept, we applied HistoWAS to analyze a total of 102 features (72 conventional object-level features and our 30 spatial features) using 385 PAS-stained WSIs from 206 participants in the Kidney Precision Medicine Project (KPMP). The code and data have been released to https://github.com/hrlblab/histoWAS.","authors":["Yuechen Yang","Junlin Guo","Yanfan Zhu","Jialin Yue","Junchao Zhu","Yu Wang","Shilin Zhao","Haichun Yang","Xingyi Guo","Jovan Tanevski","Laura Barisoni","Avi Z. Rosenberg","Yuankai Huo"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19949v1","updated":"2025-12-23T00:38:52Z","published":"2025-12-23T00:38:52Z","title":"How Much 3D Do Video Foundation Models Encode?","summary":"Videos are continuous 2D projections of 3D worlds. After training on large video data, will global 3D understanding naturally emerge? We study this by quantifying the 3D understanding of existing Video Foundation Models (VidFMs) pretrained on vast video data. We propose the first model-agnostic framework that measures the 3D awareness of various VidFMs by estimating multiple 3D properties from their features via shallow read-outs. Our study presents meaningful findings regarding the 3D awareness of VidFMs on multiple axes. In particular, we show that state-of-the-art video generation models exhibit a strong understanding of 3D objects and scenes, despite not being trained on any 3D data. Such understanding can even surpass that of large expert models specifically trained for 3D tasks. Our findings, together with the 3D benchmarking of major VidFMs, provide valuable observations for building scalable 3D models.","authors":["Zixuan Huang","Xiang Li","Zhaoyang Lv","James M. Rehg"],"pdf_url":"","comment":"Project Page: https://vidfm-3d-probe.github.io"},{"id":"http://arxiv.org/abs/2509.19073v2","updated":"2025-12-23T00:35:30Z","published":"2025-09-23T14:34:10Z","title":"WaveletGaussian: Wavelet-domain Diffusion for Sparse-view 3D Gaussian Object Reconstruction","summary":"3D Gaussian Splatting (3DGS) has become a powerful representation for image-based object reconstruction, yet its performance drops sharply in sparse-view settings. Prior works address this limitation by employing diffusion models to repair corrupted renders, subsequently using them as pseudo ground truths for later optimization. While effective, such approaches incur heavy computation from the diffusion fine-tuning and repair steps. We present WaveletGaussian, a framework for more efficient sparse-view 3D Gaussian object reconstruction. Our key idea is to shift diffusion into the wavelet domain: diffusion is applied only to the low-resolution LL subband, while high-frequency subbands are refined with a lightweight network. We further propose an efficient online random masking strategy to curate training pairs for diffusion fine-tuning, replacing the commonly used, but inefficient, leave-one-out strategy. Experiments across two benchmark datasets, Mip-NeRF 360 and OmniObject3D, show WaveletGaussian achieves competitive rendering quality while substantially reducing training time.","authors":["Hung Nguyen","Runfa Li","An Le","Truong Nguyen"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19943v1","updated":"2025-12-23T00:24:46Z","published":"2025-12-23T00:24:46Z","title":"SE360: Semantic Edit in 360$^\\circ$ Panoramas via Hierarchical Data Construction","summary":"While instruction-based image editing is emerging, extending it to 360$^\\circ$ panoramas introduces additional challenges. Existing methods often produce implausible results in both equirectangular projections (ERP) and perspective views. To address these limitations, we propose SE360, a novel framework for multi-condition guided object editing in 360$^\\circ$ panoramas. At its core is a novel coarse-to-fine autonomous data generation pipeline without manual intervention. This pipeline leverages a Vision-Language Model (VLM) and adaptive projection adjustment for hierarchical analysis, ensuring the holistic segmentation of objects and their physical context. The resulting data pairs are both semantically meaningful and geometrically consistent, even when sourced from unlabeled panoramas. Furthermore, we introduce a cost-effective, two-stage data refinement strategy to improve data realism and mitigate model overfitting to erase artifacts. Based on the constructed dataset, we train a Transformer-based diffusion model to allow flexible object editing guided by text, mask, or reference image in 360$^\\circ$ panoramas. Our experiments demonstrate that our method outperforms existing methods in both visual quality and semantic accuracy.","authors":["Haoyi Zhong","Fang-Lue Zhang","Andrew Chalmers","Taehyun Rhee"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19941v1","updated":"2025-12-23T00:18:23Z","published":"2025-12-23T00:18:23Z","title":"Block-Recurrent Dynamics in Vision Transformers","summary":"As Vision Transformers (ViTs) become standard vision backbones, a mechanistic account of their computational phenomenology is essential. Despite architectural cues that hint at dynamical structure, there is no settled framework that interprets Transformer depth as a well-characterized flow. In this work, we introduce the Block-Recurrent Hypothesis (BRH), arguing that trained ViTs admit a block-recurrent depth structure such that the computation of the original $L$ blocks can be accurately rewritten using only $k \\ll L$ distinct blocks applied recurrently. Across diverse ViTs, between-layer representational similarity matrices suggest few contiguous phases. To determine whether these phases reflect genuinely reusable computation, we train block-recurrent surrogates of pretrained ViTs: Recurrent Approximations to Phase-structured TransfORmers (Raptor). In small-scale, we demonstrate that stochastic depth and training promote recurrent structure and subsequently correlate with our ability to accurately fit Raptor. We then provide an empirical existence proof for BRH by training a Raptor model to recover $96\\%$ of DINOv2 ImageNet-1k linear probe accuracy in only 2 blocks at equivalent computational cost. Finally, we leverage our hypothesis to develop a program of Dynamical Interpretability. We find i) directional convergence into class-dependent angular basins with self-correcting trajectories under small perturbations, ii) token-specific dynamics, where cls executes sharp late reorientations while patch tokens exhibit strong late-stage coherence toward their mean direction, and iii) a collapse to low rank updates in late depth, consistent with convergence to low-dimensional attractors. Altogether, we find a compact recurrent program emerges along ViT depth, pointing to a low-complexity normative solution that enables these models to be studied through principled dynamical systems analysis.","authors":["Mozes Jacobs","Thomas Fel","Richard Hakim","Alessandra Brondetta","Demba Ba","T. Andy Keller"],"pdf_url":"","comment":"25 pages, 15 figures"},{"id":"http://arxiv.org/abs/2512.20839v1","updated":"2025-12-23T23:30:56Z","published":"2025-12-23T23:30:56Z","title":"Input-Adaptive Visual Preprocessing for Efficient Fast Vision-Language Model Inference","summary":"Vision-Language Models (VLMs) have demonstrated strong performance on multimodal reasoning tasks, but their deployment remains challenging due to high inference latency and computational cost, particularly when processing high-resolution visual inputs. While recent architectures such as FastVLM improve efficiency through optimized vision encoders, existing pipelines still rely on static visual preprocessing, leading to redundant computation for visually simple inputs. In this work, we propose an adaptive visual preprocessing method that dynamically adjusts input resolution and spatial coverage based on image content characteristics. The proposed approach combines content-aware image analysis, adaptive resolution selection, and content-aware cropping to reduce visual redundancy prior to vision encoding. Importantly, the method is integrated with FastVLM without modifying its architecture or requiring retraining. We evaluate the proposed method on a subset of the DocVQA dataset in an inference-only setting, focusing on efficiency-oriented metrics. Experimental results show that adaptive preprocessing reduces per-image inference time by over 50\\%, lowers mean full generation time, and achieves a consistent reduction of more than 55\\% in visual token count compared to the baseline pipeline. These findings demonstrate that input-aware preprocessing is an effective and lightweight strategy for improving deployment-oriented efficiency of vision-language models. To facilitate reproducibility, our implementation is provided as a fork of the FastVLM repository, incorporating the files for the proposed method, and is available at https://github.com/kmdavidds/mlfastlm.","authors":["Putu Indah Githa Cahyani","Komang David Dananjaya Suartana","Novanto Yudistira"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2503.11953v2","updated":"2025-12-23T23:21:46Z","published":"2025-03-15T01:48:54Z","title":"SPOC: Spatially-Progressing Object State Change Segmentation in Video","summary":"Object state changes in video reveal critical cues about human and agent activity. However, existing methods are limited to temporal localization of when the object is in its initial state (e.g., cheese block) versus when it has completed a state change (e.g., grated cheese), offering no insight into where the change is unfolding. We propose to deepen the problem by introducing the spatially-progressing object state change segmentation task. The goal is to segment at the pixel-level those regions of an object that are actionable and those that are transformed. We show that state-of-the-art VLMs and video segmentation methods struggle at this task, underscoring its difficulty and novelty. As an initial baseline, we design a VLM-based pseudo-labeling approach, state-change dynamics constraints, and a novel WhereToChange benchmark built on in-the-wild Internet videos. Experiments on two datasets validate both the challenge of the new task as well as the promise of our model for localizing exactly where and how fast objects are changing in video. We further demonstrate useful implications for tracking activity progress to benefit robotic agents. Overall, our work positions spatial OSC segmentation as a new frontier task for video understanding: one that challenges current SOTA methods and invites the community to build more robust, state-change-sensitive representations. Project page: https://vision.cs.utexas.edu/projects/spoc-spatially-progressing-osc","authors":["Priyanka Mandikal","Tushar Nagarajan","Alex Stoken","Zihui Xue","Kristen Grauman"],"pdf_url":"","comment":"Accepted at WACV 2026"},{"id":"http://arxiv.org/abs/2512.20833v1","updated":"2025-12-23T23:15:10Z","published":"2025-12-23T23:15:10Z","title":"CHAMMI-75: pre-training multi-channel models with heterogeneous microscopy images","summary":"Quantifying cell morphology using images and machine learning has proven to be a powerful tool to study the response of cells to treatments. However, models used to quantify cellular morphology are typically trained with a single microscopy imaging type. This results in specialized models that cannot be reused across biological studies because the technical specifications do not match (e.g., different number of channels), or because the target experimental conditions are out of distribution. Here, we present CHAMMI-75, an open access dataset of heterogeneous, multi-channel microscopy images from 75 diverse biological studies. We curated this resource from publicly available sources to investigate cellular morphology models that are channel-adaptive and can process any microscopy image type. Our experiments show that training with CHAMMI-75 can improve performance in multi-channel bioimaging tasks primarily because of its high diversity in microscopy modalities. This work paves the way to create the next generation of cellular morphology models for biological studies.","authors":["Vidit Agrawal","John Peters","Tyler N. Thompson","Mohammad Vali Sanian","Chau Pham","Nikita Moshkov","Arshad Kazi","Aditya Pillai","Jack Freeman","Byunguk Kang","Samouil L. Farhi","Ernest Fraenkel","Ron Stewart","Lassi Paavolainen","Bryan A. Plummer","Juan C. Caicedo"],"pdf_url":"","comment":"47 Pages, 23 Figures, 26 Tables"},{"id":"http://arxiv.org/abs/2512.20815v1","updated":"2025-12-23T22:28:30Z","published":"2025-12-23T22:28:30Z","title":"Learning to Sense for Driving: Joint Optics-Sensor-Model Co-Design for Semantic Segmentation","summary":"Traditional autonomous driving pipelines decouple camera design from downstream perception, relying on fixed optics and handcrafted ISPs that prioritize human viewable imagery rather than machine semantics. This separation discards information during demosaicing, denoising, or quantization, while forcing models to adapt to sensor artifacts. We present a task-driven co-design framework that unifies optics, sensor modeling, and lightweight semantic segmentation networks into a single end-to-end RAW-to-task pipeline. Building on DeepLens[19], our system integrates realistic cellphone-scale lens models, learnable color filter arrays, Poisson-Gaussian noise processes, and quantization, all optimized directly for segmentation objectives. Evaluations on KITTI-360 show consistent mIoU improvements over fixed pipelines, with optics modeling and CFA learning providing the largest gains, especially for thin or low-light-sensitive classes. Importantly, these robustness gains are achieved with a compact ~1M-parameter model running at ~28 FPS, demonstrating edge deployability. Visual and quantitative analyses further highlight how co-designed sensors adapt acquisition to semantic structure, sharpening boundaries and maintaining accuracy under blur, noise, and low bit-depth. Together, these findings establish full-stack co-optimization of optics, sensors, and networks as a principled path toward efficient, reliable, and deployable perception in autonomous systems.","authors":["Reeshad Khan amd John Gauch"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20783v1","updated":"2025-12-23T21:30:05Z","published":"2025-12-23T21:30:05Z","title":"NULLBUS: Multimodal Mixed-Supervision for Breast Ultrasound Segmentation via Nullable Global-Local Prompts","summary":"Breast ultrasound (BUS) segmentation provides lesion boundaries essential for computer-aided diagnosis and treatment planning. While promptable methods can improve segmentation performance and tumor delineation when text or spatial prompts are available, many public BUS datasets lack reliable metadata or reports, constraining training to small multimodal subsets and reducing robustness. We propose NullBUS, a multimodal mixed-supervision framework that learns from images with and without prompts in a single model. To handle missing text, we introduce nullable prompts, implemented as learnable null embeddings with presence masks, enabling fallback to image-only evidence when metadata are absent and the use of text when present. Evaluated on a unified pool of three public BUS datasets, NullBUS achieves a mean IoU of 0.8568 and a mean Dice of 0.9103, demonstrating state-of-the-art performance under mixed prompt availability.","authors":["Raja Mallina","Bryar Shareef"],"pdf_url":"","comment":"5 pages, 2 figures, and 4 tables"},{"id":"http://arxiv.org/abs/2509.22615v2","updated":"2025-12-23T21:29:42Z","published":"2025-09-26T17:41:57Z","title":"GaussianVision: Vision-Language Alignment from Compressed Image Representations using 2D Gaussian Splatting","summary":"Modern vision language pipelines are driven by RGB vision encoders trained on massive image text corpora. While these pipelines have enabled impressive zero-shot capabilities and strong transfer across tasks, they still inherit two structural inefficiencies from the pixel domain: (i) transmitting dense RGB images from edge devices to the cloud is energy-intensive and costly, and (ii) patch-based tokenization explodes sequence length, stressing attention budgets and context limits. We explore 2D Gaussian Splatting (2DGS) as an alternative visual substrate for alignment: a compact, spatially adaptive representation that parameterizes images by a set of colored anisotropic Gaussians. We develop a scalable 2DGS pipeline with structured initialization, luminance-aware pruning, and batched CUDA kernels, achieving over 90x faster fitting and about 97% GPU utilization compared to prior implementations. We further adapt contrastive language-image pre-training (CLIP) to 2DGS by reusing a frozen RGB-based transformer backbone with a lightweight splat-aware input stem and a perceiver resampler, training only 9.7% to 13.8% of the total parameters. On a 12.8M dataset from DataComp, GS encoders yield competitive zero-shot performance on 38 datasets from the CLIP benchmark while compressing inputs 3x to 23.5x relative to pixels. Our results establish 2DGS as a viable multimodal substrate, pinpoint architectural bottlenecks, and open a path toward representations that are both semantically powerful and transmission-efficient for edge-cloud learning.","authors":["Yasmine Omri","Connor Ding","Tsachy Weissman","Thierry Tambe"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20770v1","updated":"2025-12-23T21:14:55Z","published":"2025-12-23T21:14:55Z","title":"OccuFly: A 3D Vision Benchmark for Semantic Scene Completion from the Aerial Perspective","summary":"Semantic Scene Completion (SSC) is crucial for 3D perception in mobile robotics, as it enables holistic scene understanding by jointly estimating dense volumetric occupancy and per-voxel semantics. Although SSC has been widely studied in terrestrial domains such as autonomous driving, aerial scenarios like autonomous flying remain largely unexplored, thereby limiting progress on downstream applications. Furthermore, LiDAR sensors represent the primary modality for SSC data generation, which poses challenges for most uncrewed aerial vehicles (UAVs) due to flight regulations, mass and energy constraints, and the sparsity of LiDAR-based point clouds from elevated viewpoints. To address these limitations, we introduce OccuFly, the first real-world, camera-based aerial SSC benchmark, captured at altitudes of 50m, 40m, and 30m during spring, summer, fall, and winter. OccuFly covers urban, industrial, and rural scenarios, provides 22 semantic classes, and the data format adheres to established conventions to facilitate seamless integration with existing research. Crucially, we propose a LiDAR-free data generation framework based on camera modality, which is ubiquitous on modern UAVs. By utilizing traditional 3D reconstruction, our framework automates label transfer by lifting a subset of annotated 2D masks into the reconstructed point cloud, thereby substantially minimizing manual 3D annotation effort. Finally, we benchmark the state-of-the-art on OccuFly and highlight challenges specific to elevated viewpoints, yielding a comprehensive vision benchmark for holistic aerial 3D scene understanding.","authors":["Markus Gross","Sai B. Matha","Aya Fahmy","Rui Song","Daniel Cremers","Henri Meess"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2509.17329v3","updated":"2025-12-23T20:37:10Z","published":"2025-09-22T03:05:22Z","title":"SmokeSeer: 3D Gaussian Splatting for Smoke Removal and Scene Reconstruction","summary":"Smoke in real-world scenes can severely degrade image quality and hamper visibility. Recent image restoration methods either rely on data-driven priors that are susceptible to hallucinations, or are limited to static low-density smoke. We introduce SmokeSeer, a method for simultaneous 3D scene reconstruction and smoke removal from multi-view video sequences. Our method uses thermal and RGB images, leveraging the reduced scattering in thermal images to see through smoke. We build upon 3D Gaussian splatting to fuse information from the two image modalities, and decompose the scene into smoke and non-smoke components. Unlike prior work, SmokeSeer handles a broad range of smoke densities and adapts to temporally varying smoke. We validate our method on synthetic data and a new real-world smoke dataset with RGB and thermal images. We provide an open-source implementation and data on the project website.","authors":["Neham Jain","Andrew Jong","Sebastian Scherer","Ioannis Gkioulekas"],"pdf_url":"","comment":"Project website: https://imaging.cs.cmu.edu/smokeseer"},{"id":"http://arxiv.org/abs/2512.20746v1","updated":"2025-12-23T20:00:34Z","published":"2025-12-23T20:00:34Z","title":"TrashDet: Iterative Neural Architecture Search for Efficient Waste Detection","summary":"This paper addresses trash detection on the TACO dataset under strict TinyML constraints using an iterative hardware-aware neural architecture search framework targeting edge and IoT devices. The proposed method constructs a Once-for-All-style ResDets supernet and performs iterative evolutionary search that alternates between backbone and neck/head optimization, supported by a population passthrough mechanism and an accuracy predictor to reduce search cost and improve stability. This framework yields a family of deployment-ready detectors, termed TrashDets. On a five-class TACO subset (paper, plastic, bottle, can, cigarette), the strongest variant, TrashDet-l, achieves 19.5 mAP50 with 30.5M parameters, improving accuracy by up to 3.6 mAP50 over prior detectors while using substantially fewer parameters. The TrashDet family spans 1.2M to 30.5M parameters with mAP50 values between 11.4 and 19.5, providing scalable detector options for diverse TinyML deployment budgets on resource-constrained hardware. On the MAX78002 microcontroller with the TrashNet dataset, two specialized variants, TrashDet-ResNet and TrashDet-MBNet, jointly dominate the ai87-fpndetector baseline, with TrashDet-ResNet achieving 7525~$μ$J energy per inference at 26.7 ms latency and 37.45 FPS, and TrashDet-MBNet improving mAP50 by 10.2%; together they reduce energy consumption by up to 88%, latency by up to 78%, and average power by up to 53% compared to existing TinyML detectors.","authors":["Tony Tran","Bin Hu"],"pdf_url":"","comment":"10 pages. The paper has been accepted by the WACV 2026 workshop"},{"id":"http://arxiv.org/abs/2512.20735v1","updated":"2025-12-23T19:47:11Z","published":"2025-12-23T19:47:11Z","title":"VL4Gaze: Unleashing Vision-Language Models for Gaze Following","summary":"Human gaze provides essential cues for interpreting attention, intention, and social interaction in visual scenes, yet gaze understanding remains largely unexplored in current vision-language models (VLMs). While recent VLMs achieve strong scene-level reasoning across a range of visual tasks, there exists no benchmark that systematically evaluates or trains them for gaze interpretation, leaving open the question of whether gaze understanding can emerge from general-purpose vision-language pre-training. To address this gap, we introduce VL4Gaze, the first large-scale benchmark designed to investigate, evaluate, and unlock the potential of VLMs for gaze understanding. VL4Gaze contains 489K automatically generated question-answer pairs across 124K images and formulates gaze understanding as a unified VQA problem through four complementary tasks: (1) gaze object description, (2) gaze direction description, (3) gaze point location, and (4) ambiguous question recognition. We comprehensively evaluate both commercial and open-source VLMs under in-context learning and fine-tuning settings. The results show that even large-scale VLMs struggle to reliably infer gaze semantics and spatial localization without task-specific supervision. In contrast, training on VL4Gaze brings substantial and consistent improvements across all tasks, highlighting the importance of targeted multi-task supervision for developing gaze understanding capabilities in VLMs. We will release the dataset and code to support further research and development in this direction.","authors":["Shijing Wang","Chaoqun Cui","Yaping Huang","Hyung Jin Chang","Yihua Cheng"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2505.17019v2","updated":"2025-12-23T19:00:17Z","published":"2025-05-22T17:59:53Z","title":"Let Androids Dream of Electric Sheep: A Human-Inspired Image Implication Understanding and Reasoning Framework","summary":"Metaphorical comprehension in images remains a critical challenge for AI systems, as existing models struggle to grasp the nuanced cultural, emotional, and contextual implications embedded in visual content. While multimodal large language models (MLLMs) excel in general Visual Question Answer (VQA) tasks, they struggle with a fundamental limitation on image implication tasks: contextual gaps that obscure the relationships between different visual elements and their abstract meanings. Inspired by the human cognitive process, we propose Let Androids Dream (LAD), a novel framework for image implication understanding and reasoning. LAD addresses contextual missing through the three-stage framework: (1) Perception: converting visual information into rich and multi-level textual representations, (2) Search: iteratively searching and integrating cross-domain knowledge to resolve ambiguity, and (3) Reasoning: generating context-alignment image implication via explicit reasoning. Our framework with the lightweight GPT-4o-mini model achieves SOTA performance compared to 15+ MLLMs on English image implication benchmark and a huge improvement on Chinese benchmark, performing comparable with the Gemini-3.0-pro model on Multiple-Choice Question (MCQ) and outperforms the GPT-4o model 36.7% on Open-Style Question (OSQ). Generalization experiments also show that our framework can effectively benefit general VQA and visual reasoning tasks. Additionally, our work provides new insights into how AI can more effectively interpret image implications, advancing the field of vision-language reasoning and human-AI interaction. Our project is publicly available at https://github.com/MING-ZCH/Let-Androids-Dream-of-Electric-Sheep.","authors":["Chenhao Zhang","Yazhe Niu"],"pdf_url":"","comment":"19 pages, 9 figures, 7 tables. Code & Dataset: https://github.com/MING-ZCH/Let-Androids-Dream-of-Electric-Sheep"},{"id":"http://arxiv.org/abs/2501.02913v2","updated":"2025-12-23T16:12:14Z","published":"2025-01-06T10:48:31Z","title":"Pointmap-Conditioned Diffusion for Consistent Novel View Synthesis","summary":"Synthesizing extrapolated views remains a difficult task, especially in urban driving scenes, where the only reliable sources of data are limited RGB captures and sparse LiDAR points. To address this problem, we present PointmapDiff, a framework for novel view synthesis that utilizes pre-trained 2D diffusion models. Our method leverages point maps (i.e., rasterized 3D scene coordinates) as a conditioning signal, capturing geometric and photometric priors from the reference images to guide the image generation process. With the proposed reference attention layers and ControlNet for point map features, PointmapDiff can generate accurate and consistent results across varying viewpoints while respecting geometric fidelity. Experiments on real-life driving data demonstrate that our method achieves high-quality generation with flexibility over point map conditioning signals (e.g., dense depth map or even sparse LiDAR points) and can be used to distill to 3D representations such as 3D Gaussian Splatting for improving view extrapolation.","authors":["Thang-Anh-Quan Nguyen","Nathan Piasco","Luis Roldão","Moussab Bennehar","Dzmitry Tsishkou","Laurent Caraffa","Jean-Philippe Tarel","Roland Brémond"],"pdf_url":"","comment":"WACV 2026. Project page: https://ntaquan0125.github.io/pointmap-conditioned-diffusion"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2512.20612v1","updated":"2025-12-23T18:58:25Z","published":"2025-12-23T18:58:25Z","title":"Making Large Language Models Efficient Dense Retrievers","summary":"Recent work has shown that directly fine-tuning large language models (LLMs) for dense retrieval yields strong performance, but their substantial parameter counts make them computationally inefficient. While prior studies have revealed significant layer redundancy in LLMs for generative tasks, it remains unclear whether similar redundancy exists when these models are adapted for retrieval tasks, which require encoding entire sequences into fixed representations rather than generating tokens iteratively. To this end, we conduct a comprehensive analysis of layer redundancy in LLM-based dense retrievers. We find that, in contrast to generative settings, MLP layers are substantially more prunable, while attention layers remain critical for semantic aggregation. Building on this insight, we propose EffiR, a framework for developing efficient retrievers that performs large-scale MLP compression through a coarse-to-fine strategy (coarse-grained depth reduction followed by fine-grained width reduction), combined with retrieval-specific fine-tuning. Across diverse BEIR datasets and LLM backbones, EffiR achieves substantial reductions in model size and inference cost while preserving the performance of full-size models.","authors":["Yibin Lei","Shwai He","Ang Li","Andrew Yates"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20458v1","updated":"2025-12-23T15:53:33Z","published":"2025-12-23T15:53:33Z","title":"Laser: Governing Long-Horizon Agentic Search via Structured Protocol and Context Register","summary":"Recent advances in Large Language Models (LLMs) and Large Reasoning Models (LRMs) have enabled agentic search systems that interleave multi-step reasoning with external tool use. However, existing frameworks largely rely on unstructured natural-language reasoning and accumulate raw intermediate traces in the context, which often leads to unstable reasoning trajectories, context overflow, and degraded performance on complex multi-hop queries. In this study, we introduce Laser, a general framework for stabilizing and scaling agentic search. Laser defines a symbolic action protocol that organizes agent behaviors into three spaces: planning, task-solving, and retrospection. Each action is specified with explicit semantics and a deterministic execution format, enabling structured and logical reasoning processes and reliable action parsing. This design makes intermediate decisions interpretable and traceable, enhancing explicit retrospection and fine-grained control over reasoning trajectories. In coordination with parsable actions, Laser further maintains a compact context register that stores only essential states of the reasoning process, allowing the agent to reason over long horizons without uncontrolled context expansion. Experiments on Qwen2.5/3-series models across challenging multi-hop QA datasets show that Laser consistently outperforms existing agentic search baselines under both prompting-only and fine-tuning settings, demonstrating that Laser provides a principled and effective foundation for robust, scalable agentic search.","authors":["Shuting Wang","Qiaolin Xia","Hao Wang","Yu Lu"," Bobsimons","Zhicheng Dou"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.10688v4","updated":"2025-12-23T12:25:36Z","published":"2025-12-11T14:35:13Z","title":"Rethinking Popularity Bias in Collaborative Filtering via Analytical Vector Decomposition","summary":"Popularity bias fundamentally undermines the personalization capabilities of collaborative filtering (CF) models, causing them to disproportionately recommend popular items while neglecting users' genuine preferences for niche content. While existing approaches treat this as an external confounding factor, we reveal that popularity bias is an intrinsic geometric artifact of Bayesian Pairwise Ranking (BPR) optimization in CF models. Through rigorous mathematical analysis, we prove that BPR systematically organizes item embeddings along a dominant \"popularity direction\" where embedding magnitudes directly correlate with interaction frequency. This geometric distortion forces user embeddings to simultaneously handle two conflicting tasks-expressing genuine preference and calibrating against global popularity-trapping them in suboptimal configurations that favor popular items regardless of individual tastes. We propose Directional Decomposition and Correction (DDC), a universally applicable framework that surgically corrects this embedding geometry through asymmetric directional updates. DDC guides positive interactions along personalized preference directions while steering negative interactions away from the global popularity direction, disentangling preference from popularity at the geometric source. Extensive experiments across multiple BPR-based architectures demonstrate that DDC significantly outperforms state-of-the-art debiasing methods, reducing training loss to less than 5% of heavily-tuned baselines while achieving superior recommendation quality and fairness. Code is available in https://github.com/LingFeng-Liu-AI/DDC.","authors":["Lingfeng Liu","Yixin Song","Dazhong Shen","Bing Yin","Hao Li","Yanyong Zhang","Chao Wang"],"pdf_url":"","comment":"Accepted by SIGKDD 2026(First Cycle)"},{"id":"http://arxiv.org/abs/2512.20245v1","updated":"2025-12-23T10:55:32Z","published":"2025-12-23T10:55:32Z","title":"Memory as Resonance: A Biomimetic Architecture for Infinite Context Memory on Ergodic Phonetic Manifolds","summary":"The memory of contemporary Large Language Models is bound by a physical paradox: as they learn, they fill up. The linear accumulation (O(N)) of Key-Value states treats context as a warehouse of static artifacts, eventually forcing a destructive choice between amnesia and latency. We challenge this discrete orthodoxy, proposing that long-term memory is not the storage of items, but the persistence of a trajectory. We introduce Phonetic Trajectory Memory (PTM), a neuro-symbolic architecture that encodes language not as a sequence of tensors, but as a continuous path on an ergodic manifold governed by irrational rotation matrices. By decoupling the navigation (an invariant O(1) geometric signal) from the reconstruction (a probabilistic generative act), PTM achieves a compression magnitude of greater than 3,000x relative to dense caches. We demonstrate that retrieval becomes a process of resonance: the phonetic trace stabilizes the model against hallucination via \"Signal Consensus\" mechanism, securing up to approximately 92% factual accuracy. While this aggressive abstraction alters generative texture, it unlocks immediate access latency (approximately 34ms) independent of depth. Our results suggest that infinite context does not require infinite silicon; it requires treating memory not as data to be stored, but as a reconstructive process acting on a conserved, undying physical signal.","authors":["Tarik Houichime","Abdelghani Souhar","Younes El Amrani"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20174v1","updated":"2025-12-23T09:14:16Z","published":"2025-12-23T09:14:16Z","title":"Towards Natural Language-Based Document Image Retrieval: New Dataset and Benchmark","summary":"Document image retrieval (DIR) aims to retrieve document images from a gallery according to a given query. Existing DIR methods are primarily based on image queries that retrieve documents within the same coarse semantic category, e.g., newspapers or receipts. However, these methods struggle to effectively retrieve document images in real-world scenarios where textual queries with fine-grained semantics are usually provided. To bridge this gap, we introduce a new Natural Language-based Document Image Retrieval (NL-DIR) benchmark with corresponding evaluation metrics. In this work, natural language descriptions serve as semantically rich queries for the DIR task. The NL-DIR dataset contains 41K authentic document images, each paired with five high-quality, fine-grained semantic queries generated and evaluated through large language models in conjunction with manual verification. We perform zero-shot and fine-tuning evaluations of existing mainstream contrastive vision-language models and OCR-free visual document understanding (VDU) models. A two-stage retrieval method is further investigated for performance improvement while achieving both time and space efficiency. We hope the proposed NL-DIR benchmark can bring new opportunities and facilitate research for the VDU community. Datasets and codes will be publicly available at huggingface.co/datasets/nianbing/NL-DIR.","authors":["Hao Guo","Xugong Qin","Jun Jie Ou Yang","Peng Zhang","Gangyan Zeng","Yubo Li","Hailun Lin"],"pdf_url":"","comment":"CVPR 2025"},{"id":"http://arxiv.org/abs/2512.20172v1","updated":"2025-12-23T09:07:28Z","published":"2025-12-23T09:07:28Z","title":"Collaborative Group-Aware Hashing for Fast Recommender Systems","summary":"The fast online recommendation is critical for applications with large-scale databases; meanwhile, it is challenging to provide accurate recommendations in sparse scenarios. Hash technique has shown its superiority for speeding up the online recommendation by bit operations on Hamming distance computations. However, existing hashing-based recommendations suffer from low accuracy, especially with sparse settings, due to the limited representation capability of each bit and neglected inherent relations among users and items. To this end, this paper lodges a Collaborative Group-Aware Hashing (CGAH) method for both collaborative filtering (namely CGAH-CF) and content-aware recommendations (namely CGAH) by integrating the inherent group information to alleviate the sparse issue. Firstly, we extract inherent group affinities of users and items by classifying their latent vectors into different groups. Then, the preference is formulated as the inner product of the group affinity and the similarity of hash codes. By learning hash codes with the inherent group information, CGAH obtains more effective hash codes than other discrete methods with sparse interactive data. Extensive experiments on three public datasets show the superior performance of our proposed CGAH and CGAH-CF over the state-of-the-art discrete collaborative filtering methods and discrete content-aware recommendations under different sparse settings.","authors":["Yan Zhang","Li Deng","Lixin Duan","Ivor W. Tsang","Guowu Yang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2506.09898v2","updated":"2025-12-23T08:47:51Z","published":"2025-06-11T16:13:52Z","title":"Discrete Scale-invariant Metric Learning for Efficient Collaborative Filtering","summary":"Metric learning has attracted extensive interest for its ability to provide personalized recommendations based on the importance of observed user-item interactions. Current metric learning methods aim to push negative items away from the corresponding users and positive items by an absolute geometrical distance margin. However, items may come from imbalanced categories with different intra-class variations. Thus, the absolute distance margin may not be ideal for estimating the difference between user preferences over imbalanced items. To this end, we propose a new method, named discrete scale-invariant metric learning (DSIML), by adding binary constraints to users and items, which maps users and items into binary codes of a shared Hamming subspace to speed up the online recommendation. Specifically, we firstly propose a scale-invariant margin based on angles at the negative item points in the shared Hamming subspace. Then, we derive a scale-invariant triple hinge loss based on the margin. To capture more preference difference information, we integrate a pairwise ranking loss into the scale-invariant loss in the proposed model. Due to the difficulty of directly optimizing the mixed integer optimization problem formulated with \\textit{log-sum-exp} functions, we seek to optimize its variational quadratic upper bound and learn hash codes with an alternating optimization strategy. Experiments on benchmark datasets clearly show that our proposed method is superior to competitive metric learning and hashing-based baselines for recommender systems.","authors":["Yan Zhang","Li Deng","Lixin Duan","Sami Azam"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20145v1","updated":"2025-12-23T08:15:34Z","published":"2025-12-23T08:15:34Z","title":"Retrieval-augmented Prompt Learning for Pre-trained Foundation Models","summary":"The pre-trained foundation models (PFMs) have become essential for facilitating large-scale multimodal learning. Researchers have effectively employed the ``pre-train, prompt, and predict'' paradigm through prompt learning to induce improved few-shot performance. However, prompt learning approaches for PFMs still follow a parametric learning paradigm. As such, the stability of generalization in memorization and rote learning can be compromised. More specifically, conventional prompt learning might face difficulties in fully utilizing atypical instances and avoiding overfitting to shallow patterns with limited data during the process of fully-supervised training. To overcome these constraints, we present our approach, named RetroPrompt, which aims to achieve a balance between memorization and generalization by decoupling knowledge from mere memorization. Unlike traditional prompting methods, RetroPrompt leverages a publicly accessible knowledge base generated from the training data and incorporates a retrieval mechanism throughout the input, training, and inference stages. This enables the model to actively retrieve relevant contextual information from the corpus, thereby enhancing the available cues. We conduct comprehensive experiments on a variety of datasets across natural language processing and computer vision tasks to demonstrate the superior performance of our proposed approach, RetroPrompt, in both zero-shot and few-shot scenarios. Through detailed analysis of memorization patterns, we observe that RetroPrompt effectively reduces the reliance on rote memorization, leading to enhanced generalization.","authors":["Xiang Chen","Yixin Ou","Quan Feng","Lei Li","Piji Li","Haibo Ye","Sheng-Jun Huang","Shuofei Qiao","Shumin Deng","Huajun Chen","Ningyu Zhang"],"pdf_url":"","comment":"IEEE/ACM Transactions on Audio, Speech and Language Processing"},{"id":"http://arxiv.org/abs/2511.15241v3","updated":"2025-12-23T07:48:31Z","published":"2025-11-19T08:55:01Z","title":"Selective Mixup for Debiasing Question Selection in Computerized Adaptive Testing","summary":"Computerized Adaptive Testing (CAT) is a widely used technology for evaluating learners' proficiency in online education platforms. By leveraging prior estimates of proficiency to select questions and updating the estimates iteratively based on responses, CAT enables personalized learner modeling and has attracted substantial attention. Despite this progress, most existing works focus primarily on improving diagnostic accuracy, while overlooking the selection bias inherent in the adaptive process. Selection Bias arises because the question selection is strongly influenced by the estimated proficiency, such as assigning easier questions to learners with lower proficiency and harder ones to learners with higher proficiency. Since the selection depends on prior estimation, this bias propagates into the diagnosis model, which is further amplified during iterative updates, leading to misalignment and biased predictions. Moreover, the imbalanced nature of learners' historical interactions often exacerbates the bias in diagnosis models. To address this issue, we propose a debiasing framework consisting of two key modules: Cross-Attribute Examinee Retrieval and Selective Mixup-based Regularization. First, we retrieve balanced examinees with relatively even distributions of correct and incorrect responses and use them as neutral references for biased examinees. Then, mixup is applied between each biased examinee and its matched balanced counterpart under label consistency. This augmentation enriches the diversity of bias-conflicting samples and smooths selection boundaries. Finally, extensive experiments on two benchmark datasets with multiple advanced diagnosis models demonstrate that our method substantially improves both the generalization ability and fairness of question selection in CAT.","authors":["Mi Tian","Kun Zhang","Fei Liu","Jinglong Li","Yuxin Liao","Chenxi Bai","Zhengtao Tan","Le Wu","Richang Hong"],"pdf_url":"","comment":"Accepted by CIKM 2025"},{"id":"http://arxiv.org/abs/2408.02152v3","updated":"2025-12-23T05:50:45Z","published":"2024-08-04T22:00:34Z","title":"Generative Retrieval with Few-shot Indexing","summary":"Existing generative retrieval (GR) methods rely on training-based indexing, which fine-tunes a model to memorise associations between queries and the document identifiers (docids) of relevant documents. Training-based indexing suffers from high training costs, under-utilisation of pre-trained knowledge in large language models (LLMs), and limited adaptability to dynamic document corpora. To address the issues, we propose a few-shot indexing-based GR framework (Few-Shot GR). It has a few-shot indexing process without any training, where we prompt an LLM to generate docids for all documents in a corpus, ultimately creating a docid bank for the entire corpus. During retrieval, we feed a query to the same LLM and constrain it to generate a docid within the docid bank created during indexing, and then map the generated docid back to its corresponding document. Moreover, we devise few-shot indexing with one-to-many mapping to further enhance Few-Shot GR. Experiments show that Few-Shot GR achieves superior performance to state-of-the-art GR methods requiring heavy training.","authors":["Arian Askari","Chuan Meng","Mohammad Aliannejadi","Zhaochun Ren","Evangelos Kanoulas","Suzan Verberne"],"pdf_url":"","comment":"Accepted for publication at the 48th European Conference on Information Retrieval (ECIR 2026)"},{"id":"http://arxiv.org/abs/2512.20034v1","updated":"2025-12-23T03:55:45Z","published":"2025-12-23T03:55:45Z","title":"VSA:Visual-Structural Alignment for UI-to-Code","summary":"The automation of user interface development has the potential to accelerate software delivery by mitigating intensive manual implementation. Despite the advancements in Large Multimodal Models for design-to-code translation, existing methodologies predominantly yield unstructured, flat codebases that lack compatibility with component-oriented libraries such as React or Angular. Such outputs typically exhibit low cohesion and high coupling, complicating long-term maintenance. In this paper, we propose \\textbf{VSA (VSA)}, a multi-stage paradigm designed to synthesize organized frontend assets through visual-structural alignment. Our approach first employs a spatial-aware transformer to reconstruct the visual input into a hierarchical tree representation. Moving beyond basic layout extraction, we integrate an algorithmic pattern-matching layer to identify recurring UI motifs and encapsulate them into modular templates. These templates are then processed via a schema-driven synthesis engine, ensuring the Large Language Model generates type-safe, prop-drilled components suitable for production environments. Experimental results indicate that our framework yields a substantial improvement in code modularity and architectural consistency over state-of-the-art benchmarks, effectively bridging the gap between raw pixels and scalable software engineering.","authors":["Xian Wu","Ming Zhang","Zhiyu Fang","Fei Li","Bin Wang","Yong Jiang","Hao Zhou"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20022v1","updated":"2025-12-23T03:32:43Z","published":"2025-12-23T03:32:43Z","title":"LLM-Assisted Abstract Screening with OLIVER: Evaluating Calibration and Single-Model vs. Actor-Critic Configurations in Literature Reviews","summary":"Introduction: Recent work suggests large language models (LLMs) can accelerate screening, but prior evaluations focus on earlier LLMs, standardized Cochrane reviews, single-model setups, and accuracy as the primary metric, leaving generalizability, configuration effects, and calibration largely unexamined.\n  Methods: We developed OLIVER (Optimized LLM-based Inclusion and Vetting Engine for Reviews), an open-source pipeline for LLM-assisted abstract screening. We evaluated multiple contemporary LLMs across two non-Cochrane systematic reviews and performance was assessed at both the full-text screening and final inclusion stages using accuracy, AUC, and calibration metrics. We further tested an actor-critic screening framework combining two lightweight models under three aggregation rules.\n  Results: Across individual models, performance varied widely. In the smaller Review 1 (821 abstracts, 63 final includes), several models achieved high sensitivity for final includes but at the cost of substantial false positives and poor calibration. In the larger Review 2 (7741 abstracts, 71 final includes), most models were highly specific but struggled to recover true includes, with prompt design influencing recall. Calibration was consistently weak across single-model configurations despite high overall accuracy. Actor-critic screening improved discrimination and markedly reduced calibration error in both reviews, yielding higher AUCs.\n  Discussion: LLMs may eventually accelerate abstract screening, but single-model performance is highly sensitive to review characteristics, prompting, and calibration is limited. An actor-critic framework improves classification quality and confidence reliability while remaining computationally efficient, enabling large-scale screening at low cost.","authors":["Kian Godhwani","David Benrimoh"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19983v1","updated":"2025-12-23T02:13:01Z","published":"2025-12-23T02:13:01Z","title":"IGDMRec: Behavior Conditioned Item Graph Diffusion for Multimodal Recommendation","summary":"Multimodal recommender systems (MRSs) are critical for various online platforms, offering users more accurate personalized recommendations by incorporating multimodal information of items. Structure-based MRSs have achieved state-of-the-art performance by constructing semantic item graphs, which explicitly model relationships between items based on modality feature similarity. However, such semantic item graphs are often noisy due to 1) inherent noise in multimodal information and 2) misalignment between item semantics and user-item co-occurrence relationships, which introduces false links and leads to suboptimal recommendations. To address this challenge, we propose Item Graph Diffusion for Multimodal Recommendation (IGDMRec), a novel method that leverages a diffusion model with classifier-free guidance to denoise the semantic item graph by integrating user behavioral information. Specifically, IGDMRec introduces a Behavior-conditioned Graph Diffusion (BGD) module, incorporating interaction data as conditioning information to guide the denoising of the semantic item graph. Additionally, a Conditional Denoising Network (CD-Net) is designed to implement the denoising process with manageable complexity. Finally, we propose a contrastive representation augmentation scheme that leverages both the denoised item graph and the original item graph to enhance item representations. \\LL{Extensive experiments on four real-world datasets demonstrate the superiority of IGDMRec over competitive baselines, with robustness analysis validating its denoising capability and ablation studies verifying the effectiveness of its key components.","authors":["Ziyuan Guo","Jie Guo","Zhenghao Chen","Bin Song","Fei Richard Yu"],"pdf_url":"","comment":"12 pages, 6 figures. This paper has been accepted for publication in IEEE Transactions on Multimedia. The final published version will be available via IEEE Xplore"},{"id":"http://arxiv.org/abs/2512.19958v1","updated":"2025-12-23T01:10:25Z","published":"2025-12-23T01:10:25Z","title":"Towards Analysing Invoices and Receipts with Amazon Textract","summary":"This paper presents an evaluation of the AWS Textract in the context of extracting data from receipts. We analyse Textract functionalities using a dataset that includes receipts of varied formats and conditions. Our analysis provided a qualitative view of Textract strengths and limitations. While the receipts totals were consistently detected, we also observed typical issues and irregularities that were often influenced by image quality and layout. Based on the analysis of the observations, we propose mitigation strategies.","authors":["Sneha Oommen","Gabby Sanchez","Cassandra T. Britto","Di Wang","Jordan Chiou","Maria Spichkova"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20781v1","updated":"2025-12-23T21:29:45Z","published":"2025-12-23T21:29:45Z","title":"Soft Filtering: Guiding Zero-shot Composed Image Retrieval with Prescriptive and Proscriptive Constraints","summary":"Composed Image Retrieval (CIR) aims to find a target image that aligns with user intent, expressed through a reference image and a modification text. While Zero-shot CIR (ZS-CIR) methods sidestep the need for labeled training data by leveraging pretrained vision-language models, they often rely on a single fused query that merges all descriptive cues of what the user wants, tending to dilute key information and failing to account for what they wish to avoid. Moreover, current CIR benchmarks assume a single correct target per query, overlooking the ambiguity in modification texts. To address these challenges, we propose Soft Filtering with Textual constraints (SoFT), a training-free, plug-and-play filtering module for ZS-CIR. SoFT leverages multimodal large language models (LLMs) to extract two complementary constraints from the reference-modification pair: prescriptive (must-have) and proscriptive (must-avoid) constraints. These serve as semantic filters that reward or penalize candidate images to re-rank results, without modifying the base retrieval model or adding supervision. In addition, we construct a two-stage dataset pipeline that refines CIR benchmarks. We first identify multiple plausible targets per query to construct multi-target triplets, capturing the open-ended nature of user intent. Then guide multimodal LLMs to rewrite the modification text to focus on one target, while referencing contrastive distractors to ensure precision. This enables more comprehensive and reliable evaluation under varying ambiguity levels. Applied on top of CIReVL, a ZS-CIR retriever, SoFT raises R@5 to 65.25 on CIRR (+12.94), mAP@50 to 27.93 on CIRCO (+6.13), and R@50 to 58.44 on FashionIQ (+4.59), demonstrating broad effectiveness.","authors":["Youjin Jung","Seongwoo Cho","Hyun-seok Min","Sungchul Choi"],"pdf_url":"","comment":"Accepted to AAAI 2026 Workshop on New Frontiers in Information Retrieval"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2512.20618v1","updated":"2025-12-23T18:59:49Z","published":"2025-12-23T18:59:49Z","title":"LongVideoAgent: Multi-Agent Reasoning with Long Videos","summary":"Recent advances in multimodal LLMs and systems that use tools for long-video QA point to the promise of reasoning over hour-long episodes. However, many methods still compress content into lossy summaries or rely on limited toolsets, weakening temporal grounding and missing fine-grained cues. We propose a multi-agent framework in which a master LLM coordinates a grounding agent to localize question-relevant segments and a vision agent to extract targeted textual observations. The master agent plans with a step limit, and is trained with reinforcement learning to encourage concise, correct, and efficient multi-agent cooperation. This design helps the master agent focus on relevant clips via grounding, complements subtitles with visual detail, and yields interpretable trajectories. On our proposed LongTVQA and LongTVQA+ which are episode-level datasets aggregated from TVQA/TVQA+, our multi-agent system significantly outperforms strong non-agent baselines. Experiments also show reinforcement learning further strengthens reasoning and planning for the trained agent. Code and data will be shared at https://longvideoagent.github.io/.","authors":["Runtao Liu","Ziyi Liu","Jiaqi Tang","Yue Ma","Renjie Pi","Jipeng Zhang","Qifeng Chen"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20610v1","updated":"2025-12-23T18:57:53Z","published":"2025-12-23T18:57:53Z","title":"FedPOD: the deployable units of training for federated learning","summary":"This paper proposes FedPOD (Proportionally Orchestrated Derivative) for optimizing learning efficiency and communication cost in federated learning among multiple clients. Inspired by FedPIDAvg, we define a round-wise task for FedPOD to enhance training efficiency. FedPIDAvg achieved performance improvement by incorporating the training loss reduction for prediction entropy as weights using differential terms. Furthermore, by modeling data distribution with a Poisson distribution and using a PID controller, it reduced communication costs even in skewed data distribution. However, excluding participants classified as outliers based on the Poisson distribution can limit data utilization. Additionally, PID controller requires the same participants to be maintained throughout the federated learning process as it uses previous rounds' learning information in the current round. In our approach, FedPOD addresses these issues by including participants excluded as outliers, eliminating dependency on previous rounds' learning information, and applying a method for calculating validation loss at each round. In this challenge, FedPOD presents comparable performance to FedPIDAvg in metrics of Dice score, 0.78, 0.71 and 0.72 for WT, ET and TC in average, and projected convergence score, 0.74 in average. Furthermore, the concept of FedPOD draws inspiration from Kubernetes' smallest computing unit, POD, designed to be compatible with Kubernetes auto-scaling. Extending round-wise tasks of FedPOD to POD units allows flexible design by applying scale-out similar to Kubernetes' auto-scaling. This work demonstrated the potentials of FedPOD to enhance federated learning by improving efficiency, flexibility, and performance in metrics.","authors":["Daewoon Kim","Si Young Yie","Jae Sung Lee"],"pdf_url":"","comment":"12 pages, 12 figures, MICCAI"},{"id":"http://arxiv.org/abs/2512.20607v1","updated":"2025-12-23T18:55:30Z","published":"2025-12-23T18:55:30Z","title":"Saddle-to-Saddle Dynamics Explains A Simplicity Bias Across Neural Network Architectures","summary":"Neural networks trained with gradient descent often learn solutions of increasing complexity over time, a phenomenon known as simplicity bias. Despite being widely observed across architectures, existing theoretical treatments lack a unifying framework. We present a theoretical framework that explains a simplicity bias arising from saddle-to-saddle learning dynamics for a general class of neural networks, incorporating fully-connected, convolutional, and attention-based architectures. Here, simple means expressible with few hidden units, i.e., hidden neurons, convolutional kernels, or attention heads. Specifically, we show that linear networks learn solutions of increasing rank, ReLU networks learn solutions with an increasing number of kinks, convolutional networks learn solutions with an increasing number of convolutional kernels, and self-attention models learn solutions with an increasing number of attention heads. By analyzing fixed points, invariant manifolds, and dynamics of gradient descent learning, we show that saddle-to-saddle dynamics operates by iteratively evolving near an invariant manifold, approaching a saddle, and switching to another invariant manifold. Our analysis also illuminates the effects of data distribution and weight initialization on the duration and number of plateaus in learning, dissociating previously confounding factors. Overall, our theory offers a framework for understanding when and why gradient descent progressively learns increasingly complex solutions.","authors":["Yedi Zhang","Andrew Saxe","Peter E. Latham"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20605v1","updated":"2025-12-23T18:51:50Z","published":"2025-12-23T18:51:50Z","title":"Emergent temporal abstractions in autoregressive models enable hierarchical reinforcement learning","summary":"Large-scale autoregressive models pretrained on next-token prediction and finetuned with reinforcement learning (RL) have achieved unprecedented success on many problem domains. During RL, these models explore by generating new outputs, one token at a time. However, sampling actions token-by-token can result in highly inefficient learning, particularly when rewards are sparse. Here, we show that it is possible to overcome this problem by acting and exploring within the internal representations of an autoregressive model. Specifically, to discover temporally-abstract actions, we introduce a higher-order, non-causal sequence model whose outputs control the residual stream activations of a base autoregressive model. On grid world and MuJoCo-based tasks with hierarchical structure, we find that the higher-order model learns to compress long activation sequence chunks onto internal controllers. Critically, each controller executes a sequence of behaviorally meaningful actions that unfold over long timescales and are accompanied with a learned termination condition, such that composing multiple controllers over time leads to efficient exploration on novel tasks. We show that direct internal controller reinforcement, a process we term \"internal RL\", enables learning from sparse rewards in cases where standard RL finetuning fails. Our results demonstrate the benefits of latent action generation and reinforcement in autoregressive models, suggesting internal RL as a promising avenue for realizing hierarchical RL within foundation models.","authors":["Seijin Kobayashi","Yanick Schimpf","Maximilian Schlegel","Angelika Steger","Maciej Wolczyk","Johannes von Oswald","Nino Scherre","Kaitlin Maile","Guillaume Lajoie","Blake A. Richards","Rif A. Saurous","James Manyika","Blaise Agüera y Arcas","Alexander Meulemans","João Sacramento"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.08855v2","updated":"2025-12-23T18:50:53Z","published":"2025-12-09T17:48:28Z","title":"Reinforcement Learning From State and Temporal Differences","summary":"TD($λ$) with function approximation has proved empirically successful for some complex reinforcement learning problems. For linear approximation, TD($λ$) has been shown to minimise the squared error between the approximate value of each state and the true value. However, as far as policy is concerned, it is error in the relative ordering of states that is critical, rather than error in the state values. We illustrate this point, both in simple two-state and three-state systems in which TD($λ$)--starting from an optimal policy--converges to a sub-optimal policy, and also in backgammon. We then present a modified form of TD($λ$), called STD($λ$), in which function approximators are trained with respect to relative state values on binary decision problems. A theoretical analysis, including a proof of monotonic policy improvement for STD($λ$) in the context of the two-state system, is presented, along with a comparison with Bertsekas' differential training method [1]. This is followed by successful demonstrations of STD($λ$) on the two-state system and a variation on the well known acrobot problem.","authors":["Lex Weaver","Jonathan Baxter"],"pdf_url":"","comment":"Technical Report, Department of Computer Science, Australian National University, May 1999 New version uploaded 2025 after original source taken offline"},{"id":"http://arxiv.org/abs/2505.08961v2","updated":"2025-12-23T18:50:46Z","published":"2025-05-13T21:01:53Z","title":"Learning Informative Attention Weights for Person Re-Identification","summary":"Attention mechanisms have been widely used in deep learning, and recent efforts have been devoted to incorporating attention modules into deep neural networks (DNNs) for person Re-Identification (Re-ID) to enhance their discriminative feature learning capabilities. Existing attention modules, including self-attention and channel attention, learn attention weights that quantify the importance of feature tokens or feature channels. However, existing attention methods do not explicitly ensure that the attention weights are informative for predicting the identity of the person in the input image, and may consequently introduce noisy information from the input image. To address this issue, we propose a novel method termed Reduction of Information Bottleneck loss (RIB), motivated by the principle of the Information Bottleneck (IB). A novel distribution-free and efficient variational upper bound for the IB loss (IBB), which can be optimized by standard SGD, is derived and incorporated into the training loss of the RIB models. RIB is applied to DNNs with self-attention modules through a novel Differentiable Channel Selection Attention module, or DCS-Attention, that selects the most informative channels for computing attention weights, leading to competitive models termed RIB-DCS. RIB is also incorporated into DNNs with existing channel attention modules to promote the learning of informative channel attention weights, leading to models termed RIB-CA. Both RIB-DCS and RIB-CA are applied to fixed neural network backbones and learnable backbones with Differentiable Neural Architecture Search (DNAS). Extensive experiments on multiple person Re-ID benchmarks show that RIB significantly enhances the prediction accuracy of DNNs for person Re-ID, even for the occluded person Re-ID.","authors":["Yancheng Wang","Nebojsa Jojic","Yingzhen Yang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20582v1","updated":"2025-12-23T18:27:41Z","published":"2025-12-23T18:27:41Z","title":"Relu and softplus neural nets as zero-sum turn-based games","summary":"We show that the output of a ReLU neural network can be interpreted as the value of a zero-sum, turn-based, stopping game, which we call the ReLU net game. The game runs in the direction opposite to that of the network, and the input of the network serves as the terminal reward of the game. In fact, evaluating the network is the same as running the Shapley-Bellman backward recursion for the value of the game. Using the expression of the value of the game as an expected total payoff with respect to the path measure induced by the transition probabilities and a pair of optimal policies, we derive a discrete Feynman-Kac-type path-integral formula for the network output. This game-theoretic representation can be used to derive bounds on the output from bounds on the input, leveraging the monotonicity of Shapley operators, and to verify robustness properties using policies as certificates. Moreover, training the neural network becomes an inverse game problem: given pairs of terminal rewards and corresponding values, one seeks transition probabilities and rewards of a game that reproduces them. Finally, we show that a similar approach applies to neural networks with Softplus activation functions, where the ReLU net game is replaced by its entropic regularization.","authors":["Stephane Gaubert","Yiannis Vlassopoulos"],"pdf_url":"","comment":"24 pages, 2 figures"},{"id":"http://arxiv.org/abs/2303.16158v4","updated":"2025-12-23T18:23:14Z","published":"2023-03-25T03:06:43Z","title":"Behavioral Machine Learning? Regularization and Forecast Bias","summary":"Standard forecast efficiency tests interpret violations as evidence of behavioral bias. We show theoretically and empirically that rational forecasters using optimal regularization systematically violate these tests. Machine learning forecasts show near zero bias at one year horizon, but strong overreaction at two years, consistent with predictions from a model of regularization and measurement noise. We provide three complementary tests: experimental variation in regularization parameters, cross-sectional heterogeneity in firm signal quality, and quasi-experimental evidence from ML adoption around 2013. Technically trained analysts shift sharply toward overreaction post-2013. Our findings suggest reported violations may reflect statistical sophistication rather than cognitive failure.","authors":["Murray Z. Frank","Jing Gao","Keer Yang"],"pdf_url":"","comment":"stock analysts, machine learning, behavioral, overreaction"},{"id":"http://arxiv.org/abs/2512.20577v1","updated":"2025-12-23T18:21:24Z","published":"2025-12-23T18:21:24Z","title":"Improving ML Training Data with Gold-Standard Quality Metrics","summary":"Hand-tagged training data is essential to many machine learning tasks. However, training data quality control has received little attention in the literature, despite data quality varying considerably with the tagging exercise. We propose methods to evaluate and enhance the quality of hand-tagged training data using statistical approaches to measure tagging consistency and agreement. We show that agreement metrics give more reliable results if recorded over multiple iterations of tagging, where declining variance in such recordings is an indicator of increasing data quality. We also show one way a tagging project can collect high-quality training data without requiring multiple tags for every work item, and that a tagger burn-in period may not be sufficient for minimizing tagger errors.","authors":["Leslie Barrett","Michael W. Sherman"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20576v1","updated":"2025-12-23T18:20:06Z","published":"2025-12-23T18:20:06Z","title":"Performative Policy Gradient: Optimality in Performative Reinforcement Learning","summary":"Post-deployment machine learning algorithms often influence the environments they act in, and thus shift the underlying dynamics that the standard reinforcement learning (RL) methods ignore. While designing optimal algorithms in this performative setting has recently been studied in supervised learning, the RL counterpart remains under-explored. In this paper, we prove the performative counterparts of the performance difference lemma and the policy gradient theorem in RL, and further introduce the Performative Policy Gradient algorithm (PePG). PePG is the first policy gradient algorithm designed to account for performativity in RL. Under softmax parametrisation, and also with and without entropy regularisation, we prove that PePG converges to performatively optimal policies, i.e. policies that remain optimal under the distribution shifts induced by themselves. Thus, PePG significantly extends the prior works in Performative RL that achieves performative stability but not optimality. Furthermore, our empirical analysis on standard performative RL environments validate that PePG outperforms standard policy gradient algorithms and the existing performative RL algorithms aiming for stability.","authors":["Debabrota Basu","Udvas Das","Brahim Driss","Uddalak Mukherjee"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20573v1","updated":"2025-12-23T18:16:58Z","published":"2025-12-23T18:16:58Z","title":"Fail Fast, Win Big: Rethinking the Drafting Strategy in Speculative Decoding via Diffusion LLMs","summary":"Diffusion Large Language Models (dLLMs) offer fast, parallel token generation, but their standalone use is plagued by an inherent efficiency-quality tradeoff. We show that, if carefully applied, the attributes of dLLMs can actually be a strength for drafters in speculative decoding with autoregressive (AR) verifiers. Our core insight is that dLLM's speed from parallel decoding drastically lowers the risk of costly rejections, providing a practical mechanism to effectively realize the (elusive) lengthy drafts that lead to large speedups with speculative decoding. We present FailFast, a dLLM-based speculative decoding framework that realizes this approach by dynamically adapting its speculation length. It \"fails fast\" by spending minimal compute in hard-to-speculate regions to shrink speculation latency and \"wins big\" by aggressively extending draft lengths in easier regions to reduce verification latency (in many cases, speculating and accepting 70 tokens at a time!). Without any fine-tuning, FailFast delivers lossless acceleration of AR LLMs and achieves up to 4.9$\\times$ speedup over vanilla decoding, 1.7$\\times$ over the best naive dLLM drafter, and 1.4$\\times$ over EAGLE-3 across diverse models and workloads. We open-source FailFast at https://github.com/ruipeterpan/failfast.","authors":["Rui Pan","Zhuofu Chen","Ravi Netravali"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20563v1","updated":"2025-12-23T18:07:43Z","published":"2025-12-23T18:07:43Z","title":"LEAD: Minimizing Learner-Expert Asymmetry in End-to-End Driving","summary":"Simulators can generate virtually unlimited driving data, yet imitation learning policies in simulation still struggle to achieve robust closed-loop performance. Motivated by this gap, we empirically study how misalignment between privileged expert demonstrations and sensor-based student observations can limit the effectiveness of imitation learning. More precisely, experts have significantly higher visibility (e.g., ignoring occlusions) and far lower uncertainty (e.g., knowing other vehicles' actions), making them difficult to imitate reliably. Furthermore, navigational intent (i.e., the route to follow) is under-specified in student models at test time via only a single target point. We demonstrate that these asymmetries can measurably limit driving performance in CARLA and offer practical interventions to address them. After careful modifications to narrow the gaps between expert and student, our TransFuser v6 (TFv6) student policy achieves a new state of the art on all major publicly available CARLA closed-loop benchmarks, reaching 95 DS on Bench2Drive and more than doubling prior performances on Longest6~v2 and Town13. Additionally, by integrating perception supervision from our dataset into a shared sim-to-real pipeline, we show consistent gains on the NAVSIM and Waymo Vision-Based End-to-End driving benchmarks. Our code, data, and models are publicly available at https://github.com/autonomousvision/lead.","authors":["Long Nguyen","Micha Fauth","Bernhard Jaeger","Daniel Dauner","Maximilian Igl","Andreas Geiger","Kashyap Chitta"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20562v1","updated":"2025-12-23T18:05:55Z","published":"2025-12-23T18:05:55Z","title":"Shallow Neural Networks Learn Low-Degree Spherical Polynomials with Learnable Channel Attention","summary":"We study the problem of learning a low-degree spherical polynomial of degree $\\ell_0 = Θ(1) \\ge 1$ defined on the unit sphere in $\\RR^d$ by training an over-parameterized two-layer neural network (NN) with channel attention in this paper. Our main result is the significantly improved sample complexity for learning such low-degree polynomials. We show that, for any regression risk $\\eps \\in (0,1)$, a carefully designed two-layer NN with channel attention and finite width of $m \\ge Θ({n^4 \\log (2n/δ)}/{d^{2\\ell_0}})$ trained by the vanilla gradient descent (GD) requires the lowest sample complexity of $n \\asymp Θ(d^{\\ell_0}/\\eps)$ with probability $1-δ$ for every $δ\\in (0,1)$, in contrast with the representative sample complexity $Θ\\pth{d^{\\ell_0} \\max\\set{\\eps^{-2},\\log d}}$, where $n$ is the training daata size. Moreover, such sample complexity is not improvable since the trained network renders a sharp rate of the nonparametric regression risk of the order $Θ(d^{\\ell_0}/{n})$ with probability at least $1-δ$. On the other hand, the minimax optimal rate for the regression risk with a kernel of rank $Θ(d^{\\ell_0})$ is $Θ(d^{\\ell_0}/{n})$, so that the rate of the nonparametric regression risk of the network trained by GD is minimax optimal. The training of the two-layer NN with channel attention consists of two stages. In Stage 1, a provable learnable channel selection algorithm identifies the ground-truth channel number $\\ell_0$ from the initial $L \\ge \\ell_0$ channels in the first-layer activation, with high probability. This learnable selection is achieved by an efficient one-step GD update on both layers, enabling feature learning for low-degree polynomial targets. In Stage 2, the second layer is trained by standard GD using the activation function with the selected channels.","authors":["Yingzhen Yang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2511.13912v2","updated":"2025-12-23T18:00:52Z","published":"2025-11-17T21:06:52Z","title":"Compute-in-Memory Implementation of State Space Models for Event Sequence Processing","summary":"State space models (SSMs) have recently emerged as a powerful framework for long sequence processing, outperforming traditional methods on diverse benchmarks. Fundamentally, SSMs can generalize both recurrent and convolutional networks and have been shown to even capture key functions of biological systems. Here we report an approach to implement SSMs in energy-efficient compute-in-memory (CIM) hardware to achieve real-time, event-driven processing. Our work re-parameterizes the model to function with real-valued coefficients and shared decay constants, reducing the complexity of model mapping onto practical hardware systems. By leveraging device dynamics and diagonalized state transition parameters, the state evolution can be natively implemented in crossbar-based CIM systems combined with memristors exhibiting short-term memory effects. Through this algorithm and hardware co-design, we show the proposed system offers both high accuracy and high energy efficiency while supporting fully asynchronous processing for event-based vision and audio tasks.","authors":["Xiaoyu Zhang","Mingtao Hu","Sen Lu","Soohyeon Kim","Eric Yeu-Jer Lee","Yuyang Liu","Wei D. Lu"],"pdf_url":"","comment":"Xiaoyu Zhang and Mingtao Hu contributed equally to this work"},{"id":"http://arxiv.org/abs/2510.07191v2","updated":"2025-12-23T17:45:29Z","published":"2025-10-08T16:25:04Z","title":"Resolution scaling governs DINOv3 transfer performance in chest radiograph classification","summary":"Self-supervised learning (SSL) has advanced visual representation learning, but its value in chest radiography, a high-volume imaging modality with fine-grained findings, remains unclear. Meta's DINOv3 extends earlier SSL models through Gram-anchored self-distillation. Whether these design choices improve transfer learning for chest radiography has not been systematically tested. We benchmarked DINOv3 against DINOv2 and ImageNet initialization across seven datasets (n>814,000). Two representative backbones were evaluated: ViT-B/16 and ConvNeXt-B. Images were analyzed at 224x224, 512x512, and 1024x1024 pixels. We additionally assessed frozen features from a 7B model. The primary outcome was mean AUROC across labels. At 224x224, DINOv3 and DINOv2 achieved comparable performance on adult datasets. Increasing resolution to 512x512 yielded consistent improvements for DINOv3 over both DINOv2 and ImageNet. In contrast, results in pediatric cohort showed no differences across initializations. Across all settings, ConvNeXt-B outperformed ViT-B/16. Models using frozen DINOv3-7B features underperformed relative to fully finetuned 86-89M-parameter backbones, highlighting the importance of domain adaptation. Scaling to 1024x1024 did not further improve accuracy. Resolution-related gains were most evident for boundary-dependent and small focal abnormalities. In chest radiography, higher input resolution is critical for leveraging the benefits of modern self-supervised models. 512x512 pixels represent a practical upper limit where DINOv3-initialized ConvNeXt-B networks provide the strongest performance, while larger inputs offer minimal return on cost. Clinically, these findings support use of finetuned, mid-sized backbones at 512x512 for chest radiograph interpretation, with the greatest gains expected in detecting subtle or boundary-centered lesions relevant to emergency and critical care settings.","authors":["Soroosh Tayebi Arasteh","Mina Shaigan","Christiane Kuhl","Jakob Nikolas Kather","Sven Nebelung","Daniel Truhn"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20533v1","updated":"2025-12-23T17:24:39Z","published":"2025-12-23T17:24:39Z","title":"Over-the-Air Goal-Oriented Communications","summary":"Goal-oriented communications offer an attractive alternative to the Shannon-based communication paradigm, where the data is never reconstructed at the Receiver (RX) side. Rather, focusing on the case of edge inference, the Transmitter (TX) and the RX cooperate to exchange features of the input data that will be used to predict an unseen attribute of them, leveraging information from collected data sets. This chapter demonstrates that the wireless channel can be used to perform computations over the data, when equipped with programmable metasurfaces. The end-to-end system of the TX, RX, and MS-based channel is treated as a single deep neural network which is trained through backpropagation to perform inference on unseen data. Using Stacked Intelligent Metasurfaces (SIM), it is shown that this Metasurfaces-Integrated Neural Network (MINN) can achieve performance comparable to fully digital neural networks under various system parameters and data sets. By offloading computations onto the channel itself, important benefits may be achieved in terms of energy consumption, arising from reduced computations at the transceivers and smaller transmission power required for successful inference.","authors":["Kyriakos Stylianopoulos","Paolo Di Lorenzo","George C. Alexandropoulos"],"pdf_url":"","comment":"35 pages, 9 figures. Book chapter"},{"id":"http://arxiv.org/abs/2512.20523v1","updated":"2025-12-23T17:14:14Z","published":"2025-12-23T17:14:14Z","title":"ScoreMatchingRiesz: Auto-DML with Infinitesimal Classification","summary":"This study proposes Riesz representer estimation methods based on score matching. The Riesz representer is a key component in debiased machine learning for constructing $\\sqrt{n}$-consistent and efficient estimators in causal inference and structural parameter estimation. To estimate the Riesz representer, direct approaches have garnered attention, such as Riesz regression and the covariate balancing propensity score. These approaches can also be interpreted as variants of direct density ratio estimation (DRE) in several applications such as average treatment effect estimation. In DRE, it is well known that flexible models can easily overfit the observed data due to the estimand and the form of the loss function. To address this issue, recent work has proposed modeling the density ratio as a product of multiple intermediate density ratios and estimating it using score-matching techniques, which are often used in the diffusion model literature. We extend score-matching-based DRE methods to Riesz representer estimation. Our proposed method not only mitigates overfitting but also provides insights for causal inference by bridging marginal effects and average policy effects through time score functions.","authors":["Masahiro Kato"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20514v1","updated":"2025-12-23T17:02:35Z","published":"2025-12-23T17:02:35Z","title":"Explainable time-series forecasting with sampling-free SHAP for Transformers","summary":"Time-series forecasts are essential for planning and decision-making in many domains. Explainability is key to building user trust and meeting transparency requirements. Shapley Additive Explanations (SHAP) is a popular explainable AI framework, but it lacks efficient implementations for time series and often assumes feature independence when sampling counterfactuals. We introduce SHAPformer, an accurate, fast and sampling-free explainable time-series forecasting model based on the Transformer architecture. It leverages attention manipulation to make predictions based on feature subsets. SHAPformer generates explanations in under one second, several orders of magnitude faster than the SHAP Permutation Explainer. On synthetic data with ground truth explanations, SHAPformer provides explanations that are true to the data. Applied to real-world electrical load data, it achieves competitive predictive performance and delivers meaningful local and global insights, such as identifying the past load as the key predictor and revealing a distinct model behavior during the Christmas period.","authors":["Matthias Hertel","Sebastian Pütz","Ralf Mikut","Veit Hagenmeyer","Benjamin Schäfer"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20513v1","updated":"2025-12-23T17:02:17Z","published":"2025-12-23T17:02:17Z","title":"Recurrent Off-Policy Deep Reinforcement Learning Doesn't Have to be Slow","summary":"Recurrent off-policy deep reinforcement learning models achieve state-of-the-art performance but are often sidelined due to their high computational demands. In response, we introduce RISE (Recurrent Integration via Simplified Encodings), a novel approach that can leverage recurrent networks in any image-based off-policy RL setting without significant computational overheads via using both learnable and non-learnable encoder layers. When integrating RISE into leading non-recurrent off-policy RL algorithms, we observe a 35.6% human-normalized interquartile mean (IQM) performance improvement across the Atari benchmark. We analyze various implementation strategies to highlight the versatility and potential of our proposed framework.","authors":["Tyler Clark","Christine Evers","Jonathon Hare"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2509.09719v2","updated":"2025-12-23T16:32:05Z","published":"2025-09-09T22:16:24Z","title":"Spectral Bottleneck in Sinusoidal Representation Networks: Noise is All You Need","summary":"This work identifies and attempts to address a fundamental limitation of implicit neural representations with sinusoidal activation. The fitting error of SIRENs is highly sensitive to the target frequency content and to the choice of initialization. In extreme cases, this sensitivity leads to a spectral bottleneck that can result in a zero-valued output. This phenomenon is characterized by analyzing the evolution of activation spectra and the empirical neural tangent kernel (NTK) during the training process. An unfavorable distribution of energy across frequency modes was noted to give rise to this failure mode. Furthermore, the effect of Gaussian perturbations applied to the baseline uniformly initialized weights is examined, showing how these perturbations influence activation spectra and the NTK eigenbasis of SIREN. Overall, initialization emerges as a central factor governing the evolution of SIRENs, indicating the need for adaptive, target-aware strategies as the target length increases and fine-scale detail becomes essential. The proposed weight initialization scheme (WINNER) represents a simple ad hoc step in this direction and demonstrates that fitting accuracy can be significantly improved by modifying the spectral profile of network activations through a target-aware initialization. The approach achieves state-of-the-art performance on audio fitting tasks and yields notable improvements in image fitting tasks.","authors":["Hemanth Chandravamsi","Dhanush V. Shenoy","Itay Zinn","Ziv Chen","Shimon Pisnoy","Steven H. Frankel"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2508.12833v2","updated":"2025-12-23T16:25:32Z","published":"2025-08-18T11:17:59Z","title":"Toward Storage-Aware Learning with Compressed Data An Empirical Exploratory Study on JPEG","summary":"On-device machine learning is often constrained by limited storage, particularly in continuous data collection scenarios. This paper presents an empirical study on storage-aware learning, focusing on the trade-off between data quantity and quality via compression. We demonstrate that naive strategies, such as uniform data dropping or one-size-fits-all compression, are suboptimal. Our findings further reveal that data samples exhibit varying sensitivities to compression, supporting the feasibility of a sample-wise adaptive compression strategy. These insights provide a foundation for developing a new class of storage-aware learning systems. The primary contribution of this work is the systematic characterization of this under-explored challenge, offering valuable insights that advance the understanding of storage-aware learning.","authors":["Kichang Lee","Songkuk Kim","JaeYeon Park","JeongGil Ko"],"pdf_url":"","comment":"6pages, 6figures"},{"id":"http://arxiv.org/abs/2401.09986v3","updated":"2025-12-23T16:22:09Z","published":"2024-01-18T14:02:23Z","title":"Improving Local Training in Federated Learning via Temperature Scaling","summary":"Federated learning is inherently hampered by data heterogeneity: non-i.i.d. training data over local clients. We propose a novel model training approach for federated learning, FLex&Chill, which exploits the Logit Chilling method. Through extensive evaluations, we demonstrate that, in the presence of non-i.i.d. data characteristics inherent in federated learning systems, this approach can expedite model convergence and improve inference accuracy. Quantitatively, from our experiments, we observe up to 6X improvement in the global federated learning model convergence time, and up to 3.37% improvement in inference accuracy.","authors":["Kichang Lee","Pei Zhang","Songkuk Kim","JeongGil Ko"],"pdf_url":"","comment":"56 pages"},{"id":"http://arxiv.org/abs/2506.09207v3","updated":"2025-12-23T16:20:49Z","published":"2025-06-10T19:57:35Z","title":"mLaSDI: Multi-stage latent space dynamics identification","summary":"Accurately solving partial differential equations (PDEs) is essential across many scientific disciplines. However, high-fidelity solvers can be computationally prohibitive, motivating the development of reduced-order models (ROMs). Recently, Latent Space Dynamics Identification (LaSDI) was proposed as a data-driven, non-intrusive ROM framework. LaSDI compresses the training data via an autoencoder and learns user-specified ordinary differential equations (ODEs), governing the latent dynamics, enabling rapid predictions for unseen parameters. While LaSDI has produced effective ROMs for numerous problems, the autoencoder must simultaneously reconstruct the training data and satisfy the imposed latent dynamics, which are often competing objectives that limit accuracy, particularly for complex or high-frequency phenomena. To address this limitation, we propose multi-stage Latent Space Dynamics Identification (mLaSDI). With mLaSDI, we train LaSDI sequentially in stages. After training the initial autoencoder, we train additional decoders which map the latent trajectories to residuals from previous stages. This staged residual learning, combined with periodic activation functions, enables recovery of high-frequency content without sacrificing interpretability of the latent dynamics. Numerical experiments on a multiscale oscillating system, unsteady wake flow, and the 1D-1V Vlasov equation demonstrate that mLaSDI achieves significantly lower reconstruction and prediction errors, often by an order of magnitude, while requiring less training time and reduced hyperparameter tuning compared to standard LaSDI.","authors":["William Anderson","Seung Whan Chung","Robert Stephany","Youngsoo Choi"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20460v1","updated":"2025-12-23T15:55:10Z","published":"2025-12-23T15:55:10Z","title":"The Aligned Economic Index & The State Switching Model","summary":"A growing empirical literature suggests that equity-premium predictability is state dependent, with much of the forecasting power concentrated around recessionary periods \\parencite{Henkel2011,DanglHalling2012,Devpura2018}. I study U.S. stock return predictability across economic regimes and document strong evidence of time-varying expected returns across both expansionary and contractionary states. I contribute in two ways. First, I introduce a state-switching predictive regression in which the market state is defined in real time using the slope of the yield curve. Relative to the standard one-state predictive regression, the state-switching specification increases both in-sample and out-of-sample performance for the set of popular predictors considered by \\textcite{WelchGoyal2008}, improving the out-of-sample performance of most predictors in economically meaningful ways. Second, I propose a new aggregate predictor, the Aligned Economic Index, constructed via partial least squares (PLS). Under the state-switching model, the Aligned Economic Index exhibits statistically and economically significant predictive power in sample and out of sample, and it outperforms widely used benchmark predictors and alternative predictor-combination methods.","authors":["Ilias Aarab"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2509.22358v2","updated":"2025-12-23T15:51:07Z","published":"2025-09-26T13:53:56Z","title":"Stochastic activations","summary":"We introduce stochastic activations. This novel strategy randomly selects between several non-linear functions in the feed-forward layer of a large language model. In particular, we choose between SILU or RELU depending on a Bernoulli draw. This strategy circumvents the optimization problem associated with RELU, namely, the constant shape for negative inputs that prevents the gradient flow. We leverage this strategy in two ways:\n  (1) We use stochastic activations during pre-training and fine-tune the model with RELU, which is used at inference time to provide sparse latent vectors. This reduces the inference FLOPs and translates into a significant speedup in the CPU. Interestingly, this leads to much better results than training from scratch with the RELU activation function.\n  (2) We evaluate stochastic activations for generation. This strategy performs reasonably well: it is only slightly inferior to the best deterministic non-linearity, namely SILU combined with temperature scaling. This offers an alternative to existing strategies by providing a controlled way to increase the diversity of the generated text.","authors":["Maria Lomeli","Matthijs Douze","Gergely Szilvasy","Loic Cabannes","Jade Copet","Sainbayar Sukhbaatar","Jason Weston","Gabriel Synnaeve","Pierre-Emmanuel Mazaré","Hervé Jégou"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20438v1","updated":"2025-12-23T15:27:28Z","published":"2025-12-23T15:27:28Z","title":"Machine Learning to Predict Digital Frustration from Clickstream Data","summary":"Many businesses depend on their mobile apps and websites, so user frustration while trying to complete a task on these channels can cause lost sales and complaints. In this research, I use clickstream data from a real e-commerce site to predict whether a session is frustrated or not. Frustration is defined using certain rules based on rage bursts, back and forth navigation (U turns), cart churn, search struggle, and long wandering sessions, and applies these rules to 5.4 million raw clickstream events (304,881 sessions). From each session, I build tabular features and train standard classifier models. I also use the full event sequence to train a discriminative LSTM classifier. XGBoost reaches about 90% accuracy, ROC AUC of 0.9579, while the LSTM performs best with about 91% accuracy and a ROC AUC of 0.9705. Finally, the research shows that with only the first 20 to 30 interactions, the LSTM already predicts frustration reliably.","authors":["Jibin Joseph"],"pdf_url":"","comment":"17 pages, 5 figures"},{"id":"http://arxiv.org/abs/2310.05805v3","updated":"2025-12-23T15:25:13Z","published":"2023-10-09T15:43:46Z","title":"Boosted Control Functions: Distribution generalization and invariance in confounded models","summary":"Modern machine learning methods and the availability of large-scale data have significantly advanced our ability to predict target quantities from large sets of covariates. However, these methods often struggle under distributional shifts, particularly in the presence of hidden confounding. While the impact of hidden confounding is well-studied in causal effect estimation, e.g., instrumental variables, its implications for prediction tasks under shifting distributions remain underexplored. This work addresses this gap by introducing a strong notion of invariance that, unlike existing weaker notions, allows for distribution generalization even in the presence of nonlinear, non-identifiable structural functions. Central to this framework is the Boosted Control Function (BCF), a novel, identifiable target of inference that satisfies the proposed strong invariance notion and is provably worst-case optimal under distributional shifts. The theoretical foundation of our work lies in Simultaneous Equation Models for Distribution Generalization (SIMDGs), which bridge machine learning with econometrics by describing data-generating processes under distributional shifts. To put these insights into practice, we propose the ControlTwicing algorithm to estimate the BCF using nonparametric machine-learning techniques and study its generalization performance on synthetic and real-world datasets compared to robust and empirical risk minimization approaches.","authors":["Nicola Gnecco","Jonas Peters","Sebastian Engelke","Niklas Pfister"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.07490v3","updated":"2025-12-23T15:22:53Z","published":"2025-12-08T12:17:40Z","title":"Efficient Low-Tubal-Rank Tensor Estimation via Alternating Preconditioned Gradient Descent","summary":"The problem of low-tubal-rank tensor estimation is a fundamental task with wide applications across high-dimensional signal processing, machine learning, and image science. Traditional approaches tackle such a problem by performing tensor singular value decomposition, which is computationally expensive and becomes infeasible for large-scale tensors. Recent approaches address this issue by factorizing the tensor into two smaller factor tensors and solving the resulting problem using gradient descent. However, this kind of approach requires an accurate estimate of the tensor rank, and when the rank is overestimated, the convergence of gradient descent and its variants slows down significantly or even diverges. To address this problem, we propose an Alternating Preconditioned Gradient Descent (APGD) algorithm, which accelerates convergence in the over-parameterized setting by adding a preconditioning term to the original gradient and updating these two factors alternately. Based on certain geometric assumptions on the objective function, we establish linear convergence guarantees for more general low-tubal-rank tensor estimation problems. Then we further analyze the specific cases of low-tubal-rank tensor factorization and low-tubal-rank tensor recovery. Our theoretical results show that APGD achieves linear convergence even under over-parameterization, and the convergence rate is independent of the tensor condition number. Extensive simulations on synthetic data are carried out to validate our theoretical assertions.","authors":["Zhiyu Liu","Zhi Han","Yandong Tang","Jun Fan","Yao Wang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17586v2","updated":"2025-12-23T15:11:27Z","published":"2025-12-19T13:52:19Z","title":"Learning Safe Autonomous Driving Policies Using Predictive Safety Representations","summary":"Safe reinforcement learning (SafeRL) is a prominent paradigm for autonomous driving, where agents are required to optimize performance under strict safety requirements. This dual objective creates a fundamental tension, as overly conservative policies limit driving efficiency while aggressive exploration risks safety violations. The Safety Representations for Safer Policy Learning (SRPL) framework addresses this challenge by equipping agents with a predictive model of future constraint violations and has shown promise in controlled environments. This paper investigates whether SRPL extends to real-world autonomous driving scenarios. Systematic experiments on the Waymo Open Motion Dataset (WOMD) and NuPlan demonstrate that SRPL can improve the reward-safety tradeoff, achieving statistically significant improvements in success rate (effect sizes r = 0.65-0.86) and cost reduction (effect sizes r = 0.70-0.83), with p < 0.05 for observed improvements. However, its effectiveness depends on the underlying policy optimizer and the dataset distribution. The results further show that predictive safety representations play a critical role in improving robustness to observation noise. Additionally, in zero-shot cross-dataset evaluation, SRPL-augmented agents demonstrate improved generalization compared to non-SRPL methods. These findings collectively demonstrate the potential of predictive safety representations to strengthen SafeRL for autonomous driving.","authors":["Mahesh Keswani","Raunak Bhattacharyya"],"pdf_url":"","comment":"8 pages, 4 figures"},{"id":"http://arxiv.org/abs/2505.09710v3","updated":"2025-12-23T15:03:59Z","published":"2025-05-14T18:10:49Z","title":"Training Deep Morphological Neural Networks as Universal Approximators","summary":"We investigate deep morphological neural networks (DMNNs). We demonstrate that despite their inherent non-linearity, \"linear\" activations are essential for DMNNs. To preserve their inherent sparsity, we propose architectures that constraint the parameters of the \"linear\" activations: For the first (resp. second) architecture, we work under the constraint that the majority of parameters (resp. learnable parameters) should be part of morphological operations. We improve the generalization ability of our networks via residual connections and weight dropout. Our proposed networks can be successfully trained, and are more prunable than linear networks. To the best of our knowledge, we are the first to successfully train DMNNs under such constraints. Finally, we propose a hybrid network architecture combining linear and morphological layers, showing empirically that the inclusion of morphological layers significantly accelerates the convergence of gradient descent with large batches.","authors":["Konstantinos Fotopoulos","Petros Maragos"],"pdf_url":"","comment":"v3: Added acknowledgments"},{"id":"http://arxiv.org/abs/2512.20420v1","updated":"2025-12-23T15:02:12Z","published":"2025-12-23T15:02:12Z","title":"Simplifying Multi-Task Architectures Through Task-Specific Normalization","summary":"Multi-task learning (MTL) aims to leverage shared knowledge across tasks to improve generalization and parameter efficiency, yet balancing resources and mitigating interference remain open challenges. Architectural solutions often introduce elaborate task-specific modules or routing schemes, increasing complexity and overhead. In this work, we show that normalization layers alone are sufficient to address many of these challenges. Simply replacing shared normalization with task-specific variants already yields competitive performance, questioning the need for complex designs. Building on this insight, we propose Task-Specific Sigmoid Batch Normalization (TS$σ$BN), a lightweight mechanism that enables tasks to softly allocate network capacity while fully sharing feature extractors. TS$σ$BN improves stability across CNNs and Transformers, matching or exceeding performance on NYUv2, Cityscapes, CelebA, and PascalContext, while remaining highly parameter-efficient. Moreover, its learned gates provide a natural framework for analyzing MTL dynamics, offering interpretable insights into capacity allocation, filter specialization, and task relationships. Our findings suggest that complex MTL architectures may be unnecessary and that task-specific normalization offers a simple, interpretable, and efficient alternative.","authors":["Mihai Suteu","Ovidiu Serban"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20407v1","updated":"2025-12-23T14:55:08Z","published":"2025-12-23T14:55:08Z","title":"AUDRON: A Deep Learning Framework with Fused Acoustic Signatures for Drone Type Recognition","summary":"Unmanned aerial vehicles (UAVs), commonly known as drones, are increasingly used across diverse domains, including logistics, agriculture, surveillance, and defense. While these systems provide numerous benefits, their misuse raises safety and security concerns, making effective detection mechanisms essential. Acoustic sensing offers a low-cost and non-intrusive alternative to vision or radar-based detection, as drone propellers generate distinctive sound patterns. This study introduces AUDRON (AUdio-based Drone Recognition Network), a hybrid deep learning framework for drone sound detection, employing a combination of Mel-Frequency Cepstral Coefficients (MFCC), Short-Time Fourier Transform (STFT) spectrograms processed with convolutional neural networks (CNNs), recurrent layers for temporal modeling, and autoencoder-based representations. Feature-level fusion integrates complementary information before classification. Experimental evaluation demonstrates that AUDRON effectively differentiates drone acoustic signatures from background noise, achieving high accuracy while maintaining generalizability across varying conditions. AUDRON achieves 98.51 percent and 97.11 percent accuracy in binary and multiclass classification. The results highlight the advantage of combining multiple feature representations with deep learning for reliable acoustic drone detection, suggesting the framework's potential for deployment in security and surveillance applications where visual or radar sensing may be limited.","authors":["Rajdeep Chatterjee","Sudip Chakrabarty","Trishaani Acharjee","Deepanjali Mishra"],"pdf_url":"","comment":"Presented at the 2025 IEEE 22nd India Council International Conference (INDICON). 6 pages, 3 figures"},{"id":"http://arxiv.org/abs/2512.20403v1","updated":"2025-12-23T14:46:43Z","published":"2025-12-23T14:46:43Z","title":"BRIDGE: Budget-aware Reasoning via Intermediate Distillation with Guided Examples","summary":"Distilling knowledge from large proprietary models (e.g., GPT-4) to tiny deployable models (less than 1B parameters) faces a critical capacity-budget trap: the 1000x capacity gap between teachers and students prevents effective direct transfer, while API costs prohibit extensive data collection. We introduce BRIDGE (Budget-Aware Reasoning via Intermediate Distillation), a two-phase framework that resolves these constraints through strategic intermediation and budget asymmetry. In Phase 1, a mid-sized Teacher Assistant (TA; e.g., about 7B) learns from the black-box teacher on a strictly limited subset of data (e.g., 3-5%), selected via a zero-API-cost pipeline that balances entropic difficulty and semantic diversity using only local TA inference. In Phase 2, we exploit this asymmetry-teacher queries are expensive, whereas TA inference is free to amplify supervision: the refined TA generates synthetic rationales for the full dataset to train the tiny student. Crucially, we apply an instruction-tuning curriculum to establish behavioral alignment in the tiny student before transferring reasoning. Our theoretical analysis shows that BRIDGE yields tighter generalization bounds than direct distillation when data is abundant. Experiments across medical, legal, and financial benchmarks demonstrate consistent improvements: BRIDGE delivers student performance gains of 28-41%, closing the capability gap with proprietary teachers by 12-16% while using 10x fewer teacher queries. Notably, BRIDGE defies the conventional cost-performance frontier, surpassing direct distillation baselines that use 100% of the budget while consuming only 5% of the resources.","authors":["Xuan-An Le","Minh-Nam Tran","Son Nguyen"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.18689v2","updated":"2025-12-23T14:46:41Z","published":"2025-12-21T10:55:32Z","title":"Fusion of Multiscale Features Via Centralized Sparse-attention Network for EEG Decoding","summary":"Electroencephalography (EEG) signal decoding is a key technology that translates brain activity into executable commands, laying the foundation for direct brain-machine interfacing and intelligent interaction. To address the inherent spatiotemporal heterogeneity of EEG signals, this paper proposes a multi-branch parallel architecture, where each temporal scale is equipped with an independent spatial feature extraction module. To further enhance multi-branch feature fusion, we propose a Fusion of Multiscale Features via Centralized Sparse-attention Network (EEG-CSANet), a centralized sparse-attention network. It employs a main-auxiliary branch architecture, where the main branch models core spatiotemporal patterns via multiscale self-attention, and the auxiliary branch facilitates efficient local interactions through sparse cross-attention. Experimental results show that EEG-CSANet achieves state-of-the-art (SOTA) performance across five public datasets (BCIC-IV-2A, BCIC-IV-2B, HGD, SEED, and SEED-VIG), with accuracies of 88.54%, 91.09%, 99.43%, 96.03%, and 90.56%, respectively. Such performance demonstrates its strong adaptability and robustness across various EEG decoding tasks. Moreover, extensive ablation studies are conducted to enhance the interpretability of EEG-CSANet. In the future, we hope that EEG-CSANet could serve as a promising baseline model in the field of EEG signal decoding. The source code is publicly available at: https://github.com/Xiangrui-Cai/EEG-CSANet","authors":["Xiangrui Cai","Shaocheng Ma","Lei Cao","Jie Li","Tianyu Liu","Yilin Dong"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20399v1","updated":"2025-12-23T14:40:08Z","published":"2025-12-23T14:40:08Z","title":"GeoTransolver: Learning Physics on Irregumar Domains Using Multi-scale Geometry Aware Physics Attention Transformer","summary":"We present GeoTransolver, a Multiscale Geometry-Aware Physics Attention Transformer for CAE that replaces standard attention with GALE, coupling physics-aware self-attention on learned state slices with cross-attention to a shared geometry/global/boundary-condition context computed from multi-scale ball queries (inspired by DoMINO) and reused in every block. Implemented and released in NVIDIA PhysicsNeMo, GeoTransolver persistently projects geometry, global and boundary condition parameters into physical state spaces to anchor latent computations to domain structure and operating regimes. We benchmark GeoTransolver on DrivAerML, Luminary SHIFT-SUV, and Luminary SHIFT-Wing, comparing against Domino, Transolver (as released in PhysicsNeMo), and literature-reported AB-UPT, and evaluate drag/lift R2 and Relative L1 errors for field variables. GeoTransolver delivers better accuracy, improved robustness to geometry/regime shifts, and favorable data efficiency; we include ablations on DrivAerML and qualitative results such as contour plots and design trends for the best GeoTransolver models. By unifying multiscale geometry-aware context with physics-based attention in a scalable transformer, GeoTransolver advances operator learning for high-fidelity surrogate modeling across complex, irregular domains and non-linear physical regimes.","authors":["Corey Adams","Rishikesh Ranade","Ram Cherukuri","Sanjay Choudhry"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20368v1","updated":"2025-12-23T13:53:53Z","published":"2025-12-23T13:53:53Z","title":"Avoiding the Price of Adaptivity: Inference in Linear Contextual Bandits via Stability","summary":"Statistical inference in contextual bandits is complicated by the adaptive, non-i.i.d. nature of the data. A growing body of work has shown that classical least-squares inference may fail under adaptive sampling, and that constructing valid confidence intervals for linear functionals of the model parameter typically requires paying an unavoidable inflation of order $\\sqrt{d \\log T}$. This phenomenon -- often referred to as the price of adaptivity -- highlights the inherent difficulty of reliable inference under general contextual bandit policies.\n  A key structural property that circumvents this limitation is the \\emph{stability} condition of Lai and Wei, which requires the empirical feature covariance to concentrate around a deterministic limit. When stability holds, the ordinary least-squares estimator satisfies a central limit theorem, and classical Wald-type confidence intervals -- designed for i.i.d. data -- become asymptotically valid even under adaptation, \\emph{without} incurring the $\\sqrt{d \\log T}$ price of adaptivity.\n  In this paper, we propose and analyze a penalized EXP4 algorithm for linear contextual bandits. Our first main result shows that this procedure satisfies the Lai--Wei stability condition and therefore admits valid Wald-type confidence intervals for linear functionals. Our second result establishes that the same algorithm achieves regret guarantees that are minimax optimal up to logarithmic factors, demonstrating that stability and statistical efficiency can coexist within a single contextual bandit method. Finally, we complement our theory with simulations illustrating the empirical normality of the resulting estimators and the sharpness of the corresponding confidence intervals.","authors":["Samya Praharaj","Koulik Khamaru"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20363v1","updated":"2025-12-23T13:46:38Z","published":"2025-12-23T13:46:38Z","title":"Clust-PSI-PFL: A Population Stability Index Approach for Clustered Non-IID Personalized Federated Learning","summary":"Federated learning (FL) supports privacy-preserving, decentralized machine learning (ML) model training by keeping data on client devices. However, non-independent and identically distributed (non-IID) data across clients biases updates and degrades performance. To alleviate these issues, we propose Clust-PSI-PFL, a clustering-based personalized FL framework that uses the Population Stability Index (PSI) to quantify the level of non-IID data. We compute a weighted PSI metric, $WPSI^L$, which we show to be more informative than common non-IID metrics (Hellinger, Jensen-Shannon, and Earth Mover's distance). Using PSI features, we form distributionally homogeneous groups of clients via K-means++; the number of optimal clusters is chosen by a systematic silhouette-based procedure, typically yielding few clusters with modest overhead. Across six datasets (tabular, image, and text modalities), two partition protocols (Dirichlet with parameter $α$ and Similarity with parameter S), and multiple client sizes, Clust-PSI-PFL delivers up to 18% higher global accuracy than state-of-the-art baselines and markedly improves client fairness by a relative improvement of 37% under severe non-IID data. These results establish PSI-guided clustering as a principled, lightweight mechanism for robust PFL under label skew.","authors":["Daniel M. Jimenez-Gutierrez","Mehrdad Hassanzadeh","Aris Anagnostopoulos","Ioannis Chatzigiannakis","Andrea Vitaletti"],"pdf_url":"","comment":"Accepted for publication to the 40th IEEE International Parallel & Distributed Processing Symposium (IPDPS 2026)"},{"id":"http://arxiv.org/abs/2412.11800v3","updated":"2025-12-23T13:43:57Z","published":"2024-12-16T14:11:28Z","title":"Scalable Temporal Anomaly Causality Discovery in Large Systems: Achieving Computational Efficiency with Binary Anomaly Flag Data","summary":"Extracting anomaly causality facilitates diagnostics once monitoring systems detect system faults. Identifying anomaly causes in large systems involves investigating a broader set of monitoring variables across multiple subsystems. However, learning graphical causal models (GCMs) comes with a significant computational burden that restrains the applicability of most existing methods in real-time and large-scale deployments. In addition, modern monitoring applications for large systems often generate large amounts of binary alarm flags, and the distinct characteristics of binary anomaly data -- the meaning of state transition and data sparsity -- challenge existing causality learning mechanisms. This study proposes an anomaly causal discovery approach (AnomalyCD), addressing the accuracy and computational challenges of generating GCMs from temporal binary flag datasets. The AnomalyCD presents several strategies, such as anomaly data-aware causality testing, sparse data and prior link compression, and edge pruning adjustment approaches. We validate the performance of the approach on two datasets: monitoring sensor data from the readout-box system of the Compact Muon Solenoid experiment at CERN, and a public dataset from an information technology monitoring system. The results on temporal GCMs demonstrate a considerable reduction of computation overhead and a moderate enhancement of accuracy on the binary anomaly datasets Source code: https://github.com/muleina/AnomalyCD .","authors":["Mulugeta Weldezgina Asres","Christian Walter Omlin","The CMS-HCAL Collaboration"],"pdf_url":"","comment":"34 pages, 17 figures, 8 tables"},{"id":"http://arxiv.org/abs/2508.12029v3","updated":"2025-12-23T13:32:11Z","published":"2025-08-16T12:31:39Z","title":"BConformeR: A Conformer Based on Mutual Sampling for Unified Prediction of Continuous and Discontinuous Antibody Binding Sites","summary":"Accurate prediction of antibody-binding sites (epitopes) on antigens is crucial for vaccine design, immunodiagnostics, therapeutic antibody development, antibody engineering, research into autoimmune and allergic diseases, and advancing our understanding of immune responses. Despite in silico methods that have been proposed to predict both linear (continuous) and conformational (discontinuous) epitopes, they consistently underperform in predicting conformational epitopes. In this work, we propose Conformer-based models trained separately on AlphaFold-predicted structures and experimentally determined structures, leveraging convolutional neural networks (CNNs) to extract local features and Transformers to capture long-range dependencies within antigen sequences. Ablation studies demonstrate that CNN enhances the prediction of linear epitopes, and the Transformer module improves the prediction of conformational epitopes. Experimental results show that our model outperforms existing baselines in terms of MCC, ROC-AUC, PR-AUC, and F1 scores on both linear and conformational epitopes.","authors":["Zhangyu You","Jiahao Ma","Hongzong Li","Ye-Fan Hu","Jian-Dong Huang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20350v1","updated":"2025-12-23T13:31:21Z","published":"2025-12-23T13:31:21Z","title":"Field-Space Attention for Structure-Preserving Earth System Transformers","summary":"Accurate and physically consistent modeling of Earth system dynamics requires machine-learning architectures that operate directly on continuous geophysical fields and preserve their underlying geometric structure. Here we introduce Field-Space attention, a mechanism for Earth system Transformers that computes attention in the physical domain rather than in a learned latent space. By maintaining all intermediate representations as continuous fields on the sphere, the architecture enables interpretable internal states and facilitates the enforcement of scientific constraints. The model employs a fixed, non-learned multiscale decomposition and learns structure-preserving deformations of the input field, allowing coherent integration of coarse and fine-scale information while avoiding the optimization instabilities characteristic of standard single-scale Vision Transformers. Applied to global temperature super-resolution on a HEALPix grid, Field-Space Transformers converge more rapidly and stably than conventional Vision Transformers and U-Net baselines, while requiring substantially fewer parameters. The explicit preservation of field structure throughout the network allows physical and statistical priors to be embedded directly into the architecture, yielding improved fidelity and reliability in data-driven Earth system modeling. These results position Field-Space Attention as a compact, interpretable, and physically grounded building block for next-generation Earth system prediction and generative modeling frameworks.","authors":["Maximilian Witte","Johannes Meuer","Étienne Plésiat","Christopher Kadow"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2507.17454v2","updated":"2025-12-23T13:29:56Z","published":"2025-07-23T12:21:26Z","title":"C3RL: Rethinking the Combination of Channel-independence and Channel-mixing from Representation Learning","summary":"Multivariate time series forecasting has drawn increasing attention due to its practical importance. Existing approaches typically adopt either channel-mixing (CM) or channel-independence (CI) strategies. CM strategy can capture inter-variable dependencies but fails to discern variable-specific temporal patterns. CI strategy improves this aspect but fails to fully exploit cross-variable dependencies like CM. Hybrid strategies based on feature fusion offer limited generalization and interpretability. To address these issues, we propose C3RL, a novel representation learning framework that jointly models both CM and CI strategies. Motivated by contrastive learning in computer vision, C3RL treats the inputs of the two strategies as transposed views and builds a siamese network architecture: one strategy serves as the backbone, while the other complements it. By jointly optimizing contrastive and prediction losses with adaptive weighting, C3RL balances representation and forecasting performance. Extensive experiments on seven models show that C3RL boosts the best-case performance rate to 81.4% for models based on CI strategy and to 76.3% for models based on CM strategy, demonstrating strong generalization and effectiveness.","authors":["Shusen Ma","Yun-Bo Zhao","Yu Kang"],"pdf_url":"","comment":"Accepted by AAAI 2026"},{"id":"http://arxiv.org/abs/2512.20348v1","updated":"2025-12-23T13:29:26Z","published":"2025-12-23T13:29:26Z","title":"Physics-guided Neural Network-based Shaft Power Prediction for Vessels","summary":"Optimizing maritime operations, particularly fuel consumption for vessels, is crucial, considering its significant share in global trade. As fuel consumption is closely related to the shaft power of a vessel, predicting shaft power accurately is a crucial problem that requires careful consideration to minimize costs and emissions. Traditional approaches, which incorporate empirical formulas, often struggle to model dynamic conditions, such as sea conditions or fouling on vessels. In this paper, we present a hybrid, physics-guided neural network-based approach that utilizes empirical formulas within the network to combine the advantages of both neural networks and traditional techniques. We evaluate the presented method using data obtained from four similar-sized cargo vessels and compare the results with those of a baseline neural network and a traditional approach that employs empirical formulas. The experimental results demonstrate that the physics-guided neural network approach achieves lower mean absolute error, root mean square error, and mean absolute percentage error for all tested vessels compared to both the empirical formula-based method and the base neural network.","authors":["Dogan Altan","Hamza Haruna Mohammed","Glenn Terje Lines","Dusica Marijan","Arnbjørn Maressa"],"pdf_url":"","comment":"This work has been accepted for publication in the 11th Special Session on Intelligent Data Mining at IEEE BigData 2025. The final published version of this work will be available through IEEE"},{"id":"http://arxiv.org/abs/2512.20346v1","updated":"2025-12-23T13:28:15Z","published":"2025-12-23T13:28:15Z","title":"Inverse Autoregressive Flows for Zero Degree Calorimeter fast simulation","summary":"Physics-based machine learning blends traditional science with modern data-driven techniques. Rather than relying exclusively on empirical data or predefined equations, this methodology embeds domain knowledge directly into the learning process, resulting in models that are both more accurate and robust. We leverage this paradigm to accelerate simulations of the Zero Degree Calorimeter (ZDC) of the ALICE experiment at CERN. Our method introduces a novel loss function and an output variability-based scaling mechanism, which enhance the model's capability to accurately represent the spatial distribution and morphology of particle showers in detector outputs while mitigating the influence of rare artefacts on the training. Leveraging Normalizing Flows (NFs) in a teacher-student generative framework, we demonstrate that our approach not only outperforms classic data-driven model assimilation but also yields models that are 421 times faster than existing NF implementations in ZDC simulation literature.","authors":["Emilia Majerz","Witold Dzwinel","Jacek Kitowski"],"pdf_url":"","comment":"Presented as a poster at the Machine Learning and the Physical Sciences Workshop, 39th Conference on Neural Information Processing Systems (NeurIPS), 2025"},{"id":"http://arxiv.org/abs/2507.20993v2","updated":"2025-12-23T13:19:34Z","published":"2025-07-28T16:52:31Z","title":"Learning Treatment Policies From Multimodal Electronic Health Records","summary":"We study how to learn effective treatment policies from multimodal electronic health records (EHRs) that consist of tabular data and clinical text. These policies can help physicians make better treatment decisions and allocate healthcare resources more efficiently. Causal policy learning methods prioritize patients with the largest expected treatment benefit. Yet, existing estimators assume tabular covariates that satisfy strong causal assumptions, which are typically violated in the multimodal setting. As a result, predictive models of baseline risk are commonly used in practice to guide such decisions, as they extend naturally to multimodal data. However, such risk-based policies are not designed to identify which patients benefit most from treatment. We propose an extension of causal policy learning that uses expert-provided annotations during training to supervise treatment effect estimation, while using only multimodal representations as input during inference. We show that the proposed method achieves strong empirical performance across synthetic, semi-synthetic, and real-world EHR datasets, thereby offering practical insights into applying causal machine learning to realistic clinical data.","authors":["Henri Arno","Thomas Demeester"],"pdf_url":"","comment":"Preprint. Under review"},{"id":"http://arxiv.org/abs/2506.05831v3","updated":"2025-12-23T13:17:55Z","published":"2025-06-06T07:56:41Z","title":"Heartcare Suite: A Unified Multimodal ECG Suite for Dual Signal-Image Modeling and Understanding","summary":"Although electrocardiograms (ECG) play a dominant role in cardiovascular diagnosis and treatment, their intrinsic data forms and representational patterns pose significant challenges for medical multimodal large language models (Med-MLLMs) in achieving cross-modal semantic alignment. To address this gap, we propose Heartcare Suite, a unified ECG suite designed for dual signal-image modeling and understanding. (i) Heartcare-400K: We build a finegrained ECG instruction dataset on top of our data pipeline engine--HeartAgent--by integrating 12,170 high quality clinical ECG reports from top hospitals with open-source data; (ii) Heartcare-Bench: a systematic benchmark assessing performance of models in multi-perspective ECG understanding and cross-modal generalization, providing guidance for optimizing ECG comprehension models; (iii) HeartcareGPT: built upon a structure-aware discrete tokenizer Beat, we propose the DSPA (Dual Stream Projection Alignment) paradigm--a dual encoder projection alignment mechanism enabling joint optimizing and modeling native ECG signal-image within a shared feature space. Heartcare achieves consistent improvements across diverse ECG understanding tasks, validating both the effectiveness of the unified modeling paradigm and the necessity of a high-quality data pipeline, and establishing a methodological foundation for extending Med-MLLMs toward physiological signal domains. Our project is available at https://github.com/DCDmllm/Heartcare-Suite .","authors":["Yihan Xie","Sijing Li","Tianwei Lin","Zhuonan Wang","Chenglin Yang","Yu Zhong","Wenjie Yan","Wenqiao Zhang","Xiaogang Guo","Jun Xiao","Yueting Zhuang","Beng Chin Ooi"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19253v2","updated":"2025-12-23T13:00:45Z","published":"2025-12-22T10:40:03Z","title":"Machine Unlearning in the Era of Quantum Machine Learning: An Empirical Study","summary":"We present the first comprehensive empirical study of machine unlearning (MU) in hybrid quantum-classical neural networks. While MU has been extensively explored in classical deep learning, its behavior within variational quantum circuits (VQCs) and quantum-augmented architectures remains largely unexplored. First, we adapt a broad suite of unlearning methods to quantum settings, including gradient-based, distillation-based, regularization-based and certified techniques. Second, we introduce two new unlearning strategies tailored to hybrid models. Experiments across Iris, MNIST, and Fashion-MNIST, under both subset removal and full-class deletion, reveal that quantum models can support effective unlearning, but outcomes depend strongly on circuit depth, entanglement structure, and task complexity. Shallow VQCs display high intrinsic stability with minimal memorization, whereas deeper hybrid models exhibit stronger trade-offs between utility, forgetting strength, and alignment with retrain oracle. We find that certain methods, e.g. EU-k, LCA, and Certified Unlearning, consistently provide the best balance across metrics. These findings establish baseline empirical insights into quantum machine unlearning and highlight the need for quantum-aware algorithms and theoretical guarantees, as quantum machine learning systems continue to expand in scale and capability. We publicly release our code at: https://github.com/CrivoiCarla/HQML.","authors":["Carla Crivoi","Radu Tudor Ionescu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20329v1","updated":"2025-12-23T12:57:27Z","published":"2025-12-23T12:57:27Z","title":"FedDPC : Handling Data Heterogeneity and Partial Client Participation in Federated Learning","summary":"Data heterogeneity is a significant challenge in modern federated learning (FL) as it creates variance in local model updates, causing the aggregated global model to shift away from the true global optimum. Partial client participation in FL further exacerbates this issue by skewing the aggregation of local models towards the data distribution of participating clients. This creates additional variance in the global model updates, causing the global model to converge away from the optima of the global objective. These variances lead to instability in FL training, which degrades global model performance and slows down FL training. While existing literature primarily focuses on addressing data heterogeneity, the impact of partial client participation has received less attention. In this paper, we propose FedDPC, a novel FL method, designed to improve FL training and global model performance by mitigating both data heterogeneity and partial client participation. FedDPC addresses these issues by projecting each local update onto the previous global update, thereby controlling variance in both local and global updates. To further accelerate FL training, FedDPC employs adaptive scaling for each local update before aggregation. Extensive experiments on image classification tasks with multiple heterogeneously partitioned datasets validate the effectiveness of FedDPC. The results demonstrate that FedDPC outperforms state-of-the-art FL algorithms by achieving faster reduction in training loss and improved test accuracy across communication rounds.","authors":["Mrinmay Sen","Subhrajit Nag"],"pdf_url":"","comment":"10 pages, 7 figures"},{"id":"http://arxiv.org/abs/2512.20328v1","updated":"2025-12-23T12:56:18Z","published":"2025-12-23T12:56:18Z","title":"Toward Explaining Large Language Models in Software Engineering Tasks","summary":"Recent progress in Large Language Models (LLMs) has substantially advanced the automation of software engineering (SE) tasks, enabling complex activities such as code generation and code summarization. However, the black-box nature of LLMs remains a major barrier to their adoption in high-stakes and safety-critical domains, where explainability and transparency are vital for trust, accountability, and effective human supervision. Despite increasing interest in explainable AI for software engineering, existing methods lack domain-specific explanations aligned with how practitioners reason about SE artifacts. To address this gap, we introduce FeatureSHAP, the first fully automated, model-agnostic explainability framework tailored to software engineering tasks. Based on Shapley values, FeatureSHAP attributes model outputs to high-level input features through systematic input perturbation and task-specific similarity comparisons, while remaining compatible with both open-source and proprietary LLMs. We evaluate FeatureSHAP on two bi-modal SE tasks: code generation and code summarization. The results show that FeatureSHAP assigns less importance to irrelevant input features and produces explanations with higher fidelity than baseline methods. A practitioner survey involving 37 participants shows that FeatureSHAP helps practitioners better interpret model outputs and make more informed decisions. Collectively, FeatureSHAP represents a meaningful step toward practical explainable AI in software engineering. FeatureSHAP is available at https://github.com/deviserlab/FeatureSHAP.","authors":["Antonio Vitale","Khai-Nguyen Nguyen","Denys Poshyvanyk","Rocco Oliveto","Simone Scalabrino","Antonio Mastropaolo"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.13892v2","updated":"2025-12-23T12:54:15Z","published":"2025-12-15T20:50:54Z","title":"One Permutation Is All You Need: Fast, Reliable Variable Importance and Model Stress-Testing","summary":"Reliable estimation of feature contributions in machine learning models is essential for trust, transparency and regulatory compliance, especially when models are proprietary or otherwise operate as black boxes. While permutation-based methods are a standard tool for this task, classical implementations rely on repeated random permutations, introducing computational overhead and stochastic instability. In this paper, we show that by replacing multiple random permutations with a single, deterministic, and optimal permutation, we achieve a method that retains the core principles of permutation-based importance while being non-random, faster, and more stable. We validate this approach across nearly 200 scenarios, including real-world household finance and credit risk applications, demonstrating improved bias-variance tradeoffs and accuracy in challenging regimes such as small sample sizes, high dimensionality, and low signal-to-noise ratios. Finally, we introduce Systemic Variable Importance, a natural extension designed for model stress-testing that explicitly accounts for feature correlations. This framework provides a transparent way to quantify how shocks or perturbations propagate through correlated inputs, revealing dependencies that standard variable importance measures miss. Two real-world case studies demonstrate how this metric can be used to audit models for hidden reliance on protected attributes (e.g., gender or race), enabling regulators and practitioners to assess fairness and systemic risk in a principled and computationally efficient manner.","authors":["Albert Dorador"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20325v1","updated":"2025-12-23T12:49:44Z","published":"2025-12-23T12:49:44Z","title":"Top-K Exterior Power Persistent Homology: Algorithm, Structure, and Stability","summary":"Exterior powers play important roles in persistent homology in computational geometry. In the present paper we study the problem of extracting the $K$ longest intervals of the exterior-power layers of a tame persistence module. We prove a structural decomposition theorem that organizes the exterior-power layers into monotone per-anchor streams with explicit multiplicities, enabling a best-first algorithm. We also show that the Top-$K$ length vector is $2$-Lipschitz under bottleneck perturbations of the input barcode, and prove a comparison-model lower bound. Our experiments confirm the theory, showing speedups over full enumeration in high overlap cases. By enabling efficient extraction of the most prominent features, our approach makes higher-order persistence feasible for large datasets and thus broadly applicable to machine learning, data science, and scientific computing.","authors":["Yoshihiro Maruyama"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2501.18092v6","updated":"2025-12-23T12:38:34Z","published":"2025-01-30T02:03:30Z","title":"Learning Provably Improves the Convergence of Gradient Descent","summary":"Learn to Optimize (L2O) trains deep neural network-based solvers for optimization, achieving success in accelerating convex problems and improving non-convex solutions. However, L2O lacks rigorous theoretical backing for its own training convergence, as existing analyses often use unrealistic assumptions -- a gap this work highlights empirically. We bridge this gap by proving the training convergence of L2O models that learn Gradient Descent (GD) hyperparameters for quadratic programming, leveraging the Neural Tangent Kernel (NTK) theory. We propose a deterministic initialization strategy to support our theoretical results and promote stable training over extended optimization horizons by mitigating gradient explosion. Our L2O framework demonstrates over 50% better optimality than GD and superior robustness over state-of-the-art L2O methods on synthetic datasets. The code of our method can be found from https://github.com/NetX-lab/MathL2OProof-Official.","authors":["Qingyu Song","Wei Lin","Hong Xu"],"pdf_url":"","comment":"48 pages, 11 figures, NeurIPS 2025"},{"id":"http://arxiv.org/abs/2511.18417v2","updated":"2025-12-23T12:33:25Z","published":"2025-11-23T12:07:45Z","title":"Categorical Equivariant Deep Learning: Category-Equivariant Neural Networks and Universal Approximation Theorems","summary":"We develop a theory of category-equivariant neural networks (CENNs) that unifies group/groupoid-equivariant networks, poset/lattice-equivariant networks, graph and sheaf neural networks. Equivariance is formulated as naturality in a topological category with Radon measures. Formulating linear and nonlinear layers in the categorical setup, we prove the equivariant universal approximation theorem in the general setting: the class of finite-depth CENNs is dense in the space of continuous equivariant transformations. We instantiate the framework for groups/groupoids, posets/lattices, graphs and cellular sheaves, deriving universal approximation theorems for them in a systematic manner. Categorical equivariant deep learning thus allows us to expand the horizons of equivariant deep learning beyond group actions, encompassing not only geometric symmetries but also contextual and compositional symmetries.","authors":["Yoshihiro Maruyama"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20312v1","updated":"2025-12-23T12:30:37Z","published":"2025-12-23T12:30:37Z","title":"TableGPT-R1: Advancing Tabular Reasoning Through Reinforcement Learning","summary":"Tabular data serves as the backbone of modern data analysis and scientific research. While Large Language Models (LLMs) fine-tuned via Supervised Fine-Tuning (SFT) have significantly improved natural language interaction with such structured data, they often fall short in handling the complex, multi-step reasoning and robust code execution required for real-world table tasks. Reinforcement Learning (RL) offers a promising avenue to enhance these capabilities, yet its application in the tabular domain faces three critical hurdles: the scarcity of high-quality agentic trajectories with closed-loop code execution and environment feedback on diverse table structures, the extreme heterogeneity of feedback signals ranging from rigid SQL execution to open-ended data interpretation, and the risk of catastrophic forgetting of general knowledge during vertical specialization. To overcome these challenges and unlock advanced reasoning on complex tables, we introduce \\textbf{TableGPT-R1}, a specialized tabular model built on a systematic RL framework. Our approach integrates a comprehensive data engineering pipeline that synthesizes difficulty-stratified agentic trajectories for both supervised alignment and RL rollouts, a task-adaptive reward system that combines rule-based verification with a criteria-injected reward model and incorporates process-level step reward shaping with behavioral regularization, and a multi-stage training framework that progressively stabilizes reasoning before specializing in table-specific tasks. Extensive evaluations demonstrate that TableGPT-R1 achieves state-of-the-art performance on authoritative benchmarks, significantly outperforming baseline models while retaining robust general capabilities. Our model is available at https://huggingface.co/tablegpt/TableGPT-R1.","authors":["Saisai Yang","Qingyi Huang","Jing Yuan","Liangyu Zha","Kai Tang","Yuhang Yang","Ning Wang","Yucheng Wei","Liyao Li","Wentao Ye","Hao Chen","Tao Zhang","Junlin Zhou","Haobo Wang","Gang Chen","Junbo Zhao"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20311v1","updated":"2025-12-23T12:29:58Z","published":"2025-12-23T12:29:58Z","title":"Algorithm for Interpretable Graph Features via Motivic Persistent Cohomology","summary":"We present the Chromatic Persistence Algorithm (CPA), an event-driven method for computing persistent cohomological features of weighted graphs via graphic arrangements, a classical object in computational geometry. We establish rigorous complexity results: CPA is exponential in the worst case, fixed-parameter tractable in treewidth, and nearly linear for common graph families such as trees, cycles, and series-parallel graphs. Finally, we demonstrate its practical applicability through a controlled experiment on molecular-like graph structures.","authors":["Yoshihiro Maruyama"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2212.04382v4","updated":"2025-12-23T12:28:30Z","published":"2022-12-08T16:23:42Z","title":"Structure of Classifier Boundaries: Case Study for a Naive Bayes Classifier","summary":"Classifiers assign complex input data points to one of a small number of output categories. For a Bayes classifier whose input space is a graph, we study the structure of the \\emph{boundary}, which comprises those points for which at least one neighbor is classified differently. The scientific setting is assignment of DNA reads produced by \\NGSs\\ to candidate source genomes. The boundary is both large and complicated in structure. We introduce a new measure of uncertainty, Neighbor Similarity, that compares the result for an input point to the distribution of results for its neighbors. This measure not only tracks two inherent uncertainty measures for the Bayes classifier, but also can be implemented for classifiers without inherent measures of uncertainty.","authors":["Alan F. Karr","Zac Bowen","Adam A. Porter","Regina Ruane"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20305v1","updated":"2025-12-23T12:16:06Z","published":"2025-12-23T12:16:06Z","title":"KAN-AFT: An Interpretable Nonlinear Survival Model Integrating Kolmogorov-Arnold Networks with Accelerated Failure Time Analysis","summary":"Survival analysis relies fundamentally on the semi-parametric Cox Proportional Hazards (CoxPH) model and the parametric Accelerated Failure Time (AFT) model. CoxPH assumes constant hazard ratios, often failing to capture real-world dynamics, while traditional AFT models are limited by rigid distributional assumptions. Although deep learning models like DeepAFT address these constraints by improving predictive accuracy and handling censoring, they inherit the significant challenge of black-box interpretability. The recent introduction of CoxKAN demonstrated the successful integration of Kolmogorov-Arnold Networks (KANs), a novel architecture that yields highly accurate and interpretable symbolic representations, within the CoxPH framework. Motivated by the interpretability gains of CoxKAN, we introduce KAN-AFT (Kolmogorov Arnold Network-based AFT), the first framework to apply KANs to the AFT model. KAN-AFT effectively models complex nonlinear relationships within the AFT framework. Our primary contributions include: (i) a principled AFT-KAN formulation, (ii) robust optimization strategies for right-censored observations (e.g., Buckley-James and IPCW), and (iii) an interpretability pipeline that converts the learned spline functions into closed-form symbolic equations for survival time. Empirical results on multiple datasets confirm that KAN-AFT achieves performance comparable to or better than DeepAFT, while uniquely providing transparent, symbolic models of the survival process.","authors":["Mebin Jose","Jisha Francis","Sudheesh Kumar Kattumannil"],"pdf_url":"","comment":"A new development in Survival Analysis based on the celebrated Kolmogorov-Arnold Networks (KANs)"},{"id":"http://arxiv.org/abs/2510.10078v3","updated":"2025-12-23T12:11:51Z","published":"2025-10-11T07:29:32Z","title":"Improving Speech Emotion Recognition with Mutual Information Regularized Generative Model","summary":"Lack of large, well-annotated emotional speech corpora continues to limit the performance and robustness of speech emotion recognition (SER), particularly as models grow more complex and the demand for multimodal systems increases. While generative data augmentation offers a promising solution, existing approaches often produce emotionally inconsistent samples due to oversimplified conditioning on categorical labels. This paper introduces a novel mutual-information-regularised generative framework that combines cross-modal alignment with feature-level synthesis. Building on an InfoGAN-style architecture, our method first learns a semantically aligned audio-text representation space using pre-trained transformers and contrastive objectives. A feature generator is then trained to produce emotion-aware audio features while employing mutual information as a quantitative regulariser to ensure strong dependency between generated features and their conditioning variables. We extend this approach to multimodal settings, enabling the generation of novel, paired (audio, text) features. Comprehensive evaluation on three benchmark datasets (IEMOCAP, MSP-IMPROV, MSP-Podcast) demonstrates that our framework consistently outperforms existing augmentation methods, achieving state-of-the-art performance with improvements of up to 2.6% in unimodal SER and 3.2% in multimodal emotion recognition. Most importantly, we demonstrate that mutual information functions as both a regulariser and a measurable metric for generative quality, offering a systematic approach to data augmentation in affective computing.","authors":["Chung-Soo Ahn","Rajib Rana","Sunil Sivadas","Carlos Busso","Jagath C. Rajapakse"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2509.14844v2","updated":"2025-12-23T12:08:33Z","published":"2025-09-18T11:10:24Z","title":"Non-Intrusive Parametrized-Background Data-Weak Reconstruction of Cardiac Displacement Fields from Sparse MRI-like Observations","summary":"Personalized cardiac diagnostics require accurate reconstruction of myocardial displacement fields from sparse clinical imaging data, yet current methods often demand intrusive access to computational models. In this work, we apply the non-intrusive Parametrized-Background Data-Weak (PBDW) approach to three-dimensional (3D) cardiac displacement field reconstruction from limited Magnetic Resonance Image (MRI)-like observations. Our implementation requires only solution snapshots -- no governing equations, assembly routines, or solver access -- enabling immediate deployment across commercial and research codes using different constitutive models. Additionally, we introduce two enhancements: an H-size minibatch worst-case Orthogonal Matching Pursuit (wOMP) algorithm that improves Sensor Selection (SS) computational efficiency while maintaining reconstruction accuracy, and memory optimization techniques exploiting block matrix structures in vectorial problems. We demonstrate the effectiveness of the method through validation on a 3D left ventricular model with simulated scar tissue. Starting with noise-free reconstruction, we systematically incorporate Gaussian noise and spatial sparsity mimicking realistic MRI acquisition protocols. Results show exceptional accuracy in noise-free conditions (relative L2 error of order O(1e-5)), robust performance with 10% noise (relative L2 error of order O(1e-2)), and effective reconstruction from sparse measurements (relative L2 error of order O(1e-2)). The online reconstruction achieves four-order-of-magnitude computational speed-up compared to full Finite Element (FE) simulations, with reconstruction times under one tenth of second for sparse scenarios, demonstrating significant potential for integration into clinical cardiac modeling workflows.","authors":["Francesco C. Mantegazza","Federica Caforio","Christoph Augustin","Matthias A. F. Gsell","Gundolf Haase","Elias Karabelas"],"pdf_url":"","comment":"42 pages, 12 figures, 6 tables"},{"id":"http://arxiv.org/abs/2512.20291v1","updated":"2025-12-23T12:00:10Z","published":"2025-12-23T12:00:10Z","title":"Mixture-of-Experts with Gradient Conflict-Driven Subspace Topology Pruning for Emergent Modularity","summary":"Mixture-of-Experts (MoE) architectures achieve parameter efficiency through conditional computation, yet contemporary designs suffer from two fundamental limitations: structural parameter isolation that causes catastrophic forgetting, and instruction-overfitting that degrades performance in instruction-free scenarios. We propose CDSP-MoE (Conflict-Driven Subspace Pruning MoE), a framework that addresses these issues through a paradigm shift from isolated expert containers to dynamic expert instantiation within a shared physical subspace. Grounded in the Universal Weight Subspace Hypothesis, CDSP-MoE maintains a super-complete parameter backbone where logical experts are carved out via learnable topology masks. Unlike prior work that uses gradient conflict for token reassignment or optimization surgery, we leverage it as a structural supervisory signal: a Lagged Gradient Game penalizes interfering connections in the shared manifold, enabling the topology to spontaneously prune conflicting pathways and evolve interpretable modular structures. Experimental results demonstrate that CDSP-MoE achieves robust content-driven routing without human-defined task labels, maintaining semantic specialization even under strict blind inference protocols where explicit instructions are absent. Code is available at: https://github.com/konodiodaaaaa1/Conflict-Driven-Subspace-Pruning-Mixture-of-Experts","authors":["Yuxing Gan","Ziyu Lei"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2507.11662v2","updated":"2025-12-23T11:29:24Z","published":"2025-07-15T18:50:29Z","title":"Let's Think in Two Steps: Mitigating Agreement Bias in MLLMs with Self-Grounded Verification","summary":"Verifiers--functions assigning rewards to agent behavior--have been key for AI progress in domains like math and code. However, extending gains to domains without clear-cut success criteria (e.g., computer use) remains a challenge: while humans can recognize desired outcomes, translating this intuition into scalable rules is nontrivial. Multimodal Large Language Models (MLLMs) emerge as a promising solution, given their world knowledge, human-preference alignment, and reasoning skills. We evaluate MLLMs as verifiers across web navigation, computer use, and robotic manipulation, and identify a critical limitation: a strong tendency to over-validate agent behavior, a phenomenon we term agreement bias. This bias is pervasive across models, resilient to test-time scaling, and poses risks to existing methods relying on MLLM evaluations. We discuss methods to evaluate and improve MLLM verifiers and introduce Self-Grounded Verification (SGV), a lightweight method that harnesses MLLMs' own sampling mechanisms by modulating (un)conditional generation to better leverage their knowledge, alignment, and reasoning. SGV operates in two steps: first, the MLLM is elicited to generate broad priors about desired behavior, independent of the data under evaluation. Then, conditioned on self-generated priors, it reasons over and evaluates a candidate trajectory. SGV yields more human-aligned evaluations with gains of up to 25pp in failure detection, 14pp in accuracy, and benefits extending to downstream applications. In self-refinement and online supervision, SGV boosts task completion of a GUI specialist in OSWorld, a diffusion policy in robomimic, and a ReAct agent in VisualWebArena--setting a new state of the art, surpassing the previous best by 20pp. We release an updated version of VisualWebArena featuring more human-aligned evaluators, high-fidelity environment parallelism, and speedups of over 10x.","authors":["Moises Andrade","Joonhyuk Cha","Brandon Ho","Vriksha Srihari","Karmesh Yadav","Zsolt Kira"],"pdf_url":"","comment":"Our code, models, and data are publicly available at https://mshalimay.github.io/agreement-bias-sgv/"},{"id":"http://arxiv.org/abs/2512.20272v1","updated":"2025-12-23T11:25:22Z","published":"2025-12-23T11:25:22Z","title":"HGAN-SDEs: Learning Neural Stochastic Differential Equations with Hermite-Guided Adversarial Training","summary":"Neural Stochastic Differential Equations (Neural SDEs) provide a principled framework for modeling continuous-time stochastic processes and have been widely adopted in fields ranging from physics to finance. Recent advances suggest that Generative Adversarial Networks (GANs) offer a promising solution to learning the complex path distributions induced by SDEs. However, a critical bottleneck lies in designing a discriminator that faithfully captures temporal dependencies while remaining computationally efficient. Prior works have explored Neural Controlled Differential Equations (CDEs) as discriminators due to their ability to model continuous-time dynamics, but such architectures suffer from high computational costs and exacerbate the instability of adversarial training. To address these limitations, we introduce HGAN-SDEs, a novel GAN-based framework that leverages Neural Hermite functions to construct a structured and efficient discriminator. Hermite functions provide an expressive yet lightweight basis for approximating path-level dynamics, enabling both reduced runtime complexity and improved training stability. We establish the universal approximation property of our framework for a broad class of SDE-driven distributions and theoretically characterize its convergence behavior. Extensive empirical evaluations on synthetic and real-world systems demonstrate that HGAN-SDEs achieve superior sample quality and learning efficiency compared to existing generative models for SDEs","authors":["Yuanjian Xu","Yuan Shuai","Jianing Hao","Guang Zhang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20270v1","updated":"2025-12-23T11:24:45Z","published":"2025-12-23T11:24:45Z","title":"Optimality-Informed Neural Networks for Solving Parametric Optimization Problems","summary":"Many engineering tasks require solving families of nonlinear constrained optimization problems, parametrized in setting-specific variables. This is computationally demanding, particularly, if solutions have to be computed across strongly varying parameter values, e.g., in real-time control or for model-based design. Thus, we propose to learn the mapping from parameters to the primal optimal solutions and to their corresponding duals using neural networks, giving a dense estimation in contrast to gridded approaches. Our approach, Optimality-informed Neural Networks (OptINNs), combines (i) a KKT-residual loss that penalizes violations of the first-order optimality conditions under standard constraint qualifications assumptions, and (ii) problem-specific output activations that enforce simple inequality constraints (e.g., box-type/positivity) by construction. This design reduces data requirements, allows the prediction of dual variables, and improves feasibility and closeness to optimality compared to penalty-only training. Taking quadratic penalties as a baseline, since this approach has been previously proposed for the considered problem class in literature, our method simplifies hyperparameter tuning and attains tighter adherence to optimality conditions. We evaluate OptINNs on different nonlinear optimization problems ranging from low to high dimensions. On small problems, OptINNs match a quadratic-penalty baseline in primal accuracy while additionally predicting dual variables with low error. On larger problems, OptINNs achieve lower constraint violations and lower primal error compared to neural networks based on the quadratic-penalty method. These results suggest that embedding feasibility and optimality into the network architecture and loss can make learning-based surrogates more accurate, feasible, and data-efficient for parametric optimization.","authors":["Matthias K. Hoffmann","Amine Othmane","Kathrin Flaßkamp"],"pdf_url":"","comment":"Under review, 24 pages, 10 figures"},{"id":"http://arxiv.org/abs/2512.20268v1","updated":"2025-12-23T11:22:26Z","published":"2025-12-23T11:22:26Z","title":"DeepONet-accelerated Bayesian inversion for moving boundary problems","summary":"This work demonstrates that neural operator learning provides a powerful and flexible framework for building fast, accurate emulators of moving boundary systems, enabling their integration into digital twin platforms. To this end, a Deep Operator Network (DeepONet) architecture is employed to construct an efficient surrogate model for moving boundary problems in single-phase Darcy flow through porous media. The surrogate enables rapid and accurate approximation of complex flow dynamics and is coupled with an Ensemble Kalman Inversion (EKI) algorithm to solve Bayesian inverse problems.\n  The proposed inversion framework is demonstrated by estimating the permeability and porosity of fibre reinforcements for composite materials manufactured via the Resin Transfer Moulding (RTM) process. Using both synthetic and experimental in-process data, the DeepONet surrogate accelerates inversion by several orders of magnitude compared with full-model EKI. This computational efficiency enables real-time, accurate, high-resolution estimation of local variations in permeability, porosity, and other parameters, thereby supporting effective monitoring and control of RTM processes, as well as other applications involving moving boundary flows. Unlike prior approaches for RTM inversion that learn mesh-dependent mappings, the proposed neural operator generalises across spatial and temporal domains, enabling evaluation at arbitrary sensor configurations without retraining, and represents a significant step toward practical industrial deployment of digital twins.","authors":["Marco A. Iglesias","Michael. E. Causon","Mikhail Y. Matveev","Andreas Endruweit","Michael . V. Tretyakov"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20249v1","updated":"2025-12-23T11:04:34Z","published":"2025-12-23T11:04:34Z","title":"Unified Multimodal Brain Decoding via Cross-Subject Soft-ROI Fusion","summary":"Multimodal brain decoding aims to reconstruct semantic information that is consistent with visual stimuli from brain activity signals such as fMRI, and then generate readable natural language descriptions. However, multimodal brain decoding still faces key challenges in cross-subject generalization and interpretability. We propose a BrainROI model and achieve leading-level results in brain-captioning evaluation on the NSD dataset. Under the cross-subject setting, compared with recent state-of-the-art methods and representative baselines, metrics such as BLEU-4 and CIDEr show clear improvements. Firstly, to address the heterogeneity of functional brain topology across subjects, we design a new fMRI encoder. We use multi-atlas soft functional parcellations (soft-ROI) as a shared space. We extend the discrete ROI Concatenation strategy in MINDLLM to a voxel-wise gated fusion mechanism (Voxel-gate). We also ensure consistent ROI mapping through global label alignment, which enhances cross-subject transferability. Secondly, to overcome the limitations of manual and black-box prompting methods in stability and transparency, we introduce an interpretable prompt optimization process. In a small-sample closed loop, we use a locally deployed Qwen model to iteratively generate and select human-readable prompts. This process improves the stability of prompt design and preserves an auditable optimization trajectory. Finally, we impose parameterized decoding constraints during inference to further improve the stability and quality of the generated descriptions.","authors":["Xuanyu Hu"],"pdf_url":"","comment":"15 pages, 2 figures, 4 tables. Submitted to ICPR 2026"},{"id":"http://arxiv.org/abs/2512.20233v1","updated":"2025-12-23T10:46:48Z","published":"2025-12-23T10:46:48Z","title":"How I Met Your Bias: Investigating Bias Amplification in Diffusion Models","summary":"Diffusion-based generative models demonstrate state-of-the-art performance across various image synthesis tasks, yet their tendency to replicate and amplify dataset biases remains poorly understood. Although previous research has viewed bias amplification as an inherent characteristic of diffusion models, this work provides the first analysis of how sampling algorithms and their hyperparameters influence bias amplification. We empirically demonstrate that samplers for diffusion models -- commonly optimized for sample quality and speed -- have a significant and measurable effect on bias amplification. Through controlled studies with models trained on Biased MNIST, Multi-Color MNIST and BFFHQ, and with Stable Diffusion, we show that sampling hyperparameters can induce both bias reduction and amplification, even when the trained model is fixed. Source code is available at https://github.com/How-I-met-your-bias/how_i_met_your_bias.","authors":["Nathan Roos","Ekaterina Iakovleva","Ani Gjergji","Vito Paolo Pastore","Enzo Tartaglione"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20232v1","updated":"2025-12-23T10:46:18Z","published":"2025-12-23T10:46:18Z","title":"Adaptive Multi-task Learning for Probabilistic Load Forecasting","summary":"Simultaneous load forecasting across multiple entities (e.g., regions, buildings) is crucial for the efficient, reliable, and cost-effective operation of power systems. Accurate load forecasting is a challenging problem due to the inherent uncertainties in load demand, dynamic changes in consumption patterns, and correlations among entities. Multi-task learning has emerged as a powerful machine learning approach that enables the simultaneous learning across multiple related problems. However, its application to load forecasting remains underexplored and is limited to offline learning-based methods, which cannot capture changes in consumption patterns. This paper presents an adaptive multi-task learning method for probabilistic load forecasting. The proposed method can dynamically adapt to changes in consumption patterns and correlations among entities. In addition, the techniques presented provide reliable probabilistic predictions for loads of multiples entities and assess load uncertainties. Specifically, the method is based on vectorvalued hidden Markov models and uses a recursive process to update the model parameters and provide predictions with the most recent parameters. The performance of the proposed method is evaluated using datasets that contain the load demand of multiple entities and exhibit diverse and dynamic consumption patterns. The experimental results show that the presented techniques outperform existing methods both in terms of forecasting performance and uncertainty assessment.","authors":["Onintze Zaballa","Verónica Álvarez","Santiago Mazuelas"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.01498v2","updated":"2025-12-23T10:29:27Z","published":"2025-12-01T10:19:30Z","title":"No Trust Issues Here: A Technical Report on the Winning Solutions for the Rayan AI Contest","summary":"This report presents solutions to three machine learning challenges developed as part of the Rayan AI Contest: compositional image retrieval, zero-shot anomaly detection, and backdoored model detection. In compositional image retrieval, we developed a system that processes visual and textual inputs to retrieve relevant images, achieving 95.38% accuracy and ranking first with a clear margin over the second team. For zero-shot anomaly detection, we designed a model that identifies and localizes anomalies in images without prior exposure to abnormal examples, securing second place with a 73.14% score. In the backdoored model detection task, we proposed a method to detect hidden backdoor triggers in neural networks, reaching an accuracy of 78%, which placed our approach in second place. These results demonstrate the effectiveness of our methods in addressing key challenges related to retrieval, anomaly detection, and model security, with implications for real-world applications in industries such as healthcare, manufacturing, and cybersecurity. Code for all solutions is available online (https://github.com/safinal/rayan-ai-contest-solutions).","authors":["Ali Nafisi","Sina Asghari","Mohammad Saeed Arvenaghi","Hossein Shakibania"],"pdf_url":"","comment":"Code available at https://github.com/safinal/rayan-ai-contest-solutions"},{"id":"http://arxiv.org/abs/2512.20220v1","updated":"2025-12-23T10:20:11Z","published":"2025-12-23T10:20:11Z","title":"Generalisation in Multitask Fitted Q-Iteration and Offline Q-learning","summary":"We study offline multitask reinforcement learning in settings where multiple tasks share a low-rank representation of their action-value functions. In this regime, a learner is provided with fixed datasets collected from several related tasks, without access to further online interaction, and seeks to exploit shared structure to improve statistical efficiency and generalization. We analyze a multitask variant of fitted Q-iteration that jointly learns a shared representation and task-specific value functions via Bellman error minimization on offline data. Under standard realizability and coverage assumptions commonly used in offline reinforcement learning, we establish finite-sample generalization guarantees for the learned value functions. Our analysis explicitly characterizes how pooling data across tasks improves estimation accuracy, yielding a $1/\\sqrt{nT}$ dependence on the total number of samples across tasks, while retaining the usual dependence on the horizon and concentrability coefficients arising from distribution shift. In addition, we consider a downstream offline setting in which a new task shares the same underlying representation as the upstream tasks. We study how reusing the representation learned during the multitask phase affects value estimation for this new task, and show that it can reduce the effective complexity of downstream learning relative to learning from scratch. Together, our results clarify the role of shared representations in multitask offline Q-learning and provide theoretical insight into when and how multitask structure can improve generalization in model-free, value-based reinforcement learning.","authors":["Kausthubh Manda","Raghuram Bharadwaj Diddigi"],"pdf_url":"","comment":"18 pages (9 pages + Appendix and references), this is version 1"},{"id":"http://arxiv.org/abs/2410.06820v4","updated":"2025-12-23T10:16:53Z","published":"2024-10-09T12:28:32Z","title":"Learning a Neural Solver for Parametric PDE to Enhance Physics-Informed Methods","summary":"Physics-informed deep learning often faces optimization challenges due to the complexity of solving partial differential equations (PDEs), which involve exploring large solution spaces, require numerous iterations, and can lead to unstable training. These challenges arise particularly from the ill-conditioning of the optimization problem caused by the differential terms in the loss function. To address these issues, we propose learning a solver, i.e., solving PDEs using a physics-informed iterative algorithm trained on data. Our method learns to condition a gradient descent algorithm that automatically adapts to each PDE instance, significantly accelerating and stabilizing the optimization process and enabling faster convergence of physics-aware models. Furthermore, while traditional physics-informed methods solve for a single PDE instance, our approach extends to parametric PDEs. Specifically, we integrate the physical loss gradient with PDE parameters, allowing our method to solve over a distribution of PDE parameters, including coefficients, initial conditions, and boundary conditions. We demonstrate the effectiveness of our approach through empirical experiments on multiple datasets, comparing both training and test-time optimization performance. The code is available at https://github.com/2ailesB/neural-parametric-solver.","authors":["Lise Le Boudec","Emmanuel de Bezenac","Louis Serrano","Ramon Daniel Regueiro-Espino","Yuan Yin","Patrick Gallinari"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20218v1","updated":"2025-12-23T10:16:43Z","published":"2025-12-23T10:16:43Z","title":"Cost-TrustFL: Cost-Aware Hierarchical Federated Learning with Lightweight Reputation Evaluation across Multi-Cloud","summary":"Federated learning across multi-cloud environments faces critical challenges, including non-IID data distributions, malicious participant detection, and substantial cross-cloud communication costs (egress fees). Existing Byzantine-robust methods focus primarily on model accuracy while overlooking the economic implications of data transfer across cloud providers. This paper presents Cost-TrustFL, a hierarchical federated learning framework that jointly optimizes model performance and communication costs while providing robust defense against poisoning attacks. We propose a gradient-based approximate Shapley value computation method that reduces the complexity from exponential to linear, enabling lightweight reputation evaluation. Our cost-aware aggregation strategy prioritizes intra-cloud communication to minimize expensive cross-cloud data transfers. Experiments on CIFAR-10 and FEMNIST datasets demonstrate that Cost-TrustFL achieves 86.7% accuracy under 30% malicious clients while reducing communication costs by 32% compared to baseline methods. The framework maintains stable performance across varying non-IID degrees and attack intensities, making it practical for real-world multi-cloud deployments.","authors":["Jixiao Yang","Jinyu Chen","Zixiao Huang","Chengda Xu","Chi Zhang","Sijia Li"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2505.23593v3","updated":"2025-12-23T09:55:01Z","published":"2025-05-29T16:04:39Z","title":"Position: Federated Foundation Language Model Post-Training Should Focus on Open-Source Models","summary":"Post-training of foundation language models has emerged as a promising research domain in federated learning (FL) with the goal to enable privacy-preserving model improvements and adaptations to user's downstream tasks. Recent advances in this area adopt centralized post-training approaches that build upon black-box foundation language models where there is no access to model weights and architecture details. Although the use of black-box models has been successful in centralized post-training, their blind replication in FL raises several concerns. Our position is that using black-box models in FL contradicts the core principles of federation such as data privacy and autonomy. In this position paper, we critically analyze the usage of black-box models in federated post-training, and provide a detailed account of various aspects of openness and their implications for FL.","authors":["Nikita Agrawal","Simon Mertel","Ruben Mayer"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2510.03289v2","updated":"2025-12-23T09:36:38Z","published":"2025-09-29T12:07:09Z","title":"Why mask diffusion does not work","summary":"The main advantages of diffusion language models over autoregressive (AR) models lie in their ability to support parallel generation and bidirectional attention, enabling a more controllable generation process. In recent years, open-source mask diffusion language models have emerged, most of which are based on a variant known as absorbing diffusion. However, this paper demonstrates why mask diffusion faces inherent difficulties in achieving parallel generation and bidirectional attention. We also propose the most effective training and inference strategies for mask diffusion.","authors":["Haocheng Sun","Cynthia Xin Wen","Edward Hong Wang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2510.16066v3","updated":"2025-12-23T09:35:05Z","published":"2025-10-17T03:56:11Z","title":"Cash Flow Underwriting with Bank Transaction Data: Advancing MSME Financial Inclusion in Malaysia","summary":"Despite accounting for 96.1% of all businesses in Malaysia, access to financing remains one of the most persistent challenges faced by Micro, Small, and Medium Enterprises (MSMEs). Newly established businesses are often excluded from formal credit markets as traditional underwriting approaches rely heavily on credit bureau data. This study investigates the potential of bank statement data as an alternative data source for credit assessment to promote financial inclusion in emerging markets. First, we propose a cash flow-based underwriting pipeline where we utilise bank statement data for end-to-end data extraction and machine learning credit scoring. Second, we introduce a novel dataset of 611 loan applicants from a Malaysian lending institution. Third, we develop and evaluate credit scoring models based on application information and bank transaction-derived features. Empirical results show that the use of such data boosts the performance of all models on our dataset, which can improve credit scoring for new-to-lending MSMEs. Finally, we will release the anonymised bank transaction dataset to facilitate further research on MSME financial inclusion within Malaysia's emerging economy.","authors":["Chun Chet Ng","Wei Zeng Low","Jia Yu Lim","Yin Yin Boon"],"pdf_url":"","comment":"Accepted for oral presentation at the AI for Financial Inclusion, Risk Modeling and Resilience in Emerging Markets (FinRem) Workshop at ACM ICAIF 2025, Singapore. Accepted for poster presentation at the Agentic AI in Financial Services Workshop at AAAI 2026, Singapore"},{"id":"http://arxiv.org/abs/2508.06244v2","updated":"2025-12-23T09:18:27Z","published":"2025-08-08T11:56:13Z","title":"Membership Inference Attack with Partial Features","summary":"Machine learning models are vulnerable to membership inference attack, which can be used to determine whether a given sample appears in the training data. Most existing methods assume the attacker has full access to the features of the target sample. This assumption, however, does not hold in many real-world scenarios where only partial features are available, thereby limiting the applicability of these methods. In this work, we introduce Partial Feature Membership Inference (PFMI), a scenario where the adversary observes only partial features of each sample and aims to infer whether this observed subset was present in the training set. To address this problem, we propose MRAD (Memory-guided Reconstruction and Anomaly Detection), a two-stage attack framework that works in both white-box and black-box settings. In the first stage, MRAD leverages the latent memory of the target model to reconstruct the unknown features of the sample. We observe that when the known features are absent from the training set, the reconstructed sample deviates significantly from the true data distribution. Consequently, in the second stage, we use anomaly detection algorithms to measure the deviation between the reconstructed sample and the training data distribution, thereby determining whether the known features belong to a member of the training set. Empirical results demonstrate that MRAD is effective across various datasets, and maintains compatibility with off-the-shelf anomaly detection techniques. For example, on STL-10, our attack exceeds an AUC of around 0.75 even with 60% of the missing features.","authors":["Xurun Wang","Guangrui Liu","Xinjie Li","Haoyu He","Lin Yao","Zhongyun Hua","Weizhe Zhang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20177v1","updated":"2025-12-23T09:16:44Z","published":"2025-12-23T09:16:44Z","title":"NeuralCrop: Combining physics and machine learning for improved crop yield predictions","summary":"Global gridded crop models (GGCMs) simulate daily crop growth by explicitly representing key biophysical processes and project end-of-season yield time series. They are a primary tool to quantify the impacts of climate change on agricultural productivity and assess associated risks for food security. Despite decades of development, state-of-the-art GGCMs still have substantial uncertainties in simulating complex biophysical processes due to limited process understanding. Recently, machine learning approaches trained on observational data have shown great potential in crop yield predictions. However, these models have not demonstrated improved performance over classical GGCMs and are not suitable for simulating crop yields under changing climate conditions due to problems in generalizing outside their training distributions. Here we introduce NeuralCrop, a hybrid GGCM that combines the strengths of an advanced process-based GGCM, resolving important processes explicitly, with data-driven machine learning components. The model is first trained to emulate a competitive GGCM before it is fine-tuned on observational data. We show that NeuralCrop outperforms state-of-the-art GGCMs across site-level and large-scale cropping regions. Across moisture conditions, NeuralCrop reproduces the interannual yield anomalies in European wheat regions and the US Corn Belt more accurately during the period from 2000 to 2019 with particularly strong improvements under drought extremes. When generalizing to conditions unseen during training, NeuralCrop continues to make robust projections, while pure machine learning models exhibit substantial performance degradation. Our results show that our hybrid crop modelling approach offers overall improved crop modeling and more reliable yield projections under climate change and intensifying extreme weather conditions.","authors":["Yunan Lin","Sebastian Bathiany","Maha Badri","Maximilian Gelbrecht","Philipp Hess","Brian Groenke","Jens Heinke","Christoph Müller","Niklas Boers"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19576v2","updated":"2025-12-23T09:09:53Z","published":"2025-12-22T17:00:25Z","title":"LeLaR: The First In-Orbit Demonstration of an AI-Based Satellite Attitude Controller","summary":"Attitude control is essential for many satellite missions. Classical controllers, however, are time-consuming to design and sensitive to model uncertainties and variations in operational boundary conditions. Deep Reinforcement Learning (DRL) offers a promising alternative by learning adaptive control strategies through autonomous interaction with a simulation environment. Overcoming the Sim2Real gap, which involves deploying an agent trained in simulation onto the real physical satellite, remains a significant challenge. In this work, we present the first successful in-orbit demonstration of an AI-based attitude controller for inertial pointing maneuvers. The controller was trained entirely in simulation and deployed to the InnoCube 3U nanosatellite, which was developed by the Julius-Maximilians-Universität Würzburg in cooperation with the Technische Universität Berlin, and launched in January 2025. We present the AI agent design, the methodology of the training procedure, the discrepancies between the simulation and the observed behavior of the real satellite, and a comparison of the AI-based attitude controller with the classical PD controller of InnoCube. Steady-state metrics confirm the robust performance of the AI-based controller during repeated in-orbit maneuvers.","authors":["Kirill Djebko","Tom Baumann","Erik Dilger","Frank Puppe","Sergio Montenegro"],"pdf_url":"","comment":"55 pages, 27 figures, 29 tables. The maneuver telemetry datasets generated and analyzed during this work are available in the GitHub repository under https://github.com/kdjebko/lelar-in-orbit-data"},{"id":"http://arxiv.org/abs/2512.20169v1","updated":"2025-12-23T08:56:49Z","published":"2025-12-23T08:56:49Z","title":"Learning to Reason in LLMs by Expectation Maximization","summary":"Large language models (LLMs) solve reasoning problems by first generating a rationale and then answering. We formalize reasoning as a latent variable model and derive an expectation-maximization (EM) objective for learning to reason. This view connects EM and modern reward-based optimization, and shows that the main challenge lies in designing a sampling distribution that generates rationales that justify correct answers. We instantiate and compare several sampling schemes: rejection sampling with a budget, self-taught reasoner (STaR), and prompt posterior sampling (PPS), which only keeps the rationalization stage of STaR. Our experiments on the ARC, MMLU, and OpenBookQA datasets with the Llama and Qwen models show that the sampling scheme can significantly affect the accuracy of learned reasoning models. Despite its simplicity, we observe that PPS outperforms the other sampling schemes.","authors":["Junghyun Lee","Branislav Kveton","Sunav Choudhary","Subhojyoti Mukherjee","Anup Rao","Ryan A. Rossi","Alexa Siu"],"pdf_url":"","comment":"12 pages, 3 figures, 1 table"},{"id":"http://arxiv.org/abs/2512.20168v1","updated":"2025-12-23T08:53:36Z","published":"2025-12-23T08:53:36Z","title":"Odysseus: Jailbreaking Commercial Multimodal LLM-integrated Systems via Dual Steganography","summary":"By integrating language understanding with perceptual modalities such as images, multimodal large language models (MLLMs) constitute a critical substrate for modern AI systems, particularly intelligent agents operating in open and interactive environments. However, their increasing accessibility also raises heightened risks of misuse, such as generating harmful or unsafe content. To mitigate these risks, alignment techniques are commonly applied to align model behavior with human values. Despite these efforts, recent studies have shown that jailbreak attacks can circumvent alignment and elicit unsafe outputs. Currently, most existing jailbreak methods are tailored for open-source models and exhibit limited effectiveness against commercial MLLM-integrated systems, which often employ additional filters. These filters can detect and prevent malicious input and output content, significantly reducing jailbreak threats. In this paper, we reveal that the success of these safety filters heavily relies on a critical assumption that malicious content must be explicitly visible in either the input or the output. This assumption, while often valid for traditional LLM-integrated systems, breaks down in MLLM-integrated systems, where attackers can leverage multiple modalities to conceal adversarial intent, leading to a false sense of security in existing MLLM-integrated systems. To challenge this assumption, we propose Odysseus, a novel jailbreak paradigm that introduces dual steganography to covertly embed malicious queries and responses into benign-looking images. Extensive experiments on benchmark datasets demonstrate that our Odysseus successfully jailbreaks several pioneering and realistic MLLM-integrated systems, achieving up to 99% attack success rate. It exposes a fundamental blind spot in existing defenses, and calls for rethinking cross-modal security in MLLM-integrated systems.","authors":["Songze Li","Jiameng Cheng","Yiming Li","Xiaojun Jia","Dacheng Tao"],"pdf_url":"","comment":"This paper is accepted by Network and Distributed System Security Symposium (NDSS) 2026"},{"id":"http://arxiv.org/abs/2510.23649v3","updated":"2025-12-23T08:47:31Z","published":"2025-10-25T11:43:27Z","title":"Efficient Low Rank Attention for Long-Context Inference in Large Language Models","summary":"As the length of input text increases, the key-value (KV) cache in LLMs imposes prohibitive GPU memory costs and limits long-context inference on resource constrained devices. Existing approaches, such as KV quantization and pruning, reduce memory usage but suffer from numerical precision loss or suboptimal retention of key-value pairs. In this work, Low Rank Query and Key attention (LRQK) is introduced, a two-stage framework that jointly decomposes full-precision query and key matrices into compact rank-\\(r\\) factors during the prefill stage, and then employs these low-dimensional projections to compute proxy attention scores in \\(\\mathcal{O}(lr)\\) time at each decode step. By selecting only the top-\\(k\\) tokens and a small fixed set of recent tokens, LRQK employs a mixed GPU-CPU cache with a hit-and-miss mechanism where only missing full-precision KV pairs are transferred, thereby preserving exact attention outputs while reducing CPU-GPU data movement. Extensive experiments on the RULER and LongBench benchmarks with LLaMA-3-8B and Qwen2.5-7B demonstrate that LRQK matches or surpasses leading sparse-attention methods in long context settings, while delivering significant memory savings with minimal accuracy loss. Our code is available at https://github.com/tenghuilee/LRQK.","authors":["Tenghui Li","Guoxu Zhou","Xuyang Zhao","Yuning Qiu","Qibin Zhao"],"pdf_url":"","comment":"https://neurips.cc/virtual/2025/loc/san-diego/poster/118451"},{"id":"http://arxiv.org/abs/2508.20295v2","updated":"2025-12-23T08:30:32Z","published":"2025-08-27T22:03:19Z","title":"FedReFT: Federated Representation Fine-Tuning with All-But-Me Aggregation","summary":"Parameter-efficient fine-tuning (PEFT) adapts large pre-trained models by updating only a small subset of parameters. Recently, Representation Fine-Tuning (ReFT) has emerged as an effective alternative. ReFT shifts the fine-tuning paradigm from updating model weights to directly manipulating hidden representations that capture rich semantic information, and outperforms state-of-the-art PEFTs in standalone settings. However, its application in Federated Learning (FL) remains challenging due to heterogeneity in clients' data distributions, model capacities, and computational resources. To address these challenges, we introduce Federated Representation Fine-Tuning (FedReFT), a novel approach to fine-tune clients' hidden representations. FedReFT applies sparse intervention layers to steer hidden representations directly, offering a lightweight and semantically rich fine-tuning alternative ideal for edge devices. However, representation-level updates are especially vulnerable to aggregation mismatch under different task heterogeneity, where naive averaging can corrupt semantic alignment. To mitigate this issue, we propose All-But-Me (ABM) aggregation, where each client receives the aggregated updates of others and partially incorporates them, enabling stable and personalized learning by balancing local focus with global knowledge. We further design an adaptive update strategy inspired by Test-Time Computing (TTC) to balance local and global contributions under heterogeneous conditions. FedReFT achieves state-of-the-art performance on commonsense reasoning, arithmetic reasoning, and GLUE benchmarks, while delivering 1-49 times higher parameter efficiency compared to leading LoRA-based methods.","authors":["Fatema Siddika","Md Anwar Hossen","J. Pablo Muñoz","Tanya Roosta","Anuj Sharma","Ali Jannesari"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.01457v4","updated":"2025-12-23T08:18:03Z","published":"2025-12-01T09:44:31Z","title":"Zero-Overhead Introspection for Adaptive Test-Time Compute","summary":"Large language models excel at reasoning but lack key aspects of introspection, including anticipating their own success and the computation required to achieve it. Humans use real-time introspection to decide how much effort to invest, when to make multiple attempts, when to stop, and when to signal success or failure. Without this, LLMs struggle to make intelligent meta-cognition decisions. Test-time scaling methods like Best-of-N drive up cost and latency by using a fixed budget of samples regardless of the marginal benefit of each one at any point in generation, and the absence of confidence signals can mislead people, prevent appropriate escalation to better tools, and undermine trustworthiness. Learned verifiers or reward models can provide confidence estimates, but do not enable adaptive inference and add substantial cost by requiring extra models or forward passes. We present ZIP-RC, which equips models with zero-overhead introspective predictions of reward and cost. At every token, ZIP-RC reuses reserved or unused logits in the same forward pass as next-token prediction to output a joint distribution over final reward and remaining length -- no extra models, architecture change, or inference overhead. This full joint distribution is used to compute a sampling utility which is the linear combination of the expected maximum reward, total compute, and latency of set of samples if generated to completion. During inference, we maximize this utility with meta-actions that determine which prefix of tokens to continue or initiate sampling from. On mixed-difficulty mathematical benchmarks, ZIP-RC improves accuracy by up to 12% over majority voting at equal or lower average cost, and traces smooth Pareto frontiers between quality, compute, and latency. By providing real-time reward-cost introspection, ZIP-RC enables adaptive, efficient reasoning.","authors":["Rohin Manvi","Joey Hong","Tim Seyde","Maxime Labonne","Mathias Lechner","Sergey Levine"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20145v1","updated":"2025-12-23T08:15:34Z","published":"2025-12-23T08:15:34Z","title":"Retrieval-augmented Prompt Learning for Pre-trained Foundation Models","summary":"The pre-trained foundation models (PFMs) have become essential for facilitating large-scale multimodal learning. Researchers have effectively employed the ``pre-train, prompt, and predict'' paradigm through prompt learning to induce improved few-shot performance. However, prompt learning approaches for PFMs still follow a parametric learning paradigm. As such, the stability of generalization in memorization and rote learning can be compromised. More specifically, conventional prompt learning might face difficulties in fully utilizing atypical instances and avoiding overfitting to shallow patterns with limited data during the process of fully-supervised training. To overcome these constraints, we present our approach, named RetroPrompt, which aims to achieve a balance between memorization and generalization by decoupling knowledge from mere memorization. Unlike traditional prompting methods, RetroPrompt leverages a publicly accessible knowledge base generated from the training data and incorporates a retrieval mechanism throughout the input, training, and inference stages. This enables the model to actively retrieve relevant contextual information from the corpus, thereby enhancing the available cues. We conduct comprehensive experiments on a variety of datasets across natural language processing and computer vision tasks to demonstrate the superior performance of our proposed approach, RetroPrompt, in both zero-shot and few-shot scenarios. Through detailed analysis of memorization patterns, we observe that RetroPrompt effectively reduces the reliance on rote memorization, leading to enhanced generalization.","authors":["Xiang Chen","Yixin Ou","Quan Feng","Lei Li","Piji Li","Haibo Ye","Sheng-Jun Huang","Shuofei Qiao","Shumin Deng","Huajun Chen","Ningyu Zhang"],"pdf_url":"","comment":"IEEE/ACM Transactions on Audio, Speech and Language Processing"},{"id":"http://arxiv.org/abs/2509.10825v3","updated":"2025-12-23T08:14:20Z","published":"2025-09-13T14:44:45Z","title":"ORACLE: Explaining Feature Interactions in Neural Networks with ANOVA","summary":"We introduce ORACLE, a framework for explaining neural networks on tabular data and scientific factorial designs. ORACLE summarizes a trained network's prediction surface with main effects and pairwise interactions by treating the network as a black-box response, discretizing the inputs onto a grid, and fitting an orthogonal factorial (ANOVA-style) surrogate -- the $L^2$ orthogonal projection of the model response onto a finite-dimensional factorial subspace. A simple centering and $μ$-rebalancing step then expresses this surrogate as main- and interaction-effect tables that remain faithful to the original model in the $L^2$ sense. The resulting grid-based interaction maps are easy to visualize, comparable across backbones, and directly aligned with classical design-of-experiments practice. On synthetic factorial benchmarks and low- to medium-dimensional tabular regression tasks, ORACLE more accurately recovers ground-truth interaction structure and hotspots than Monte Carlo SHAP-family interaction methods, as measured by ranking, localization, and cross-backbone stability. In latent image and text settings, ORACLE clarifies its scope: grid-based factorial surrogates are most effective when features admit an interpretable factorial structure, making ORACLE particularly well-suited to scientific and engineering workflows that require stable, DoE-style interaction summaries.","authors":["Dongseok Kim","Hyoungsun Choi","Mohamed Jismy Aashik Rasool","Gisung Oh"],"pdf_url":"","comment":"v3: Minor wording edits for clarity; no technical changes"},{"id":"http://arxiv.org/abs/2512.18190v2","updated":"2025-12-23T08:10:47Z","published":"2025-12-20T03:27:11Z","title":"External Hippocampus: Topological Cognitive Maps for Guiding Large Language Model Reasoning","summary":"This paper proposes the External Hippocampus framework, which models language model reasoning from a cognitive dynamics perspective as the flow of information energy in semantic space. Unlike traditional weight-space optimization methods, this framework constructs topological cognitive maps through dimensionality reduction projection, enabling precise navigation and intervention of energy flow at test time while avoiding substantial computational requirements and demonstrating predictable intervention patterns. The method effectively addresses the cognitive deadlock problem in multi-step reasoning for small models. Experiments on models <=7B parameters show: map-guided methods achieve 81.20% accuracy on 500 challenging problems (relative baseline +16.80%), reduce reasoning time by >= 15x, with key findings revealing that reasoning stagnation manifests as \"Cognitive Vortex\" and low-entropy potential wells, while temperature perturbations effectively restart energy flow. The framework requires no additional training, possesses autonomous growth capability, and provides an efficient and controllable topological-aware solution for small model reasoning.","authors":["Jian Yan"],"pdf_url":"","comment":"12 pages, 7 figures"},{"id":"http://arxiv.org/abs/2509.23129v2","updated":"2025-12-23T07:56:48Z","published":"2025-09-27T05:24:51Z","title":"C$^2$GSPG: Confidence-calibrated Group Sequence Policy Gradient towards Self-aware Reasoning","summary":"Reinforcement Learning (RL) methods, exemplified by Group Relative Policy Optimization (GRPO) and its variants, play a central role in developing reasoning models. However, these methods often suffer from a critical overconfidence issue, which prevents them from achieving self-aware reasoning models. In this study, we propose a simple yet effective confidence-calibration group sequence policy gradient method, called C$^2$GSPG, which simultaneously enhances reasoning performance while suppressing overconfidence. In principle, we propose a Group Sequence Policy Gradient (GSPG) framework for learning reasoning models, which eliminates the token-level bias commonly appearing in GRPO and its variants. In this framework, we define the model confidence for each reasoning problem using the normalized sequence-level probability, and then apply a cross-entropy regularizer to calibrate the model confidence to the sequence's reward. We demonstrate that the confidence calibration regularizer and GSPG are collaborative for binary rewards, as their objectives always share the same gradient direction. For non-binary rewards, we apply nonlinear reward normalization and adaptive regularizer clipping, mitigating the potential conflict between the two objectives. Applying C$^2$GSPG to post-train large language models in logical and mathematical reasoning tasks, we show its superiority over state-of-the-art methods in both reasoning accuracy and confidence calibration. The code of C$^2$GSPG is available at https://github.com/HaotianLiu123/CCGSPG.","authors":["Haotian Liu","Shuo Wang","Hongteng Xu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20115v1","updated":"2025-12-23T07:19:19Z","published":"2025-12-23T07:19:19Z","title":"Sample-Efficient Policy Constraint Offline Deep Reinforcement Learning based on Sample Filtering","summary":"Offline reinforcement learning (RL) aims to learn a policy that maximizes the expected return using a given static dataset of transitions. However, offline RL faces the distribution shift problem. The policy constraint offline RL method is proposed to solve the distribution shift problem. During the policy constraint offline RL training, it is important to ensure the difference between the learned policy and behavior policy within a given threshold. Thus, the learned policy heavily relies on the quality of the behavior policy. However, a problem exists in existing policy constraint methods: if the dataset contains many low-reward transitions, the learned will be contained with a suboptimal reference policy, leading to slow learning speed, low sample efficiency, and inferior performances. This paper shows that the sampling method in policy constraint offline RL that uses all the transitions in the dataset can be improved. A simple but efficient sample filtering method is proposed to improve the sample efficiency and the final performance. First, we evaluate the score of the transitions by average reward and average discounted reward of episodes in the dataset and extract the transition samples of high scores. Second, the high-score transition samples are used to train the offline RL algorithms. We verify the proposed method in a series of offline RL algorithms and benchmark tasks. Experimental results show that the proposed method outperforms baselines.","authors":["Yuanhao Chen","Qi Liu","Pengbin Chen","Zhongjian Qiao","Yanjie Li"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20111v1","updated":"2025-12-23T07:11:26Z","published":"2025-12-23T07:11:26Z","title":"ABBEL: LLM Agents Acting through Belief Bottlenecks Expressed in Language","summary":"As the length of sequential decision-making tasks increases, it becomes computationally impractical to keep full interaction histories in context. We introduce a general framework for LLM agents to maintain concise contexts through multi-step interaction: Acting through Belief Bottlenecks Expressed in Language (ABBEL), and methods to further improve ABBEL agents with RL post-training. ABBEL replaces long multi-step interaction history by a belief state, i.e., a natural language summary of what has been discovered about task-relevant unknowns. Under ABBEL, at each step the agent first updates a prior belief with the most recent observation from the environment to form a posterior belief, then uses only the posterior to select an action. We systematically evaluate frontier models under ABBEL across six diverse multi-step environments, finding that ABBEL supports generating interpretable beliefs while maintaining near-constant memory use over interaction steps. However, bottleneck approaches are generally prone to error propagation, which we observe causing inferior performance when compared to the full context setting due to errors in belief updating. Therefore, we train LLMs to generate and act on beliefs within the ABBEL framework via reinforcement learning (RL). We experiment with belief grading, to reward higher quality beliefs, as well as belief length penalties to reward more compressed beliefs. Our experiments demonstrate the ability of RL to improve ABBEL's performance beyond the full context setting, while using less memory than contemporaneous approaches.","authors":["Aly Lidayan","Jakob Bjorner","Satvik Golechha","Kartik Goyal","Alane Suhr"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2401.15894v3","updated":"2025-12-23T06:57:26Z","published":"2024-01-29T05:26:17Z","title":"Enhancing Topological Dependencies in Spatio-Temporal Graphs with Cycle Message Passing Blocks","summary":"Graph Neural Networks (GNNs) and Transformer-based models have been increasingly adopted to learn the complex vector representations of spatio-temporal graphs, capturing intricate spatio-temporal dependencies crucial for applications such as traffic datasets. Although many existing methods utilize multi-head attention mechanisms and message-passing neural networks (MPNNs) to capture both spatial and temporal relations, these approaches encode temporal and spatial relations independently, and reflect the graph's topological characteristics in a limited manner. In this work, we introduce the Cycle to Mixer (Cy2Mixer), a novel spatio-temporal GNN based on topological non-trivial invariants of spatio-temporal graphs with gated multi-layer perceptrons (gMLP). The Cy2Mixer is composed of three blocks based on MLPs: A temporal block for capturing temporal properties, a message-passing block for encapsulating spatial information, and a cycle message-passing block for enriching topological information through cyclic subgraphs. We bolster the effectiveness of Cy2Mixer with mathematical evidence emphasizing that our cycle message-passing block is capable of offering differentiated information to the deep learning model compared to the message-passing block. Furthermore, empirical evaluations substantiate the efficacy of the Cy2Mixer, demonstrating state-of-the-art performances across various spatio-temporal benchmark datasets. The source code is available at https://github.com/leemingo/cy2mixer.","authors":["Minho Lee","Yun Young Choi","Sun Woo Park","Seunghwan Lee","Joohwan Ko","Jaeyoung Hong"],"pdf_url":"","comment":"Proceedings of the Third Learning on Graphs Conference (LoG 2024)"},{"id":"http://arxiv.org/abs/2512.20096v1","updated":"2025-12-23T06:49:33Z","published":"2025-12-23T06:49:33Z","title":"Information-directed sampling for bandits: a primer","summary":"The Multi-Armed Bandit problem provides a fundamental framework for analyzing the tension between exploration and exploitation in sequential learning. This paper explores Information Directed Sampling (IDS) policies, a class of heuristics that balance immediate regret against information gain. We focus on the tractable environment of two-state Bernoulli bandits as a minimal model to rigorously compare heuristic strategies against the optimal policy. We extend the IDS framework to the discounted infinite-horizon setting by introducing a modified information measure and a tuning parameter to modulate the decision-making behavior. We examine two specific problem classes: symmetric bandits and the scenario involving one fair coin. In the symmetric case we show that IDS achieves bounded cumulative regret, whereas in the one-fair-coin scenario the IDS policy yields a regret that scales logarithmically with the horizon, in agreement with classical asymptotic lower bounds. This work serves as a pedagogical synthesis, aiming to bridge concepts from reinforcement learning and information theory for an audience of statistical physicists.","authors":["Annika Hirling","Giorgio Nicoletti","Antonio Celani"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20094v1","updated":"2025-12-23T06:44:31Z","published":"2025-12-23T06:44:31Z","title":"Jensen-Shannon Divergence Message-Passing for Rich-Text Graph Representation Learning","summary":"In this paper, we investigate how the widely existing contextual and structural divergence may influence the representation learning in rich-text graphs. To this end, we propose Jensen-Shannon Divergence Message-Passing (JSDMP), a new learning paradigm for rich-text graph representation learning. Besides considering similarity regarding structure and text, JSDMP further captures their corresponding dissimilarity by Jensen-Shannon divergence. Similarity and dissimilarity are then jointly used to compute new message weights among text nodes, thus enabling representations to learn with contextual and structural information from truly correlated text nodes. With JSDMP, we propose two novel graph neural networks, namely Divergent message-passing graph convolutional network (DMPGCN) and Divergent message-passing Page-Rank graph neural networks (DMPPRG), for learning representations in rich-text graphs. DMPGCN and DMPPRG have been extensively texted on well-established rich-text datasets and compared with several state-of-the-art baselines. The experimental results show that DMPGCN and DMPPRG can outperform other baselines, demonstrating the effectiveness of the proposed Jensen-Shannon Divergence Message-Passing paradigm","authors":["Zuo Wang","Ye Yuan"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2511.09586v3","updated":"2025-12-23T06:43:03Z","published":"2025-11-12T12:56:25Z","title":"Environment Scaling for Interactive Agentic Experience Collection: A Survey","summary":"LLM-based agents can autonomously accomplish complex tasks across various domains. However, to further cultivate capabilities such as adaptive behavior and long-term decision-making, training on static datasets built from human-level knowledge is insufficient. These datasets are costly to construct and lack both dynamism and realism. A growing consensus is that agents should instead interact directly with environments and learn from experience through reinforcement learning. We formalize this iterative process as the Generation-Execution-Feedback (GEF) loop, where environments generate tasks to challenge agents, return observations in response to agents' actions during task execution, and provide evaluative feedback on rollouts for subsequent learning. Under this paradigm, environments function as indispensable producers of experiential data, highlighting the need to scale them toward greater complexity, realism, and interactivity. In this survey, we systematically review representative methods for environment scaling from a pioneering environment-centric perspective and organize them along the stages of the GEF loop, namely task generation, task execution, and feedback. We further analyze implementation frameworks, challenges, and applications, consolidating fragmented advances and outlining future research directions for agent intelligence.","authors":["Yuchen Huang","Sijia Li","Minghao Liu","Wei Liu","Shijue Huang","Zhiyuan Fan","Hou Pong Chan","Yi R. Fung"],"pdf_url":"","comment":"22 pages, 5 figures, SEA Workshop @ NeurIPS 2025"},{"id":"http://arxiv.org/abs/2108.03336v2","updated":"2025-12-23T06:30:15Z","published":"2021-08-06T23:52:30Z","title":"Estimating Graph Dimension with Cross-validated Eigenvalues","summary":"In applied multivariate statistics, estimating the number of latent dimensions or the number of clusters, $k$, is a fundamental and recurring problem. We study a sequence of statistics called \"cross-validated eigenvalues.\" Under a large class of random graph models, including both Poisson and Bernoulli edges, without parametric assumptions, we provide a $p$-value for each cross-validated eigenvalue. It tests the null hypothesis that the sample eigenvector is orthogonal to (i.e., uncorrelated with) the true latent dimensions. This approach naturally adapts to problems where some dimensions are not statistically detectable. In scenarios where all $k$ dimensions can be estimated, we show that our procedure consistently estimates $k$. In simulations and data example, the proposed estimator compares favorably to alternative approaches in both computational and statistical performance.","authors":["Fan Chen","Sebastien Roch","Karl Rohe","Shuqi Yu"],"pdf_url":"","comment":"63 pages, 12 figures"},{"id":"http://arxiv.org/abs/2512.20086v1","updated":"2025-12-23T06:28:12Z","published":"2025-12-23T06:28:12Z","title":"Spatio-Temporal Graphs Beyond Grids: Benchmark for Maritime Anomaly Detection","summary":"Spatio-temporal graph neural networks (ST-GNNs) have achieved notable success in structured domains such as road traffic and public transportation, where spatial entities can be naturally represented as fixed nodes. In contrast, many real-world systems including maritime traffic lack such fixed anchors, making the construction of spatio-temporal graphs a fundamental challenge. Anomaly detection in these non-grid environments is particularly difficult due to the absence of canonical reference points, the sparsity and irregularity of trajectories, and the fact that anomalies may manifest at multiple granularities. In this work, we introduce a novel benchmark dataset for anomaly detection in the maritime domain, extending the Open Maritime Traffic Analysis Dataset (OMTAD) into a benchmark tailored for graph-based anomaly detection. Our dataset enables systematic evaluation across three different granularities: node-level, edge-level, and graph-level anomalies. We plan to employ two specialized LLM-based agents: \\emph{Trajectory Synthesizer} and \\emph{Anomaly Injector} to construct richer interaction contexts and generate semantically meaningful anomalies. We expect this benchmark to promote reproducibility and to foster methodological advances in anomaly detection for non-grid spatio-temporal systems.","authors":["Jeehong Kim","Youngseok Hwang","Minchan Kim","Sungho Bae","Hyunwoo Park"],"pdf_url":"","comment":"Accepted at NeurIPS 2025 Workshop in AI for Science: The Reach and Limits of AI for Scientific Discovery"},{"id":"http://arxiv.org/abs/2512.20084v1","updated":"2025-12-23T06:27:30Z","published":"2025-12-23T06:27:30Z","title":"QE-Catalytic: A Graph-Language Multimodal Base Model for Relaxed-Energy Prediction in Catalytic Adsorption","summary":"Adsorption energy is a key descriptor of catalytic reactivity. It is fundamentally defined as the difference between the relaxed total energy of the adsorbate-surface system and that of an appropriate reference state; therefore, the accuracy of relaxed-energy prediction directly determines the reliability of machine-learning-driven catalyst screening. E(3)-equivariant graph neural networks (GNNs) can natively operate on three-dimensional atomic coordinates under periodic boundary conditions and have demonstrated strong performance on such tasks. In contrast, language-model-based approaches, while enabling human-readable textual descriptions and reducing reliance on explicit graph -- thereby broadening applicability -- remain insufficient in both adsorption-configuration energy prediction accuracy and in distinguishing ``the same system with different configurations,'' even with graph-assisted pretraining in the style of GAP-CATBERTa.\n  To this end, we propose QE-Catalytic, a multimodal framework that deeply couples a large language model (\\textbf{Q}wen) with an E(3)-equivariant graph Transformer (\\textbf{E}quiformer-V2), enabling unified support for adsorption-configuration property prediction and inverse design on complex catalytic surfaces. During prediction, QE-Catalytic jointly leverages three-dimensional structures and structured configuration text, and injects ``3D geometric information'' into the language channel via graph-text alignment, allowing it to function as a high-performance text-based predictor when precise coordinates are unavailable, while also autoregressively generating CIF files for target-energy-driven structure design and information completion. On OC20, QE-Catalytic reduces the MAE of relaxed adsorption energy from 0.713~eV to 0.486~eV, and consistently outperforms baseline models such as CatBERTa and GAP-CATBERTa across multiple evaluation protocols.","authors":["Yanjie Li","Jian Xu","Xueqing Chen","Lina Yu","Shiming Xiang","Weijun Li","Cheng-lin Liu"],"pdf_url":"","comment":"25 pages"},{"id":"http://arxiv.org/abs/2402.04536v3","updated":"2025-12-23T06:22:42Z","published":"2024-02-07T02:50:56Z","title":"Tactile-based Object Retrieval From Granular Media","summary":"We introduce GEOTACT, the first robotic system capable of grasping and retrieving objects of potentially unknown shapes buried in a granular environment. While important in many applications, ranging from mining and exploration to search and rescue, this type of interaction with granular media is difficult due to the uncertainty stemming from visual occlusion and noisy contact signals. To address these challenges, we use a learning method relying exclusively on touch feedback, trained end-to-end with simulated sensor noise. We show that our problem formulation leads to the natural emergence of learned pushing behaviors that the manipulator uses to reduce uncertainty and funnel the object to a stable grasp despite spurious and noisy tactile readings. We introduce a training curriculum that bootstraps learning in simulated granular environments, enabling zero-shot transfer to real hardware. Despite being trained only on seven objects with primitive shapes, our method is shown to successfully retrieve 35 different objects, including rigid, deformable, and articulated objects with complex shapes. Videos and additional information can be found at https://jxu.ai/geotact.","authors":["Jingxi Xu","Yinsen Jia","Dongxiao Yang","Patrick Meng","Xinyue Zhu","Zihan Guo","Shuran Song","Matei Ciocarlie"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2505.08528v2","updated":"2025-12-23T06:18:57Z","published":"2025-05-13T13:01:38Z","title":"GradMix: Gradient-based Selective Mixup for Robust Data Augmentation in Class-Incremental Learning","summary":"In the context of continual learning, acquiring new knowledge while maintaining previous knowledge presents a significant challenge. Existing methods often use experience replay techniques that store a small portion of previous task data for training. In experience replay approaches, data augmentation has emerged as a promising strategy to further improve the model performance by mixing limited previous task data with sufficient current task data. However, we theoretically and empirically analyze that training with mixed samples from random sample pairs may harm the knowledge of previous tasks and cause greater catastrophic forgetting. We then propose GradMix, a robust data augmentation method specifically designed for mitigating catastrophic forgetting in class-incremental learning. GradMix performs gradient-based selective mixup using a class-based criterion that mixes only samples from helpful class pairs and not from detrimental class pairs for reducing catastrophic forgetting. Our experiments on various real datasets show that GradMix outperforms data augmentation baselines in accuracy by minimizing the forgetting of previous knowledge.","authors":["Minsu Kim","Seong-Hyeon Hwang","Steven Euijong Whang"],"pdf_url":"","comment":"Accepted to KDD 2026"},{"id":"http://arxiv.org/abs/2510.04944v2","updated":"2025-12-23T05:51:37Z","published":"2025-10-06T15:46:50Z","title":"On Structured State-Space Duality","summary":"Structured State-Space Duality (SSD) [Dao & Gu, ICML 2024] is an equivalence between a simple Structured State-Space Model (SSM) and a masked attention mechanism. In particular, a state-space model with a scalar-times-identity state matrix is equivalent to a masked self-attention with a $1$-semiseparable causal mask. Consequently, the same sequence transformation (model) has two algorithmic realizations: as a linear-time $O(T)$ recurrence or as a quadratic-time $O(T^2)$ attention. In this note, we formalize and generalize this duality: (i) we extend SSD from the scalar-identity case to general diagonal SSMs (diagonal state matrices); (ii) we show that these diagonal SSMs match the scalar case's training complexity lower bounds while supporting richer dynamics; (iii) we establish a necessary and sufficient condition under which an SSM is equivalent to $1$-semiseparable masked attention; and (iv) we show that such duality fails to extend to standard softmax attention due to rank explosion. Together, these results tighten bridge between recurrent SSMs and Transformers, and widen the design space for expressive yet efficient sequence models.","authors":["Jerry Yao-Chieh Hu","Xiwen Zhang","Ali ElSheikh","Weimin Wu","Han Liu"],"pdf_url":"","comment":"v2 fixed typos and added numerical results (Appendix B)"},{"id":"http://arxiv.org/abs/2408.02152v3","updated":"2025-12-23T05:50:45Z","published":"2024-08-04T22:00:34Z","title":"Generative Retrieval with Few-shot Indexing","summary":"Existing generative retrieval (GR) methods rely on training-based indexing, which fine-tunes a model to memorise associations between queries and the document identifiers (docids) of relevant documents. Training-based indexing suffers from high training costs, under-utilisation of pre-trained knowledge in large language models (LLMs), and limited adaptability to dynamic document corpora. To address the issues, we propose a few-shot indexing-based GR framework (Few-Shot GR). It has a few-shot indexing process without any training, where we prompt an LLM to generate docids for all documents in a corpus, ultimately creating a docid bank for the entire corpus. During retrieval, we feed a query to the same LLM and constrain it to generate a docid within the docid bank created during indexing, and then map the generated docid back to its corresponding document. Moreover, we devise few-shot indexing with one-to-many mapping to further enhance Few-Shot GR. Experiments show that Few-Shot GR achieves superior performance to state-of-the-art GR methods requiring heavy training.","authors":["Arian Askari","Chuan Meng","Mohammad Aliannejadi","Zhaochun Ren","Evangelos Kanoulas","Suzan Verberne"],"pdf_url":"","comment":"Accepted for publication at the 48th European Conference on Information Retrieval (ECIR 2026)"},{"id":"http://arxiv.org/abs/2412.17231v3","updated":"2025-12-23T05:48:23Z","published":"2024-12-23T02:58:12Z","title":"FedMeld: A Model-dispersal Federated Learning Framework for Space-ground Integrated Networks","summary":"To bridge the digital divide, space-ground integrated networks (SGINs) are expected to deliver artificial intelligence (AI) services to every corner of the world. One key mission of SGINs is to support federated learning (FL) at a global scale. However, existing space-ground integrated FL frameworks involve ground stations or costly inter-satellite links, entailing excessive training latency and communication costs. To overcome these limitations, we propose an infrastructure-free federated learning framework based on a model dispersal (FedMeld) strategy, which exploits periodic movement patterns and store-carry-forward capabilities of satellites to enable parameter mixing across large-scale geographical regions. We theoretically show that FedMeld leads to global model convergence and quantify the effects of round interval and mixing ratio between adjacent areas on its learning performance. Based on the theoretical results, we formulate a joint optimization problem to design the staleness control and mixing ratio (SC-MR) for minimizing the training loss. By decomposing the problem into sequential SC and MR subproblems without compromising the optimality, we derive the round interval solution in a closed form and the mixing ratio in a semi-closed form to achieve the optimal latency-accuracy tradeoff. Experiments using various datasets demonstrate that FedMeld achieves superior model accuracy while significantly reducing communication costs as compared with traditional FL schemes for SGINs.","authors":["Qian Chen","Xianhao Chen","Kaibin Huang"],"pdf_url":"","comment":"17 pages, 10 figures. This paper has been accepted by IEEE Transactions on Mobile Computing"},{"id":"http://arxiv.org/abs/2512.20063v1","updated":"2025-12-23T05:31:56Z","published":"2025-12-23T05:31:56Z","title":"PairFlow: Closed-Form Source-Target Coupling for Few-Step Generation in Discrete Flow Models","summary":"We introduce $\\texttt{PairFlow}$, a lightweight preprocessing step for training Discrete Flow Models (DFMs) to achieve few-step sampling without requiring a pretrained teacher. DFMs have recently emerged as a new class of generative models for discrete data, offering strong performance. However, they suffer from slow sampling due to their iterative nature. Existing acceleration methods largely depend on finetuning, which introduces substantial additional training overhead. $\\texttt{PairFlow}$ addresses this issue with a lightweight preprocessing step. Inspired by ReFlow and its extension to DFMs, we train DFMs from coupled samples of source and target distributions, without requiring any pretrained teacher. At the core of our approach is a closed-form inversion for DFMs, which allows efficient construction of paired source-target samples. Despite its extremely low cost, taking only up to 1.7% of the compute needed for full model training, $\\texttt{PairFlow}$ matches or even surpasses the performance of two-stage training involving finetuning. Furthermore, models trained with our framework provide stronger base models for subsequent distillation, yielding further acceleration after finetuning. Experiments on molecular data as well as binary and RGB images demonstrate the broad applicability and effectiveness of our approach.","authors":["Mingue Park","Jisung Hwang","Seungwoo Yoo","Kyeongmin Yeo","Minhyuk Sung"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20059v1","updated":"2025-12-23T05:23:06Z","published":"2025-12-23T05:23:06Z","title":"DS-HGCN: A Dual-Stream Hypergraph Convolutional Network for Predicting Student Engagement via Social Contagion","summary":"Student engagement is a critical factor influencing academic success and learning outcomes. Accurately predicting student engagement is essential for optimizing teaching strategies and providing personalized interventions. However, most approaches focus on single-dimensional feature analysis and assessing engagement based on individual student factors. In this work, we propose a dual-stream multi-feature fusion model based on hypergraph convolutional networks (DS-HGCN), incorporating social contagion of student engagement. DS-HGCN enables accurate prediction of student engagement states by modeling multi-dimensional features and their propagation mechanisms between students. The framework constructs a hypergraph structure to encode engagement contagion among students and captures the emotional and behavioral differences and commonalities by multi-frequency signals. Furthermore, we introduce a hypergraph attention mechanism to dynamically weigh the influence of each student, accounting for individual differences in the propagation process. Extensive experiments on public benchmark datasets demonstrate that our proposed method achieves superior performance and significantly outperforms existing state-of-the-art approaches.","authors":["Ziyang Fan","Li Tao","Yi Wang","Jingwei Qu","Ying Wang","Fei Jiang"],"pdf_url":"","comment":"14pages,Accepted by MMM2026"},{"id":"http://arxiv.org/abs/2512.20058v1","updated":"2025-12-23T05:20:22Z","published":"2025-12-23T05:20:22Z","title":"Deep Eigenspace Network and Its Application to Parametric Non-selfadjoint Eigenvalue Problems","summary":"We consider operator learning for efficiently solving parametric non-selfadjoint eigenvalue problems. To overcome the spectral instability and mode switching inherent in non-selfadjoint operators, we introduce a hybrid framework that learns the stable invariant eigensubspace mapping rather than individual eigenfunctions. We proposed a Deep Eigenspace Network (DEN) architecture integrating Fourier Neural Operators, geometry-adaptive POD bases, and explicit banded cross-mode mixing mechanisms to capture complex spectral dependencies on unstructured meshes. We apply DEN to the parametric non-selfadjoint Steklov eigenvalue problem and provide theoretical proofs for the Lipschitz continuity of the eigensubspace with respect to the parameters. In addition, we derive error bounds for the reconstruction of the eigenspace. Numerical experiments validate DEN's high accuracy and zero-shot generalization capabilities across different discretizations.","authors":["H. Li","J. Sun","Z. Zhang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2509.16189v3","updated":"2025-12-23T05:18:37Z","published":"2025-09-19T17:49:25Z","title":"Latent learning: episodic memory complements parametric learning by enabling flexible reuse of experiences","summary":"When do machine learning systems fail to generalize, and what mechanisms could improve their generalization? Here, we draw inspiration from cognitive science to argue that one weakness of parametric machine learning systems is their failure to exhibit latent learning -- learning information that is not relevant to the task at hand, but that might be useful in a future task. We show how this perspective links failures ranging from the reversal curse in language modeling to new findings on agent-based navigation. We then highlight how cognitive science points to episodic memory as a potential part of the solution to these issues. Correspondingly, we show that a system with an oracle retrieval mechanism can use learning experiences more flexibly to generalize better across many of these challenges. We also identify some of the essential components for effectively using retrieval, including the importance of within-example in-context learning for acquiring the ability to use information across retrieved examples. In summary, our results illustrate one possible contributor to the relative data inefficiency of current machine learning systems compared to natural intelligence, and help to understand how retrieval methods can complement parametric learning to improve generalization. We close by discussing some of the links between these findings and prior results in cognitive science and neuroscience, and the broader implications.","authors":["Andrew Kyle Lampinen","Martin Engelcke","Yuxuan Li","Arslan Chaudhry","James L. McClelland"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20053v1","updated":"2025-12-23T05:03:54Z","published":"2025-12-23T05:03:54Z","title":"An Optimal Policy for Learning Controllable Dynamics by Exploration","summary":"Controllable Markov chains describe the dynamics of sequential decision making tasks and are the central component in optimal control and reinforcement learning. In this work, we give the general form of an optimal policy for learning controllable dynamics in an unknown environment by exploring over a limited time horizon. This policy is simple to implement and efficient to compute, and allows an agent to ``learn by exploring\" as it maximizes its information gain in a greedy fashion by selecting controls from a constraint set that changes over time during exploration. We give a simple parameterization for the set of controls, and present an algorithm for finding an optimal policy. The reason for this policy is due to the existence of certain types of states that restrict control of the dynamics; such as transient states, absorbing states, and non-backtracking states. We show why the occurrence of these states makes a non-stationary policy essential for achieving optimal exploration. Six interesting examples of controllable dynamics are treated in detail. Policy optimality is demonstrated using counting arguments, comparing with suboptimal policies, and by making use of a sequential improvement property from dynamic programming.","authors":["Peter N. Loxley"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17052v2","updated":"2025-12-23T04:20:29Z","published":"2025-12-18T20:40:25Z","title":"Dynamic Tool Dependency Retrieval for Efficient Function Calling","summary":"Function calling agents powered by Large Language Models (LLMs) select external tools to automate complex tasks. On-device agents typically use a retrieval module to select relevant tools, improving performance and reducing context length. However, existing retrieval methods rely on static and limited inputs, failing to capture multi-step tool dependencies and evolving task context. This limitation often introduces irrelevant tools that mislead the agent, degrading efficiency and accuracy. We propose Dynamic Tool Dependency Retrieval (DTDR), a lightweight retrieval method that conditions on both the initial query and the evolving execution context. DTDR models tool dependencies from function calling demonstrations, enabling adaptive retrieval as plans unfold. We benchmark DTDR against state-of-the-art retrieval methods across multiple datasets and LLM backbones, evaluating retrieval precision, downstream task accuracy, and computational efficiency. Additionally, we explore strategies to integrate retrieved tools into prompts. Our results show that dynamic tool retrieval improves function calling success rates between $23\\%$ and $104\\%$ compared to state-of-the-art static retrievers.","authors":["Bhrij Patel","Davide Belli","Amir Jalalirad","Maximilian Arnold","Aleksandr Ermolov","Bence Major"],"pdf_url":"","comment":"18 pages, 5 figures, 6 tables"},{"id":"http://arxiv.org/abs/2512.20039v1","updated":"2025-12-23T04:14:56Z","published":"2025-12-23T04:14:56Z","title":"Optimal Anytime-Valid Tests for Composite Nulls","summary":"We consider the problem of designing optimal level-$α$ power-one tests for composite nulls. Given a parameter $α\\in (0,1)$ and a stream of $\\mathcal{X}$-valued observations $\\{X_n: n \\geq 1\\} \\overset{i.i.d.}{\\sim} P$, the goal is to design a level-$α$ power-one test $τ_α$ for the null $H_0: P \\in \\mathcal{P}_0 \\subset \\mathcal{P}(\\mathcal{X})$. Prior works have shown that any such $τ_α$ must satisfy $\\mathbb{E}_P[τ_α] \\geq \\tfrac{\\log(1/α)}{γ^*(P, \\mathcal{P}_0)}$, where $γ^*(P, \\mathcal{P}_0)$ is the so-called $\\mathrm{KL}_{\\inf}$ or minimum divergence of $P$ to the null class. In this paper, our objective is to develop and analyze constructive schemes that match this lower bound as $α\\downarrow 0$.\n  We first consider the finite-alphabet case~($|\\mathcal{X}| = m < \\infty$), and show that a test based on \\emph{universal} $e$-process~(formed by the ratio of a universal predictor and the running null MLE) is optimal in the above sense. The proof relies on a Donsker-Varadhan~(DV) based saddle-point representation of $\\mathrm{KL}_{\\inf}$, and an application of Sion's minimax theorem. This characterization motivates a general method for arbitrary $\\mathcal{X}$: construct an $e$-process based on the empirical solutions to the saddle-point representation over a sufficiently rich class of test functions. We give sufficient conditions for the optimality of this test for compact convex nulls, and verify them for Hölder smooth density models. We end the paper with a discussion on the computational aspects of implementing our proposed tests in some practical settings.","authors":["Shubhanshu Shekhar"],"pdf_url":"","comment":"24 pages, 1 figure"},{"id":"http://arxiv.org/abs/2504.11671v4","updated":"2025-12-23T04:02:12Z","published":"2025-04-16T00:02:28Z","title":"Computational Basis of LLM's Decision Making in Social Simulation","summary":"Large language models (LLMs) increasingly serve as human-like decision-making agents in social science and applied settings. These LLM-agents are typically assigned human-like characters and placed in real-life contexts. However, how these characters and contexts shape an LLM's behavior remains underexplored. This study proposes and tests methods for probing, quantifying, and modifying an LLM's internal representations in a Dictator Game, a classic behavioral experiment on fairness and prosocial behavior. We extract ``vectors of variable variations'' (e.g., ``male'' to ``female'') from the LLM's internal state. Manipulating these vectors during the model's inference can substantially alter how those variables relate to the model's decision-making. This approach offers a principled way to study and regulate how social concepts can be encoded and engineered within transformer-based models, with implications for alignment, debiasing, and designing AI agents for social simulations in both academic and commercial applications, strengthening sociological theory and measurement.","authors":["Ji Ma"],"pdf_url":"","comment":"Forthcoming: Sociological Methodology; USPTO patent pending"},{"id":"http://arxiv.org/abs/2512.20028v1","updated":"2025-12-23T03:44:49Z","published":"2025-12-23T03:44:49Z","title":"DecoKAN: Interpretable Decomposition for Forecasting Cryptocurrency Market Dynamics","summary":"Accurate and interpretable forecasting of multivariate time series is crucial for understanding the complex dynamics of cryptocurrency markets in digital asset systems. Advanced deep learning methodologies, particularly Transformer-based and MLP-based architectures, have achieved competitive predictive performance in cryptocurrency forecasting tasks. However, cryptocurrency data is inherently composed of long-term socio-economic trends and local high-frequency speculative oscillations. Existing deep learning-based 'black-box' models fail to effectively decouple these composite dynamics or provide the interpretability needed for trustworthy financial decision-making. To overcome these limitations, we propose DecoKAN, an interpretable forecasting framework that integrates multi-level Discrete Wavelet Transform (DWT) for decoupling and hierarchical signal decomposition with Kolmogorov-Arnold Network (KAN) mixers for transparent and interpretable nonlinear modeling. The DWT component decomposes complex cryptocurrency time series into distinct frequency components, enabling frequency-specific analysis, while KAN mixers provide intrinsically interpretable spline-based mappings within each decomposed subseries. Furthermore, interpretability is enhanced through a symbolic analysis pipeline involving sparsification, pruning, and symbolization, which produces concise analytical expressions offering symbolic representations of the learned patterns. Extensive experiments demonstrate that DecoKAN achieves the lowest average Mean Squared Error on all tested real-world cryptocurrency datasets (BTC, ETH, XMR), consistently outperforming a comprehensive suite of competitive state-of-the-art baselines. These results validate DecoKAN's potential to bridge the gap between predictive accuracy and model transparency, advancing trustworthy decision support within complex cryptocurrency markets.","authors":["Yuan Gao","Zhenguo Dong","Xuelong Wang","Zhiqiang Wang","Yong Zhang","Shaofan Wang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19025v2","updated":"2025-12-23T03:34:28Z","published":"2025-12-22T04:42:41Z","title":"The Erasure Illusion: Stress-Testing the Generalization of LLM Forgetting Evaluation","summary":"Machine unlearning aims to remove specific data influences from trained models, a capability essential for adhering to copyright laws and ensuring AI safety. Current unlearning metrics typically measure success by monitoring the model's performance degradation on the specific unlearning dataset ($D_u$). We argue that for Large Language Models (LLMs), this evaluation paradigm is insufficient and potentially misleading. Many real-world uses of unlearning--motivated by copyright or safety--implicitly target not only verbatim content in $D_u$, but also behaviors influenced by the broader generalizations the model derived from it. We demonstrate that LLMs can pass standard unlearning evaluation and appear to have \"forgotten\" the target knowledge, while simultaneously retaining strong capabilities on content that is semantically adjacent to $D_u$. This phenomenon indicates that erasing exact sentences does not necessarily equate to removing the underlying knowledge. To address this gap, we propose Proximal Surrogate Generation (PSG), an automated stress-testing framework that generates a surrogate dataset, $\\tilde{D}_u$. This surrogate set is constructed to be semantically derived from $D_u$ yet sufficiently distinct in embedding space. By comparing unlearning metric scores between $D_u$ and $\\tilde{D}_u$, we can stress-test the reliability of the metric itself. Our extensive evaluation across three LLM families (Llama-3-8B, Qwen2.5-7B, and Zephyr-7B-$β$), three distinct datasets, and seven standard metrics reveals widespread inconsistencies. We find that current metrics frequently overestimate unlearning success, failing to detect retained knowledge exposed by our stress-test datasets.","authors":["Hengrui Jia","Taoran Li","Jonas Guan","Varun Chandrasekaran"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20021v1","updated":"2025-12-23T03:31:35Z","published":"2025-12-23T03:31:35Z","title":"Gaussian Process Assisted Meta-learning for Image Classification and Object Detection Models","summary":"Collecting operationally realistic data to inform machine learning models can be costly. Before collecting new data, it is helpful to understand where a model is deficient. For example, object detectors trained on images of rare objects may not be good at identification in poorly represented conditions. We offer a way of informing subsequent data acquisition to maximize model performance by leveraging the toolkit of computer experiments and metadata describing the circumstances under which the training data was collected (e.g., season, time of day, location). We do this by evaluating the learner as the training data is varied according to its metadata. A Gaussian process (GP) surrogate fit to that response surface can inform new data acquisitions. This meta-learning approach offers improvements to learner performance as compared to data with randomly selected metadata, which we illustrate on both classic learning examples, and on a motivating application involving the collection of aerial images in search of airplanes.","authors":["Anna R. Flowers","Christopher T. Franck","Robert B. Gramacy","Justin A. Krometis"],"pdf_url":"","comment":"15 pages, 8 figures"},{"id":"http://arxiv.org/abs/2410.23066v3","updated":"2025-12-23T03:28:14Z","published":"2024-10-30T14:41:23Z","title":"Don't Pay Attention, PLANT It: Pretraining Attention via Learning-to-Rank","summary":"State-of-the-art Extreme Multi-Label Text Classification models rely on multi-label attention to focus on key tokens in input text, but learning good attention weights is challenging. We introduce PLANT - Pretrained and Leveraged Attention - a plug-and-play strategy for initializing attention. PLANT works by planting label-specific attention using a pretrained Learning-to-Rank model guided by mutual information gain. This architecture-agnostic approach integrates seamlessly with large language model backbones such as Mistral-7B, LLaMA3-8B, DeepSeek-V3, and Phi-3. PLANT outperforms state-of-the-art methods across tasks including ICD coding, legal topic classification, and content recommendation. Gains are especially pronounced in few-shot settings, with substantial improvements on rare labels. Ablation studies confirm that attention initialization is a key driver of these gains. For code and trained models, see https://github.com/debjyotiSRoy/xcube/tree/plant","authors":["Debjyoti Saha Roy","Byron C. Wallace","Javed A. Aslam"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2511.17902v2","updated":"2025-12-23T03:27:52Z","published":"2025-11-22T03:39:13Z","title":"Statistically-Guided Dual-Domain Meta-Learning with Adaptive Multi-Prototype Aggregation for Distributed Fiber Optic Sensing","summary":"Distributed Fiber Optic Sensing (DFOS) is promising for long-range perimeter security, yet practical deployment faces three key obstacles: severe cross-deployment domain shift, scarce or unavailable labels at new sites, and limited within-class coverage even in source deployments. We propose DUPLE, a prototype-based meta-learning framework tailored for cross-deployment DFOS recognition. The core idea is to jointly exploit complementary time- and frequency-domain cues and adapt class representations to sample-specific statistics: (i) a dual-domain learner constructs multi-prototype class representations to cover intra-class heterogeneity; (ii) a lightweight statistical guidance mechanism estimates the reliability of each domain from raw signal statistics; and (iii) a query-adaptive aggregation strategy selects and combines the most relevant prototypes for each query. Extensive experiments on two real-world cross-deployment benchmarks demonstrate consistent improvements over strong deep learning and meta-learning baselines, achieving more accurate and stable recognition under label-scarce target deployments.","authors":["Yifan He","Haodong Zhang","Qiuheng Song","Lin Lei","Zhenxuan Zeng","Haoyang He","Hongyan Wu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2505.15015v2","updated":"2025-12-23T03:19:42Z","published":"2025-05-21T01:35:56Z","title":"Multi-Scale Harmonic Encoding for Feature-Wise Graph Message Passing","summary":"Most Graph Neural Networks (GNNs) propagate messages by treating node embeddings as holistic feature vectors, implicitly assuming uniform relevance across feature dimensions. This limits their ability to selectively transmit informative components, especially when graph structures exhibit distinct frequency characteristics. We propose MSH-GNN (Multi-Scale Harmonic Graph Neural Network), a frequency-aware message passing framework that performs feature-wise adaptive propagation. Each node projects incoming messages onto node-conditioned feature subspaces derived from its own representation, enabling selective extraction of frequency-relevant components. Learnable multi-scale harmonic modulations further allow the model to capture both smooth and oscillatory structural patterns. A frequency-aware attention pooling mechanism is introduced for graph-level readout. We show that MSH-GNN admits an interpretation as a learnable Fourier-feature approximation of kernelized message functions and matches the expressive power of the 1-Weisfeiler-Lehman (1-WL) test. Extensive experiments on node- and graph-level benchmarks demonstrate consistent improvements over state-of-the-art methods, particularly in joint structure-frequency analysis tasks.","authors":["Longlong Li","Mengyang Zhao","Guanghui Wang","Cunquan Qu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20012v1","updated":"2025-12-23T03:10:09Z","published":"2025-12-23T03:10:09Z","title":"Reliable LLM-Based Edge-Cloud-Expert Cascades for Telecom Knowledge Systems","summary":"Large language models (LLMs) are emerging as key enablers of automation in domains such as telecommunications, assisting with tasks including troubleshooting, standards interpretation, and network optimization. However, their deployment in practice must balance inference cost, latency, and reliability. In this work, we study an edge-cloud-expert cascaded LLM-based knowledge system that supports decision-making through a question-and-answer pipeline. In it, an efficient edge model handles routine queries, a more capable cloud model addresses complex cases, and human experts are involved only when necessary. We define a misalignment-cost constrained optimization problem, aiming to minimize average processing cost, while guaranteeing alignment of automated answers with expert judgments. We propose a statistically rigorous threshold selection method based on multiple hypothesis testing (MHT) for a query processing mechanism based on knowledge and confidence tests. The approach provides finite-sample guarantees on misalignment risk. Experiments on the TeleQnA dataset -- a telecom-specific benchmark -- demonstrate that the proposed method achieves superior cost-efficiency compared to conventional cascaded baselines, while ensuring reliability at prescribed confidence levels.","authors":["Qiushuo Hou","Sangwoo Park","Matteo Zecchin","Yunlong Cai","Guanding Yu","Osvaldo Simeone","Tommaso Melodia"],"pdf_url":"","comment":"This paper has been submitted to a journal"},{"id":"http://arxiv.org/abs/2512.20007v1","updated":"2025-12-23T03:05:26Z","published":"2025-12-23T03:05:26Z","title":"Semiparametric KSD test: unifying score and distance-based approaches for goodness-of-fit testing","summary":"Goodness-of-fit (GoF) tests are fundamental for assessing model adequacy. Score-based tests are appealing because they require fitting the model only once under the null. However, extending them to powerful nonparametric alternatives is difficult due to the lack of suitable score functions. Through a class of exponentially tilted models, we show that the resulting score-based GoF tests are equivalent to the tests based on integral probability metrics (IPMs) indexed by a function class. When the class is rich, the test is universally consistent. This simple yet insightful perspective enables reinterpretation of classical distance-based testing procedures-including those based on Kolmogorov-Smirnov distance, Wasserstein-1 distance, and maximum mean discrepancy-as arising from score-based constructions. Building on this insight, we propose a new nonparametric score-based GoF test through a special class of IPM induced by kernelized Stein's function class, called semiparametric kernelized Stein discrepancy (SKSD) test. Compared with other nonparametric score-based tests, the SKSD test is computationally efficient and accommodates general nuisance-parameter estimators, supported by a generic parametric bootstrap procedure. The SKSD test is universally consistent and attains Pitman efficiency. Moreover, SKSD test provides simple GoF tests for models with intractable likelihoods but tractable scores with the help of Stein's identity and we use two popular models, kernel exponential family and conditional Gaussian models, to illustrate the power of our method. Our method achieves power comparable to task-specific normality tests such as Anderson-Darling and Lilliefors, despite being designed for general nonparametric alternatives.","authors":["Zhihan Huang","Ziang Niu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20006v1","updated":"2025-12-23T03:05:25Z","published":"2025-12-23T03:05:25Z","title":"Orthogonal Activation with Implicit Group-Aware Bias Learning for Class Imbalance","summary":"Class imbalance is a common challenge in machine learning and data mining, often leading to suboptimal performance in classifiers. While deep learning excels in feature extraction, its performance still deteriorates under imbalanced data. In this work, we propose a novel activation function, named OGAB, designed to alleviate class imbalance in deep learning classifiers. OGAB incorporates orthogonality and group-aware bias learning to enhance feature distinguishability in imbalanced scenarios without explicitly requiring label information. Our key insight is that activation functions can be used to introduce strong inductive biases that can address complex data challenges beyond traditional non-linearity. Our work demonstrates that orthogonal transformations can preserve information about minority classes by maintaining feature independence, thereby preventing the dominance of majority classes in the embedding space. Further, the proposed group-aware bias mechanism automatically identifies data clusters and adjusts embeddings to enhance class separability without the need for explicit supervision. Unlike existing approaches that address class imbalance through preprocessing data modifications or post-processing corrections, our proposed approach tackles class imbalance during the training phase at the embedding learning level, enabling direct integration with the learning process. We demonstrate the effectiveness of our solution on both real-world and synthetic imbalanced datasets, showing consistent performance improvements over both traditional and learnable activation functions.","authors":["Sukumar Kishanthan","Asela Hevapathige"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.16334v3","updated":"2025-12-23T02:58:42Z","published":"2025-12-18T09:17:45Z","title":"Pretrained Battery Transformer (PBT): A battery life prediction foundation model","summary":"Early prediction of battery cycle life is essential for accelerating battery research, manufacturing, and deployment. Although machine learning methods have shown encouraging results, progress is hindered by data scarcity and heterogeneity arising from diverse aging conditions. In other fields, foundation models (FMs) trained on diverse datasets have achieved broad generalization through transfer learning, but no FMs have been reported for battery cycle life prediction yet. Here we present the Pretrained Battery Transformer (PBT), the first FM for battery life prediction, developed through domain-knowledge-encoded mixture-of-expert layers. Validated on the largest public battery life database, PBT learns transferable representations from 13 lithium-ion battery datasets, outperforming existing models by an average of 19.8%. With transfer learning, PBT achieves state-of-the-art performance across 15 diverse datasets encompassing various operating conditions, formation protocols, and chemistries. This work establishes a foundation model pathway for battery lifetime prediction, paving the way toward universal battery lifetime prediction systems.","authors":["Ruifeng Tan","Weixiang Hong","Jia Li","Jiaqiang Huang","Tong-Yi Zhang"],"pdf_url":"","comment":"5 figures in the main content"},{"id":"http://arxiv.org/abs/2512.20004v1","updated":"2025-12-23T02:57:33Z","published":"2025-12-23T02:57:33Z","title":"IoT-based Android Malware Detection Using Graph Neural Network With Adversarial Defense","summary":"Since the Internet of Things (IoT) is widely adopted using Android applications, detecting malicious Android apps is essential. In recent years, Android graph-based deep learning research has proposed many approaches to extract relationships from applications as graphs to generate graph embeddings. First, we demonstrate the effectiveness of graph-based classification using a Graph Neural Network (GNN)-based classifier to generate API graph embeddings. The graph embeddings are combined with Permission and Intent features to train multiple machine learning and deep learning models for Android malware detection. The proposed classification approach achieves an accuracy of 98.33 percent on the CICMaldroid dataset and 98.68 percent on the Drebin dataset. However, graph-based deep learning models are vulnerable, as attackers can add fake relationships to evade detection by the classifier. Second, we propose a Generative Adversarial Network (GAN)-based attack algorithm named VGAE-MalGAN targeting graph-based GNN Android malware classifiers. The VGAE-MalGAN generator produces adversarial malware API graphs, while the VGAE-MalGAN substitute detector attempts to mimic the target detector. Experimental results show that VGAE-MalGAN can significantly reduce the detection rate of GNN-based malware classifiers. Although the model initially fails to detect adversarial malware, retraining with generated adversarial samples improves robustness and helps mitigate adversarial attacks.","authors":["Rahul Yumlembam","Biju Issac","Seibu Mary Jacob","Longzhi Yang"],"pdf_url":"","comment":"13 pages"},{"id":"http://arxiv.org/abs/2512.20003v1","updated":"2025-12-23T02:55:14Z","published":"2025-12-23T02:55:14Z","title":"Control Variate Score Matching for Diffusion Models","summary":"Diffusion models offer a robust framework for sampling from unnormalized probability densities, which requires accurately estimating the score of the noise-perturbed target distribution. While the standard Denoising Score Identity (DSI) relies on data samples, access to the target energy function enables an alternative formulation via the Target Score Identity (TSI). However, these estimators face a fundamental variance trade-off: DSI exhibits high variance in low-noise regimes, whereas TSI suffers from high variance at high noise levels. In this work, we reconcile these approaches by unifying both estimators within the principled framework of control variates. We introduce the Control Variate Score Identity (CVSI), deriving an optimal, time-dependent control coefficient that theoretically guarantees variance minimization across the entire noise spectrum. We demonstrate that CVSI serves as a robust, low-variance plug-in estimator that significantly enhances sample efficiency in both data-free sampler learning and inference-time diffusion sampling.","authors":["Khaled Kahouli","Romuald Elie","Klaus-Robert Müller","Quentin Berthet","Oliver T. Unke","Arnaud Doucet"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20002v1","updated":"2025-12-23T02:55:04Z","published":"2025-12-23T02:55:04Z","title":"LoFT-LLM: Low-Frequency Time-Series Forecasting with Large Language Models","summary":"Time-series forecasting in real-world applications such as finance and energy often faces challenges due to limited training data and complex, noisy temporal dynamics. Existing deep forecasting models typically supervise predictions using full-length temporal windows, which include substantial high-frequency noise and obscure long-term trends. Moreover, auxiliary variables containing rich domain-specific information are often underutilized, especially in few-shot settings. To address these challenges, we propose LoFT-LLM, a frequency-aware forecasting pipeline that integrates low-frequency learning with semantic calibration via a large language model (LLM). Firstly, a Patch Low-Frequency forecasting Module (PLFM) extracts stable low-frequency trends from localized spectral patches. Secondly, a residual learner then models high-frequency variations. Finally, a fine-tuned LLM refines the predictions by incorporating auxiliary context and domain knowledge through structured natural language prompts. Extensive experiments on financial and energy datasets demonstrate that LoFT-LLM significantly outperforms strong baselines under both full-data and few-shot regimes, delivering superior accuracy, robustness, and interpretability.","authors":["Jiacheng You","Jingcheng Yang","Yuhang Xie","Zhongxuan Wu","Xiucheng Li","Feng Li","Pengjie Wang","Jian Xu","Bo Zheng","Xinyang Chen"],"pdf_url":"","comment":"Accepted at KDD 2026. 9 pages"},{"id":"http://arxiv.org/abs/2412.06867v2","updated":"2025-12-23T02:53:15Z","published":"2024-12-09T09:37:54Z","title":"Lossless Model Compression via Joint Low-Rank Factorization Optimization","summary":"Low-rank factorization is a popular model compression technique that minimizes the error $δ$ between approximated and original weight matrices. Despite achieving performances close to the original models when $δ$ is optimized, a performance discrepancy remains due to the separate optimization processes for low-rank factorization and model performance, resulting in unavoidable losses. We address this issue by introducing a novel joint optimization strategy for lossless low-rank weight factorization, which, for the first time, enhances the model's performance beyond the original. Our approach begins with a theoretical analysis of the relationship between low-rank factorization and model optimization objectives, establishing a precise perturbation range for matrix factorization errors on model performance. This challenge is then reformulated as a numerical rank deficiency problem with inequality constraints and develop a joint objective that simultaneously addresses factorization error and model performance. Based on the above analysis, we propose two optimization algorithms: \\textbf{a lossless optimization algorithm} that maximizes model accuracy while ensuring compression, and \\textbf{a compact optimization algorithm} that minimizes model size while preserving performance. These algorithms do not require fine-tuning and can directly compress numerous deep models to achieve lossless results. Our methods demonstrate robust efficacy across various vision and language tasks. For example, the compressed model reduced by 70\\% on ResNext50 outperforms the original. Our code will be made public.","authors":["Boyang Zhang","Daning Cheng","Yunquan Zhang","Fangming Liu","Jiake Tian"],"pdf_url":"","comment":"Under Review"},{"id":"http://arxiv.org/abs/2502.15802v2","updated":"2025-12-23T02:46:06Z","published":"2025-02-19T06:12:43Z","title":"A General Error-Theoretical Analysis Framework for Constructing Compression Strategies","summary":"The exponential growth in parameter size and computational complexity of deep models poses significant challenges for efficient deployment. The core problem of existing compression methods is that different layers of the model have significant differences in their tolerance to compression levels. For instance, the first layer of a model can typically sustain a higher compression level compared to the last layer without compromising performance. Thus, the key challenge lies in how to allocate compression levels across layers in a way that minimizes performance loss while maximizing parameter reduction. To address this challenge, we propose a Compression Error Theory (CET) framework, designed to determine the optimal compression level for each layer. Taking quantization as an example, CET leverages differential expansion and algebraic geometry to reconstruct the quadratic form of quantization error as ellipsoids and hyperbolic paraboloids, and utilizes their geometric structures to define an error subspace. To identify the error subspace with minimal performance loss, by performing orthogonal decomposition of the geometric space, CET transforms the optimization process of the error subspace into a complementary problem. The final theoretical analysis shows that constructing the quantization subspace along the major axis results in minimal performance degradation. Through experimental verification of the theory, CET can greatly retain performance while compressing. Specifically, on the ResNet-34 model, CET achieves nearly 11$\\times$ parameter compression while even surpassing performance comparable to the original model.","authors":["Boyang Zhang","Daning Cheng","Yunquan Zhang","Meiqi Tu","Fangming Liu","Jiake Tian"],"pdf_url":"","comment":"Under Review"},{"id":"http://arxiv.org/abs/2507.09001v2","updated":"2025-12-23T02:45:24Z","published":"2025-07-11T20:08:07Z","title":"Surprisingly High Redundancy in Electronic Structure Data","summary":"Accurate prediction of electronic structure underpins advances in chemistry, materials science, and condensed matter physics. In recent years, Machine Learning (ML) has enabled the development of powerful surrogate models that can enable the prediction of the ground state electron density and related properties at a fraction of the computational cost of conventional first principles simulations. Such ML models typically rely on massive datasets generated through expensive Kohn-Sham Density Functional Theory calculations. A key reason for relying on such large datasets is the lack of prior knowledge about which portions of the data are essential, and which are redundant. This study reveals significant redundancies in electronic structure datasets across various material systems, including molecules, simple metals, and chemically complex alloys -- challenging the notion that extensive datasets are essential for accurate ML-based electronic structure predictions. We demonstrate that even random pruning can substantially reduce dataset size with minimal loss in predictive accuracy. Furthermore, a state-of-the-art coverage-based pruning strategy that selects data across all learning difficulties, retains chemical accuracy and model generalizability using up to 100-fold less data, while reducing training time by threefold or greater. By contrast, widely used importance-based pruning methods, which eliminate easy-to-learn data, can catastrophically fail at higher pruning factors due to significant reduction in data coverage. This heretofore unexplored high redundancy in electronic structure data holds the potential to identify a minimal, essential dataset representative of each material class.","authors":["Sazzad Hossain","Ponkrshnan Thiagarajan","Shashank Pathrudkar","Stephanie Taylor","Abhijeet S. Gangan","Amartya S. Banerjee","Susanta Ghosh"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19995v1","updated":"2025-12-23T02:44:25Z","published":"2025-12-23T02:44:25Z","title":"Schoenfeld's Anatomy of Mathematical Reasoning by Language Models","summary":"Large language models increasingly expose reasoning traces, yet their underlying cognitive structure and steps remain difficult to identify and analyze beyond surface-level statistics. We adopt Schoenfeld's Episode Theory as an inductive, intermediate-scale lens and introduce ThinkARM (Anatomy of Reasoning in Models), a scalable framework that explicitly abstracts reasoning traces into functional reasoning steps such as Analysis, Explore, Implement, Verify, etc. When applied to mathematical problem solving by diverse models, this abstraction reveals reproducible thinking dynamics and structural differences between reasoning and non-reasoning models, which are not apparent from token-level views. We further present two diagnostic case studies showing that exploration functions as a critical branching step associated with correctness, and that efficiency-oriented methods selectively suppress evaluative feedback steps rather than uniformly shortening responses. Together, our results demonstrate that episode-level representations make reasoning steps explicit, enabling systematic analysis of how reasoning is structured, stabilized, and altered in modern language models.","authors":["Ming Li","Chenrui Fan","Yize Cheng","Soheil Feizi","Tianyi Zhou"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19991v1","updated":"2025-12-23T02:33:57Z","published":"2025-12-23T02:33:57Z","title":"Bloom Filter Encoding for Machine Learning","summary":"We present a method that uses the Bloom filter transform to preprocess data for machine learning. Each sample is encoded into a compact, privacy-preserving bit array. This reduces memory use and protects the original data while keeping enough structure for accurate classification. We test the method on six datasets: SMS Spam Collection, ECG200, Adult 50K, CDC Diabetes, MNIST, and Fashion MNIST. Four classifiers are used: Extreme Gradient Boosting, Deep Neural Networks, Convolutional Neural Networks, and Logistic Regression. Results show that models trained on Bloom filter encodings achieve accuracy similar to models trained on raw data or other transforms. At the same time, the method provides memory savings while enhancing privacy. These results suggest that the Bloom filter transform is an efficient preprocessing approach for diverse machine learning tasks.","authors":["John Cartmell","Mihaela Cardei","Ionut Cardei"],"pdf_url":"","comment":"14 pages, 7 figures"},{"id":"http://arxiv.org/abs/2512.19989v1","updated":"2025-12-23T02:30:50Z","published":"2025-12-23T02:30:50Z","title":"A Novel CNN Gradient Boosting Ensemble for Guava Disease Detection","summary":"As a significant agricultural country, Bangladesh utilizes its fertile land for guava cultivation and dedicated labor to boost its economic development. In a nation like Bangladesh, enhancing guava production and agricultural practices plays a crucial role in its economy. Anthracnose and fruit fly infection can lower the quality and productivity of guava, a crucial tropical fruit. Expert systems that detect diseases early can reduce losses and safeguard the harvest. Images of guava fruits classified into the Healthy, Fruit Flies, and Anthracnose classes are included in the Guava Fruit Disease Dataset 2024 (GFDD24), which comes from plantations in Rajshahi and Pabna, Bangladesh. This study aims to create models using CNN alongside traditional machine learning techniques that can effectively identify guava diseases in locally cultivated varieties in Bangladesh. In order to achieve the highest classification accuracy of approximately 99.99% for the guava dataset, we propose utilizing ensemble models that combine CNNML with Gradient Boosting Machine. In general, the CNN-ML cascade framework exhibits strong, high-accuracy guava disease detection that is appropriate for real-time agricultural monitoring systems.","authors":["Tamim Ahasan Rijon","Yeasin Arafath"],"pdf_url":"","comment":"Accepted at IEEE ICCIT 2025. This is the author accepted manuscript"},{"id":"http://arxiv.org/abs/2412.06865v2","updated":"2025-12-23T02:30:32Z","published":"2024-12-09T08:50:28Z","title":"FP=xINT:Representing Neural Networks via Low-Bit Series Basis Functions","summary":"Post-Training Quantization (PTQ) converts pre-trained Full-Precision (FP) models into quantized versions without training. While existing methods reduce size and computational costs, they also significantly degrade performance and quantization efficiency at extremely low settings due to quantization noise. We introduce a deep model series expansion framework to address this issue, enabling rapid and accurate approximation of unquantized models without calibration sets or fine-tuning. This is the first use of series expansion for neural network quantization. Specifically, our method expands the FP model into multiple low-bit basis models. To ensure accurate quantization, we develop low-bit basis model expansions at different granularities (tensor, layer, model), and theoretically confirm their convergence to the dense model, thus restoring FP model accuracy. Additionally, we design AbelianAdd/Mul operations between isomorphic models in the low-bit expansion, forming an Abelian group to ensure operation parallelism and commutativity. The experiments show that our algorithm achieves state-of-the-art performance in low-bit settings; for example, 4-bit quantization of ResNet-50 surpasses the original accuracy, reaching 77.03%. The code will be made public.","authors":["Boyang Zhang","Daning Cheng","Yunquan Zhang","Jiake Tian","Jing Li","Fangming Liu"],"pdf_url":"","comment":"AAAI2026"},{"id":"http://arxiv.org/abs/2512.01801v3","updated":"2025-12-23T02:23:42Z","published":"2025-12-01T15:33:59Z","title":"GR-RL: Going Dexterous and Precise for Long-Horizon Robotic Manipulation","summary":"We present GR-RL, a robotic learning framework that turns a generalist vision-language-action (VLA) policy into a highly capable specialist for long-horizon dexterous manipulation. Assuming the optimality of human demonstrations is core to existing VLA policies. However, we claim that in highly dexterous and precise manipulation tasks, human demonstrations are noisy and suboptimal. GR-RL proposes a multi-stage training pipeline that filters, augments, and reinforces the demonstrations by reinforcement learning. First, GR-RL learns a vision-language-conditioned task progress, filters the demonstration trajectories, and only keeps the transitions that contribute positively to the progress. Specifically, we show that by directly applying offline RL with sparse reward, the resulting $Q$-values can be treated as a robust progress function. Next, we introduce morphological symmetry augmentation that greatly improves the generalization and performance of GR-RL. Lastly, to better align the VLA policy with its deployment behaviors for high-precision control, we perform online RL by learning a latent space noise predictor. With this pipeline, GR-RL is, to our knowledge, the first learning-based policy that can autonomously lace up a shoe by threading shoelaces through multiple eyelets with an 83.3% success rate, a task requiring long-horizon reasoning, millimeter-level precision, and compliant soft-body interaction. We hope GR-RL provides a step toward enabling generalist robot foundation models to specialize into reliable real-world experts.","authors":["Yunfei Li","Xiao Ma","Jiafeng Xu","Yu Cui","Zhongren Cui","Zhigang Han","Liqun Huang","Tao Kong","Yuxiao Liu","Hao Niu","Wanli Peng","Jingchao Qiao","Zeyu Ren","Haixin Shi","Zhi Su","Jiawen Tian","Yuyang Xiao","Shenyu Zhang","Liwei Zheng","Hang Li","Yonghui Wu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19986v1","updated":"2025-12-23T02:22:53Z","published":"2025-12-23T02:22:53Z","title":"Covariance-Aware Simplex Projection for Cardinality-Constrained Portfolio Optimization","summary":"Metaheuristic algorithms for cardinality-constrained portfolio optimization require repair operators to map infeasible candidates onto the feasible region. Standard Euclidean projection treats assets as independent and can ignore the covariance structure that governs portfolio risk, potentially producing less diversified portfolios. This paper introduces Covariance-Aware Simplex Projection (CASP), a two-stage repair operator that (i) selects a target number of assets using volatility-normalized scores and (ii) projects the candidate weights using a covariance-aware geometry aligned with tracking-error risk. This provides a portfolio-theoretic foundation for using a covariance-induced distance in repair operators. On S&P 500 data (2020-2024), CASP-Basic delivers materially lower portfolio variance than standard Euclidean repair without relying on return estimates, with improvements that are robust across assets and statistically significant. Ablation results indicate that volatility-normalized selection drives most of the variance reduction, while the covariance-aware projection provides an additional, consistent improvement. We further show that optional return-aware extensions can improve Sharpe ratios, and out-of-sample tests confirm that gains transfer to realized performance. CASP integrates as a drop-in replacement for Euclidean projection in metaheuristic portfolio optimizers.","authors":["Nikolaos Iliopoulos"],"pdf_url":"","comment":"9 pages, 3 figures, 5 tables"},{"id":"http://arxiv.org/abs/2512.16251v2","updated":"2025-12-23T02:11:19Z","published":"2025-12-18T07:05:25Z","title":"Interpretable Deep Learning for Stock Returns: A Consensus-Bottleneck Asset Pricing Model","summary":"We introduce the Consensus-Bottleneck Asset Pricing Model (CB-APM), a partially interpretable neural network that replicates the reasoning processes of sell-side analysts by capturing how dispersed investor beliefs are compressed into asset prices through a consensus formation process. By modeling this \"bottleneck\" to summarize firm- and macro-level information, CB-APM not only predicts future risk premiums of U.S. equities but also links belief aggregation to expected returns in a structurally interpretable manner. The model improves long-horizon return forecasts and outperforms standard deep learning approaches in both predictive accuracy and explanatory power. Comprehensive portfolio analyses show that CB-APM's out-of-sample predictions translate into economically meaningful payoffs, with monotonic return differentials and stable long-short performance across regularization settings. Empirically, CB-APM leverages consensus as a regularizer to amplify long-horizon predictability and yields interpretable consensus-based components that clarify how information is priced in returns. Moreover, regression and Gibbons-Ross-Shanken (GRS)-based pricing diagnostics reveal that the learned consensus representations capture priced variation only partially spanned by traditional factor models, demonstrating that CB-APM uncovers belief-driven structure in expected returns beyond the canonical factor space. Overall, CB-APM provides an interpretable and empirically grounded framework for understanding belief-driven return dynamics.","authors":["Bong-Gyu Jang","Younwoo Jeong","Changeun Kim"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2509.06580v4","updated":"2025-12-23T01:41:33Z","published":"2025-09-08T11:49:52Z","title":"AI for Scientific Discovery is a Social Problem","summary":"Artificial intelligence (AI) is increasingly applied to scientific research, but its benefits remain unevenly distributed across communities and disciplines. While technical challenges such as limited data, fragmented standards, and unequal access to computational resources exist, social and institutional factors are often the primary constraints. Narratives emphasizing autonomous \"AI scientists,\" under-recognition of data and infrastructure work, misaligned incentives, and gaps between domain experts and machine learning researchers all limit the impact of AI on scientific discovery. This paper highlights four interconnected challenges: community coordination, misalignment of research priorities with upstream needs, data fragmentation, and infrastructure inequities. We argue that addressing these challenges requires not only technical innovation but also intentional efforts in community-building, cross-disciplinary education, shared benchmarks, and accessible infrastructure. We call for reframing AI for science as a collective social project, where sustainable collaboration and equitable participation are treated as prerequisites for technical progress","authors":["Georgia Channing","Avijit Ghosh"],"pdf_url":"","comment":"Both authors contributed equally"},{"id":"http://arxiv.org/abs/2512.19970v1","updated":"2025-12-23T01:32:32Z","published":"2025-12-23T01:32:32Z","title":"Spatio-Temporal Graph Neural Networks for Dairy Farm Sustainability Forecasting and Counterfactual Policy Analysis","summary":"This study introduces a novel data-driven framework and the first-ever county-scale application of Spatio-Temporal Graph Neural Networks (STGNN) to forecast composite sustainability indices from herd-level operational records. The methodology employs a novel, end-to-end pipeline utilizing a Variational Autoencoder (VAE) to augment Irish Cattle Breeding Federation (ICBF) datasets, preserving joint distributions while mitigating sparsity. A first-ever pillar-based scoring formulation is derived via Principal Component Analysis, identifying Reproductive Efficiency, Genetic Management, Herd Health, and Herd Management, to construct weighted composite indices. These indices are modelled using a novel STGNN architecture that explicitly encodes geographic dependencies and non-linear temporal dynamics to generate multi-year forecasts for 2026-2030.","authors":["Surya Jayakumar","Kieran Sullivan","John McLaughlin","Christine O'Meara","Indrakshi Dey"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2509.25535v2","updated":"2025-12-23T01:30:53Z","published":"2025-09-29T21:44:00Z","title":"Meta-Router: Bridging Gold-standard and Preference-based Evaluations in Large Language Model Routing","summary":"In language tasks that require extensive human--model interaction, deploying a single \"best\" model for every query can be expensive. To reduce inference cost while preserving the quality of the responses, a large language model (LLM) router selects the most appropriate model from a pool of candidates for each query. A central challenge to training a high-quality router is the scarcity of reliable supervision. Gold-standard data (e.g., expert-verified labels or rubric-based scores) provide accurate quality evaluations of LLM responses but are costly and difficult to scale. In contrast, preference-based data, collected via crowdsourcing or LLM-as-a-judge systems, are cheaper and more scalable, yet often biased in reflecting the true quality of responses. We cast the problem of LLM router training with combined gold-standard and preference-based data into a causal inference framework by viewing the response evaluation mechanism as the treatment assignment. This perspective further reveals that the bias in preference-based data corresponds to the well-known causal estimand: the conditional average treatment effect. Based on this new perspective, we develop an integrative causal router training framework that corrects preference-data bias, address imbalances between two data sources, and improve routing robustness and efficiency. Numerical experiments demonstrate that our approach delivers more accurate routing and improves the trade-off between cost and quality.","authors":["Yichi Zhang","Fangzheng Xie","Shu Yang","Chong Wu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2505.18345v2","updated":"2025-12-23T01:23:23Z","published":"2025-05-23T20:03:36Z","title":"Diffusion Self-Weighted Guidance for Offline Reinforcement Learning","summary":"Offline reinforcement learning (RL) recovers the optimal policy $π$ given historical observations of an agent. In practice, $π$ is modeled as a weighted version of the agent's behavior policy $μ$, using a weight function $w$ working as a critic of the agent's behavior. Though recent approaches to offline RL based on diffusion models have exhibited promising results, the computation of the required scores is challenging due to their dependence on the unknown $w$. In this work, we alleviate this issue by constructing a diffusion over both the actions and the weights. With the proposed setting, the required scores are directly obtained from the diffusion model without learning extra networks. Our main conceptual contribution is a novel guidance method, where guidance (which is a function of $w$) comes from the same diffusion model, therefore, our proposal is termed Self-Weighted Guidance (SWG). We show that SWG generates samples from the desired distribution on toy examples and performs on par with state-of-the-art methods on D4RL's challenging environments, while maintaining a streamlined training pipeline. We further validate SWG through ablation studies on weight formulations and scalability.","authors":["Augusto Tagle","Javier Ruiz-del-Solar","Felipe Tobar"],"pdf_url":"","comment":"Published in Transactions on Machine Learning Research (TMLR). 21 pages, 6 figures"},{"id":"http://arxiv.org/abs/2512.10229v3","updated":"2025-12-23T01:17:01Z","published":"2025-12-11T02:25:27Z","title":"Adaptive Information Routing for Multimodal Time Series Forecasting","summary":"Time series forecasting is a critical task for artificial intelligence with numerous real-world applications. Traditional approaches primarily rely on historical time series data to predict the future values. However, in practical scenarios, this is often insufficient for accurate predictions due to the limited information available. To address this challenge, multimodal time series forecasting methods which incorporate additional data modalities, mainly text data, alongside time series data have been explored. In this work, we introduce the Adaptive Information Routing (AIR) framework, a novel approach for multimodal time series forecasting. Unlike existing methods that treat text data on par with time series data as interchangeable auxiliary features for forecasting, AIR leverages text information to dynamically guide the time series model by controlling how and to what extent multivariate time series information should be combined. We also present a text-refinement pipeline that employs a large language model to convert raw text data into a form suitable for multimodal forecasting, and we introduce a benchmark that facilitates multimodal forecasting experiments based on this pipeline. Experiment results with the real world market data such as crude oil price and exchange rates demonstrate that AIR effectively modulates the behavior of the time series model using textual inputs, significantly enhancing forecasting accuracy in various time series forecasting tasks.","authors":["Jun Seo","Hyeokjun Choe","Seohui Bae","Soyeon Park","Wonbin Ahn","Taeyoon Lim","Junhyeok Kang","Sangjun Han","Jaehoon Lee","Dongwan Kang","Minjae Kim","Sungdong Yoo","Soonyoung Lee"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19941v1","updated":"2025-12-23T00:18:23Z","published":"2025-12-23T00:18:23Z","title":"Block-Recurrent Dynamics in Vision Transformers","summary":"As Vision Transformers (ViTs) become standard vision backbones, a mechanistic account of their computational phenomenology is essential. Despite architectural cues that hint at dynamical structure, there is no settled framework that interprets Transformer depth as a well-characterized flow. In this work, we introduce the Block-Recurrent Hypothesis (BRH), arguing that trained ViTs admit a block-recurrent depth structure such that the computation of the original $L$ blocks can be accurately rewritten using only $k \\ll L$ distinct blocks applied recurrently. Across diverse ViTs, between-layer representational similarity matrices suggest few contiguous phases. To determine whether these phases reflect genuinely reusable computation, we train block-recurrent surrogates of pretrained ViTs: Recurrent Approximations to Phase-structured TransfORmers (Raptor). In small-scale, we demonstrate that stochastic depth and training promote recurrent structure and subsequently correlate with our ability to accurately fit Raptor. We then provide an empirical existence proof for BRH by training a Raptor model to recover $96\\%$ of DINOv2 ImageNet-1k linear probe accuracy in only 2 blocks at equivalent computational cost. Finally, we leverage our hypothesis to develop a program of Dynamical Interpretability. We find i) directional convergence into class-dependent angular basins with self-correcting trajectories under small perturbations, ii) token-specific dynamics, where cls executes sharp late reorientations while patch tokens exhibit strong late-stage coherence toward their mean direction, and iii) a collapse to low rank updates in late depth, consistent with convergence to low-dimensional attractors. Altogether, we find a compact recurrent program emerges along ViT depth, pointing to a low-complexity normative solution that enables these models to be studied through principled dynamical systems analysis.","authors":["Mozes Jacobs","Thomas Fel","Richard Hakim","Alessandra Brondetta","Demba Ba","T. Andy Keller"],"pdf_url":"","comment":"25 pages, 15 figures"},{"id":"http://arxiv.org/abs/2512.20848v1","updated":"2025-12-23T23:54:32Z","published":"2025-12-23T23:54:32Z","title":"Nemotron 3 Nano: Open, Efficient Mixture-of-Experts Hybrid Mamba-Transformer Model for Agentic Reasoning","summary":"We present Nemotron 3 Nano 30B-A3B, a Mixture-of-Experts hybrid Mamba-Transformer language model. Nemotron 3 Nano was pretrained on 25 trillion text tokens, including more than 3 trillion new unique tokens over Nemotron 2, followed by supervised fine tuning and large-scale RL on diverse environments. Nemotron 3 Nano achieves better accuracy than our previous generation Nemotron 2 Nano while activating less than half of the parameters per forward pass. It achieves up to 3.3x higher inference throughput than similarly-sized open models like GPT-OSS-20B and Qwen3-30B-A3B-Thinking-2507, while also being more accurate on popular benchmarks. Nemotron 3 Nano demonstrates enhanced agentic, reasoning, and chat abilities and supports context lengths up to 1M tokens. We release both our pretrained Nemotron 3 Nano 30B-A3B Base and post-trained Nemotron 3 Nano 30B-A3B checkpoints on Hugging Face.","authors":[" NVIDIA"," :","Aaron Blakeman","Aaron Grattafiori","Aarti Basant","Abhibha Gupta","Abhinav Khattar","Adi Renduchintala","Aditya Vavre","Akanksha Shukla","Akhiad Bercovich","Aleksander Ficek","Aleksandr Shaposhnikov","Alex Kondratenko","Alexander Bukharin","Alexandre Milesi","Ali Taghibakhshi","Alisa Liu","Amelia Barton","Ameya Sunil Mahabaleshwarkar","Amir Klein","Amit Zuker","Amnon Geifman","Amy Shen","Anahita Bhiwandiwalla","Andrew Tao","Ann Guan","Anubhav Mandarwal","Arham Mehta","Ashwath Aithal","Ashwin Poojary","Asif Ahamed","Asma Kuriparambil Thekkumpate","Ayush Dattagupta","Banghua Zhu","Bardiya Sadeghi","Barnaby Simkin","Ben Lanir","Benedikt Schifferer","Besmira Nushi","Bilal Kartal","Bita Darvish Rouhani","Boris Ginsburg","Brandon Norick","Brandon Soubasis","Branislav Kisacanin","Brian Yu","Bryan Catanzaro","Carlo del Mundo","Chantal Hwang","Charles Wang","Cheng-Ping Hsieh","Chenghao Zhang","Chenhan Yu","Chetan Mungekar","Chintan Patel","Chris Alexiuk","Christopher Parisien","Collin Neale","Damon Mosk-Aoyama","Dan Su","Dane Corneil","Daniel Afrimi","Daniel Rohrer","Daniel Serebrenik","Daria Gitman","Daria Levy","Darko Stosic","David Mosallanezhad","Deepak Narayanan","Dhruv Nathawani","Dima Rekesh","Dina Yared","Divyanshu Kakwani","Dong Ahn","Duncan Riach","Dusan Stosic","Edgar Minasyan","Edward Lin","Eileen Long","Eileen Peters Long","Elena Lantz","Ellie Evans","Elliott Ning","Eric Chung","Eric Harper","Eric Tramel","Erick Galinkin","Erik Pounds","Evan Briones","Evelina Bakhturina","Faisal Ladhak","Fay Wang","Fei Jia","Felipe Soares","Feng Chen","Ferenc Galko","Frankie Siino","Gal Hubara Agam","Ganesh Ajjanagadde","Gantavya Bhatt","Gargi Prasad","George Armstrong","Gerald Shen","Gorkem Batmaz","Grigor Nalbandyan","Haifeng Qian","Harsh Sharma","Hayley Ross","Helen Ngo","Herman Sahota","Hexin Wang","Himanshu Soni","Hiren Upadhyay","Huizi Mao","Huy C Nguyen","Huy Q Nguyen","Iain Cunningham","Ido Shahaf","Igor Gitman","Ilya Loshchilov","Ivan Moshkov","Izzy Putterman","Jan Kautz","Jane Polak Scowcroft","Jared Casper","Jatin Mitra","Jeffrey Glick","Jenny Chen","Jesse Oliver","Jian Zhang","Jiaqi Zeng","Jie Lou","Jimmy Zhang","Jining Huang","Joey Conway","Joey Guman","John Kamalu","Johnny Greco","Jonathan Cohen","Joseph Jennings","Joyjit Daw","Julien Veron Vialard","Junkeun Yi","Jupinder Parmar","Kai Xu","Kan Zhu","Kari Briski","Katherine Cheung","Katherine Luna","Keshav Santhanam","Kevin Shih","Kezhi Kong","Khushi Bhardwaj","Krishna C. Puvvada","Krzysztof Pawelec","Kumar Anik","Lawrence McAfee","Laya Sleiman","Leon Derczynski","Li Ding","Lucas Liebenwein","Luis Vega","Maanu Grover","Maarten Van Segbroeck","Maer Rodrigues de Melo","Makesh Narsimhan Sreedhar","Manoj Kilaru","Maor Ashkenazi","Marc Romeijn","Mark Cai","Markus Kliegl","Maryam Moosaei","Matvei Novikov","Mehrzad Samadi","Melissa Corpuz","Mengru Wang","Meredith Price","Michael Boone","Michael Evans","Miguel Martinez","Mike Chrzanowski","Mohammad Shoeybi","Mostofa Patwary","Nabin Mulepati","Natalie Hereth","Nave Assaf","Negar Habibi","Neta Zmora","Netanel Haber","Nicola Sessions","Nidhi Bhatia","Nikhil Jukar","Nikki Pope","Nikolai Ludwig","Nima Tajbakhsh","Nirmal Juluru","Oleksii Hrinchuk","Oleksii Kuchaiev","Olivier Delalleau","Oluwatobi Olabiyi","Omer Ullman Argov","Ouye Xie","Parth Chadha","Pasha Shamis","Pavlo Molchanov","Pawel Morkisz","Peter Dykas","Peter Jin","Pinky Xu","Piotr Januszewski","Pranav Prashant Thombre","Prasoon Varshney","Pritam Gundecha","Qing Miao","Rabeeh Karimi Mahabadi","Ran El-Yaniv","Ran Zilberstein","Rasoul Shafipour","Rich Harang","Rick Izzo","Rima Shahbazyan","Rishabh Garg","Ritika Borkar","Ritu Gala","Riyad Islam","Roger Waleffe","Rohit Watve","Roi Koren","Ruoxi Zhang","Russell J. Hewett","Ryan Prenger","Ryan Timbrook","Sadegh Mahdavi","Sahil Modi","Samuel Kriman","Sanjay Kariyappa","Sanjeev Satheesh","Saori Kaji","Satish Pasumarthi","Sean Narentharen","Sean Narenthiran","Seonmyeong Bak","Sergey Kashirsky","Seth Poulos","Shahar Mor","Shanmugam Ramasamy","Shantanu Acharya","Shaona Ghosh","Sharath Turuvekere Sreenivas","Shelby Thomas","Shiqing Fan","Shreya Gopal","Shrimai Prabhumoye","Shubham Pachori","Shubham Toshniwal","Shuoyang Ding","Siddharth Singh","Simeng Sun","Smita Ithape","Somshubra Majumdar","Soumye Singhal","Stefania Alborghetti","Stephen Ge","Sugam Dipak Devare","Sumeet Kumar Barua","Suseella Panguluri","Suyog Gupta","Sweta Priyadarshi","Syeda Nahida Akter","Tan Bui","Teodor-Dumitru Ene","Terry Kong","Thanh Do","Tijmen Blankevoort","Tom Balough","Tomer Asida","Tomer Bar Natan","Tugrul Konuk","Twinkle Vashishth","Udi Karpas","Ushnish De","Vahid Noorozi","Vahid Noroozi","Venkat Srinivasan","Venmugil Elango","Vijay Korthikanti","Vitaly Kurin","Vitaly Lavrukhin","Wanli Jiang","Wasi Uddin Ahmad","Wei Du","Wei Ping","Wenfei Zhou","Will Jennings","William Zhang","Wojciech Prazuch","Xiaowei Ren","Yashaswi Karnati","Yejin Choi","Yev Meyer","Yi-Fu Wu","Yian Zhang","Ying Lin","Yonatan Geifman","Yonggan Fu","Yoshi Subara","Yoshi Suhara","Yubo Gao","Zach Moshe","Zhen Dong","Zihan Liu","Zijia Chen","Zijie Yan"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2504.17836v4","updated":"2025-12-23T23:38:19Z","published":"2025-04-24T17:48:03Z","title":"Learning Enhanced Ensemble Filters","summary":"The filtering distribution in hidden Markov models evolves according to the law of a mean-field model in state-observation space. The ensemble Kalman filter (EnKF) approximates this mean-field model with an ensemble of interacting particles, employing a Gaussian ansatz for the joint distribution of the state and observation at each observation time. These methods are robust, but the Gaussian ansatz limits accuracy. Here this shortcoming is addressed by using machine learning to map the joint predicted state and observation to the updated state estimate. The derivation of methods from a mean field formulation of the true filtering distribution suggests a single parametrization of the algorithm that can be deployed at different ensemble sizes. And we use a mean field formulation of the ensemble Kalman filter as an inductive bias for our architecture.\n  To develop this perspective, in which the mean-field limit of the algorithm and finite interacting ensemble particle approximations share a common set of parameters, a novel form of neural operator is introduced, taking probability distributions as input: a measure neural mapping (MNM). A MNM is used to design a novel approach to filtering, the MNM-enhanced ensemble filter (MNMEF), which is defined in both the mean-field limit and for interacting ensemble particle approximations. The ensemble approach uses empirical measures as input to the MNM and is implemented using the set transformer, which is invariant to ensemble permutation and allows for different ensemble sizes. In practice fine-tuning of a small number of parameters, for specific ensemble sizes, further enhances the accuracy of the scheme. The promise of the approach is demonstrated by its superior root-mean-square-error performance relative to leading methods in filtering the Lorenz '96 and Kuramoto-Sivashinsky models.","authors":["Eviatar Bach","Ricardo Baptista","Edoardo Calvello","Bohan Chen","Andrew Stuart"],"pdf_url":"","comment":"Accepted by the Journal of Computational Physics"},{"id":"http://arxiv.org/abs/2512.20833v1","updated":"2025-12-23T23:15:10Z","published":"2025-12-23T23:15:10Z","title":"CHAMMI-75: pre-training multi-channel models with heterogeneous microscopy images","summary":"Quantifying cell morphology using images and machine learning has proven to be a powerful tool to study the response of cells to treatments. However, models used to quantify cellular morphology are typically trained with a single microscopy imaging type. This results in specialized models that cannot be reused across biological studies because the technical specifications do not match (e.g., different number of channels), or because the target experimental conditions are out of distribution. Here, we present CHAMMI-75, an open access dataset of heterogeneous, multi-channel microscopy images from 75 diverse biological studies. We curated this resource from publicly available sources to investigate cellular morphology models that are channel-adaptive and can process any microscopy image type. Our experiments show that training with CHAMMI-75 can improve performance in multi-channel bioimaging tasks primarily because of its high diversity in microscopy modalities. This work paves the way to create the next generation of cellular morphology models for biological studies.","authors":["Vidit Agrawal","John Peters","Tyler N. Thompson","Mohammad Vali Sanian","Chau Pham","Nikita Moshkov","Arshad Kazi","Aditya Pillai","Jack Freeman","Byunguk Kang","Samouil L. Farhi","Ernest Fraenkel","Ron Stewart","Lassi Paavolainen","Bryan A. Plummer","Juan C. Caicedo"],"pdf_url":"","comment":"47 Pages, 23 Figures, 26 Tables"},{"id":"http://arxiv.org/abs/2512.20831v1","updated":"2025-12-23T23:12:53Z","published":"2025-12-23T23:12:53Z","title":"Context-Sensitive Abstractions for Reinforcement Learning with Parameterized Actions","summary":"Real-world sequential decision-making often involves parameterized action spaces that require both, decisions regarding discrete actions and decisions about continuous action parameters governing how an action is executed. Existing approaches exhibit severe limitations in this setting -- planning methods demand hand-crafted action models, and standard reinforcement learning (RL) algorithms are designed for either discrete or continuous actions but not both, and the few RL methods that handle parameterized actions typically rely on domain-specific engineering and fail to exploit the latent structure of these spaces. This paper extends the scope of RL algorithms to long-horizon, sparse-reward settings with parameterized actions by enabling agents to autonomously learn both state and action abstractions online. We introduce algorithms that progressively refine these abstractions during learning, increasing fine-grained detail in the critical regions of the state-action space where greater resolution improves performance. Across several continuous-state, parameterized-action domains, our abstraction-driven approach enables TD($λ$) to achieve markedly higher sample efficiency than state-of-the-art baselines.","authors":["Rashmeet Kaur Nayyar","Naman Shah","Siddharth Srivastava"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20821v1","updated":"2025-12-23T22:46:06Z","published":"2025-12-23T22:46:06Z","title":"Defending against adversarial attacks using mixture of experts","summary":"Machine learning is a powerful tool enabling full automation of a huge number of tasks without explicit programming. Despite recent progress of machine learning in different domains, these models have shown vulnerabilities when they are exposed to adversarial threats. Adversarial threats aim to hinder the machine learning models from satisfying their objectives. They can create adversarial perturbations, which are imperceptible to humans' eyes but have the ability to cause misclassification during inference. Moreover, they can poison the training data to harm the model's performance or they can query the model to steal its sensitive information. In this paper, we propose a defense system, which devises an adversarial training module within mixture-of-experts architecture to enhance its robustness against adversarial threats. In our proposed defense system, we use nine pre-trained experts with ResNet-18 as their backbone. During end-to-end training, the parameters of expert models and gating mechanism are jointly updated allowing further optimization of the experts. Our proposed defense system outperforms state-of-the-art defense systems and plain classifiers, which use a more complex architecture than our model's backbone.","authors":["Mohammad Meymani","Roozbeh Razavi-Far"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20814v1","updated":"2025-12-23T22:25:11Z","published":"2025-12-23T22:25:11Z","title":"FedMPDD: Communication-Efficient Federated Learning with Privacy Preservation Attributes via Projected Directional Derivative","summary":"This paper introduces \\texttt{FedMPDD} (\\textbf{Fed}erated Learning via \\textbf{M}ulti-\\textbf{P}rojected \\textbf{D}irectional \\textbf{D}erivatives), a novel algorithm that simultaneously optimizes bandwidth utilization and enhances privacy in Federated Learning. The core idea of \\texttt{FedMPDD} is to encode each client's high-dimensional gradient by computing its directional derivatives along multiple random vectors. This compresses the gradient into a much smaller message, significantly reducing uplink communication costs from $\\mathcal{O}(d)$ to $\\mathcal{O}(m)$, where $m \\ll d$. The server then decodes the aggregated information by projecting it back onto the same random vectors. Our key insight is that averaging multiple projections overcomes the dimension-dependent convergence limitations of a single projection. We provide a rigorous theoretical analysis, establishing that \\texttt{FedMPDD} converges at a rate of $\\mathcal{O}(1/\\sqrt{K})$, matching the performance of FedSGD. Furthermore, we demonstrate that our method provides some inherent privacy against gradient inversion attacks due to the geometric properties of low-rank projections, offering a tunable privacy-utility trade-off controlled by the number of projections. Extensive experiments on benchmark datasets validate our theory and demonstrates our results.","authors":["Mohammadreza Rostami","Solmaz S. Kia"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20813v1","updated":"2025-12-23T22:23:23Z","published":"2025-12-23T22:23:23Z","title":"GraphFire-X: Physics-Informed Graph Attention Networks and Structural Gradient Boosting for Building-Scale Wildfire Preparedness at the Wildland-Urban Interface","summary":"As wildfires increasingly evolve into urban conflagrations, traditional risk models that treat structures as isolated assets fail to capture the non-linear contagion dynamics characteristic of the wildland urban interface (WUI). This research bridges the gap between mechanistic physics and data driven learning by establishing a novel dual specialist ensemble framework that disentangles vulnerability into two distinct vectors, environmental contagion and structural fragility. The architecture integrates two specialized predictive streams, an environmental specialist, implemented as a graph neural network (GNN) that operationalizes the community as a directed contagion graph weighted by physics informed convection, radiation, and ember probabilities, and enriched with high dimensional Google AlphaEarth Foundation embeddings, and a Structural Specialist, implemented via XGBoost to isolate granular asset level resilience. Applied to the 2025 Eaton Fire, the framework reveals a critical dichotomy in risk drivers. The GNN demonstrates that neighborhood scale environmental pressure overwhelmingly dominates intrinsic structural features in defining propagation pathways, while the XGBoost model identifies eaves as the primary micro scale ingress vector. By synthesizing these divergent signals through logistic stacking, the ensemble achieves robust classification and generates a diagnostic risk topology. This capability empowers decision makers to move beyond binary loss prediction and precisely target mitigation prioritizing vegetation management for high connectivity clusters and structural hardening for architecturally vulnerable nodes thereby operationalizing a proactive, data driven approach to community resilience.","authors":["Miguel Esparza","Vamshi Battal","Ali Mostafavi"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20811v1","updated":"2025-12-23T22:20:34Z","published":"2025-12-23T22:20:34Z","title":"Weighted MCC: A Robust Measure of Multiclass Classifier Performance for Observations with Individual Weights","summary":"Several performance measures are used to evaluate binary and multiclass classification tasks.\n  But individual observations may often have distinct weights, and none of these measures are sensitive to such varying weights.\n  We propose a new weighted Pearson-Matthews Correlation Coefficient (MCC) for binary classification as well as weighted versions of related multiclass measures. The weighted MCC varies between $-1$ and $1$. But crucially, the weighted MCC values are higher for classifiers that perform better on highly weighted observations, and hence is able to distinguish them from classifiers that have a similar overall performance and ones that perform better on the lowly weighted observations.\n  Furthermore, we prove that the weighted measures are robust with respect to the choice of weights in a precise manner:\n  if the weights are changed by at most $ε$, the value of the weighted measure changes at most by a factor of $ε$ in the binary case\n  and by a factor of $ε^2$ in the multiclass case.\n  Our computations demonstrate that the weighted measures clearly identify classifiers that perform better on higher weighted observations, while the unweighted measures remain completely indifferent to the choices of weights.","authors":["Rommel Cortez","Bala Krishnamoorthy"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.13868v2","updated":"2025-12-23T21:56:57Z","published":"2025-12-15T19:56:39Z","title":"Safe Online Control-Informed Learning","summary":"This paper proposes a Safe Online Control-Informed Learning framework for safety-critical autonomous systems. The framework unifies optimal control, parameter estimation, and safety constraints into an online learning process. It employs an extended Kalman filter to incrementally update system parameters in real time, enabling robust and data-efficient adaptation under uncertainty. A softplus barrier function enforces constraint satisfaction during learning and control while eliminating the dependence on high-quality initial guesses. Theoretical analysis establishes convergence and safety guarantees, and the framework's effectiveness is demonstrated on cart-pole and robot-arm systems.","authors":["Tianyu Zhou","Zihao Liang","Zehui Lu","Shaoshuai Mou"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2412.17228v2","updated":"2025-12-23T21:35:27Z","published":"2024-12-23T02:44:35Z","title":"MatchMiner-AI: An Open-Source Solution for Cancer Clinical Trial Matching","summary":"Clinical trials drive improvements in cancer treatments and outcomes. However, most adults with cancer do not participate in trials, and trials often fail to enroll enough patients to answer their scientific questions. Artificial intelligence could accelerate identification of appropriate clinical trials for patients, but data restrictions have precluded sharing AI models trained on patient records. Here, we describe the development and evaluation of the open-source MatchMiner-AI platform, trained on synthetic data, for clinical trial searching and ranking. It focuses on matching patients to potential trials based on core criteria describing clinical \"spaces,\" or target populations. The pipeline includes modules to extract key elements of the history from a patient's longitudinal electronic health record, rapidly rank candidate trial-patient matches based on embeddings in vector space, and reason about whether a candidate match represents an appropriate clinical consideration. Another module predicts whether the patient meets common exclusion criteria across clinical trials, such as end-organ dysfunction. Training code is available at https://github.com/dfci/matchminer-ai-training . Examples of inference code are at https://github.com/dfci/matchminer-ai-inference . To facilitate deployment across contexts, demonstration apps, all synthetic data, as well as patient/trial embedding, cross-encoding/match classification, and generative reasoning models are available at https://huggingface.co/ksg-dfci .","authors":["Jennifer Altreuter","Pavel Trukhanov","Morgan A. Paul","Michael J. Hassett","Irbaz B. Riaz","Muhammad Umar Afzal","Arshad A. Mohammed","Sarah Sammons","James Lindsay","Emily Mallaber","Harry R. Klein","Gufran Gungor","Matthew Galvin","Michael Deletto","Stephen C. Van Nostrand","James Provencher","Joyce Yu","Naeem Tahir","Jonathan Wischhusen","Olga Kozyreva","Taylor Ortiz","Hande Tuncer","Jad El Masri","Alys Malcolm","Tali Mazor","Ethan Cerami","Kenneth L. Kehl"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20785v1","updated":"2025-12-23T21:33:11Z","published":"2025-12-23T21:33:11Z","title":"Symbolic regression for defect interactions in 2D materials","summary":"Machine learning models have become firmly established across all scientific fields. Extracting features from data and making inferences based on them with neural network models often yields high accuracy; however, this approach has several drawbacks. Symbolic regression is a powerful technique for discovering analytical equations that describe data, providing interpretable and generalizable models capable of predicting unseen data. Symbolic regression methods have gained new momentum with the advancement of neural network technologies and offer several advantages, the main one being the interpretability of results. In this work, we examined the application of the deep symbolic regression algorithm SEGVAE to determine the properties of two-dimensional materials with defects. Comparing the results with state-of-the-art graph neural network-based methods shows comparable or, in some cases, even identical outcomes. We also discuss the applicability of this class of methods in natural sciences.","authors":["Mikhail Lazarev","Andrey Ustyuzhanin"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20783v1","updated":"2025-12-23T21:30:05Z","published":"2025-12-23T21:30:05Z","title":"NULLBUS: Multimodal Mixed-Supervision for Breast Ultrasound Segmentation via Nullable Global-Local Prompts","summary":"Breast ultrasound (BUS) segmentation provides lesion boundaries essential for computer-aided diagnosis and treatment planning. While promptable methods can improve segmentation performance and tumor delineation when text or spatial prompts are available, many public BUS datasets lack reliable metadata or reports, constraining training to small multimodal subsets and reducing robustness. We propose NullBUS, a multimodal mixed-supervision framework that learns from images with and without prompts in a single model. To handle missing text, we introduce nullable prompts, implemented as learnable null embeddings with presence masks, enabling fallback to image-only evidence when metadata are absent and the use of text when present. Evaluated on a unified pool of three public BUS datasets, NullBUS achieves a mean IoU of 0.8568 and a mean Dice of 0.9103, demonstrating state-of-the-art performance under mixed prompt availability.","authors":["Raja Mallina","Bryar Shareef"],"pdf_url":"","comment":"5 pages, 2 figures, and 4 tables"},{"id":"http://arxiv.org/abs/2512.20777v1","updated":"2025-12-23T21:25:40Z","published":"2025-12-23T21:25:40Z","title":"Improving Matrix Exponential for Generative AI Flows: A Taylor-Based Approach Beyond Paterson--Stockmeyer","summary":"The matrix exponential is a fundamental operator in scientific computing and system simulation, with applications ranging from control theory and quantum mechanics to modern generative machine learning. While Padé approximants combined with scaling and squaring have long served as the standard, recent Taylor-based methods, which utilize polynomial evaluation schemes that surpass the classical Paterson--Stockmeyer technique, offer superior accuracy and reduced computational complexity. This paper presents an optimized Taylor-based algorithm for the matrix exponential, specifically designed for the high-throughput requirements of generative AI flows. We provide a rigorous error analysis and develop a dynamic selection strategy for the Taylor order and scaling factor to minimize computational effort under a prescribed error tolerance. Extensive numerical experiments demonstrate that our approach provides significant acceleration and maintains high numerical stability compared to existing state-of-the-art implementations. These results establish the proposed method as a highly efficient tool for large-scale generative modeling.","authors":["Jorge Sastre","Daniel Faronbi","José Miguel Alonso","Peter Traver","Javier Ibáñez","Nuria Lloret"],"pdf_url":"","comment":"41 pages, 35 figures"},{"id":"http://arxiv.org/abs/2506.06303v2","updated":"2025-12-23T21:21:10Z","published":"2025-05-21T16:15:01Z","title":"Reward Is Enough: LLMs Are In-Context Reinforcement Learners","summary":"Reinforcement learning (RL) is a framework for solving sequential decision-making problems. In this work, we demonstrate that, surprisingly, RL emerges during the inference time of large language models (LLMs), a phenomenon we term in-context RL (ICRL). To reveal this capability, we introduce a simple multi-round prompting framework, we call ICRL prompting, for inference-time self-improvement. The goal of ICRL prompting is to guide LLMs to perform reinforcement learning during inference for self-improvement on a given task. After each response, the model receives numerical scalar feedback, denoted as a reward. In the next round, we prompt the LLM again together with a context that concatenates all prior responses and their associated rewards. We consistently observe that response quality improves as the context grows. In other words, the LLM can optimize scalar reward signals during inference, exhibiting behavior analogous to reinforcement learning. We evaluate ICRL prompting on Game of 24, creative writing, ScienceWorld, and Olympiad-level math competitions (AIME and HMMT), demonstrating significant improvements over baselines such as Self-Refine and Reflexion. Notably, even when the reward signals are generated by the same LLM, ICRL prompting still improves performance, highlighting a promising new paradigm for test-time scaling.","authors":["Kefan Song","Amir Moeini","Peng Wang","Lei Gong","Rohan Chandra","Yanjun Qi","Shangtong Zhang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20762v1","updated":"2025-12-23T20:49:05Z","published":"2025-12-23T20:49:05Z","title":"Subgroup Discovery with the Cox Model","summary":"We study the problem of subgroup discovery for survival analysis, where the goal is to find an interpretable subset of the data on which a Cox model is highly accurate. Our work is the first to study this particular subgroup problem, for which we make several contributions.\n  Subgroup discovery methods generally require a \"quality function\" in order to sift through and select the most advantageous subgroups. We first examine why existing natural choices for quality functions are insufficient to solve the subgroup discovery problem for the Cox model. To address the shortcomings of existing metrics, we introduce two technical innovations: the *expected prediction entropy (EPE)*, a novel metric for evaluating survival models which predict a hazard function; and the *conditional rank statistics (CRS)*, a statistical object which quantifies the deviation of an individual point to the distribution of survival times in an existing subgroup. We study the EPE and CRS theoretically and show that they can solve many of the problems with existing metrics.\n  We introduce a total of eight algorithms for the Cox subgroup discovery problem. The main algorithm is able to take advantage of both the EPE and the CRS, allowing us to give theoretical correctness results for this algorithm in a well-specified setting. We evaluate all of the proposed methods empirically on both synthetic and real data. The experiments confirm our theory, showing that our contributions allow for the recovery of a ground-truth subgroup in well-specified cases, as well as leading to better model fit compared to naively fitting the Cox model to the whole dataset in practical settings. Lastly, we conduct a case study on jet engine simulation data from NASA. The discovered subgroups uncover known nonlinearities/homogeneity in the data, and which suggest design choices which have been mirrored in practice.","authors":["Zachary Izzo","Iain Melvin"],"pdf_url":"","comment":"43 pages, 2 figures"},{"id":"http://arxiv.org/abs/2512.20761v1","updated":"2025-12-23T20:48:11Z","published":"2025-12-23T20:48:11Z","title":"TS-Arena Technical Report -- A Pre-registered Live Forecasting Platform","summary":"While Time Series Foundation Models (TSFMs) offer transformative capabilities for forecasting, they simultaneously risk triggering a fundamental evaluation crisis. This crisis is driven by information leakage due to overlapping training and test sets across different models, as well as the illegitimate transfer of global patterns to test data. While the ability to learn shared temporal dynamics represents a primary strength of these models, their evaluation on historical archives often permits the exploitation of observed global shocks, which violates the independence required for valid benchmarking. We introduce TS-Arena, a platform that restores the operational integrity of forecasting by treating the genuinely unknown future as the definitive test environment. By implementing a pre-registration mechanism on live data streams, the platform ensures that evaluation targets remain physically non-existent during inference, thereby enforcing a strict global temporal split. This methodology establishes a moving temporal frontier that prevents historical contamination and provides an authentic assessment of model generalization. Initially applied within the energy sector, TS-Arena provides a sustainable infrastructure for comparing foundation models under real-world constraints. A prototype of the platform is available at https://huggingface.co/spaces/DAG-UPB/TS-Arena.","authors":["Marcel Meyer","Sascha Kaltenpoth","Kevin Zalipski","Henrik Albers","Oliver Müller"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20760v1","updated":"2025-12-23T20:45:31Z","published":"2025-12-23T20:45:31Z","title":"Generalization of RLVR Using Causal Reasoning as a Testbed","summary":"Reinforcement learning with verifiable rewards (RLVR) has emerged as a promising paradigm for post-training large language models (LLMs) on complex reasoning tasks. Yet, the conditions under which RLVR yields robust generalization remain poorly understood. This paper provides an empirical study of RLVR generalization in the setting of probabilistic inference over causal graphical models. This setting offers two natural axes along which to examine generalization: (i) the level of the probabilistic query -- associational, interventional, or counterfactual -- and (ii) the structural complexity of the query, measured by the size of its relevant subgraph. We construct datasets of causal graphs and queries spanning these difficulty axes and fine-tune Qwen-2.5-Instruct models using RLVR or supervised fine-tuning (SFT). We vary both the model scale (3B-32B) and the query level included in training. We find that RLVR yields stronger within-level and across-level generalization than SFT, but only for specific combinations of model size and training query level. Further analysis shows that RLVR's effectiveness depends on the model's initial reasoning competence. With sufficient initial competence, RLVR improves an LLM's marginalization strategy and reduces errors in intermediate probability calculations, producing substantial accuracy gains, particularly on more complex queries. These findings show that RLVR can improve specific causal reasoning subskills, with its benefits emerging only when the model has sufficient initial competence.","authors":["Brian Lu","Hongyu Zhao","Shuo Sun","Hao Peng","Rui Ding","Hongyuan Mei"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20757v1","updated":"2025-12-23T20:43:06Z","published":"2025-12-23T20:43:06Z","title":"TokSuite: Measuring the Impact of Tokenizer Choice on Language Model Behavior","summary":"Tokenizers provide the fundamental basis through which text is represented and processed by language models (LMs). Despite the importance of tokenization, its role in LM performance and behavior is poorly understood due to the challenge of measuring the impact of tokenization in isolation. To address this need, we present TokSuite, a collection of models and a benchmark that supports research into tokenization's influence on LMs. Specifically, we train fourteen models that use different tokenizers but are otherwise identical using the same architecture, dataset, training budget, and initialization. Additionally, we curate and release a new benchmark that specifically measures model performance subject to real-world perturbations that are likely to influence tokenization. Together, TokSuite allows robust decoupling of the influence of a model's tokenizer, supporting a series of novel findings that elucidate the respective benefits and shortcomings of a wide range of popular tokenizers.","authors":["Gül Sena Altıntaş","Malikeh Ehghaghi","Brian Lester","Fengyuan Liu","Wanru Zhao","Marco Ciccone","Colin Raffel"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20755v1","updated":"2025-12-23T20:36:54Z","published":"2025-12-23T20:36:54Z","title":"Bridging Efficiency and Safety: Formal Verification of Neural Networks with Early Exits","summary":"Ensuring the safety and efficiency of AI systems is a central goal of modern research. Formal verification provides guarantees of neural network robustness, while early exits improve inference efficiency by enabling intermediate predictions. Yet verifying networks with early exits introduces new challenges due to their conditional execution paths. In this work, we define a robustness property tailored to early exit architectures and show how off-the-shelf solvers can be used to assess it. We present a baseline algorithm, enhanced with an early stopping strategy and heuristic optimizations that maintain soundness and completeness. Experiments on multiple benchmarks validate our framework's effectiveness and demonstrate the performance gains of the improved algorithm. Alongside the natural inference acceleration provided by early exits, we show that they also enhance verifiability, enabling more queries to be solved in less time compared to standard networks. Together with a robustness analysis, we show how these metrics can help users navigate the inherent trade-off between accuracy and efficiency.","authors":["Yizhak Yisrael Elboher","Avraham Raviv","Amihay Elboher","Zhouxing Shi","Omri Azencot","Hillel Kugler","Guy Katz"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2505.11785v3","updated":"2025-12-23T20:21:51Z","published":"2025-05-17T01:51:28Z","title":"Improving Coverage in Combined Prediction Sets with Weighted p-values","summary":"Conformal prediction quantifies the uncertainty of machine learning models by augmenting point predictions with valid prediction sets. For complex scenarios involving multiple trials, models, or data sources, conformal prediction sets can be aggregated to create a prediction set that captures the overall uncertainty, often improving precision. However, aggregating multiple prediction sets with individual $1-α$ coverage inevitably weakens the overall guarantee, typically resulting in $1-2α$ worst-case coverage. In this work, we propose a framework for the weighted aggregation of prediction sets, where weights are assigned to each prediction set based on their contribution. Our framework offers flexible control over how the sets are aggregated, achieving tighter coverage bounds that interpolate between the $1-2α$ guarantee of the combined models and the $1-α$ guarantee of an individual model depending on the distribution of weights. Importantly, our framework generalizes to data-dependent weights, as we derive a procedure for weighted aggregation that maintains finite-sample validity even when the weights depend on the data. This extension makes our framework broadly applicable to settings where weights are learned, such as mixture-of-experts (MoE), and we demonstrate through experiments in the MoE setting that our methods achieve adaptive coverage.","authors":["Gina Wong","Drew Prinster","Suchi Saria","Rama Chellappa","Anqi Liu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20749v1","updated":"2025-12-23T20:12:01Z","published":"2025-12-23T20:12:01Z","title":"Stabilizing Multimodal Autoencoders: A Theoretical and Empirical Analysis of Fusion Strategies","summary":"In recent years, the development of multimodal autoencoders has gained significant attention due to their potential to handle multimodal complex data types and improve model performance. Understanding the stability and robustness of these models is crucial for optimizing their training, architecture, and real-world applicability. This paper presents an analysis of Lipschitz properties in multimodal autoencoders, combining both theoretical insights and empirical validation to enhance the training stability of these models. We begin by deriving the theoretical Lipschitz constants for aggregation methods within the multimodal autoencoder framework. We then introduce a regularized attention-based fusion method, developed based on our theoretical analysis, which demonstrates improved stability and performance during training. Through a series of experiments, we empirically validate our theoretical findings by estimating the Lipschitz constants across multiple trials and fusion strategies. Our results demonstrate that our proposed fusion function not only aligns with theoretical predictions but also outperforms existing strategies in terms of consistency, convergence speed, and accuracy. This work provides a solid theoretical foundation for understanding fusion in multimodal autoencoders and contributes a solution for enhancing their performance.","authors":["Diyar Altinses","Andreas Schwung"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20747v1","updated":"2025-12-23T20:07:37Z","published":"2025-12-23T20:07:37Z","title":"A Physics Informed Neural Network For Deriving MHD State Vectors From Global Active Regions Observations","summary":"Solar active regions (ARs) do not appear randomly but cluster along longitudinally warped toroidal bands ('toroids') that encode information about magnetic structures in the tachocline, where global-scale organization likely originates. Global MagnetoHydroDynamic Shallow-Water Tachocline (MHD-SWT) models have shown potential to simulate such toroids, matching observations qualitatively. For week-scale early prediction of flare-producing AR emergence, forward-integration of these toroids is necessary. This requires model initialization with a dynamically self-consistent MHD state-vector that includes magnetic, flow fields, and shell-thickness variations. However, synoptic magnetograms provide only geometric shape of toroids, not the state-vector needed to initialize MHD-SWT models. To address this challenging task, we develop PINNBARDS, a novel Physics-Informed Neural Network (PINN)-Based AR Distribution Simulator, that uses observational toroids and MHD-SWT equations to derive initial state-vector. Using Feb-14-2024 SDO/HMI synoptic map, we show that PINN converges to physically consistent, predominantly antisymmetric toroids, matching observed ones. Although surface data provides north and south toroids' central latitudes, and their latitudinal widths, they cannot determine tachocline field strengths, connected to AR emergence. We explore here solutions across a broad parameter range, finding hydrodynamically-dominated structures for weak fields (~2 kG) and overly rigid behavior for strong fields (~100 kG). We obtain best agreement with observations for 20-30 kG toroidal fields, and ~10 degree bandwidth, consistent with low-order longitudinal mode excitation. To our knowledge, PINNBARDS serves as the first method for reconstructing state-vectors for hidden tachocline magnetic structures from surface patterns; potentially leading to weeks ahead prediction of flare-producing AR-emergence.","authors":["Subhamoy Chatterjee","Mausumi Dikpati"],"pdf_url":"","comment":"25 pages, 12 figures, accepted for publication in The Astrophysical Journal"},{"id":"http://arxiv.org/abs/2512.20746v1","updated":"2025-12-23T20:00:34Z","published":"2025-12-23T20:00:34Z","title":"TrashDet: Iterative Neural Architecture Search for Efficient Waste Detection","summary":"This paper addresses trash detection on the TACO dataset under strict TinyML constraints using an iterative hardware-aware neural architecture search framework targeting edge and IoT devices. The proposed method constructs a Once-for-All-style ResDets supernet and performs iterative evolutionary search that alternates between backbone and neck/head optimization, supported by a population passthrough mechanism and an accuracy predictor to reduce search cost and improve stability. This framework yields a family of deployment-ready detectors, termed TrashDets. On a five-class TACO subset (paper, plastic, bottle, can, cigarette), the strongest variant, TrashDet-l, achieves 19.5 mAP50 with 30.5M parameters, improving accuracy by up to 3.6 mAP50 over prior detectors while using substantially fewer parameters. The TrashDet family spans 1.2M to 30.5M parameters with mAP50 values between 11.4 and 19.5, providing scalable detector options for diverse TinyML deployment budgets on resource-constrained hardware. On the MAX78002 microcontroller with the TrashNet dataset, two specialized variants, TrashDet-ResNet and TrashDet-MBNet, jointly dominate the ai87-fpndetector baseline, with TrashDet-ResNet achieving 7525~$μ$J energy per inference at 26.7 ms latency and 37.45 FPS, and TrashDet-MBNet improving mAP50 by 10.2%; together they reduce energy consumption by up to 88%, latency by up to 78%, and average power by up to 53% compared to existing TinyML detectors.","authors":["Tony Tran","Bin Hu"],"pdf_url":"","comment":"10 pages. The paper has been accepted by the WACV 2026 workshop"},{"id":"http://arxiv.org/abs/2512.20745v1","updated":"2025-12-23T19:57:49Z","published":"2025-12-23T19:57:49Z","title":"AgentMath: Empowering Mathematical Reasoning for Large Language Models via Tool-Augmented Agent","summary":"Large Reasoning Models (LRMs) like o3 and DeepSeek-R1 have achieved remarkable progress in natural language reasoning with long chain-of-thought. However, they remain computationally inefficient and struggle with accuracy when solving problems requiring complex mathematical operations. In this work, we present AgentMath, an agent framework that seamlessly integrates language models' reasoning capabilities with code interpreters' computational precision to efficiently tackle complex mathematical problems. Our approach introduces three key innovations: (1) An automated method that converts natural language chain-of-thought into structured tool-augmented trajectories, generating high-quality supervised fine-tuning (SFT) data to alleviate data scarcity; (2) A novel agentic reinforcement learning (RL) paradigm that dynamically interleaves natural language generation with real-time code execution. This enables models to autonomously learn optimal tool-use strategies through multi-round interactive feedback, while fostering emergent capabilities in code refinement and error correction; (3) An efficient training system incorporating innovative techniques, including request-level asynchronous rollout scheduling, agentic partial rollout, and prefix-aware weighted load balancing, achieving 4-5x speedup and making efficient RL training feasible on ultra-long sequences with scenarios with massive tool calls.Extensive evaluations show that AgentMath achieves state-of-the-art performance on challenging mathematical competition benchmarks including AIME24, AIME25, and HMMT25. Specifically, AgentMath-30B-A3B attains 90.6%, 86.4%, and 73.8% accuracy respectively, achieving advanced capabilities.These results validate the effectiveness of our approach and pave the way for building more sophisticated and scalable mathematical reasoning agents.","authors":["Haipeng Luo","Huawen Feng","Qingfeng Sun","Can Xu","Kai Zheng","Yufei Wang","Tao Yang","Han Hu","Yansong Tang","Di Wang"],"pdf_url":"","comment":"LLM, Mathematical Reasoning"},{"id":"http://arxiv.org/abs/2507.01020v2","updated":"2025-12-23T19:52:29Z","published":"2025-04-18T08:38:56Z","title":"AutoAdv: Automated Adversarial Prompting for Multi-Turn Jailbreaking of Large Language Models","summary":"Large Language Models (LLMs) continue to exhibit vulnerabilities to jailbreaking attacks: carefully crafted malicious inputs intended to circumvent safety guardrails and elicit harmful responses. As such, we present AutoAdv, a novel framework that automates adversarial prompt generation to systematically evaluate and expose vulnerabilities in LLM safety mechanisms. Our approach leverages a parametric attacker LLM to produce semantically disguised malicious prompts through strategic rewriting techniques, specialized system prompts, and optimized hyperparameter configurations. The primary contribution of our work is a dynamic, multi-turn attack methodology that analyzes failed jailbreak attempts and iteratively generates refined follow-up prompts, leveraging techniques such as roleplaying, misdirection, and contextual manipulation. We quantitatively evaluate attack success rate (ASR) using the StrongREJECT (arXiv:2402.10260 [cs.CL]) framework across sequential interaction turns. Through extensive empirical evaluation of state-of-the-art models--including ChatGPT, Llama, and DeepSeek--we reveal significant vulnerabilities, with our automated attacks achieving jailbreak success rates of up to 86% for harmful content generation. Our findings reveal that current safety mechanisms remain susceptible to sophisticated multi-turn attacks, emphasizing the urgent need for more robust defense strategies.","authors":["Aashray Reddy","Andrew Zagula","Nicholas Saban"],"pdf_url":"","comment":"We encountered issues with the paper being hosted under my personal account, so we republished it under a different account associated with a university email, which makes updates and management easier. As a result, this version is a duplicate of arXiv:2511.02376"},{"id":"http://arxiv.org/abs/2512.20739v1","updated":"2025-12-23T19:50:13Z","published":"2025-12-23T19:50:13Z","title":"AI-Driven Green Cognitive Radio Networks for Sustainable 6G Communication","summary":"The 6G wireless aims at the Tb/s peak data rates are expected, a sub-millisecond latency, massive Internet of Things/vehicle connectivity, which requires sustainable access to audio over the air and energy-saving functionality. Cognitive Radio Networks CCNs help in alleviating the problem of spectrum scarcity, but classical sensing and allocation are still energy-consumption intensive, and sensitive to rapid spectrum variations. Our framework which centers on AI driven green CRN aims at integrating deep reinforcement learning (DRL) with transfer learning, energy harvesting (EH), reconfigurable intelligent surfaces (RIS) with other light-weight genetic refinement operations that optimally combine sensing timelines, transmit power, bandwidth distribution and RIS phase selection. Compared to two baselines, the utilization of MATLAB + NS-3 under dense loads, a traditional CRN with energy sensing under fixed policies, and a hybrid CRN with cooperative sensing under heuristic distribution of resource, there are (25-30%) fewer energy reserves used, sensing AUC greater than 0.90 and +6-13 p.p. higher PDR. The integrated framework is easily scalable to large IoT and vehicular applications, and it provides a feasible and sustainable roadmap to 6G CRNs.\n  Index Terms--Cognitive Radio Networks (CRNs), 6G, Green Communication, Energy Efficiency, Deep Reinforcement Learning (DRL), Spectrum Sensing, RIS, Energy Harvesting, QoS, IoT.","authors":["Anshul Sharma","Shujaatali Badami","Biky Chouhan","Pushpanjali Pandey","Brijeena Rana","Navneet Kaur"],"pdf_url":"","comment":"10 pages, 8 figures. Full research article with MATLAB and NS-3 simulations"},{"id":"http://arxiv.org/abs/2504.16172v3","updated":"2025-12-23T19:47:25Z","published":"2025-04-22T18:01:45Z","title":"Physics-Informed Inference Time Scaling for Solving High-Dimensional PDE via Defect Correction","summary":"Solving high-dimensional partial differential equations (PDEs) is a critical challenge where modern data-driven solvers often lack reliability and rigorous error guarantees. We introduce Simulation-Calibrated Scientific Machine Learning (SCaSML), a framework that systematically improves pre-trained PDE solvers at inference time without any retraining. Our core idea is to use defect correction method that derive a new PDE, termed Structural-preserving Law of Defect, that precisely describes the error of a given surrogate model. Since it retains the structure of the original problem, we can solve it efficiently with traditional stochastic simulators and correct the initial machine-learned solution. We prove that SCaSML achieves a faster convergence rate, with a final error bounded by the product of the surrogate and simulation errors. On challenging PDEs up to 160 dimensions, SCaSML reduces the error of various surrogate models, including PINNs and Gaussian Processes, by 20-80%. Code of SCaSML is available at https://github.com/Francis-Fan-create/SCaSML.","authors":["Zexi Fan","Yan Sun","Shihao Yang","Yiping Lu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20732v1","updated":"2025-12-23T19:40:51Z","published":"2025-12-23T19:40:51Z","title":"FEM-Bench: A Structured Scientific Reasoning Benchmark for Evaluating Code-Generating LLMs","summary":"As LLMs advance their reasoning capabilities about the physical world, the absence of rigorous benchmarks for evaluating their ability to generate scientifically valid physical models has become a critical gap. Computational mechanics, which develops and applies mathematical models and numerical methods to predict the behavior of physical systems under forces, deformation, and constraints, provides an ideal foundation for structured scientific reasoning evaluation. Problems follow clear mathematical structure, enforce strict physical and numerical constraints, and support objective verification. The discipline requires constructing explicit models of physical systems and reasoning about geometry, spatial relationships, and material behavior, connecting directly to emerging AI goals in physical reasoning and world modeling. We introduce FEM-Bench, a computational mechanics benchmark designed to evaluate the ability of LLMs to generate correct finite element method (FEM) and related code. FEM-Bench 2025 contains a suite of introductory but nontrivial tasks aligned with material from a first graduate course on computational mechanics. These tasks capture essential numerical and physical modeling challenges while representing only a small fraction of the complexity present in the discipline. Despite their simplicity, state-of-the-art LLMs do not reliably solve all of them. In a five attempt run, the best performing model at function writing, Gemini 3 Pro, completed 30/33 tasks at least once and 26/33 tasks all five times. The best performing model at unit test writing, GPT-5, had an Average Joint Success Rate of 73.8%. Other popular models showed broad performance variation. FEM-Bench establishes a structured foundation for evaluating AI-generated scientific code, and future iterations will incorporate increasingly sophisticated tasks to track progress as models evolve.","authors":["Saeed Mohammadzadeh","Erfan Hamdi","Joel Shor","Emma Lejeune"],"pdf_url":"","comment":"40 pages, 5 figures, 6 tables, 7 listings"},{"id":"http://arxiv.org/abs/2512.20712v1","updated":"2025-12-23T19:19:45Z","published":"2025-12-23T19:19:45Z","title":"Real-World Adversarial Attacks on RF-Based Drone Detectors","summary":"Radio frequency (RF) based systems are increasingly used to detect drones by analyzing their RF signal patterns, converting them into spectrogram images which are processed by object detection models. Existing RF attacks against image based models alter digital features, making over-the-air (OTA) implementation difficult due to the challenge of converting digital perturbations to transmittable waveforms that may introduce synchronization errors and interference, and encounter hardware limitations. We present the first physical attack on RF image based drone detectors, optimizing class-specific universal complex baseband (I/Q) perturbation waveforms that are transmitted alongside legitimate communications. We evaluated the attack using RF recordings and OTA experiments with four types of drones. Our results show that modest, structured I/Q perturbations are compatible with standard RF chains and reliably reduce target drone detection while preserving detection of legitimate drones.","authors":["Omer Gazit","Yael Itzhakev","Yuval Elovici","Asaf Shabtai"],"pdf_url":"","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2512.20292v1","updated":"2025-12-23T12:01:18Z","published":"2025-12-23T12:01:18Z","title":"SlideTailor: Personalized Presentation Slide Generation for Scientific Papers","summary":"Automatic presentation slide generation can greatly streamline content creation. However, since preferences of each user may vary, existing under-specified formulations often lead to suboptimal results that fail to align with individual user needs. We introduce a novel task that conditions paper-to-slides generation on user-specified preferences. We propose a human behavior-inspired agentic framework, SlideTailor, that progressively generates editable slides in a user-aligned manner. Instead of requiring users to write their preferences in detailed textual form, our system only asks for a paper-slides example pair and a visual template - natural and easy-to-provide artifacts that implicitly encode rich user preferences across content and visual style. Despite the implicit and unlabeled nature of these inputs, our framework effectively distills and generalizes the preferences to guide customized slide generation. We also introduce a novel chain-of-speech mechanism to align slide content with planned oral narration. Such a design significantly enhances the quality of generated slides and enables downstream applications like video presentations. To support this new task, we construct a benchmark dataset that captures diverse user preferences, with carefully designed interpretable metrics for robust evaluation. Extensive experiments demonstrate the effectiveness of our framework.","authors":["Wenzheng Zeng","Mingyu Ouyang","Langyuan Cui","Hwee Tou Ng"],"pdf_url":"","comment":"AAAI 2026 (with appendix)"},{"id":"http://arxiv.org/abs/2512.20093v1","updated":"2025-12-23T06:41:07Z","published":"2025-12-23T06:41:07Z","title":"Neural Compression of 360-Degree Equirectangular Videos using Quality Parameter Adaptation","summary":"This study proposes a practical approach for compressing 360-degree equirectangular videos using pretrained neural video compression (NVC) models. Without requiring additional training or changes in the model architectures, the proposed method extends quantization parameter adaptation techniques from traditional video codecs to NVC, utilizing the spatially varying sampling density in equirectangular projections. We introduce latitude-based adaptive quality parameters through rate-distortion optimization for NVC. The proposed method utilizes vector bank interpolation for latent modulation, enabling flexible adaptation with arbitrary quality parameters and mitigating the limitations caused by rounding errors in the adaptive quantization parameters. Experimental results demonstrate that applying this method to the DCVC-RT framework yields BD-Rate savings of 5.2% in terms of the weighted spherical peak signal-to-noise ratio for JVET class S1 test sequences, with only a 0.3% increase in processing time.","authors":["Daichi Arai","Yuichi Kondo","Kyohei Unno","Yasuko Sugito","Yuichi Kusakabe"],"pdf_url":"","comment":"Picture Coding Symposium (PCS), 2025"},{"id":"http://arxiv.org/abs/2512.20059v1","updated":"2025-12-23T05:23:06Z","published":"2025-12-23T05:23:06Z","title":"DS-HGCN: A Dual-Stream Hypergraph Convolutional Network for Predicting Student Engagement via Social Contagion","summary":"Student engagement is a critical factor influencing academic success and learning outcomes. Accurately predicting student engagement is essential for optimizing teaching strategies and providing personalized interventions. However, most approaches focus on single-dimensional feature analysis and assessing engagement based on individual student factors. In this work, we propose a dual-stream multi-feature fusion model based on hypergraph convolutional networks (DS-HGCN), incorporating social contagion of student engagement. DS-HGCN enables accurate prediction of student engagement states by modeling multi-dimensional features and their propagation mechanisms between students. The framework constructs a hypergraph structure to encode engagement contagion among students and captures the emotional and behavioral differences and commonalities by multi-frequency signals. Furthermore, we introduce a hypergraph attention mechanism to dynamically weigh the influence of each student, accounting for individual differences in the propagation process. Extensive experiments on public benchmark datasets demonstrate that our proposed method achieves superior performance and significantly outperforms existing state-of-the-art approaches.","authors":["Ziyang Fan","Li Tao","Yi Wang","Jingwei Qu","Ying Wang","Fei Jiang"],"pdf_url":"","comment":"14pages,Accepted by MMM2026"}]},"2025-12-24T00:00:00Z":{"Artificial Intelligence":[{"id":"http://arxiv.org/abs/2512.21336v1","updated":"2025-12-24T18:59:51Z","published":"2025-12-24T18:59:51Z","title":"Optimizing Decoding Paths in Masked Diffusion Models by Quantifying Uncertainty","summary":"Masked Diffusion Models (MDMs) offer flexible, non-autoregressive generation, but this freedom introduces a challenge: final output quality is highly sensitive to the decoding order. We are the first to formalize this issue, attributing the variability in output quality to the cumulative predictive uncertainty along a generative path. To quantify this uncertainty, we introduce Denoising Entropy, a computable metric that serves as an internal signal for evaluating generative process. Leveraging this metric, we propose two algorithms designed to optimize the decoding path: a post-hoc selection method and a real-time guidance strategy. Experiments demonstrate that our entropy-guided methods significantly improve generation quality, consistently boosting accuracy on challenging reasoning, planning, and code benchmarks. Our work establishes Denoising Entropy as a principled tool for understanding and controlling generation, effectively turning the uncertainty in MDMs from a liability into a key advantage for discovering high-quality solutions.","authors":["Ziyu Chen","Xinbei Jiang","Peng Sun","Tao Lin"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.21332v1","updated":"2025-12-24T18:59:01Z","published":"2025-12-24T18:59:01Z","title":"C2LLM Technical Report: A New Frontier in Code Retrieval via Adaptive Cross-Attention Pooling","summary":"We present C2LLM - Contrastive Code Large Language Models, a family of code embedding models in both 0.5B and 7B sizes. Building upon Qwen-2.5-Coder backbones, C2LLM adopts a Pooling by Multihead Attention (PMA) module for generating sequence embedding from token embeddings, effectively 1) utilizing the LLM's causal representations acquired during pretraining, while also 2) being able to aggregate information from all tokens in the sequence, breaking the information bottleneck in EOS-based sequence embeddings, and 3) supporting flexible adaptation of embedding dimension, serving as an alternative to MRL. Trained on three million publicly available data, C2LLM models set new records on MTEB-Code among models of similar sizes, with C2LLM-7B ranking 1st on the overall leaderboard.","authors":["Jin Qin","Zihan Liao","Ziyin Zhang","Hang Yu","Peng Di","Rui Wang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.21326v1","updated":"2025-12-24T18:54:37Z","published":"2025-12-24T18:54:37Z","title":"Measuring all the noises of LLM Evals","summary":"Separating signal from noise is central to experimental science. Applying well-established statistical method effectively to LLM evals requires consideration of their unique noise characteristics. We clearly define and measure three types of noise: prediction noise from generating different answers on a given question, data noise from sampling questions, and their combined total noise following the law of total variance. To emphasize relative comparisons and gain statistical power, we propose the all-pairs paired method, which applies the paired analysis to all pairs of LLMs and measures all the noise components based on millions of question-level predictions across many evals and settings. These measurements revealed clear patterns. First, each eval exhibits a characteristic and highly predictable total noise level across all model pairs. Second, paired prediction noise typically exceeds paired data noise, which means reducing prediction noise by averaging can significantly increase statistical power. These findings enable practitioners to assess significance without custom testing and to detect much smaller effects in controlled experiments.","authors":["Sida Wang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.21316v1","updated":"2025-12-24T18:24:29Z","published":"2025-12-24T18:24:29Z","title":"Scaling Laws for Economic Productivity: Experimental Evidence in LLM-Assisted Consulting, Data Analyst, and Management Tasks","summary":"This paper derives `Scaling Laws for Economic Impacts' -- empirical relationships between the training compute of Large Language Models (LLMs) and professional productivity. In a preregistered experiment, over 500 consultants, data analysts, and managers completed professional tasks using one of 13 LLMs. We find that each year of AI model progress reduced task time by 8%, with 56% of gains driven by increased compute and 44% by algorithmic progress. However, productivity gains were significantly larger for non-agentic analytical tasks compared to agentic workflows requiring tool use. These findings suggest continued model scaling could boost U.S. productivity by approximately 20% over the next decade.","authors":["Ali Merali"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17083v2","updated":"2025-12-24T18:05:57Z","published":"2025-12-18T21:29:43Z","title":"When F1 Fails: Granularity-Aware Evaluation for Dialogue Topic Segmentation","summary":"Dialogue topic segmentation supports summarization, retrieval, memory management, and conversational continuity. Despite decades of work, evaluation practice remains dominated by strict boundary matching and F1-based metrics. Modern large language model (LLM) based conversational systems increasingly rely on segmentation to manage conversation history beyond fixed context windows. In such systems, unstructured context accumulation degrades efficiency and coherence.\n  This paper introduces an evaluation framework that reports boundary density and segment alignment diagnostics (purity and coverage) alongside window-tolerant F1 (W-F1). By separating boundary scoring from boundary selection, we evaluate segmentation quality across density regimes rather than at a single operating point. Cross-dataset evaluation shows that reported performance differences often reflect annotation granularity mismatch rather than boundary placement quality alone.\n  We evaluate structurally distinct segmentation strategies across eight dialogue datasets spanning task-oriented, open-domain, meeting-style, and synthetic interactions. Boundary-based metrics are strongly coupled to boundary density: threshold sweeps produce larger W-F1 changes than switching between methods. These findings support viewing topic segmentation as a granularity selection problem rather than prediction of a single correct boundary set. This motivates separating boundary scoring from boundary selection for analyzing and tuning segmentation under varying annotation granularities.","authors":["Michael H. Coen"],"pdf_url":"","comment":"32 pages, 4 figures. Evaluation and methodology study on dialogue topic segmentation"},{"id":"http://arxiv.org/abs/2502.06096v4","updated":"2025-12-24T17:17:29Z","published":"2025-02-10T02:01:30Z","title":"Post-detection inference for sequential changepoint localization","summary":"This paper addresses a fundamental but largely unexplored challenge in sequential changepoint analysis: conducting inference following a detected change. We develop a very general framework to construct confidence sets for the unknown changepoint using only the data observed up to a data-dependent stopping time at which an arbitrary sequential detection algorithm declares a change. Our framework is nonparametric, making no assumption on the composite post-change class, the observation space, or the sequential detection procedure used, and is non-asymptotically valid. We also extend it to handle composite pre-change classes under a suitable assumption, and also derive confidence sets for the change magnitude in parametric settings. We provide theoretical guarantees on the width of our confidence intervals. Extensive simulations demonstrate that the produced sets have reasonable size, and slightly conservative coverage. In summary, we present the first general method for sequential changepoint localization, which is theoretically sound and broadly applicable in practice.","authors":["Aytijhya Saha","Aaditya Ramdas"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2510.09016v2","updated":"2025-12-24T17:16:37Z","published":"2025-10-10T05:39:45Z","title":"DiTSinger: Scaling Singing Voice Synthesis with Diffusion Transformer and Implicit Alignment","summary":"Recent progress in diffusion-based Singing Voice Synthesis (SVS) demonstrates strong expressiveness but remains limited by data scarcity and model scalability. We introduce a two-stage pipeline: a compact seed set of human-sung recordings is constructed by pairing fixed melodies with diverse LLM-generated lyrics, and melody-specific models are trained to synthesize over 500 hours of high-quality Chinese singing data. Building on this corpus, we propose DiTSinger, a Diffusion Transformer with RoPE and qk-norm, systematically scaled in depth, width, and resolution for enhanced fidelity. Furthermore, we design an implicit alignment mechanism that obviates phoneme-level duration labels by constraining phoneme-to-acoustic attention within character-level spans, thereby improving robustness under noisy or uncertain alignments. Extensive experiments validate that our approach enables scalable, alignment-free, and high-fidelity SVS.","authors":["Zongcai Du","Guilin Deng","Xiaofeng Guo","Xin Gao","Linke Li","Kaichang Cheng","Fubo Han","Siyu Yang","Peng Liu","Pan Zhong","Qiang Fu"],"pdf_url":"","comment":"ICASSP26 under review. Demo page: https://nju-jet.github.io/DiTSinger"},{"id":"http://arxiv.org/abs/2512.10688v5","updated":"2025-12-24T17:12:02Z","published":"2025-12-11T14:35:13Z","title":"Rethinking Popularity Bias in Collaborative Filtering via Analytical Vector Decomposition","summary":"Popularity bias fundamentally undermines the personalization capabilities of collaborative filtering (CF) models, causing them to disproportionately recommend popular items while neglecting users' genuine preferences for niche content. While existing approaches treat this as an external confounding factor, we reveal that popularity bias is an intrinsic geometric artifact of Bayesian Pairwise Ranking (BPR) optimization in CF models. Through rigorous mathematical analysis, we prove that BPR systematically organizes item embeddings along a dominant \"popularity direction\" where embedding magnitudes directly correlate with interaction frequency. This geometric distortion forces user embeddings to simultaneously handle two conflicting tasks-expressing genuine preference and calibrating against global popularity-trapping them in suboptimal configurations that favor popular items regardless of individual tastes. We propose Directional Decomposition and Correction (DDC), a universally applicable framework that surgically corrects this embedding geometry through asymmetric directional updates. DDC guides positive interactions along personalized preference directions while steering negative interactions away from the global popularity direction, disentangling preference from popularity at the geometric source. Extensive experiments across multiple BPR-based architectures demonstrate that DDC significantly outperforms state-of-the-art debiasing methods, reducing training loss to less than 5% of heavily-tuned baselines while achieving superior recommendation quality and fairness. Code is available in https://github.com/LingFeng-Liu-AI/DDC.","authors":["Lingfeng Liu","Yixin Song","Dazhong Shen","Bing Yin","Hao Li","Yanyong Zhang","Chao Wang"],"pdf_url":"","comment":"Accepted by SIGKDD 2026(First Cycle)"},{"id":"http://arxiv.org/abs/2512.21288v1","updated":"2025-12-24T17:10:44Z","published":"2025-12-24T17:10:44Z","title":"Model Merging via Multi-Teacher Knowledge Distillation","summary":"Model merging has emerged as a lightweight alternative to joint multi-task learning (MTL), yet the generalization properties of merged models remain largely unexplored. Establishing such theoretical guarantees is non-trivial, as the merging process typically forbids access to the original training data and involves combining fine-tuned models trained on fundamentally heterogeneous data distributions. Without a principled understanding of these dynamics, current methods often rely on heuristics to approximate the optimal combination of parameters. This dependence is most critical in coefficient scaling, the weighting factors that modulate the magnitude of each fine-tuned model's contribution to the shared parameter. However, without a principled objective to guide their selection, these methods lead to brittle performance and are highly sensitive to scaling initialization. We address this gap by (i) establishing a novel flatness-aware PAC-Bayes generalization bound specifically for the model merging setting. This analysis introduces a \"cross-task heterogeneity\" term that formally captures the mismatch between diverse fine-tuned model priors and the target multi-task distributions. Guided by this theoretical insight, (ii) we frame model merging as multi-teacher knowledge distillation on scarce, unlabeled data. We formally demonstrate that minimizing the student-teacher Kullback-Leibler divergence directly tightens the upper bound on the merged model's excess risk. Guided by the flatness-aware bound derived, (iii) we operationalize this objective via SAMerging, a method that employs Sharpness-Aware Minimization (SAM) to find flat minima. Empirically, SAMerging establishes a new state of the art across vision and NLP benchmarks, achieving remarkable performance. The code is available at https://github.com/arshandalili/SAMerging.","authors":["Seyed Arshan Dalili","Mehrdad Mahdavi"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.21280v1","updated":"2025-12-24T16:59:04Z","published":"2025-12-24T16:59:04Z","title":"SMART SLM: Structured Memory and Reasoning Transformer, A Small Language Model for Accurate Document Assistance","summary":"The user of Engineering Manuals (EM) finds it difficult to read EM s because they are long, have a dense format which includes written documents, step by step procedures, and standard parameter lists for engineering equipment. Off the shelf transformers, especially compact ones, treat this material as a flat stream of tokens. This approach leads to confident but incorrect numeric answers and forces the models to memorize separate facts inefficiently. SMART (Structured Memory and Reasoning Transformer) offers a different and practical solution to the above problem. SMART structures its processing by using a hierarchical approach, and is based upon three main job categories (1) A syntax-aware Fact Extractor (Grammarian) Tree LSTM which extracts facts as subject relation object relations from EM sentences (2) A compact indexed memory MANN (Memory Augmented Neural Network) that indexes these Rational Subject Relation Objects as 384 dimensional vectors that are associated with the source of the information, and (3) A 6 layer Transformer that learns to fuse the previously retrieved facts into its generated response. The entire SMART model utilizes 45.51M parameters, which is 64% less than GPT-2 (124M) and 69% less than BERT (133M), and it achieves a 21.3% higher accuracy than GPT-2, indicating that SMART fits the data better with the least amount of processing requirements. SMART employs dual modes of inference an indexed fast path for known documents (sub-second answer times) and an indexed dynamic path assisted by RAGs for new uploads (FAISS Top 20 results with memory severed at 64 slots). In real world deployment, this framework leads to more well supported results with reduced hallucinations than comparable small transformer models.","authors":["Divij Dudeja","Mayukha Pal"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.21246v1","updated":"2025-12-24T15:43:58Z","published":"2025-12-24T15:43:58Z","title":"Learning Factors in AI-Augmented Education: A Comparative Study of Middle and High School Students","summary":"The increasing integration of AI tools in education has led prior research to explore their impact on learning processes. Nevertheless, most existing studies focus on higher education and conventional instructional contexts, leaving open questions about how key learning factors are related in AI-mediated learning environments and how these relationships may vary across different age groups. Addressing these gaps, our work investigates whether four critical learning factors, experience, clarity, comfort, and motivation, maintain coherent interrelationships in AI-augmented educational settings, and how the structure of these relationships differs between middle and high school students. The study was conducted in authentic classroom contexts where students interacted with AI tools as part of programming learning activities to collect data on the four learning factors and students' perceptions. Using a multimethod quantitative analysis, which combined correlation analysis and text mining, we revealed markedly different dimensional structures between the two age groups. Middle school students exhibit strong positive correlations across all dimensions, indicating holistic evaluation patterns whereby positive perceptions in one dimension generalise to others. In contrast, high school students show weak or near-zero correlations between key dimensions, suggesting a more differentiated evaluation process in which dimensions are assessed independently. These findings reveal that perception dimensions actively mediate AI-augmented learning and that the developmental stage moderates their interdependencies. This work establishes a foundation for the development of AI integration strategies that respond to learners' developmental levels and account for age-specific dimensional structures in student-AI interactions.","authors":["Gaia Ebli","Bianca Raimondi","Maurizio Gabbrielli"],"pdf_url":"","comment":"Preprint. Under review"},{"id":"http://arxiv.org/abs/2512.17864v2","updated":"2025-12-24T15:38:23Z","published":"2025-12-19T18:11:15Z","title":"Interpretable Plant Leaf Disease Detection Using Attention-Enhanced CNN","summary":"Plant diseases pose a significant threat to global food security, necessitating accurate and interpretable disease detection methods. This study introduces an interpretable attention-guided Convolutional Neural Network (CNN), CBAM-VGG16, for plant leaf disease detection. By integrating Convolution Block Attention Module (CBAM) at each convolutional stage, the model enhances feature extraction and disease localization. Trained on five diverse plant disease datasets, our approach outperforms recent techniques, achieving high accuracy (up to 98.87%) and demonstrating robust generalization. Here, we show the effectiveness of our method through comprehensive evaluation and interpretability analysis using CBAM attention maps, Grad-CAM, Grad-CAM++, and Layer-wise Relevance Propagation (LRP). This study advances the application of explainable AI in agricultural diagnostics, offering a transparent and reliable system for smart farming. The code of our proposed work is available at https://github.com/BS0111/PlantAttentionCBAM.","authors":["Balram Singh","Ram Prakash Sharma","Somnath Dey"],"pdf_url":"","comment":"27 pages, 12 figures"},{"id":"http://arxiv.org/abs/2512.21243v1","updated":"2025-12-24T15:36:21Z","published":"2025-12-24T15:36:21Z","title":"LookPlanGraph: Embodied Instruction Following Method with VLM Graph Augmentation","summary":"Methods that use Large Language Models (LLM) as planners for embodied instruction following tasks have become widespread. To successfully complete tasks, the LLM must be grounded in the environment in which the robot operates. One solution is to use a scene graph that contains all the necessary information. Modern methods rely on prebuilt scene graphs and assume that all task-relevant information is available at the start of planning. However, these approaches do not account for changes in the environment that may occur between the graph construction and the task execution. We propose LookPlanGraph - a method that leverages a scene graph composed of static assets and object priors. During plan execution, LookPlanGraph continuously updates the graph with relevant objects, either by verifying existing priors or discovering new entities. This is achieved by processing the agents egocentric camera view using a Vision Language Model. We conducted experiments with changed object positions VirtualHome and OmniGibson simulated environments, demonstrating that LookPlanGraph outperforms methods based on predefined static scene graphs. To demonstrate the practical applicability of our approach, we also conducted experiments in a real-world setting. Additionally, we introduce the GraSIF (Graph Scenes for Instruction Following) dataset with automated validation framework, comprising 514 tasks drawn from SayPlan Office, BEHAVIOR-1K, and VirtualHome RobotHow. Project page available at https://lookplangraph.github.io .","authors":["Anatoly O. Onishchenko","Alexey K. Kovalev","Aleksandr I. Panov"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.21241v1","updated":"2025-12-24T15:35:03Z","published":"2025-12-24T15:35:03Z","title":"Improving the Convergence Rate of Ray Search Optimization for Query-Efficient Hard-Label Attacks","summary":"In hard-label black-box adversarial attacks, where only the top-1 predicted label is accessible, the prohibitive query complexity poses a major obstacle to practical deployment. In this paper, we focus on optimizing a representative class of attacks that search for the optimal ray direction yielding the minimum $\\ell_2$-norm perturbation required to move a benign image into the adversarial region. Inspired by Nesterov's Accelerated Gradient (NAG), we propose a momentum-based algorithm, ARS-OPT, which proactively estimates the gradient with respect to a future ray direction inferred from accumulated momentum. We provide a theoretical analysis of its convergence behavior, showing that ARS-OPT enables more accurate directional updates and achieves faster, more stable optimization. To further accelerate convergence, we incorporate surrogate-model priors into ARS-OPT's gradient estimation, resulting in PARS-OPT with enhanced performance. The superiority of our approach is supported by theoretical guarantees under standard assumptions. Extensive experiments on ImageNet and CIFAR-10 demonstrate that our method surpasses 13 state-of-the-art approaches in query efficiency.","authors":["Xinjie Xu","Shuyu Cheng","Dongwei Xu","Qi Xuan","Chen Ma"],"pdf_url":"","comment":"Published at AAAI 2026 (Oral). This version corresponds to the conference proceedings; v2 will include the appendix"},{"id":"http://arxiv.org/abs/2512.21236v1","updated":"2025-12-24T15:25:31Z","published":"2025-12-24T15:25:31Z","title":"Casting a SPELL: Sentence Pairing Exploration for LLM Limitation-breaking","summary":"Large language models (LLMs) have revolutionized software development through AI-assisted coding tools, enabling developers with limited programming expertise to create sophisticated applications. However, this accessibility extends to malicious actors who may exploit these powerful tools to generate harmful software. Existing jailbreaking research primarily focuses on general attack scenarios against LLMs, with limited exploration of malicious code generation as a jailbreak target. To address this gap, we propose SPELL, a comprehensive testing framework specifically designed to evaluate the weakness of security alignment in malicious code generation. Our framework employs a time-division selection strategy that systematically constructs jailbreaking prompts by intelligently combining sentences from a prior knowledge dataset, balancing exploration of novel attack patterns with exploitation of successful techniques. Extensive evaluation across three advanced code models (GPT-4.1, Claude-3.5, and Qwen2.5-Coder) demonstrates SPELL's effectiveness, achieving attack success rates of 83.75%, 19.38%, and 68.12% respectively across eight malicious code categories. The generated prompts successfully produce malicious code in real-world AI development tools such as Cursor, with outputs confirmed as malicious by state-of-the-art detection systems at rates exceeding 73%. These findings reveal significant security gaps in current LLM implementations and provide valuable insights for improving AI safety alignment in code generation applications.","authors":["Yifan Huang","Xiaojun Jia","Wenbo Guo","Yuqiang Sun","Yihao Huang","Chong Wang","Yang Liu"],"pdf_url":"","comment":"Accepted to FSE 2026"},{"id":"http://arxiv.org/abs/2512.21227v1","updated":"2025-12-24T15:07:36Z","published":"2025-12-24T15:07:36Z","title":"PhononBench:A Large-Scale Phonon-Based Benchmark for Dynamical Stability in Crystal Generation","summary":"In this work, we introduce PhononBench, the first large-scale benchmark for dynamical stability in AI-generated crystals. Leveraging the recently developed MatterSim interatomic potential, which achieves DFT-level accuracy in phonon predictions across more than 10,000 materials, PhononBench enables efficient large-scale phonon calculations and dynamical-stability analysis for 108,843 crystal structures generated by six leading crystal generation models. PhononBench reveals a widespread limitation of current generative models in ensuring dynamical stability: the average dynamical-stability rate across all generated structures is only 25.83%, with the top-performing model, MatterGen, reaching just 41.0%. Further case studies show that in property-targeted generation-illustrated here by band-gap conditioning with MatterGen--the dynamical-stability rate remains as low as 23.5% even at the optimal band-gap condition of 0.5 eV. In space-group-controlled generation, higher-symmetry crystals exhibit better stability (e.g., cubic systems achieve rates up to 49.2%), yet the average stability across all controlled generations is still only 34.4%. An important additional outcome of this study is the identification of 28,119 crystal structures that are phonon-stable across the entire Brillouin zone, providing a substantial pool of reliable candidates for future materials exploration. By establishing the first large-scale dynamical-stability benchmark, this work systematically highlights the current limitations of crystal generation models and offers essential evaluation criteria and guidance for their future development toward the design and discovery of physically viable materials. All model-generated crystal structures, phonon calculation results, and the high-throughput evaluation workflows developed in PhononBench will be openly released at https://github.com/xqh19970407/PhononBench","authors":["Xiao-Qi Han","Ze-Feng Gao","Peng-Jie Guo","Zhong-Yi Lu"],"pdf_url":"","comment":"19 pages, 6 figures"},{"id":"http://arxiv.org/abs/2512.21221v1","updated":"2025-12-24T15:02:33Z","published":"2025-12-24T15:02:33Z","title":"Leveraging Lightweight Entity Extraction for Scalable Event-Based Image Retrieval","summary":"Retrieving images from natural language descriptions is a core task at the intersection of computer vision and natural language processing, with wide-ranging applications in search engines, media archiving, and digital content management. However, real-world image-text retrieval remains challenging due to vague or context-dependent queries, linguistic variability, and the need for scalable solutions. In this work, we propose a lightweight two-stage retrieval pipeline that leverages event-centric entity extraction to incorporate temporal and contextual signals from real-world captions. The first stage performs efficient candidate filtering using BM25 based on salient entities, while the second stage applies BEiT-3 models to capture deep multimodal semantics and rerank the results. Evaluated on the OpenEvents v1 benchmark, our method achieves a mean average precision of 0.559, substantially outperforming prior baselines. These results highlight the effectiveness of combining event-guided filtering with long-text vision-language modeling for accurate and efficient retrieval in complex, real-world scenarios. Our code is available at https://github.com/PhamPhuHoa-23/Event-Based-Image-Retrieval","authors":["Dao Sy Duy Minh","Huynh Trung Kiet","Nguyen Lam Phu Quy","Phu-Hoa Pham","Tran Chi Nguyen"],"pdf_url":"","comment":"System description paper for EVENTA Grand Challenge Track 2 at ACM Multimedia 2025 (MM '25). Ranked 4th place. 6 pages, 1 figure, 2 tables"},{"id":"http://arxiv.org/abs/2512.21220v1","updated":"2025-12-24T15:01:26Z","published":"2025-12-24T15:01:26Z","title":"RoboSafe: Safeguarding Embodied Agents via Executable Safety Logic","summary":"Embodied agents powered by vision-language models (VLMs) are increasingly capable of executing complex real-world tasks, yet they remain vulnerable to hazardous instructions that may trigger unsafe behaviors. Runtime safety guardrails, which intercept hazardous actions during task execution, offer a promising solution due to their flexibility. However, existing defenses often rely on static rule filters or prompt-level control, which struggle to address implicit risks arising in dynamic, temporally dependent, and context-rich environments. To address this, we propose RoboSafe, a hybrid reasoning runtime safeguard for embodied agents through executable predicate-based safety logic. RoboSafe integrates two complementary reasoning processes on a Hybrid Long-Short Safety Memory. We first propose a Backward Reflective Reasoning module that continuously revisits recent trajectories in short-term memory to infer temporal safety predicates and proactively triggers replanning when violations are detected. We then propose a Forward Predictive Reasoning module that anticipates upcoming risks by generating context-aware safety predicates from the long-term safety memory and the agent's multimodal observations. Together, these components form an adaptive, verifiable safety logic that is both interpretable and executable as code. Extensive experiments across multiple agents demonstrate that RoboSafe substantially reduces hazardous actions (-36.8% risk occurrence) compared with leading baselines, while maintaining near-original task performance. Real-world evaluations on physical robotic arms further confirm its practicality. Code will be released upon acceptance.","authors":["Le Wang","Zonghao Ying","Xiao Yang","Quanchen Zou","Zhenfei Yin","Tianlin Li","Jian Yang","Yaodong Yang","Aishan Liu","Xianglong Liu"],"pdf_url":"","comment":"11 pages, 6 figures"},{"id":"http://arxiv.org/abs/2508.21010v2","updated":"2025-12-24T14:52:45Z","published":"2025-08-28T17:10:53Z","title":"ChainReaction: Causal Chain-Guided Reasoning for Modular and Explainable Causal-Why Video Question Answering","summary":"Existing Causal-Why Video Question Answering (VideoQA) models often struggle with higher-order reasoning, relying on opaque, monolithic pipelines that entangle video understanding, causal inference, and answer generation. These black-box approaches offer limited interpretability and tend to depend on shallow heuristics. We propose a novel, modular paradigm that explicitly decouples causal reasoning from answer generation, introducing natural language causal chains as interpretable intermediate representations. Inspired by human cognitive models, these structured cause-effect sequences bridge low-level video content with high-level causal reasoning, enabling transparent and logically coherent inference. Our two-stage architecture comprises a Causal Chain Extractor (CCE) that generates causal chains from video-question pairs, and a Causal Chain-Driven Answerer (CCDA) that derives answers grounded in these chains. To address the lack of annotated reasoning traces, we introduce a scalable method for generating accurate causal chains from existing datasets. We construct human verified causal chains for 46K samples. We also propose CauCo, a new evaluation metric for causality-oriented captioning. Experiments on three large-scale benchmarks demonstrate that our approach not only outperforms state-of-the-art models, but also yields substantial gains in explainability, user trust, and generalization -- positioning the CCE as a reusable causal reasoning engine across diverse domains. Project page: https://paritoshparmar.github.io/chainreaction/","authors":["Paritosh Parmar","Eric Peh","Basura Fernando"],"pdf_url":"","comment":"Project page: https://paritoshparmar.github.io/chainreaction/"},{"id":"http://arxiv.org/abs/2507.04346v5","updated":"2025-12-24T14:44:39Z","published":"2025-07-06T11:19:34Z","title":"Improving Action Smoothness for a Cascaded Online Learning Flight Control System","summary":"This paper aims to improve the action smoothness of a cascaded online learning flight control system. Although the cascaded structure is widely used in flight control design, its stability can be compromised by oscillatory control actions, which poses challenges for practical engineering applications. To address this issue, we introduce an online temporal smoothness technique and a low-pass filter to reduce the amplitude and frequency of the control actions. Fast Fourier Transform (FFT) is used to analyze policy performance in the frequency domain. Simulation results demonstrate the improvements achieved by the two proposed techniques.","authors":["Yifei Li","Erik-jan van Kampen"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.16378v2","updated":"2025-12-24T14:39:27Z","published":"2025-12-18T10:21:14Z","title":"Hearing to Translate: The Effectiveness of Speech Modality Integration into LLMs","summary":"As Large Language Models (LLMs) expand beyond text, integrating speech as a native modality has given rise to SpeechLLMs, which aim to translate spoken language directly, thereby bypassing traditional transcription-based pipelines. Whether this integration improves speech-to-text translation quality over established cascaded architectures, however, remains an open question. We present Hearing to Translate, the first comprehensive test suite rigorously benchmarking 5 state-of-the-art SpeechLLMs against 16 strong direct and cascade systems that couple leading speech foundation models (SFM), with multilingual LLMs. Our analysis spans 16 benchmarks, 13 language pairs, and 9 challenging conditions, including disfluent, noisy, and long-form speech. Across this extensive evaluation, we find that cascaded systems remain the most reliable overall, while current SpeechLLMs only match cascades in selected settings and SFMs lag behind both, highlighting that integrating an LLM, either within the model or in a pipeline, is essential for high-quality speech translation.","authors":["Sara Papi","Javier Garcia Gilabert","Zachary Hopton","Vilém Zouhar","Carlos Escolano","Gerard I. Gállego","Jorge Iranzo-Sánchez","Ahrii Kim","Dominik Macháček","Patricia Schmidtova","Maike Züfle"],"pdf_url":"","comment":"Project available at https://github.com/sarapapi/hearing2translate"},{"id":"http://arxiv.org/abs/2512.21204v1","updated":"2025-12-24T14:33:16Z","published":"2025-12-24T14:33:16Z","title":"SpidR-Adapt: A Universal Speech Representation Model for Few-Shot Adaptation","summary":"Human infants, with only a few hundred hours of speech exposure, acquire basic units of new languages, highlighting a striking efficiency gap compared to the data-hungry self-supervised speech models. To address this gap, this paper introduces SpidR-Adapt for rapid adaptation to new languages using minimal unlabeled data. We cast such low-resource speech representation learning as a meta-learning problem and construct a multi-task adaptive pre-training (MAdaPT) protocol which formulates the adaptation process as a bi-level optimization framework. To enable scalable meta-training under this framework, we propose a novel heuristic solution, first-order bi-level optimization (FOBLO), avoiding heavy computation costs. Finally, we stabilize meta-training by using a robust initialization through interleaved supervision which alternates self-supervised and supervised objectives. Empirically, SpidR-Adapt achieves rapid gains in phonemic discriminability (ABX) and spoken language modeling (sWUGGY, sBLIMP, tSC), improving over in-domain language models after training on less than 1h of target-language audio, over $100\\times$ more data-efficient than standard training. These findings highlight a practical, architecture-agnostic path toward biologically inspired, data-efficient representations. We open-source the training code and model checkpoints at https://github.com/facebookresearch/spidr-adapt.","authors":["Mahi Luthra","Jiayi Shen","Maxime Poli","Angelo Ortiz","Yosuke Higuchi","Youssef Benchekroun","Martin Gleize","Charles-Eric Saint-James","Dongyan Lin","Phillip Rust","Angel Villar","Surya Parimi","Vanessa Stark","Rashel Moritz","Juan Pino","Yann LeCun","Emmanuel Dupoux"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.21201v1","updated":"2025-12-24T14:28:17Z","published":"2025-12-24T14:28:17Z","title":"Schrödinger's Navigator: Imagining an Ensemble of Futures for Zero-Shot Object Navigation","summary":"Zero-shot object navigation (ZSON) requires a robot to locate a target object in a previously unseen environment without relying on pre-built maps or task-specific training. However, existing ZSON methods often struggle in realistic and cluttered environments, particularly when the scene contains heavy occlusions, unknown risks, or dynamically moving target objects. To address these challenges, we propose \\textbf{Schrödinger's Navigator}, a navigation framework inspired by Schrödinger's thought experiment on uncertainty. The framework treats unobserved space as a set of plausible future worlds and reasons over them before acting. Conditioned on egocentric visual inputs and three candidate trajectories, a trajectory-conditioned 3D world model imagines future observations along each path. This enables the agent to see beyond occlusions and anticipate risks in unseen regions without requiring extra detours or dense global mapping. The imagined 3D observations are fused into the navigation map and used to update a value map. These updates guide the policy toward trajectories that avoid occlusions, reduce exposure to uncertain space, and better track moving targets. Experiments on a Go2 quadruped robot across three challenging scenarios, including severe static occlusions, unknown risks, and dynamically moving targets, show that Schrödinger's Navigator consistently outperforms strong ZSON baselines in self-localization, object localization, and overall Success Rate in occlusion-heavy environments. These results demonstrate the effectiveness of trajectory-conditioned 3D imagination in enabling robust zero-shot object navigation.","authors":["Yu He","Da Huang","Zhenyang Liu","Zixiao Gu","Qiang Sun","Guangnan Ye","Yanwei Fu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2509.18527v4","updated":"2025-12-24T14:04:28Z","published":"2025-09-23T01:47:44Z","title":"FERA: A Pose-Based Semantic Pipeline for Automated Foil Fencing Refereeing","summary":"Many multimedia tasks map raw video into structured semantic representations for downstream decision-making. Sports officiating is a representative case, where fast, subtle interactions must be judged via symbolic rules. We present FERA (FEncing Referee Assistant), a pose-based framework that turns broadcast foil fencing video into action tokens and rule-grounded explanations. From monocular footage, FERA extracts 2D poses, converts them into a 101-dimensional kinematic representation, and applies an encoder-only transformer (FERA-MDT) to recognize per-fencer footwork, blade actions, and blade-line position. To obtain a consistent single-fencer representation for both athletes, FERA processes each clip and a horizontally flipped copy, yielding time-aligned left/right predictions without requiring a multi-person pose pipeline. A dynamic temporal windowing scheme enables inference on untrimmed pose tracks. These structured predictions serve as tokens for a language model (FERA-LM) that applies simplified right-of-way rules to generate textual decisions. On 1,734 clips (2,386 annotated actions), FERA-MDT achieves a macro-F1 of 0.549 under 5-fold cross-validation, outperforming BiLSTM and TCN baselines. Combined with FERA-LM, the full pipeline recovers referee priority with 77.7% accuracy on 969 exchanges. FERA provides a case-study benchmark for pose-based semantic grounding in a two-person sport and illustrates a general pipeline for connecting video understanding with rule-based reasoning.","authors":["Ziwen Chen","Zhong Wang"],"pdf_url":"","comment":"Updated Methodology and polished sections"},{"id":"http://arxiv.org/abs/2510.16416v3","updated":"2025-12-24T13:40:37Z","published":"2025-10-18T09:22:40Z","title":"SSL4RL: Revisiting Self-supervised Learning as Intrinsic Reward for Visual-Language Reasoning","summary":"Vision-language models (VLMs) have shown remarkable abilities by integrating large language models with visual inputs. However, they often fail to utilize visual evidence adequately, either depending on linguistic priors in vision-centric tasks or resorting to textual shortcuts during reasoning. Although reinforcement learning (RL) can align models with desired behaviors, its application to VLMs has been hindered by the lack of scalable and reliable reward mechanisms. To overcome this challenge, we propose SSL4RL, a novel framework that leverages self-supervised learning (SSL) tasks as a source of verifiable rewards for RL-based fine-tuning. Our approach reformulates SSL objectives-such as predicting image rotation or reconstructing masked patches-into dense, automatic reward signals, eliminating the need for human preference data or unreliable AI evaluators. Experiments show that SSL4RL substantially improves performance on both vision-centric and vision-language reasoning benchmarks. Furthermore, through systematic ablations, we identify key factors-such as task difficulty, model scale, and semantic alignment with the target domain-that influence the effectiveness of SSL4RL tasks, offering new design principles for future work. We also demonstrate the framework's generality by applying it to graph learning, where it yields significant gains. SSL4RL establishes a versatile and effective paradigm for aligning multimodal models using verifiable, self-supervised objectives.","authors":["Xiaojun Guo","Runyu Zhou","Yifei Wang","Qi Zhang","Chenheng Zhang","Stefanie Jegelka","Xiaohan Wang","Jiajun Chai","Guojun Yin","Wei Lin","Yisen Wang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2511.12867v2","updated":"2025-12-24T13:31:20Z","published":"2025-11-17T01:41:14Z","title":"Bootstrapping LLMs via Preference-Based Policy Optimization","summary":"Bootstrapping large language models (LLMs) through preference-based policy optimization offers a promising direction for aligning model behavior with human preferences without relying on extensive manual annotations. In this work, we propose a novel preference-based policy optimization (PbPO) framework that formulates the learning process as a min-max game between the main policy and a reward model (RM). The RM is constrained within a confidence set derived from preference data to ensure reliable exploitation. Our iterative online algorithm actively collects preference data through guided exploration of the evolving policy, enabling continual self-improvement of both the policy and the RM. We provide theoretical guarantees for our method, establishing high-probability regret bounds for both settings with sequence-level RM and token-level RM, demonstrating its effectiveness in bootstrapping LLMs. Extensive experiments on five benchmarks show that our approach consistently outperforms existing state-of-the-art preference optimization techniques.","authors":["Chen Jia"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.21165v1","updated":"2025-12-24T13:25:36Z","published":"2025-12-24T13:25:36Z","title":"BALLAST: Bandit-Assisted Learning for Latency-Aware Stable Timeouts in Raft","summary":"Randomized election timeouts are a simple and effective liveness heuristic for Raft, but they become brittle under long-tail latency, jitter, and partition recovery, where repeated split votes can inflate unavailability. This paper presents BALLAST, a lightweight online adaptation mechanism that replaces static timeout heuristics with contextual bandits. BALLAST selects from a discrete set of timeout \"arms\" using efficient linear contextual bandits (LinUCB variants), and augments learning with safe exploration to cap risk during unstable periods. We evaluate BALLAST on a reproducible discrete-event simulation with long-tail delay, loss, correlated bursts, node heterogeneity, and partition/recovery turbulence. Across challenging WAN regimes, BALLAST substantially reduces recovery time and unwritable time compared to standard randomized timeouts and common heuristics, while remaining competitive on stable LAN/WAN settings.","authors":["Qizhi Wang"],"pdf_url":"","comment":"15 pages, 22 tables, 11 figures"},{"id":"http://arxiv.org/abs/2512.21152v1","updated":"2025-12-24T12:43:40Z","published":"2025-12-24T12:43:40Z","title":"MODE: Multi-Objective Adaptive Coreset Selection","summary":"We present Mode(Multi-Objective adaptive Data Efficiency), a framework that dynamically combines coreset selection strategies based on their evolving contribution to model performance. Unlike static methods, \\mode adapts selection criteria to training phases: emphasizing class balance early, diversity during representation learning, and uncertainty at convergence. We show that MODE achieves (1-1/e)-approximation with O(n \\log n) complexity and demonstrates competitive accuracy while providing interpretable insights into data utility evolution. Experiments show \\mode reduces memory requirements","authors":["Tanmoy Mukherjee","Pierre Marquis","Zied Bouraoui"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.15249v2","updated":"2025-12-24T12:33:48Z","published":"2025-12-17T09:47:29Z","title":"Intersectional Fairness in Vision-Language Models for Medical Image Disease Classification","summary":"Medical artificial intelligence (AI) systems, particularly multimodal vision-language models (VLM), often exhibit intersectional biases where models are systematically less confident in diagnosing marginalised patient subgroups. Such bias can lead to higher rates of inaccurate and missed diagnoses due to demographically skewed data and divergent distributions of diagnostic certainty. Current fairness interventions frequently fail to address these gaps or compromise overall diagnostic performance to achieve statistical parity among the subgroups. In this study, we developed Cross-Modal Alignment Consistency (CMAC-MMD), a training framework that standardises diagnostic certainty across intersectional patient subgroups. Unlike traditional debiasing methods, this approach equalises the model's decision confidence without requiring sensitive demographic data during clinical inference. We evaluated this approach using 10,015 skin lesion images (HAM10000) with external validation on 12,000 images (BCN20000), and 10,000 fundus images for glaucoma detection (Harvard-FairVLMed), stratifying performance by intersectional age, gender, and race attributes. In the dermatology cohort, the proposed method reduced the overall intersectional missed diagnosis gap (difference in True Positive Rate, $Δ$TPR) from 0.50 to 0.26 while improving the overall Area Under the Curve (AUC) from 0.94 to 0.97 compared to standard training. Similarly, for glaucoma screening, the method reduced $Δ$TPR from 0.41 to 0.31, achieving a better AUC of 0.72 (vs. 0.71 baseline). This establishes a scalable framework for developing high-stakes clinical decision support systems that are both accurate and can perform equitably across diverse patient subgroups, ensuring reliable performance without increasing privacy risks.","authors":["Yupeng Zhang","Adam G. Dunn","Usman Naseem","Jinman Kim"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.21135v1","updated":"2025-12-24T12:06:26Z","published":"2025-12-24T12:06:26Z","title":"TGC-Net: A Structure-Aware and Semantically-Aligned Framework for Text-Guided Medical Image Segmentation","summary":"Text-guided medical segmentation enhances segmentation accuracy by utilizing clinical reports as auxiliary information. However, existing methods typically rely on unaligned image and text encoders, which necessitate complex interaction modules for multimodal fusion. While CLIP provides a pre-aligned multimodal feature space, its direct application to medical imaging is limited by three main issues: insufficient preservation of fine-grained anatomical structures, inadequate modeling of complex clinical descriptions, and domain-specific semantic misalignment. To tackle these challenges, we propose TGC-Net, a CLIP-based framework focusing on parameter-efficient, task-specific adaptations. Specifically, it incorporates a Semantic-Structural Synergy Encoder (SSE) that augments CLIP's ViT with a CNN branch for multi-scale structural refinement, a Domain-Augmented Text Encoder (DATE) that injects large-language-model-derived medical knowledge, and a Vision-Language Calibration Module (VLCM) that refines cross-modal correspondence in a unified feature space. Experiments on five datasets across chest X-ray and thoracic CT modalities demonstrate that TGC-Net achieves state-of-the-art performance with substantially fewer trainable parameters, including notable Dice gains on challenging benchmarks.","authors":["Gaoren Lin","Huangxuan Zhao","Yuan Xiong","Lefei Zhang","Bo Du","Wentao Zhu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.21132v1","updated":"2025-12-24T12:02:00Z","published":"2025-12-24T12:02:00Z","title":"AutoBaxBuilder: Bootstrapping Code Security Benchmarking","summary":"As LLMs see wide adoption in software engineering, the reliable assessment of the correctness and security of LLM-generated code is crucial. Notably, prior work has demonstrated that security is often overlooked, exposing that LLMs are prone to generating code with security vulnerabilities. These insights were enabled by specialized benchmarks, crafted through significant manual effort by security experts. However, relying on manually-crafted benchmarks is insufficient in the long term, because benchmarks (i) naturally end up contaminating training data, (ii) must extend to new tasks to provide a more complete picture, and (iii) must increase in difficulty to challenge more capable LLMs. In this work, we address these challenges and present AutoBaxBuilder, a framework that generates tasks and tests for code security benchmarking from scratch. We introduce a robust pipeline with fine-grained plausibility checks, leveraging the code understanding capabilities of LLMs to construct functionality tests and end-to-end security-probing exploits. To confirm the quality of the generated benchmark, we conduct both a qualitative analysis and perform quantitative experiments, comparing it against tasks constructed by human experts. We use AutoBaxBuilder to construct entirely new tasks and release them to the public as AutoBaxBench, together with a thorough evaluation of the security capabilities of LLMs on these tasks. We find that a new task can be generated in under 2 hours, costing less than USD 10.","authors":["Tobias von Arx","Niels Mündler","Mark Vero","Maximilian Baader","Martin Vechev"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.21127v1","updated":"2025-12-24T11:58:49Z","published":"2025-12-24T11:58:49Z","title":"A Real-World Evaluation of LLM Medication Safety Reviews in NHS Primary Care","summary":"Large language models (LLMs) often match or exceed clinician-level performance on medical benchmarks, yet very few are evaluated on real clinical data or examined beyond headline metrics. We present, to our knowledge, the first evaluation of an LLM-based medication safety review system on real NHS primary care data, with detailed characterisation of key failure behaviours across varying levels of clinical complexity. In a retrospective study using a population-scale EHR spanning 2,125,549 adults in NHS Cheshire and Merseyside, we strategically sampled patients to capture a broad range of clinical complexity and medication safety risk, yielding 277 patients after data-quality exclusions. An expert clinician reviewed these patients and graded system-identified issues and proposed interventions. Our primary LLM system showed strong performance in recognising when a clinical issue is present (sensitivity 100\\% [95\\% CI 98.2--100], specificity 83.1\\% [95\\% CI 72.7--90.1]), yet correctly identified all issues and interventions in only 46.9\\% [95\\% CI 41.1--52.8] of patients. Failure analysis reveals that, in this setting, the dominant failure mechanism is contextual reasoning rather than missing medication knowledge, with five primary patterns: overconfidence in uncertainty, applying standard guidelines without adjusting for patient context, misunderstanding how healthcare is delivered in practice, factual errors, and process blindness. These patterns persisted across patient complexity and demographic strata, and across a range of state-of-the-art models and configurations. We provide 45 detailed vignettes that comprehensively cover all identified failure cases. This work highlights shortcomings that must be addressed before LLM-based clinical AI can be safely deployed. It also begs larger-scale, prospective evaluations and deeper study of LLM behaviours in clinical contexts.","authors":["Oliver Normand","Esther Borsi","Mitch Fruin","Lauren E Walker","Jamie Heagerty","Chris C. Holmes","Anthony J Avery","Iain E Buchan","Harry Coppock"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.21118v1","updated":"2025-12-24T11:34:44Z","published":"2025-12-24T11:34:44Z","title":"STLDM: Spatio-Temporal Latent Diffusion Model for Precipitation Nowcasting","summary":"Precipitation nowcasting is a critical spatio-temporal prediction task for society to prevent severe damage owing to extreme weather events. Despite the advances in this field, the complex and stochastic nature of this task still poses challenges to existing approaches. Specifically, deterministic models tend to produce blurry predictions while generative models often struggle with poor accuracy. In this paper, we present a simple yet effective model architecture termed STLDM, a diffusion-based model that learns the latent representation from end to end alongside both the Variational Autoencoder and the conditioning network. STLDM decomposes this task into two stages: a deterministic forecasting stage handled by the conditioning network, and an enhancement stage performed by the latent diffusion model. Experimental results on multiple radar datasets demonstrate that STLDM achieves superior performance compared to the state of the art, while also improving inference efficiency. The code is available in https://github.com/sqfoo/stldm_official.","authors":["Shi Quan Foo","Chi-Ho Wong","Zhihan Gao","Dit-Yan Yeung","Ka-Hing Wong","Wai-Kin Wong"],"pdf_url":"","comment":"Accepted by TMLR. Camera-ready submission"},{"id":"http://arxiv.org/abs/2409.00162v2","updated":"2025-12-24T11:32:24Z","published":"2024-08-30T16:14:35Z","title":"Sequence to Sequence Reward Modeling: Improving RLHF by Language Feedback","summary":"Aligning the behavior of Large language models (LLMs) with human intentions and values remains a critical challenge. Reinforcement learning from human feedback (RLHF) aligns LLMs by training a reward model (RM) on human preferences and fine-tuning the LLMs to maximize RM feedback. Despite its effectiveness and popularity, RLHF is prone to biased local optimization. It means RM fails to provide feedback that accurately aligns with human preference, causing LLMs to explore unexpected generalizations, and failing to achieve alignment objectives. To mitigate this issue, we propose a novel \\textit{sequence-to-sequence (seq2seq) reward modeling} method. Its key insight is that learning from language feedback rather than scalar feedback improves RLHF without additional annotations. We replaced the reward modeling target from binary maximum likelihood estimation (MLE) with sequence MLE. This method enables richer and fine-grained language feedback without additional annotations, models, or training stages. Our experiments demonstrated its effectiveness, specifically, reducing the refusal-to-response paradigm in single-turn safety dialogues and the long-response bias in text summarization tasks. We provide further analysis that seq2seq RM improves RLHF performance across 2B and 7B LLMs on 3 NLP tasks, achieving an average win rate of 76.9\\%. We further show that seq2seq RM can still improve the performance of RLHF under out-of-distribution prompts.","authors":["Jiayi Zhou","Jiaming Ji","Juntao Dai","Dong Li","Yaodong Yang"],"pdf_url":"","comment":"7 pages"},{"id":"http://arxiv.org/abs/2510.11822v2","updated":"2025-12-24T11:22:59Z","published":"2025-10-13T18:19:23Z","title":"Beyond Consensus: Mitigating the Agreeableness Bias in LLM Judge Evaluations","summary":"New Large Language Models (LLMs) become available every few weeks, and modern application developers confronted with the unenviable task of having to decide if they should switch to a new model. While human evaluation remains the gold standard, it is costly and unscalable. The state-of-the-art approach is to use LLMs as evaluators ( LLM-as-a-judge), but this suffers from a critical flaw: LLMs exhibit a strong positive bias. We provide empirical evidence showing that while LLMs can identify valid outputs with high accuracy (i.e., True Positive Rate 96%), they are remarkably poor at identifying invalid ones (i.e., True Negative Rate <25%). This systematic bias, coupled with class imbalance, often leads to inflated reliability scores.\n  While ensemble-based methods like majority voting can help, we show that they are not good enough. We introduce an optimal minority-veto strategy that is resilient to missing data and mitigates this bias to a large extent. For scenarios requiring even higher precision, we propose a novel regression-based framework that directly models the validator bias using a small set of human-annotated ground truth data. On a challenging code feedback task over 366 high-school Python programs, our regression approach reduces the maximum absolute error to just 1.2%, achieving a 2x improvement over the best-performing ensemble of 14 state-of-the-art LLMs.","authors":["Suryaansh Jain","Umair Z. Ahmed","Shubham Sahai","Ben Leong"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.21110v1","updated":"2025-12-24T11:15:57Z","published":"2025-12-24T11:15:57Z","title":"Beyond Context: Large Language Models Failure to Grasp Users Intent","summary":"Current Large Language Models (LLMs) safety approaches focus on explicitly harmful content while overlooking a critical vulnerability: the inability to understand context and recognize user intent. This creates exploitable vulnerabilities that malicious users can systematically leverage to circumvent safety mechanisms. We empirically evaluate multiple state-of-the-art LLMs, including ChatGPT, Claude, Gemini, and DeepSeek. Our analysis demonstrates the circumvention of reliable safety mechanisms through emotional framing, progressive revelation, and academic justification techniques. Notably, reasoning-enabled configurations amplified rather than mitigated the effectiveness of exploitation, increasing factual precision while failing to interrogate the underlying intent. The exception was Claude Opus 4.1, which prioritized intent detection over information provision in some use cases. This pattern reveals that current architectural designs create systematic vulnerabilities. These limitations require paradigmatic shifts toward contextual understanding and intent recognition as core safety capabilities rather than post-hoc protective mechanisms.","authors":["Ahmed M. Hussain","Salahuddin Salahuddin","Panos Papadimitratos"],"pdf_url":"","comment":"22 pages and 23 figures"},{"id":"http://arxiv.org/abs/2512.21107v1","updated":"2025-12-24T11:12:09Z","published":"2025-12-24T11:12:09Z","title":"Semi-Supervised Learning for Large Language Models Safety and Content Moderation","summary":"Safety for Large Language Models (LLMs) has been an ongoing research focus since their emergence and is even more relevant nowadays with the increasing capacity of those models. Currently, there are several guardrails in place for all public LLMs and multiple proposed datasets for training safety classifiers. However, training these safety classifiers relies on large quantities of labeled data, which can be problematic to acquire, prone to labeling errors, or often include synthetic data. To address these issues, we suggest a different approach: utilizing semi-supervised learning techniques, which leverage both labeled and unlabeled data, to improve the performance on the safety task. We analyze the improvements that these techniques can offer for both prompts given to Large Language Models and the responses to those requests. Moreover, since augmentation is the central part of semi-supervised algorithms, we demonstrate the importance of using task-specific augmentations, which significantly increase the performance when compared to general-purpose augmentation techniques.","authors":["Eduard Stefan Dinuta","Iustin Sirbu","Traian Rebedea"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.21106v1","updated":"2025-12-24T11:10:28Z","published":"2025-12-24T11:10:28Z","title":"Semantic Refinement with LLMs for Graph Representations","summary":"Graph-structured data exhibit substantial heterogeneity in where their predictive signals originate: in some domains, node-level semantics dominate, while in others, structural patterns play a central role. This structure-semantics heterogeneity implies that no graph learning model with a fixed inductive bias can generalize optimally across diverse graph domains. However, most existing methods address this challenge from the model side by incrementally injecting new inductive biases, which remains fundamentally limited given the open-ended diversity of real-world graphs. In this work, we take a data-centric perspective and treat node semantics as a task-adaptive variable. We propose a Data-Adaptive Semantic Refinement framework DAS for graph representation learning, which couples a fixed graph neural network (GNN) and a large language model (LLM) in a closed feedback loop. The GNN provides implicit supervisory signals to guide the semantic refinement of LLM, and the refined semantics are fed back to update the same graph learner. We evaluate our approach on both text-rich and text-free graphs. Results show consistent improvements on structure-dominated graphs while remaining competitive on semantics-rich graphs, demonstrating the effectiveness of data-centric semantic adaptation under structure-semantics heterogeneity.","authors":["Safal Thapaliya","Zehong Wang","Jiazheng Li","Ziming Li","Yanfang Ye","Chuxu Zhang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.21099v1","updated":"2025-12-24T10:50:04Z","published":"2025-12-24T10:50:04Z","title":"TexAvatars : Hybrid Texel-3D Representations for Stable Rigging of Photorealistic Gaussian Head Avatars","summary":"Constructing drivable and photorealistic 3D head avatars has become a central task in AR/XR, enabling immersive and expressive user experiences. With the emergence of high-fidelity and efficient representations such as 3D Gaussians, recent works have pushed toward ultra-detailed head avatars. Existing approaches typically fall into two categories: rule-based analytic rigging or neural network-based deformation fields. While effective in constrained settings, both approaches often fail to generalize to unseen expressions and poses, particularly in extreme reenactment scenarios. Other methods constrain Gaussians to the global texel space of 3DMMs to reduce rendering complexity. However, these texel-based avatars tend to underutilize the underlying mesh structure. They apply minimal analytic deformation and rely heavily on neural regressors and heuristic regularization in UV space, which weakens geometric consistency and limits extrapolation to complex, out-of-distribution deformations. To address these limitations, we introduce TexAvatars, a hybrid avatar representation that combines the explicit geometric grounding of analytic rigging with the spatial continuity of texel space. Our approach predicts local geometric attributes in UV space via CNNs, but drives 3D deformation through mesh-aware Jacobians, enabling smooth and semantically meaningful transitions across triangle boundaries. This hybrid design separates semantic modeling from geometric control, resulting in improved generalization, interpretability, and stability. Furthermore, TexAvatars captures fine-grained expression effects, including muscle-induced wrinkles, glabellar lines, and realistic mouth cavity geometry, with high fidelity. Our method achieves state-of-the-art performance under extreme pose and expression variations, demonstrating strong generalization in challenging head reenactment settings.","authors":["Jaeseong Lee","Junyeong Ahn","Taewoong Kang","Jaegul Choo"],"pdf_url":"","comment":"3DV 2026, Project page with videos: https://summertight.github.io/TexAvatars/"},{"id":"http://arxiv.org/abs/2512.21080v1","updated":"2025-12-24T09:56:00Z","published":"2025-12-24T09:56:00Z","title":"LLM Personas as a Substitute for Field Experiments in Method Benchmarking","summary":"Field experiments (A/B tests) are often the most credible benchmark for methods in societal systems, but their cost and latency create a major bottleneck for iterative method development. LLM-based persona simulation offers a cheap synthetic alternative, yet it is unclear whether replacing humans with personas preserves the benchmark interface that adaptive methods optimize against. We prove an if-and-only-if characterization: when (i) methods observe only the aggregate outcome (aggregate-only observation) and (ii) evaluation depends only on the submitted artifact and not on the algorithm's identity or provenance (algorithm-blind evaluation), swapping humans for personas is just panel change from the method's point of view, indistinguishable from changing the evaluation population (e.g., New York to Jakarta). Furthermore, we move from validity to usefulness: we define an information-theoretic discriminability of the induced aggregate channel and show that making persona benchmarking as decision-relevant as a field experiment is fundamentally a sample-size question, yielding explicit bounds on the number of independent persona evaluations required to reliably distinguish meaningfully different methods at a chosen resolution.","authors":["Enoch Hyunwook Kang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.00020v2","updated":"2025-12-24T09:40:00Z","published":"2025-10-29T04:14:43Z","title":"Large Language Model for Verilog Code Generation: Literature Review and the Road Ahead","summary":"Code generation has emerged as a critical research area at the intersection of Software Engineering (SE) and Artificial Intelligence (AI), attracting significant attention from both academia and industry. Within this broader landscape, Verilog, as a representative hardware description language (HDL), plays a fundamental role in digital circuit design and verification, making its automated generation particularly significant for Electronic Design Automation (EDA). Consequently, recent research has increasingly focused on applying Large Language Models (LLMs) to Verilog code generation, particularly at the Register Transfer Level (RTL), exploring how these AI-driven techniques can be effectively integrated into hardware design workflows. Despite substantial research efforts have explored LLM applications in this domain, a comprehensive survey synthesizing these developments remains absent from the literature. This review fill addresses this gap by providing a systematic literature review of LLM-based methods for Verilog code generation, examining their effectiveness, limitations, and potential for advancing automated hardware design. The review encompasses research work from conferences and journals in the fields of SE, AI, and EDA, encompassing 70 papers published on venues, along with 32 high-quality preprint papers, bringing the total to 102 papers. By answering four key research questions, we aim to (1) identify the LLMs used for Verilog generation, (2) examine the datasets and metrics employed in evaluation, (3) categorize the techniques proposed for Verilog generation, and (4) analyze LLM alignment approaches for Verilog generation. Based on our findings, we have identified a series of limitations of existing studies. Finally, we have outlined a roadmap highlighting potential opportunities for future research endeavors in LLM-assisted hardware design.","authors":["Guang Yang","Wei Zheng","Xiang Chen","Dong Liang","Peng Hu","Yukui Yang","Shaohang Peng","Zhenghan Li","Jiahui Feng","Xiao Wei","Kexin Sun","Deyuan Ma","Haotian Cheng","Yiheng Shen","Xing Hu","Terry Yue Zhuo","David Lo"],"pdf_url":"","comment":"Under Review"},{"id":"http://arxiv.org/abs/2512.21075v1","updated":"2025-12-24T09:39:04Z","published":"2025-12-24T09:39:04Z","title":"Understanding Scaling Laws in Deep Neural Networks via Feature Learning Dynamics","summary":"The empirical success of deep learning is often attributed to scaling laws that predict consistent gains as model, data, and compute grow; however, large models can exhibit training instability and diminishing returns, suggesting that scaling laws describe what success looks like but not when and why scaling succeeds or fails. A central obstacle is the lack of a rigorous understanding of feature learning at large depth. While muP characterizes feature-learning dynamics in the infinite-width limit and enables hyperparameter transfer across width, its depth extension (depth-muP) breaks down for residual blocks with more than one internal layer. We derive Neural Feature Dynamics (NFD) for ResNets with single-layer residual blocks, characterizing feature learning via a coupled forward-backward stochastic system in the joint infinite-width and infinite-depth limit. In this regime, NFD identifies when scaling-law trends persist and explains diminishing returns. It also reveals a vanishing mechanism induced by the 1/sqrt(depth) residual scaling under which the gradient-independence assumption (GIA), known to fail during training at finite depth, becomes provably valid again at infinite depth, yielding an analytically tractable regime for end-to-end feature learning. Motivated by this insight, we study two-layer residual blocks and show that the same mechanism causes feature-learning collapse in the first internal layer at large depth, providing a structural explanation for the empirical failure of depth-muP. Based on this diagnosis, we propose a depth-aware learning-rate correction that counteracts the collapse and empirically restores depth-wise hyperparameter transfer, yielding stronger performance in deeper ResNets.","authors":["Zihan Yao","Ruoyu Wu","Tianxiang Gao"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.21066v1","updated":"2025-12-24T09:19:15Z","published":"2025-12-24T09:19:15Z","title":"Agentic Explainable Artificial Intelligence (Agentic XAI) Approach To Explore Better Explanation","summary":"Explainable artificial intelligence (XAI) enables data-driven understanding of factor associations with response variables, yet communicating XAI outputs to laypersons remains challenging, hindering trust in AI-based predictions. Large language models (LLMs) have emerged as promising tools for translating technical explanations into accessible narratives, yet the integration of agentic AI, where LLMs operate as autonomous agents through iterative refinement, with XAI remains unexplored. This study proposes an agentic XAI framework combining SHAP-based explainability with multimodal LLM-driven iterative refinement to generate progressively enhanced explanations. As a use case, we tested this framework as an agricultural recommendation system using rice yield data from 26 fields in Japan. The Agentic XAI initially provided a SHAP result and explored how to improve the explanation through additional analysis iteratively across 11 refinement rounds (Rounds 0-10). Explanations were evaluated by human experts (crop scientists) (n=12) and LLMs (n=14) against seven metrics: Specificity, Clarity, Conciseness, Practicality, Contextual Relevance, Cost Consideration, and Crop Science Credibility. Both evaluator groups confirmed that the framework successfully enhanced recommendation quality with an average score increase of 30-33% from Round 0, peaking at Rounds 3-4. However, excessive refinement showed a substantial drop in recommendation quality, indicating a bias-variance trade-off where early rounds lacked explanation depth (bias) while excessive iteration introduced verbosity and ungrounded abstraction (variance), as revealed by metric-specific analysis. These findings suggest that strategic early stopping (regularization) is needed for optimizing practical utility, challenging assumptions about monotonic improvement and providing evidence-based design principles for agentic XAI systems.","authors":["Tomoaki Yamaguchi","Yutong Zhou","Masahiro Ryo","Keisuke Katsura"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2412.18047v6","updated":"2025-12-24T09:13:05Z","published":"2024-12-23T23:45:45Z","title":"Towards Hierarchical Multi-Agent Decision-Making for Uncertainty-Aware EV Charging","summary":"Recent advances in bidirectional EV charging and discharging systems have spurred interest in workplace applications. However, real-world deployments face various dynamic factors, such as fluctuating electricity prices and uncertain EV departure times, that hinder effective energy management. To address these issues and minimize building electricity costs while meeting EV charging requirements, we design a hierarchical multi-agent structure in which a high-level agent coordinates overall charge or discharge decisions based on real-time pricing, while multiple low-level agents manage individual power level accordingly. For uncertain EV departure times, we propose a novel uncertainty-aware critic augmentation mechanism for low-level agents that improves the evaluation of power-level decisions and ensures robust control under such uncertainty. Building upon these two key designs, we introduce HUCA, a real-time charging control framework that coordinates energy supply among the building and EVs. Experiments on real-world electricity datasets show that HUCA significantly reduces electricity costs and maintains competitive performance in meeting EV charging requirements under both simulated certain and uncertain departure scenarios. The results further highlight the importance of hierarchical control and the proposed critic augmentation under the uncertain departure scenario. A case study illustrates HUCA's capability to allocate energy between the building and EVs in real time, underscoring its potential for practical use.","authors":["Lo Pang-Yun Ting","Ali Şenol","Huan-Yang Wang","Hsu-Chao Lai","Kun-Ta Chuang","Huan Liu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2504.04973v3","updated":"2025-12-24T08:54:34Z","published":"2025-04-07T11:58:19Z","title":"Ensuring Safety in an Uncertain Environment: Constrained MDPs via Stochastic Thresholds","summary":"This paper studies constrained Markov decision processes (CMDPs) with constraints against stochastic thresholds, aiming at safety of reinforcement learning in unknown and uncertain environments. We leverage a Growing-Window estimator sampling from interactions with the uncertain environment to estimate the thresholds, based on which we design Stochastic Pessimistic-Optimistic Thresholding (SPOT), a novel model-based primal-dual algorithm for multiple constraints against stochastic thresholds. SPOT enables reinforcement learning under both pessimistic and optimistic threshold settings. We prove that our algorithm achieves sublinear regret and constraint violation; i.e., a reward regret of $\\tilde{\\mathcal{O}}(\\sqrt{T})$ while allowing an $\\tilde{\\mathcal{O}}(\\sqrt{T})$ constraint violation over $T$ episodes. The theoretical guarantees show that our algorithm achieves performance comparable to that of an approach relying on fixed and clear thresholds. To the best of our knowledge, SPOT is the first reinforcement learning algorithm that realises theoretical guaranteed performance in an uncertain environment where even thresholds are unknown.","authors":["Qian Zuo","Fengxiang He"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2507.01061v3","updated":"2025-12-24T08:52:29Z","published":"2025-06-30T09:06:16Z","title":"Epitome: Pioneering an Experimental Platform for AI-Social Science Integration","summary":"Large Language Models (LLMs) enable unprecedented social science experimentation by creating controlled hybrid human-AI environments. We introduce Epitome (www.epitome-ai.com), an open experimental platform that operationalizes this paradigm through Matrix-like social worlds where researchers can study isolated human subjects and groups interacting with LLM agents. This maintains ecological validity while enabling precise manipulation of social dynamics. Epitome approaches three frontiers: (1) methodological innovation using LLM confederates to reduce complexity while scaling interactions; (2) empirical investigation of human behavior in AI-saturated environments; and (3) exploration of emergent properties in hybrid collectives. Drawing on interdisciplinary foundations from management, communication, sociology, psychology, and ethics, the platform's modular architecture spans foundation model deployment through data collection. We validate Epitome through replication of three seminal experiments, demonstrating capacity to generate robust findings while reducing experimental complexity. This tool provides crucial insights for understanding how humans navigate AI-mediated social realities, knowledge essential for policy, education, and human-centered AI design.","authors":["Jingjing Qu","Kejia Hu","Jun Zhu","Yulei Ye","Wenhao Li","Teng Wang","Zhiyun Chen","Chaochao Lu","Aimin Zhou","Xiangfeng Wang","Xia Hu","James Evans"],"pdf_url":"","comment":"25 pages, 6figures"},{"id":"http://arxiv.org/abs/2509.22358v3","updated":"2025-12-24T08:45:34Z","published":"2025-09-26T13:53:56Z","title":"Stochastic activations","summary":"We introduce stochastic activations. This novel strategy randomly selects between several non-linear functions in the feed-forward layer of a large language model. In particular, we choose between SILU or RELU depending on a Bernoulli draw. This strategy circumvents the optimization problem associated with RELU, namely, the constant shape for negative inputs that prevents the gradient flow. We leverage this strategy in two ways:\n  (1) We use stochastic activations during pre-training and fine-tune the model with RELU, which is used at inference time to provide sparse latent vectors. This reduces the inference FLOPs and translates into a significant speedup on CPU and GPU. This leads to better results than training from scratch with the RELU activation function.\n  (2) We evaluate stochastic activations for sequence generation. This strategy performs reasonably well: it has higher diversity and has only slightly inferior performance to the best deterministic non-linearity, SILU, combined with temperature sampling. This provides an alternative way to increase the diversity of generated text.","authors":["Maria Lomeli","Matthijs Douze","Gergely Szilvasy","Loic Cabannes","Jade Copet","Sainbayar Sukhbaatar","Jason Weston","Gabriel Synnaeve","Pierre-Emmanuel Mazaré","Hervé Jégou"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2510.15905v4","updated":"2025-12-24T08:45:19Z","published":"2025-09-16T20:19:53Z","title":"\"She's Like a Person but Better\": Characterizing Companion-Assistant Dynamics in Human-AI Relationships","summary":"Large language models are increasingly used for both task-based assistance and social companionship, yet research has typically focused on one or the other. Drawing on a survey (N = 202) and 30 interviews with high-engagement ChatGPT and Replika users, we characterize digital companionship as an emerging form of human-AI relationship. With both systems, users were drawn to humanlike qualities, such as emotional resonance and personalized responses, and non-humanlike qualities, such as constant availability and inexhaustible tolerance. This led to fluid chatbot uses, such as Replika as a writing assistant and ChatGPT as an emotional confidant, despite their distinct branding. However, we observed challenging tensions in digital companionship dynamics: participants grappled with bounded personhood, forming deep attachments while denying chatbots \"real\" human qualities, and struggled to reconcile chatbot relationships with social norms. These dynamics raise questions for the design of digital companions and the rise of hybrid, general-purpose AI systems.","authors":["Aikaterina Manoli","Janet V. T. Pauketat","Ali Ladak","Hayoun Noh","Angel Hsing-Chi Hwang","Jacy Reese Anthis"],"pdf_url":"","comment":"Improved visualizations, and corrected analysis error that had swapped reports of \"Respect\" and \"Shame.\" Fixed small errors in participant quotes"},{"id":"http://arxiv.org/abs/2512.21054v1","updated":"2025-12-24T08:44:58Z","published":"2025-12-24T08:44:58Z","title":"DexAvatar: 3D Sign Language Reconstruction with Hand and Body Pose Priors","summary":"The trend in sign language generation is centered around data-driven generative methods that require vast amounts of precise 2D and 3D human pose data to achieve an acceptable generation quality. However, currently, most sign language datasets are video-based and limited to automatically reconstructed 2D human poses (i.e., keypoints) and lack accurate 3D information. Furthermore, existing state-of-the-art for automatic 3D human pose estimation from sign language videos is prone to self-occlusion, noise, and motion blur effects, resulting in poor reconstruction quality. In response to this, we introduce DexAvatar, a novel framework to reconstruct bio-mechanically accurate fine-grained hand articulations and body movements from in-the-wild monocular sign language videos, guided by learned 3D hand and body priors. DexAvatar achieves strong performance in the SGNify motion capture dataset, the only benchmark available for this task, reaching an improvement of 35.11% in the estimation of body and hand poses compared to the state-of-the-art. The official website of this work is: https://github.com/kaustesseract/DexAvatar.","authors":["Kaustubh Kundu","Hrishav Bakul Barua","Lucy Robertson-Bell","Zhixi Cai","Kalin Stefanov"],"pdf_url":"","comment":"Accepted in WACV 2026"},{"id":"http://arxiv.org/abs/2512.20605v2","updated":"2025-12-24T08:32:45Z","published":"2025-12-23T18:51:50Z","title":"Emergent temporal abstractions in autoregressive models enable hierarchical reinforcement learning","summary":"Large-scale autoregressive models pretrained on next-token prediction and finetuned with reinforcement learning (RL) have achieved unprecedented success on many problem domains. During RL, these models explore by generating new outputs, one token at a time. However, sampling actions token-by-token can result in highly inefficient learning, particularly when rewards are sparse. Here, we show that it is possible to overcome this problem by acting and exploring within the internal representations of an autoregressive model. Specifically, to discover temporally-abstract actions, we introduce a higher-order, non-causal sequence model whose outputs control the residual stream activations of a base autoregressive model. On grid world and MuJoCo-based tasks with hierarchical structure, we find that the higher-order model learns to compress long activation sequence chunks onto internal controllers. Critically, each controller executes a sequence of behaviorally meaningful actions that unfold over long timescales and are accompanied with a learned termination condition, such that composing multiple controllers over time leads to efficient exploration on novel tasks. We show that direct internal controller reinforcement, a process we term \"internal RL\", enables learning from sparse rewards in cases where standard RL finetuning fails. Our results demonstrate the benefits of latent action generation and reinforcement in autoregressive models, suggesting internal RL as a promising avenue for realizing hierarchical RL within foundation models.","authors":["Seijin Kobayashi","Yanick Schimpf","Maximilian Schlegel","Angelika Steger","Maciej Wolczyk","Johannes von Oswald","Nino Scherrer","Kaitlin Maile","Guillaume Lajoie","Blake A. Richards","Rif A. Saurous","James Manyika","Blaise Agüera y Arcas","Alexander Meulemans","João Sacramento"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2510.00915v3","updated":"2025-12-24T08:25:26Z","published":"2025-10-01T13:56:44Z","title":"Reinforcement Learning with Verifiable yet Noisy Rewards under Imperfect Verifiers","summary":"Reinforcement Learning with Verifiable Rewards (RLVR) replaces costly human labeling with automated verifiers. To reduce verifier hacking, many RLVR systems binarize rewards to $\\{0,1\\}$, but imperfect verifiers inevitably introduce \\emph{false negatives} (rejecting correct answers) and \\emph{false positives} (accepting incorrect ones). We formalize verifier unreliability as a stochastic reward channel with asymmetric noise rates $ρ_0$ and $ρ_1$ -- the FP rate and the FN rate, respectively. From this abstraction we derive two lightweight corrections: (i) a \\emph{backward} correction that yields an unbiased surrogate reward and thus an unbiased policy-gradient estimator in expectation, and (ii) a \\emph{forward} correction that reweights score-function terms so the expected update aligns with the clean gradient direction and requires only the FN rate. We implement both as lightweight hooks in a group relative policy optimization pipeline, both corrections improve RLVR for math reasoning under synthetic and real verifier noise, with the forward variant being more stable under heavier noise. Finally, an appeals mechanism with a lightweight LLM verifier estimates the FN rate online and further improves performance.","authors":["Xin-Qiang Cai","Wei Wang","Feng Liu","Tongliang Liu","Gang Niu","Masashi Sugiyama"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2511.17129v2","updated":"2025-12-24T07:50:00Z","published":"2025-11-21T10:45:44Z","title":"Learning to Compress: Unlocking the Potential of Large Language Models for Text Representation","summary":"Text representation plays a critical role in tasks like clustering, retrieval, and other downstream applications. With the emergence of large language models (LLMs), there is increasing interest in harnessing their capabilities for this purpose. However, most of the LLMs are inherently causal and optimized for next-token prediction, making them suboptimal for producing holistic representations. To address this, recent studies introduced pretext tasks to adapt LLMs for text representation. Most of these tasks, however, rely on token-level prediction objectives, such as the masked next-token prediction (MNTP) used in LLM2Vec. In this work, we explore the untapped potential of context compression as a pretext task for unsupervised adaptation of LLMs. During compression pre-training, the model learns to generate compact memory tokens, which substitute the whole context for downstream sequence prediction. Experiments demonstrate that a well-designed compression objective can significantly enhance LLM-based text representations, outperforming models trained with token-level pretext tasks. Further improvements through contrastive learning produce a strong representation model (LLM2Comp) that outperforms contemporary LLM-based text encoders on a wide range of tasks while being more sample-efficient, requiring significantly less training data. Code is available at https://github.com/longtaizi13579/LLM2Comp.","authors":["Yeqin Zhang","Yizheng Zhao","Chen Hu","Binxing Jiao","Daxin Jiang","Ruihang Miao","Cam-Tu Nguyen"],"pdf_url":"","comment":"Accepted by AAAI'26"},{"id":"http://arxiv.org/abs/2512.12284v3","updated":"2025-12-24T07:46:59Z","published":"2025-12-13T11:02:04Z","title":"V-Rex: Real-Time Streaming Video LLM Acceleration via Dynamic KV Cache Retrieval","summary":"Streaming video large language models (LLMs) are increasingly used for real-time multimodal tasks such as video captioning, question answering, conversational agents, and augmented reality. However, these models face fundamental memory and computational challenges because their key-value (KV) caches grow substantially with continuous streaming video input. This process requires an iterative prefill stage, which is a unique feature of streaming video LLMs. Due to its iterative prefill stage, it suffers from significant limitations, including extensive computation, substantial data transfer, and degradation in accuracy. Crucially, this issue is exacerbated for edge deployment, which is the primary target for these models.\n  In this work, we propose V-Rex, the first software-hardware co-designed accelerator that comprehensively addresses both algorithmic and hardware bottlenecks in streaming video LLM inference. At its core, V-Rex introduces ReSV, a training-free dynamic KV cache retrieval algorithm. ReSV exploits temporal and spatial similarity-based token clustering to reduce excessive KV cache memory across video frames. To fully realize these algorithmic benefits, V-Rex offers a compact, low-latency hardware accelerator with a dynamic KV cache retrieval engine (DRE), featuring bit-level and early-exit based computing units. V-Rex achieves unprecedented real-time of 3.9-8.3 FPS and energy-efficient streaming video LLM inference on edge deployment with negligible accuracy loss. While DRE only accounts for 2.2% power and 2.0% area, the system delivers 1.9-19.7x speedup and 3.1-18.5x energy efficiency improvements over AGX Orin GPU. This work is the first to comprehensively tackle KV cache retrieval across algorithms and hardware, enabling real-time streaming video LLM inference on resource-constrained edge devices.","authors":["Donghyuk Kim","Sejeong Yang","Wonjin Shin","Joo-Young Kim"],"pdf_url":"","comment":"14 pages, 20 figures, conference, accepted by HPCA 2026"},{"id":"http://arxiv.org/abs/2512.21024v1","updated":"2025-12-24T07:42:10Z","published":"2025-12-24T07:42:10Z","title":"Policy-Conditioned Policies for Multi-Agent Task Solving","summary":"In multi-agent tasks, the central challenge lies in the dynamic adaptation of strategies. However, directly conditioning on opponents' strategies is intractable in the prevalent deep reinforcement learning paradigm due to a fundamental ``representational bottleneck'': neural policies are opaque, high-dimensional parameter vectors that are incomprehensible to other agents. In this work, we propose a paradigm shift that bridges this gap by representing policies as human-interpretable source code and utilizing Large Language Models (LLMs) as approximate interpreters. This programmatic representation allows us to operationalize the game-theoretic concept of \\textit{Program Equilibrium}. We reformulate the learning problem by utilizing LLMs to perform optimization directly in the space of programmatic policies. The LLM functions as a point-wise best-response operator that iteratively synthesizes and refines the ego agent's policy code to respond to the opponent's strategy. We formalize this process as \\textit{Programmatic Iterated Best Response (PIBR)}, an algorithm where the policy code is optimized by textual gradients, using structured feedback derived from game utility and runtime unit tests. We demonstrate that this approach effectively solves several standard coordination matrix games and a cooperative Level-Based Foraging environment.","authors":["Yue Lin","Shuhui Zhu","Wenhao Li","Ang Li","Dan Qiao","Pascal Poupart","Hongyuan Zha","Baoxiang Wang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.21017v1","updated":"2025-12-24T07:24:31Z","published":"2025-12-24T07:24:31Z","title":"Rethinking Supervised Fine-Tuning: Emphasizing Key Answer Tokens for Improved LLM Accuracy","summary":"With the rapid advancement of Large Language Models (LLMs), the Chain-of-Thought (CoT) component has become significant for complex reasoning tasks. However, in conventional Supervised Fine-Tuning (SFT), the model could allocate disproportionately more attention to CoT sequences with excessive length. This reduces focus on the much shorter but essential Key portion-the final answer, whose correctness directly determines task success and evaluation quality. To address this limitation, we propose SFTKey, a two-stage training scheme. In the first stage, conventional SFT is applied to ensure proper output format, while in the second stage, only the Key portion is fine-tuned to improve accuracy. Extensive experiments across multiple benchmarks and model families demonstrate that SFTKey achieves an average accuracy improvement exceeding 5\\% over conventional SFT, while preserving the ability to generate correct formats. Overall, this study advances LLM fine-tuning by explicitly balancing CoT learning with additional optimization on answer-relevant tokens.","authors":["Xiaofeng Shi","Qian Kou","Yuduo Li","Hua Zhou"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.21010v1","updated":"2025-12-24T07:14:31Z","published":"2025-12-24T07:14:31Z","title":"LLM Swiss Round: Aggregating Multi-Benchmark Performance via Competitive Swiss-System Dynamics","summary":"The rapid proliferation of Large Language Models (LLMs) and diverse specialized benchmarks necessitates a shift from fragmented, task-specific metrics to a holistic, competitive ranking system that effectively aggregates performance across multiple ability dimensions. Primarily using static scoring, current evaluation methods are fundamentally limited. They struggle to determine the proper mix ratio across diverse benchmarks, and critically, they fail to capture a model's dynamic competitive fitness or its vulnerability when confronted with sequential, high-stakes tasks. To address this, we introduce the novel Competitive Swiss-System Dynamics (CSD) framework. CSD simulates a multi-round, sequential contest where models are dynamically paired across a curated sequence of benchmarks based on their accumulated win-loss record. And Monte Carlo Simulation ($N=100,000$ iterations) is used to approximate the statistically robust Expected Win Score ($E[S_m]$), which eliminates the noise of random pairing and early-round luck. Furthermore, we implement a Failure Sensitivity Analysis by parameterizing the per-round elimination quantity ($T_k$), which allows us to profile models based on their risk appetite--distinguishing between robust generalists and aggressive specialists. We demonstrate that CSD provides a more nuanced and context-aware ranking than traditional aggregate scoring and static pairwise models, representing a vital step towards risk-informed, next-generation LLM evaluation.","authors":["Jiashuo Liu","Jiayun Wu","Chunjie Wu","Jingkai Liu","Zaiyuan Wang","Huan Zhou","Wenhao Huang","Hongseok Namkoong"],"pdf_url":"","comment":"18 pages"},{"id":"http://arxiv.org/abs/2512.21002v1","updated":"2025-12-24T06:57:35Z","published":"2025-12-24T06:57:35Z","title":"Distilling the Essence: Efficient Reasoning Distillation via Sequence Truncation","summary":"Distilling the reasoning capabilities from a large language model (LLM) to a smaller student model often involves training on substantial amounts of reasoning data. However, distillation over lengthy sequences with prompt (P), chain-of-thought (CoT), and answer (A) segments makes the process computationally expensive. In this work, we investigate how the allocation of supervision across different segments (P, CoT, A) affects student performance. Our analysis shows that selective knowledge distillation over only the CoT tokens can be effective when the prompt and answer information is encompassed by it. Building on this insight, we establish a truncation protocol to quantify computation-quality tradeoffs as a function of sequence length. We observe that training on only the first $50\\%$ of tokens of every training sequence can retain, on average, $\\approx94\\%$ of full-sequence performance on math benchmarks while reducing training time, memory usage, and FLOPs by about $50\\%$ each. These findings suggest that reasoning distillation benefits from prioritizing early reasoning tokens and provides a simple lever for computation-quality tradeoffs. Codes are available at https://github.com/weiruichen01/distilling-the-essence.","authors":["Wei-Rui Chen","Vignesh Kothapalli","Ata Fatahibaarzi","Hejian Sang","Shao Tang","Qingquan Song","Zhipeng Wang","Muhammad Abdul-Mageed"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20996v1","updated":"2025-12-24T06:48:04Z","published":"2025-12-24T06:48:04Z","title":"TrafficSimAgent: A Hierarchical Agent Framework for Autonomous Traffic Simulation with MCP Control","summary":"Traffic simulation is important for transportation optimization and policy making. While existing simulators such as SUMO and MATSim offer fully-featured platforms and utilities, users without too much knowledge about these platforms often face significant challenges when conducting experiments from scratch and applying them to their daily work. To solve this challenge, we propose TrafficSimAgent, an LLM-based agent framework that serves as an expert in experiment design and decision optimization for general-purpose traffic simulation tasks. The framework facilitates execution through cross-level collaboration among expert agents: high-level expert agents comprehend natural language instructions with high flexibility, plan the overall experiment workflow, and invoke corresponding MCP-compatible tools on demand; meanwhile, low-level expert agents select optimal action plans for fundamental elements based on real-time traffic conditions. Extensive experiments across multiple scenarios show that TrafficSimAgent effectively executes simulations under various conditions and consistently produces reasonable outcomes even when user instructions are ambiguous. Besides, the carefully designed expert-level autonomous decision-driven optimization in TrafficSimAgent yields superior performance when compared with other systems and SOTA LLM based methods.","authors":["Yuwei Du","Jun Zhang","Jie Feng","Zhicheng Liu","Jian Yuan","Yong Li"],"pdf_url":"","comment":"The code will be available at: https://github.com/tsinghua-fib-lab/TrafficSimAgent"},{"id":"http://arxiv.org/abs/2512.18748v2","updated":"2025-12-24T06:47:27Z","published":"2025-12-21T14:28:51Z","title":"Code2Doc: A Quality-First Curated Dataset for Code Documentation","summary":"The performance of automatic code documentation generation models depends critically on the quality of the training data used for supervision. However, most existing code documentation datasets are constructed through large scale scraping of public repositories with limited quality control. As a result, they often contain noisy documentation, extensive duplication, and increasing contamination from AI generated content. These issues weaken the supervision signal available to learning-based models and complicate evaluation.\n  We introduce Code2Doc, a quality-first curated dataset for function-level code documentation generation. Code2Doc consists of 13,358 high-quality function-documentation pairs extracted from widely used open-source repositories spanning five programming languages: Python, Java, TypeScript, JavaScript, and C++. The dataset is constructed using a four-stage curation pipeline that enforces documentation completeness and clarity, filters functions based on structural and complexity criteria, removes exact and near-duplicate code, and identifies documentation likely to be AI generated. Starting from 52,069 extracted candidates, only 25.6% satisfy all quality constraints.\n  We provide a detailed analysis of the resulting dataset, which achieves a mean documentation quality score of 6.93 out of 10. Overall, 86.9% of samples contain explicit type annotations, and only 2.9% are flagged as potentially AI generated. Baseline experiments show that fine-tuning a large language model on Code2Doc yields relative improvements of 29.47% in BLEU and 24.04% in ROUGE-L over zero shot performance, despite the modest dataset size. We release both the dataset and the full curation pipeline to support reproducible research on automatic code documentation generation.","authors":["Recep Kaan Karaman","Meftun Akarsu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2507.11783v3","updated":"2025-12-24T06:45:54Z","published":"2025-07-15T22:52:44Z","title":"EEG Foundation Models: A Critical Review of Current Progress and Future Directions","summary":"Premise. Patterns of electrical brain activity recorded via electroencephalography (EEG) offer immense value for scientific and clinical investigations. The inability of supervised EEG encoders to learn robust EEG patterns and their over-reliance on expensive signal annotations have sparked a transition towards general-purpose self-supervised EEG encoders, i.e., EEG foundation models (EEG-FMs), for robust and scalable EEG feature extraction. However, the real-world readiness of early EEG-FMs and the rubrics for long-term research progress remain unclear. Objective. In this work, we conduct a review of ten early EEG-FMs to capture common trends and identify key directions for future development of EEG-FMs. Methods. We comparatively analyze each EEG-FM using three fundamental pillars of foundation modeling, namely the representation of input data, self-supervised modeling, and the evaluation strategy. Based on this analysis, we present a critical synthesis of EEG-FM methodology, empirical findings, and outstanding research gaps. Results. We find that most EEG-FMs adopt a sequence-based modeling scheme that relies on transformer-based backbones and the reconstruction of masked temporal EEG sequences for self-supervision. However, model evaluations remain heterogeneous and largely limited, making it challenging to assess their practical off-the-shelf utility. In addition to adopting standardized and realistic evaluations, future work should demonstrate more substantial scaling effects and make principled and trustworthy choices throughout the EEG representation learning pipeline. Significance. Our review indicates that the development of benchmarks, software tools, technical methodologies, and applications in collaboration with domain experts may advance the translational utility and real-world adoption of EEG-FMs.","authors":["Gayal Kuruppu","Neeraj Wagh","Vaclav Kremen","Sandipan Pati","Gregory Worrell","Yogatheesan Varatharajah"],"pdf_url":"","comment":"22 pages (main), 5 figures (main), 4 tables (main + supplement)"},{"id":"http://arxiv.org/abs/2512.14693v2","updated":"2025-12-24T06:34:30Z","published":"2025-12-16T18:58:45Z","title":"Universal Reasoning Model","summary":"Universal transformers (UTs) have been widely used for complex reasoning tasks such as ARC-AGI and Sudoku, yet the specific sources of their performance gains remain underexplored. In this work, we systematically analyze UTs variants and show that improvements on ARC-AGI primarily arise from the recurrent inductive bias and strong nonlinear components of Transformer, rather than from elaborate architectural designs. Motivated by this finding, we propose the Universal Reasoning Model (URM), which enhances the UT with short convolution and truncated backpropagation. Our approach substantially improves reasoning performance, achieving state-of-the-art 53.8% pass@1 on ARC-AGI 1 and 16.0% pass@1 on ARC-AGI 2. Our code is avaliable at https://github.com/UbiquantAI/URM.","authors":["Zitian Gao","Lynx Chen","Yihao Xiao","He Xing","Ran Tao","Haoming Luo","Joey Zhou","Bryan Dai"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20991v1","updated":"2025-12-24T06:33:17Z","published":"2025-12-24T06:33:17Z","title":"FinAgent: An Agentic AI Framework Integrating Personal Finance and Nutrition Planning","summary":"The issue of limited household budgets and nutritional demands continues to be a challenge especially in the middle-income environment where food prices fluctuate. This paper introduces a price aware agentic AI system, which combines personal finance management with diet optimization. With household income and fixed expenditures, medical and well-being status, as well as real-time food costs, the system creates nutritionally sufficient meals plans at comparatively reasonable prices that automatically adjust to market changes. The framework is implemented in a modular multi-agent architecture, which has specific agents (budgeting, nutrition, price monitoring, and health personalization). These agents share the knowledge base and use the substitution graph to ensure that the nutritional quality is maintained at a minimum cost. Simulations with a representative Saudi household case study show a steady 12-18\\% reduction in costs relative to a static weekly menu, nutrient adequacy of over 95\\% and high performance with price changes of 20-30%. The findings indicate that the framework can locally combine affordability with nutritional adequacy and provide a viable avenue of capacity-building towards sustainable and fair diet planning in line with Sustainable Development Goals on Zero Hunger and Good Health.","authors":["Toqeer Ali Syed","Abdulaziz Alshahrani","Ali Ullah","Ali Akarma","Sohail Khan","Muhammad Nauman","Salman Jan"],"pdf_url":"","comment":"This paper was presented at the IEEE International Conference on Computing and Applications (ICCA 2025), Bahrain"},{"id":"http://arxiv.org/abs/2512.20985v1","updated":"2025-12-24T06:20:28Z","published":"2025-12-24T06:20:28Z","title":"A Blockchain-Monitored Agentic AI Architecture for Trusted Perception-Reasoning-Action Pipelines","summary":"The application of agentic AI systems in autonomous decision-making is growing in the areas of healthcare, smart cities, digital forensics, and supply chain management. Even though these systems are flexible and offer real-time reasoning, they also raise concerns of trust and oversight, and integrity of the information and activities upon which they are founded. The paper suggests a single architecture model comprising of LangChain-based multi-agent system with a permissioned blockchain to guarantee constant monitoring, policy enforcement, and immutable auditability of agentic action. The framework relates the perception conceptualization-action cycle to a blockchain layer of governance that verifies the inputs, evaluates recommended actions, and documents the outcomes of the execution. A Hyperledger Fabric-based system, action executors MCP-integrated, and LangChain agent are introduced and experiments of smart inventory management, traffic-signal control, and healthcare monitoring are done. The results suggest that blockchain-security verification is efficient in preventing unauthorized practices, offers traceability throughout the whole decision-making process, and maintains operational latency within reasonable ranges. The suggested framework provides a universal system of implementing high-impact agentic AI applications that are autonomous yet responsible.","authors":["Salman Jan","Hassan Ali Razzaqi","Ali Akarma","Mohammad Riyaz Belgaum"],"pdf_url":"","comment":"This paper was presented at the IEEE International Conference on Computing and Applications (ICCA 2025), Bahrain"},{"id":"http://arxiv.org/abs/2512.20983v1","updated":"2025-12-24T06:17:21Z","published":"2025-12-24T06:17:21Z","title":"Automatic Replication of LLM Mistakes in Medical Conversations","summary":"Large language models (LLMs) are increasingly evaluated in clinical settings using multi-dimensional rubrics which quantify reasoning quality, safety, and patient-centeredness. Yet, replicating specific mistakes in other LLM models is not straightforward and often requires manual effort. We introduce MedMistake, an automatic pipeline that extracts mistakes LLMs make in patient-doctor conversations and converts them into a benchmark of single-shot QA pairs. Our pipeline (1) creates complex, conversational data between an LLM patient and LLM doctor, (2) runs an evaluation with a committee of 2 LLM judges across a variety of dimensions and (3) creates simplified single-shot QA scenarios from those mistakes. We release MedMistake-All, a dataset of 3,390 single-shot QA pairs where GPT-5 and Gemini 2.5 Pro are currently failing to answer correctly, as judged by two LLM judges. We used medical experts to validate a subset of 211/3390 questions (MedMistake-Bench), which we used to run a final evaluation of 12 frontier LLMs: Claude Opus 4.5, Claude Sonnet 4.5, DeepSeek-Chat, Gemini 2.5 Pro, Gemini 3 Pro, GPT-4o, GPT-5, GPT-5.1, GPT-5.2, Grok 4, Grok 4.1, Mistral Large. We found that GPT models, Claude and Grok obtained the best performance on MedMistake-Bench. We release both the doctor-validated benchmark (MedMistake-Bench), as well as the full dataset (MedMistake-All) at https://huggingface.co/datasets/TheLumos/MedicalMistakeBenchmark.","authors":["Oleksii Proniakin","Diego Fajardo","Ruslan Nazarenko","Razvan Marinescu"],"pdf_url":"","comment":"48 pages, 3 figures, 4 tables"},{"id":"http://arxiv.org/abs/2512.20978v1","updated":"2025-12-24T06:13:02Z","published":"2025-12-24T06:13:02Z","title":"GenTSE: Enhancing Target Speaker Extraction via a Coarse-to-Fine Generative Language Model","summary":"Language Model (LM)-based generative modeling has emerged as a promising direction for TSE, offering potential for improved generalization and high-fidelity speech. We present GenTSE, a two-stage decoder-only generative LM approach for TSE: Stage-1 predicts coarse semantic tokens, and Stage-2 generates fine acoustic tokens. Separating semantics and acoustics stabilizes decoding and yields more faithful, content-aligned target speech. Both stages use continuous SSL or codec embeddings, offering richer context than discretized-prompt methods. To reduce exposure bias, we employ a Frozen-LM Conditioning training strategy that conditions the LMs on predicted tokens from earlier checkpoints to reduce the gap between teacher-forcing training and autoregressive inference. We further employ DPO to better align outputs with human perceptual preferences. Experiments on Libri2Mix show that GenTSE surpasses previous LM-based systems in speech quality, intelligibility, and speaker consistency.","authors":["Haoyang Li","Xuyi Zhuang","Azmat Adnan","Ye Ni","Wei Rao","Shreyas Gopal","Eng Siong Chng"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2408.06672v3","updated":"2025-12-24T06:12:56Z","published":"2024-08-13T06:47:59Z","title":"TimeBridge: Better Diffusion Prior Design with Bridge Models for Time Series Generation","summary":"Time series generation is widely used in real-world applications such as simulation, data augmentation, and hypothesis testing. Recently, diffusion models have emerged as the de facto approach to time series generation, enabling diverse synthesis scenarios. However, the fixed standard-Gaussian diffusion prior may be ill-suited for time series data, which exhibit properties such as temporal order and fixed time points. In this paper, we propose TimeBridge, a framework that flexibly synthesizes time series data by using diffusion bridges to learn paths between a chosen prior and the data distribution. We then explore several prior designs tailored to time series synthesis. Our framework covers (i) data- and time-dependent priors for unconditional generation and (ii) scale-preserving priors for conditional generation. Experiments show that our framework with data-driven priors outperforms standard diffusion models on time series generation.","authors":["Jinseong Park","Seungyun Lee","Woojin Jeong","Yujin Choi","Jaewook Lee"],"pdf_url":"","comment":"KDD 2026"},{"id":"http://arxiv.org/abs/2512.20974v1","updated":"2025-12-24T06:00:51Z","published":"2025-12-24T06:00:51Z","title":"Generalised Linear Models in Deep Bayesian RL with Learnable Basis Functions","summary":"Bayesian Reinforcement Learning (BRL) provides a framework for generalisation of Reinforcement Learning (RL) problems from its use of Bayesian task parameters in the transition and reward models. However, classical BRL methods assume known forms of transition and reward models, reducing their applicability in real-world problems. As a result, recent deep BRL methods have started to incorporate model learning, though the use of neural networks directly on the joint data and task parameters requires optimising the Evidence Lower Bound (ELBO). ELBOs are difficult to optimise and may result in indistinctive task parameters, hence compromised BRL policies. To this end, we introduce a novel deep BRL method, Generalised Linear Models in Deep Bayesian RL with Learnable Basis Functions (GLiBRL), that enables efficient and accurate learning of transition and reward models, with fully tractable marginal likelihood and Bayesian inference on task parameters and model noises. On challenging MetaWorld ML10/45 benchmarks, GLiBRL improves the success rate of one of the state-of-the-art deep BRL methods, VariBAD, by up to 2.7x. Comparing against representative or recent deep BRL / Meta-RL methods, such as MAML, RL2, SDVT, TrMRL and ECET, GLiBRL also demonstrates its low-variance and decent performance consistently.","authors":["Jingyang You","Hanna Kurniawati"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20968v1","updated":"2025-12-24T05:48:58Z","published":"2025-12-24T05:48:58Z","title":"Mesh-Attention: A New Communication-Efficient Distributed Attention with Improved Data Locality","summary":"Distributed attention is a fundamental problem for scaling context window for Large Language Models (LLMs). The state-of-the-art method, Ring-Attention, suffers from scalability limitations due to its excessive communication traffic. This paper proposes a new distributed attention algorithm, Mesh-Attention, by rethinking the design space of distributed attention with a new matrix-based model. Our method assigns a two-dimensional tile -- rather than one-dimensional row or column -- of computation blocks to each GPU to achieve higher efficiency through lower communication-computation (CommCom) ratio. The general approach covers Ring-Attention as a special case, and allows the tuning of CommCom ratio with different tile shapes. Importantly, we propose a greedy algorithm that can efficiently search the scheduling space within the tile with restrictions that ensure efficient communication among GPUs. The theoretical analysis shows that Mesh-Attention leads to a much lower communication complexity and exhibits good scalability comparing to other current algorithms.\n  Our extensive experiment results show that Mesh-Attention can achieve up to 3.4x speedup (2.9x on average) and reduce the communication volume by up to 85.4% (79.0% on average) on 256 GPUs. Our scalability results further demonstrate that Mesh-Attention sustains superior performance as the system scales, substantially reducing overhead in large-scale deployments. The results convincingly confirm the advantage of Mesh-Attention.","authors":["Sirui Chen","Jingji Chen","Siqi Zhu","Ziheng Jiang","Yanghua Peng","Xuehai Qian"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2511.11990v2","updated":"2025-12-24T05:42:44Z","published":"2025-11-15T02:05:11Z","title":"Improving Autoformalization Using Direct Dependency Retrieval","summary":"The convergence of deep learning and formal mathematics has spurred research in formal verification. Statement autoformalization, a crucial first step in this process, aims to translate informal descriptions into machine-verifiable representations but remains a significant challenge. The core difficulty lies in the fact that existing methods often suffer from a lack of contextual awareness, leading to hallucination of formal definitions and theorems. Furthermore, current retrieval-augmented approaches exhibit poor precision and recall for formal library dependency retrieval, and lack the scalability to effectively leverage ever-growing public datasets. To bridge this gap, we propose a novel retrieval-augmented framework based on DDR (\\textit{Direct Dependency Retrieval}) for statement autoformalization. Our DDR method directly generates candidate library dependencies from natural language mathematical descriptions and subsequently verifies their existence within the formal library via an efficient suffix array check. Leveraging this efficient search mechanism, we constructed a dependency retrieval dataset of over 500,000 samples and fine-tuned a high-precision DDR model. Experimental results demonstrate that our DDR model significantly outperforms SOTA methods in both retrieval precision and recall. Consequently, an autoformalizer equipped with DDR shows consistent performance advantages in both single-attempt accuracy and multi-attempt stability compared to models using traditional selection-based RAG methods.","authors":["Shaoqi Wang","Lu Yu","Siwei Lou","Feng Yan","Chunjie Yang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19512v2","updated":"2025-12-24T05:32:41Z","published":"2025-12-22T16:06:36Z","title":"Anatomy-R1: Enhancing Anatomy Reasoning in Multimodal Large Language Models via Anatomical Similarity Curriculum and Group Diversity Augmentation","summary":"Multimodal Large Language Models (MLLMs) have achieved impressive progress in natural image reasoning, yet their potential in medical imaging remains underexplored, especially in clinical anatomical surgical images. Anatomy understanding tasks demand precise understanding and clinically coherent answers, which are difficult to achieve due to the complexity of medical data and the scarcity of high-quality expert annotations. These challenges limit the effectiveness of conventional Supervised Fine-Tuning (SFT) strategies. While recent work has demonstrated that Group Relative Policy Optimization (GRPO) can enhance reasoning in MLLMs without relying on large amounts of data, we find two weaknesses that hinder GRPO's reasoning performance in anatomy recognition: 1) knowledge cannot be effectively shared between different anatomical structures, resulting in uneven information gain and preventing the model from converging, and 2) the model quickly converges to a single reasoning path, suppressing the exploration of diverse strategies. To overcome these challenges, we propose two novel methods. First, we implement a progressive learning strategy called Anatomical Similarity Curriculum Learning by controlling question difficulty via the similarity of answer choices, enabling the model to master complex problems incrementally. Second, we utilize question augmentation referred to as Group Diversity Question Augmentation to expand the model's search space for difficult queries, mitigating the tendency to produce uniform responses. Comprehensive experiments on the SGG-VQA and OmniMedVQA benchmarks show our method achieves a significant improvement across the two benchmarks, demonstrating its effectiveness in enhancing the medical reasoning capabilities of MLLMs. The code can be found in https://github.com/tomato996/Anatomy-R1","authors":["Ziyang Song","Zelin Zang","Zuyao Chen","Xusheng Liang","Dong Yi","Jinlin Wu","Hongbin Liu","Jiebo Luo","Zhen. Lei"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20959v1","updated":"2025-12-24T05:31:42Z","published":"2025-12-24T05:31:42Z","title":"Can Agentic AI Match the Performance of Human Data Scientists?","summary":"Data science plays a critical role in transforming complex data into actionable insights across numerous domains. Recent developments in large language models (LLMs) have significantly automated data science workflows, but a fundamental question persists: Can these agentic AI systems truly match the performance of human data scientists who routinely leverage domain-specific knowledge? We explore this question by designing a prediction task where a crucial latent variable is hidden in relevant image data instead of tabular features. As a result, agentic AI that generates generic codes for modeling tabular data cannot perform well, while human experts could identify the important hidden variable using domain knowledge. We demonstrate this idea with a synthetic dataset for property insurance. Our experiments show that agentic AI that relies on generic analytics workflow falls short of methods that use domain-specific insights. This highlights a key limitation of the current agentic AI for data science and underscores the need for future research to develop agentic AI systems that can better recognize and incorporate domain knowledge.","authors":["An Luo","Jin Du","Fangqiao Tian","Xun Xian","Robert Specht","Ganghua Wang","Xuan Bi","Charles Fleming","Jayanth Srinivasa","Ashish Kundu","Mingyi Hong","Jie Ding"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.13725v2","updated":"2025-12-24T05:31:23Z","published":"2025-12-13T17:54:15Z","title":"Compressed Causal Reasoning: Quantization and GraphRAG Effects on Interventional and Counterfactual Accuracy","summary":"Causal reasoning in Large Language Models spanning association, intervention, and counterfactual inference is essential for reliable decision making in high stakes settings. As deployment shifts toward edge and resource constrained environments, quantized models such as INT8 and NF4 are becoming standard. Yet the impact of precision reduction on formal causal reasoning is poorly understood. To our knowledge, this is the first study to systematically evaluate quantization effects across all three levels of Pearls Causal Ladder. Using a 3000 sample stratified CLadder benchmark, we find that rung level accuracy in Llama 3 8B remains broadly stable under quantization, with NF4 showing less than one percent overall degradation. Interventional queries at rung 2 are the most sensitive to precision loss, whereas counterfactual reasoning at rung 3 is comparatively stable but exhibits heterogeneous weaknesses across query types such as collider bias and backdoor adjustment. Experiments on the CRASS benchmark show near identical performance across precisions, indicating that existing commonsense counterfactual datasets lack the structural sensitivity needed to reveal quantization induced reasoning drift. We further evaluate Graph Retrieval Augmented Generation using ground truth causal graphs and observe a consistent improvement in NF4 interventional accuracy of plus 1.7 percent, partially offsetting compression related degradation. These results suggest that causal reasoning is unexpectedly robust to four bit quantization, graph structured augmentation can selectively reinforce interventional reasoning, and current counterfactual benchmarks fail to capture deeper causal brittleness. This work provides an initial empirical map of compressed causal reasoning and practical guidance for deploying efficient and structurally supported causal AI systems.","authors":["Steve Nwaiwu","Nipat Jongsawat","Anucha Tungkasthan"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20958v1","updated":"2025-12-24T05:29:35Z","published":"2025-12-24T05:29:35Z","title":"ReACT-Drug: Reaction-Template Guided Reinforcement Learning for de novo Drug Design","summary":"De novo drug design is a crucial component of modern drug development, yet navigating the vast chemical space to find synthetically accessible, high-affinity candidates remains a significant challenge. Reinforcement Learning (RL) enhances this process by enabling multi-objective optimization and exploration of novel chemical space - capabilities that traditional supervised learning methods lack. In this work, we introduce \\textbf{ReACT-Drug}, a fully integrated, target-agnostic molecular design framework based on Reinforcement Learning. Unlike models requiring target-specific fine-tuning, ReACT-Drug utilizes a generalist approach by leveraging ESM-2 protein embeddings to identify similar proteins for a given target from a knowledge base such as Protein Data Base (PDB). Thereafter, the known drug ligands corresponding to such proteins are decomposed to initialize a fragment-based search space, biasing the agent towards biologically relevant subspaces. For each such fragment, the pipeline employs a Proximal Policy Optimization (PPO) agent guiding a ChemBERTa-encoded molecule through a dynamic action space of chemically valid, reaction-template-based transformations. This results in the generation of \\textit{de novo} drug candidates with competitive binding affinities and high synthetic accessibility, while ensuring 100\\% chemical validity and novelty as per MOSES benchmarking. This architecture highlights the potential of integrating structural biology, deep representation learning, and chemical synthesis rules to automate and accelerate rational drug design. The dataset and code are available at https://github.com/YadunandanRaman/ReACT-Drug/.","authors":["R Yadunandan","Nimisha Ghosh"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20957v1","updated":"2025-12-24T05:27:53Z","published":"2025-12-24T05:27:53Z","title":"One Tool Is Enough: Reinforcement Learning for Repository-Level LLM Agents","summary":"Locating the files and functions requiring modification in large open-source software (OSS) repositories is challenging due to their scale and structural complexity. Existing large language model (LLM)-based methods typically treat this as a repository-level retrieval task and rely on multiple auxiliary tools, which overlook code execution logic and complicate model control. We propose RepoNavigator, an LLM agent equipped with a single execution-aware tool-jumping to the definition of an invoked symbol. This unified design reflects the actual flow of code execution while simplifying tool manipulation. RepoNavigator is trained end-to-end via Reinforcement Learning (RL) directly from a pretrained model, without any closed-source distillation. Experiments demonstrate that RL-trained RepoNavigator achieves state-of-the-art performance, with the 7B model outperforming 14B baselines, the 14B model surpassing 32B competitors, and even the 32B model exceeding closed-source models such as Claude-3.7. These results confirm that integrating a single, structurally grounded tool with RL training provides an efficient and scalable solution for repository-level issue localization.","authors":["Zhaoxi Zhang","Yitong Duan","Yanzhi Zhang","Yiming Xu","Jiyan He","Yunfang Wu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20954v1","updated":"2025-12-24T05:25:17Z","published":"2025-12-24T05:25:17Z","title":"Reflection Pretraining Enables Token-Level Self-Correction in Biological Sequence Models","summary":"Chain-of-Thought (CoT) prompting has significantly advanced task-solving capabilities in natural language processing with large language models. Unlike standard prompting, CoT encourages the model to generate intermediate reasoning steps, non-answer tokens, that help guide the model toward more accurate final outputs. These intermediate steps enable more complex reasoning processes such as error correction, memory management, future planning, and self-reflection. However, applying CoT to non-natural language domains, such as protein and RNA language models, is not yet possible, primarily due to the limited expressiveness of their token spaces (e.g., amino acid tokens). In this work, we propose and define the concept of language expressiveness: the ability of a given language, using its tokens and grammar, to encode information. We show that the limited expressiveness of protein language severely restricts the applicability of CoT-style reasoning. To overcome this, we introduce reflection pretraining, for the first time in a biological sequence model, which enables the model to engage in intermediate reasoning through the generation of auxiliary \"thinking tokens\" beyond simple answer tokens. Theoretically, we demonstrate that our augmented token set significantly enhances biological language expressiveness, thereby improving the overall reasoning capacity of the model. Experimentally, our pretraining approach teaches protein models to self-correct and leads to substantial performance gains compared to standard pretraining.","authors":["Xiang Zhang","Jiaqi Wei","Yuejin Yang","Zijie Qiu","Yuhan Chen","Zhiqiang Gao","Muhammad Abdul-Mageed","Laks V. S. Lakshmanan","Wanli Ouyang","Chenyu You","Siqi Sun"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2510.04265v2","updated":"2025-12-24T05:18:52Z","published":"2025-10-05T16:14:03Z","title":"Don't Pass@k: A Bayesian Framework for Large Language Model Evaluation","summary":"Pass$@k$ is widely used to report performance for LLM reasoning, but it often yields unstable, misleading rankings, especially when the number of trials (samples) is limited and compute is constrained. We present a principled Bayesian evaluation framework that replaces Pass$@k$ and average accuracy over $N$ trials (avg$@N$) with posterior estimates of a model's underlying success probability and credible intervals, yielding stable rankings and a transparent decision rule for differences. Evaluation outcomes are modeled as categorical (not just 0/1) with a Dirichlet prior, giving closed-form expressions for the posterior mean and uncertainty of any weighted rubric and enabling the use of prior evidence when appropriate. Theoretically, under a uniform prior, the Bayesian posterior mean is order-equivalent to average accuracy (Pass$@1$), explaining its empirical robustness while adding principled uncertainty. Empirically, in simulations with known ground-truth success rates and on AIME'24/'25, HMMT'25, and BrUMO'25, the Bayesian/avg procedure achieves faster convergence and greater rank stability than Pass$@k$ and recent variants, enabling reliable comparisons at far smaller sample counts. The framework clarifies when observed gaps are statistically meaningful (non-overlapping credible intervals) versus noise, and it naturally extends to graded, rubric-based evaluations. Together, these results recommend replacing Pass$@k$ for LLM evaluation and ranking with a posterior-based, compute-efficient protocol that unifies binary and non-binary evaluation while making uncertainty explicit. Code is available at https://github.com/mohsenhariri/scorio","authors":["Mohsen Hariri","Amirhossein Samandar","Michael Hinczewski","Vipin Chaudhary"],"pdf_url":"","comment":"Code and simulations: https://github.com/mohsenhariri/scorio"},{"id":"http://arxiv.org/abs/2512.20950v1","updated":"2025-12-24T05:14:40Z","published":"2025-12-24T05:14:40Z","title":"MultiMind at SemEval-2025 Task 7: Crosslingual Fact-Checked Claim Retrieval via Multi-Source Alignment","summary":"This paper presents our system for SemEval-2025 Task 7: Multilingual and Crosslingual Fact-Checked Claim Retrieval. In an era where misinformation spreads rapidly, effective fact-checking is increasingly critical. We introduce TriAligner, a novel approach that leverages a dual-encoder architecture with contrastive learning and incorporates both native and English translations across different modalities. Our method effectively retrieves claims across multiple languages by learning the relative importance of different sources in alignment. To enhance robustness, we employ efficient data preprocessing and augmentation using large language models while incorporating hard negative sampling to improve representation learning. We evaluate our approach on monolingual and crosslingual benchmarks, demonstrating significant improvements in retrieval accuracy and fact-checking performance over baselines.","authors":["Mohammad Mahdi Abootorabi","Alireza Ghahramani Kure","Mohammadali Mohammadkhani","Sina Elahimanesh","Mohammad Ali Ali Panah"],"pdf_url":"","comment":"11 pages Published at the SemEval-2025 workshop"},{"id":"http://arxiv.org/abs/2512.20949v1","updated":"2025-12-24T05:10:19Z","published":"2025-12-24T05:10:19Z","title":"Neural Probe-Based Hallucination Detection for Large Language Models","summary":"Large language models(LLMs) excel at text generation and knowledge question-answering tasks, but they are prone to generating hallucinated content, severely limiting their application in high-risk domains. Current hallucination detection methods based on uncertainty estimation and external knowledge retrieval suffer from the limitation that they still produce erroneous content at high confidence levels and rely heavily on retrieval efficiency and knowledge coverage. In contrast, probe methods that leverage the model's hidden-layer states offer real-time and lightweight advantages. However, traditional linear probes struggle to capture nonlinear structures in deep semantic spaces.To overcome these limitations, we propose a neural network-based framework for token-level hallucination detection. By freezing language model parameters, we employ lightweight MLP probes to perform nonlinear modeling of high-level hidden states. A multi-objective joint loss function is designed to enhance detection stability and semantic disambiguity. Additionally, we establish a layer position-probe performance response model, using Bayesian optimization to automatically search for optimal probe insertion layers and achieve superior training results.Experimental results on LongFact, HealthBench, and TriviaQA demonstrate that MLP probes significantly outperform state-of-the-art methods in accuracy, recall, and detection capability under low false-positive conditions.","authors":["Shize Liang","Hongzhi Wang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2504.02080v2","updated":"2025-12-24T04:57:32Z","published":"2025-04-02T19:33:07Z","title":"Evolving Security in LLMs: A Study of Jailbreak Attacks and Defenses","summary":"Large Language Models (LLMs) are increasingly popular, powering a wide range of applications. Their widespread use has sparked concerns, especially through jailbreak attacks that bypass safety measures to produce harmful content.\n  In this paper, we present a comprehensive security analysis of large language models (LLMs), addressing critical research questions on the evolution and determinants of model safety.\n  Specifically, we begin by identifying the most effective techniques for detecting jailbreak attacks. Next, we investigate whether newer versions of LLMs offer improved security compared to their predecessors. We also assess the impact of model size on overall security and explore the potential benefits of integrating multiple defense strategies to enhance the security.\n  Our study evaluates both open-source (e.g., LLaMA and Mistral) and closed-source models (e.g., GPT-4) by employing four state-of-the-art attack techniques and assessing the efficacy of three new defensive approaches.","authors":["Zhengchun Shang","Wenlan Wei","Weiheng Bai"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2406.16087v8","updated":"2025-12-24T04:53:14Z","published":"2024-06-23T12:02:17Z","title":"Imperative Learning: A Self-supervised Neuro-Symbolic Learning Framework for Robot Autonomy","summary":"Data-driven methods such as reinforcement and imitation learning have achieved remarkable success in robot autonomy. However, their data-centric nature still hinders them from generalizing well to ever-changing environments. Moreover, labeling data for robotic tasks is often impractical and expensive. To overcome these challenges, we introduce a new self-supervised neuro-symbolic (NeSy) computational framework, imperative learning (IL), for robot autonomy, leveraging the generalization abilities of symbolic reasoning. The framework of IL consists of three primary components: a neural module, a reasoning engine, and a memory system. We formulate IL as a special bilevel optimization (BLO), which enables reciprocal learning over the three modules. This overcomes the label-intensive obstacles associated with data-driven approaches and takes advantage of symbolic reasoning concerning logical reasoning, physical principles, geometric analysis, etc. We discuss several optimization techniques for IL and verify their effectiveness in five distinct robot autonomy tasks including path planning, rule induction, optimal control, visual odometry, and multi-robot routing. Through various experiments, we show that IL can significantly enhance robot autonomy capabilities and we anticipate that it will catalyze further research across diverse domains.","authors":["Chen Wang","Kaiyi Ji","Junyi Geng","Zhongqiang Ren","Taimeng Fu","Fan Yang","Yifan Guo","Haonan He","Xiangyu Chen","Zitong Zhan","Qiwei Du","Shaoshu Su","Bowen Li","Yuheng Qiu","Yi Du","Qihang Li","Yifan Yang","Xiao Lin","Zhipeng Zhao"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20941v1","updated":"2025-12-24T04:53:11Z","published":"2025-12-24T04:53:11Z","title":"A Multi-fidelity Double-Delta Wing Dataset and Empirical Scaling Laws for GNN-based Aerodynamic Field Surrogate","summary":"Data-driven surrogate models are increasingly adopted to accelerate vehicle design. However, open-source multi-fidelity datasets and empirical guidelines linking dataset size to model performance remain limited. This study investigates the relationship between training data size and prediction accuracy for a graph neural network (GNN) based surrogate model for aerodynamic field prediction. We release an open-source, multi-fidelity aerodynamic dataset for double-delta wings, comprising 2448 flow snapshots across 272 geometries evaluated at angles of attack from 11 (degree) to 19 (degree) at Ma=0.3 using both Vortex Lattice Method (VLM) and Reynolds-Averaged Navier-Stokes (RANS) solvers. The geometries are generated using a nested Saltelli sampling scheme to support future dataset expansion and variance-based sensitivity analysis. Using this dataset, we conduct a preliminary empirical scaling study of the MF-VortexNet surrogate by constructing six training datasets with sizes ranging from 40 to 1280 snapshots and training models with 0.1 to 2.4 million parameters under a fixed training budget. We find that the test error decreases with data size with a power-law exponent of -0.6122, indicating efficient data utilization. Based on this scaling law, we estimate that the optimal sampling density is approximately eight samples per dimension in a d-dimensional design space. The results also suggest improved data utilization efficiency for larger surrogate models, implying a potential trade-off between dataset generation cost and model training budget.","authors":["Yiren Shen","Juan J. Alonso"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2412.08893v2","updated":"2025-12-24T04:42:33Z","published":"2024-12-12T03:14:47Z","title":"Optimal Control with Natural Images: Efficient Reinforcement Learning using Overcomplete Sparse Codes","summary":"Optimal control and sequential decision making are widely used in many complex tasks. Optimal control over a sequence of natural images is a first step towards understanding the role of vision in control. Here, we formalize this problem as a reinforcement learning task, and derive general conditions under which an image includes enough information to implement an optimal policy. Reinforcement learning is shown to provide a computationally efficient method for finding optimal policies when natural images are encoded into \"efficient\" image representations. This is demonstrated by introducing a new reinforcement learning benchmark that easily scales to large numbers of states and long horizons. In particular, by representing each image as an overcomplete sparse code, we are able to efficiently solve an optimal control task that is orders of magnitude larger than those tasks solvable using complete codes. Theoretical justification for this behaviour is provided. This work also demonstrates that deep learning is not necessary for efficient optimal control with natural images.","authors":["Peter N. Loxley"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20934v1","updated":"2025-12-24T04:30:21Z","published":"2025-12-24T04:30:21Z","title":"Transductive Visual Programming: Evolving Tool Libraries from Experience for Spatial Reasoning","summary":"Spatial reasoning in 3D scenes requires precise geometric calculations that challenge vision-language models. Visual programming addresses this by decomposing problems into steps calling specialized tools, yet existing methods rely on either fixed toolsets or speculative tool induction before solving problems, resulting in suboptimal programs and poor utilization of induced tools. We present Transductive Visual Programming (TVP), a novel framework that builds new tools from its own experience rather than speculation. TVP first solves problems using basic tools while accumulating experiential solutions into an Example Library, then abstracts recurring patterns from these programs into reusable higher-level tools for an evolving Tool Library. This allows TVP to tackle new problems with increasingly powerful tools learned from experience. On Omni3D-Bench, TVP achieves state-of-the-art performance, outperforming GPT-4o by 22% and the previous best visual programming system by 11%. Our transductively learned tools are used 5x more frequently as core program dependency than inductively created ones, demonstrating more effective tool discovery and reuse. The evolved tools also show strong generalization to unseen spatial tasks, achieving superior performance on benchmarks from SpatialScore-Hard collection without any testset-specific modification. Our work establishes experience-driven transductive tool creation as a powerful paradigm for building self-evolving visual programming agents that effectively tackle challenging spatial reasoning tasks. We release our code at https://transductive-visualprogram.github.io/.","authors":["Shengguang Wu","Xiaohan Wang","Yuhui Zhang","Hao Zhu","Serena Yeung-Levy"],"pdf_url":"","comment":"Project Website: https://transductive-visualprogram.github.io/"},{"id":"http://arxiv.org/abs/2512.20932v1","updated":"2025-12-24T04:25:31Z","published":"2025-12-24T04:25:31Z","title":"Guardrailed Elasticity Pricing: A Churn-Aware Forecasting Playbook for Subscription Strategy","summary":"This paper presents a marketing analytics framework that operationalizes subscription pricing as a dynamic, guardrailed decision system, uniting multivariate demand forecasting, segment-level price elasticity, and churn propensity to optimize revenue, margin, and retention. The approach blends seasonal time-series models with tree-based learners, runs Monte Carlo scenario tests to map risk envelopes, and solves a constrained optimization that enforces business guardrails on customer experience, margin floors, and allowable churn. Validated across heterogeneous SaaS portfolios, the method consistently outperforms static tiers and uniform uplifts by reallocating price moves toward segments with higher willingness-to-pay while protecting price-sensitive cohorts. The system is designed for real-time recalibration via modular APIs and includes model explainability for governance and compliance. Managerially, the framework functions as a strategy playbook that clarifies when to shift from flat to dynamic pricing, how to align pricing with CLV and MRR targets, and how to embed ethical guardrails, enabling durable growth without eroding customer trust.","authors":["Deepit Sapru"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20920v1","updated":"2025-12-24T03:56:58Z","published":"2025-12-24T03:56:58Z","title":"RevFFN: Memory-Efficient Full-Parameter Fine-Tuning of Mixture-of-Experts LLMs with Reversible Blocks","summary":"Full parameter fine tuning is a key technique for adapting large language models (LLMs) to downstream tasks, but it incurs substantial memory overhead due to the need to cache extensive intermediate activations for backpropagation. This bottleneck makes full fine tuning of contemporary large scale LLMs challenging in practice. Existing distributed training frameworks such as DeepSpeed alleviate this issue using techniques like ZeRO and FSDP, which rely on multi GPU memory or CPU offloading, but often require additional hardware resources and reduce training speed. We introduce RevFFN, a memory efficient fine tuning paradigm for mixture of experts (MoE) LLMs. RevFFN employs carefully designed reversible Transformer blocks that allow reconstruction of layer input activations from outputs during backpropagation, eliminating the need to store most intermediate activations in memory. While preserving the expressive capacity of MoE architectures, this approach significantly reduces peak memory consumption for full parameter fine tuning. As a result, RevFFN enables efficient full fine tuning on a single consumer grade or server grade GPU.","authors":["Ningyuan Liu","Jing Yang","Kaitong Cai","Keze Wang"],"pdf_url":"","comment":"Under submission"},{"id":"http://arxiv.org/abs/2512.15745v2","updated":"2025-12-24T03:46:46Z","published":"2025-12-10T09:26:18Z","title":"LLaDA2.0: Scaling Up Diffusion Language Models to 100B","summary":"This paper presents LLaDA2.0 -- a tuple of discrete diffusion large language models (dLLM) scaling up to 100B total parameters through systematic conversion from auto-regressive (AR) models -- establishing a new paradigm for frontier-scale deployment. Instead of costly training from scratch, LLaDA2.0 upholds knowledge inheritance, progressive adaption and efficiency-aware design principle, and seamless converts a pre-trained AR model into dLLM with a novel 3-phase block-level WSD based training scheme: progressive increasing block-size in block diffusion (warm-up), large-scale full-sequence diffusion (stable) and reverting back to compact-size block diffusion (decay). Along with post-training alignment with SFT and DPO, we obtain LLaDA2.0-mini (16B) and LLaDA2.0-flash (100B), two instruction-tuned Mixture-of-Experts (MoE) variants optimized for practical deployment. By preserving the advantages of parallel decoding, these models deliver superior performance and efficiency at the frontier scale. Both models were open-sourced.","authors":["Tiwei Bie","Maosong Cao","Kun Chen","Lun Du","Mingliang Gong","Zhuochen Gong","Yanmei Gu","Jiaqi Hu","Zenan Huang","Zhenzhong Lan","Chengxi Li","Chongxuan Li","Jianguo Li","Zehuan Li","Huabin Liu","Lin Liu","Guoshan Lu","Xiaocheng Lu","Yuxin Ma","Jianfeng Tan","Lanning Wei","Ji-Rong Wen","Yipeng Xing","Xiaolu Zhang","Junbo Zhao","Da Zheng","Jun Zhou","Junlin Zhou","Zhanchao Zhou","Liwang Zhu","Yihong Zhuang"],"pdf_url":"","comment":"19 pages"},{"id":"http://arxiv.org/abs/2512.20082v2","updated":"2025-12-24T03:42:59Z","published":"2025-12-23T06:27:12Z","title":"Adaptive Financial Sentiment Analysis for NIFTY 50 via Instruction-Tuned LLMs , RAG and Reinforcement Learning Approaches","summary":"Financial sentiment analysis plays a crucial role in informing investment decisions, assessing market risk, and predicting stock price trends. Existing works in financial sentiment analysis have not considered the impact of stock prices or market feedback on sentiment analysis. In this paper, we propose an adaptive framework that integrates large language models (LLMs) with real-world stock market feedback to improve sentiment classification in the context of the Indian stock market. The proposed methodology fine-tunes the LLaMA 3.2 3B model using instruction-based learning on the SentiFin dataset. To enhance sentiment predictions, a retrieval-augmented generation (RAG) pipeline is employed that dynamically selects multi-source contextual information based on the cosine similarity of the sentence embeddings. Furthermore, a feedback-driven module is introduced that adjusts the reliability of the source by comparing predicted sentiment with actual next-day stock returns, allowing the system to iteratively adapt to market behavior. To generalize this adaptive mechanism across temporal data, a reinforcement learning agent trained using proximal policy optimization (PPO) is incorporated. The PPO agent learns to optimize source weighting policies based on cumulative reward signals from sentiment-return alignment. Experimental results on NIFTY 50 news headlines collected from 2024 to 2025 demonstrate that the proposed system significantly improves classification accuracy, F1-score, and market alignment over baseline models and static retrieval methods. The results validate the potential of combining instruction-tuned LLMs with dynamic feedback and reinforcement learning for robust, market-aware financial sentiment modeling.","authors":[" Chaithra","Kamesh Kadimisetty","Biju R Mohan"],"pdf_url":"","comment":"Accepted in CODS 2025"},{"id":"http://arxiv.org/abs/2512.00617v2","updated":"2025-12-24T03:42:55Z","published":"2025-11-29T20:16:11Z","title":"ART: Adaptive Response Tuning Framework -- A Multi-Agent Tournament-Based Approach to LLM Response Optimization","summary":"Large Language Models (LLMs) have demonstrated remarkable capabilities in natural language understanding and generation. However, single-model responses often exhibit inconsistencies, hallucinations, and varying quality across different query domains. This paper presents ART (Adaptive Response Tuning), a novel framework that employs tournament-style ELO ranking and multi-agent reasoning to systematically optimize LLM outputs. By enabling multiple LLM agents to compete, critique, and collaborate through structured tournament workflows, ART produces consensus responses that outperform individual model outputs. Our framework introduces configurable tournament parameters, dynamic agent selection, and multiple consensus fusion strategies. Experimental evaluations demonstrate significant improvements in response accuracy, coherence, and reliability compared to baseline single-model approaches. The ART framework provides a scalable, production-ready solution for applications requiring high-quality, vetted LLM responses, achieving an 8.4% improvement in overall quality metrics and R^2 values exceeding 0.96 in ELO rating convergence.","authors":["Omer Jauhar Khan"],"pdf_url":"","comment":"14 pages, 11 figures, 5 tables. IEEE conference-style paper with appendices"},{"id":"http://arxiv.org/abs/2512.10952v2","updated":"2025-12-24T03:33:33Z","published":"2025-12-11T18:59:55Z","title":"Hierarchical Dataset Selection for High-Quality Data Sharing","summary":"The success of modern machine learning hinges on access to high-quality training data. In many real-world scenarios, such as acquiring data from public repositories or sharing across institutions, data is naturally organized into discrete datasets that vary in relevance, quality, and utility. Selecting which repositories or institutions to search for useful datasets, and which datasets to incorporate into model training are therefore critical decisions, yet most existing methods select individual samples and treat all data as equally relevant, ignoring differences between datasets and their sources. In this work, we formalize the task of dataset selection: selecting entire datasets from a large, heterogeneous pool to improve downstream performance under resource constraints. We propose Dataset Selection via Hierarchies (DaSH), a dataset selection method that models utility at both dataset and group (e.g., collections, institutions) levels, enabling efficient generalization from limited observations. Across two public benchmarks (Digit-Five and DomainNet), DaSH outperforms state-of-the-art data selection baselines by up to 26.2% in accuracy, while requiring significantly fewer exploration steps. Ablations show DaSH is robust to low-resource settings and lack of relevant datasets, making it suitable for scalable and adaptive dataset selection in practical multi-source learning workflows.","authors":["Xiaona Zhou","Yingyan Zeng","Ran Jin","Ismini Lourentzou"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20905v1","updated":"2025-12-24T03:10:00Z","published":"2025-12-24T03:10:00Z","title":"DiEC: Diffusion Embedded Clustering","summary":"Deep clustering hinges on learning representations that are inherently clusterable. However, using a single encoder to produce a fixed embedding ignores the representation trajectory formed by a pretrained diffusion model across network hierarchies and noise timesteps, where clusterability varies substantially. We propose DiEC (Diffusion Embedded Clustering), which performs unsupervised clustering by directly reading internal activations from a pretrained diffusion U-Net.\n  DiEC formulates representation selection as a two-dimensional search over layer x timestep, and exploits a weak-coupling property to decompose it into two stages. Specifically, we first fix the U-Net bottleneck layer as the Clustering-friendly Middle Layer (CML), and then use Optimal Timestep Search (OTS) to identify the clustering-optimal timestep (t*). During training, we extract bottleneck features at the fixed t* and obtain clustering representations via a lightweight residual mapping. We optimize a DEC-style KL self-training objective, augmented with adaptive graph regularization and entropy regularization to strengthen cluster structures. In parallel, we introduce a denoising-consistency branch at random timesteps to stabilize the representations and preserve generative consistency. Experiments show that DiEC achieves competitive clustering performance on multiple standard benchmarks.","authors":["Haidong Hu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2506.11718v2","updated":"2025-12-24T03:08:54Z","published":"2025-06-13T12:34:15Z","title":"Interaction, Process, Infrastructure: A Unified Framework for Human-Agent Collaboration","summary":"While AI tools are increasingly prevalent in knowledge work, they remain fragmented, lacking the architectural foundation for sustained, adaptive collaboration. We argue this limitation stems from their inability to represent and manage the structure of collaborative work. To bridge this gap, we propose a layered conceptual framework for human-agent systems that integrates Interaction, Process, and Infrastructure. Crucially, our framework elevates Process to a first-class concern, an explicit, inspectable structural representation of activities. The central theoretical construct is Structural Adaptation, enabling the process to dynamically reorganize itself in response to evolving goals. We introduce a five-module Process Model as the representational basis for this adaptation. This model offers a unified theoretical grounding, reimagining human-agent collaboration as a coherent system for complex real-world work.","authors":["Yun Wang","Yan Lu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20902v1","updated":"2025-12-24T03:06:37Z","published":"2025-12-24T03:06:37Z","title":"Embodied AI-Enhanced IoMT Edge Computing: UAV Trajectory Optimization and Task Offloading with Mobility Prediction","summary":"Due to their inherent flexibility and autonomous operation, unmanned aerial vehicles (UAVs) have been widely used in Internet of Medical Things (IoMT) to provide real-time biomedical edge computing service for wireless body area network (WBAN) users. In this paper, considering the time-varying task criticality characteristics of diverse WBAN users and the dual mobility between WBAN users and UAV, we investigate the dynamic task offloading and UAV flight trajectory optimization problem to minimize the weighted average task completion time of all the WBAN users, under the constraint of UAV energy consumption. To tackle the problem, an embodied AI-enhanced IoMT edge computing framework is established. Specifically, we propose a novel hierarchical multi-scale Transformer-based user trajectory prediction model based on the users' historical trajectory traces captured by the embodied AI agent (i.e., UAV). Afterwards, a prediction-enhanced deep reinforcement learning (DRL) algorithm that integrates predicted users' mobility information is designed for intelligently optimizing UAV flight trajectory and task offloading decisions. Real-word movement traces and simulation results demonstrate the superiority of the proposed methods in comparison with the existing benchmarks.","authors":["Siqi Mu","Shuo Wen","Yang Lu","Ruihong Jiang","Bo Ai"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20898v1","updated":"2025-12-24T02:47:22Z","published":"2025-12-24T02:47:22Z","title":"DGSAN: Dual-Graph Spatiotemporal Attention Network for Pulmonary Nodule Malignancy Prediction","summary":"Lung cancer continues to be the leading cause of cancer-related deaths globally. Early detection and diagnosis of pulmonary nodules are essential for improving patient survival rates. Although previous research has integrated multimodal and multi-temporal information, outperforming single modality and single time point, the fusion methods are limited to inefficient vector concatenation and simple mutual attention, highlighting the need for more effective multimodal information fusion. To address these challenges, we introduce a Dual-Graph Spatiotemporal Attention Network, which leverages temporal variations and multimodal data to enhance the accuracy of predictions. Our methodology involves developing a Global-Local Feature Encoder to better capture the local, global, and fused characteristics of pulmonary nodules. Additionally, a Dual-Graph Construction method organizes multimodal features into inter-modal and intra-modal graphs. Furthermore, a Hierarchical Cross-Modal Graph Fusion Module is introduced to refine feature integration. We also compiled a novel multimodal dataset named the NLST-cmst dataset as a comprehensive source of support for related research. Our extensive experiments, conducted on both the NLST-cmst and curated CSTL-derived datasets, demonstrate that our DGSAN significantly outperforms state-of-the-art methods in classifying pulmonary nodules with exceptional computational efficiency.","authors":["Xiao Yu","Zhaojie Fang","Guanyu Zhou","Yin Shen","Huoling Luo","Ye Li","Ahmed Elazab","Xiang Wan","Ruiquan Ge","Changmiao Wang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.14554v4","updated":"2025-12-24T02:46:01Z","published":"2025-12-16T16:28:32Z","title":"VLegal-Bench: Cognitively Grounded Benchmark for Vietnamese Legal Reasoning of Large Language Models","summary":"The rapid advancement of large language models (LLMs) has enabled new possibilities for applying artificial intelligence within the legal domain. Nonetheless, the complexity, hierarchical organization, and frequent revisions of Vietnamese legislation pose considerable challenges for evaluating how well these models interpret and utilize legal knowledge. To address this gap, the Vietnamese Legal Benchmark (VLegal-Bench) is introduced, the first comprehensive benchmark designed to systematically assess LLMs on Vietnamese legal tasks. Informed by Bloom's cognitive taxonomy, VLegal-Bench encompasses multiple levels of legal understanding through tasks designed to reflect practical usage scenarios. The benchmark comprises 10,450 samples generated through a rigorous annotation pipeline, where legal experts label and cross-validate each instance using our annotation system to ensure every sample is grounded in authoritative legal documents and mirrors real-world legal assistant workflows, including general legal questions and answers, retrieval-augmented generation, multi-step reasoning, and scenario-based problem solving tailored to Vietnamese law. By providing a standardized, transparent, and cognitively informed evaluation framework, VLegal-Bench establishes a solid foundation for assessing LLM performance in Vietnamese legal contexts and supports the development of more reliable, interpretable, and ethically aligned AI-assisted legal systems. To facilitate access and reproducibility, we provide a public landing page for this benchmark at https://vilegalbench.cmcai.vn/.","authors":["Nguyen Tien Dong","Minh-Anh Nguyen","Thanh Dat Hoang","Nguyen Tuan Ngoc","Dao Xuan Quang Minh","Phan Phi Hai","Nguyen Thi Ngoc Anh","Dang Van Tu","Binh Vu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2510.18802v3","updated":"2025-12-24T02:39:19Z","published":"2025-10-21T16:57:40Z","title":"Computational Foundations for Strategic Coopetition: Formalizing Interdependence and Complementarity","summary":"Coopetition refers to simultaneous cooperation and competition among actors wherein actors 'cooperate to grow the pie and compete to split it up.' Modern socio-technical systems are characterized by strategic coopetition wherein actors concomitantly cooperate to create value and compete to capture it. While conceptual modeling languages such as i* provide rich qualitative representations of strategic dependencies, they lack mechanisms for quantitative analysis of dynamic trade-offs. Conversely, classical game theory offers mathematical rigor but strips away contextual richness. This report bridges this gap by developing computational foundations that formalize two critical dimensions of coopetition: interdependence and complementarity. We ground interdependence in i* structural dependency analysis, translating depender-dependee-dependum relationships into quantitative interdependence coefficients via a structured translation framework. We formalize complementarity following Brandenburger and Nalebuff's Added Value concept, modeling synergistic value creation with validated parameterization. We integrate structural dependencies with bargaining power in value appropriation and introduce a game-theoretic formulation where Nash Equilibrium incorporates structural interdependence. Validation combines over 22,000 experimental trials across power and logarithmic specifications with the Samsung-Sony S-LCD joint venture (2004-2011). Under strict historical alignment scoring, logarithmic specifications achieve 58/60 compared to power functions (46/60), producing realistic 41% cooperation increases aligning with documented S-LCD patterns while power functions produce 166% increases exceeding realistic bounds. Statistical significance confirmed at p < 0.001, Cohen's d > 9.","authors":["Vik Pant","Eric Yu"],"pdf_url":"","comment":"39 pages, 9 figures, This technical report serves as the foundational reference for a coordinated research program examining strategic coopetition in requirements engineering and multi-agent systems, with companion work addressing trust dynamics, team production, and reciprocity mechanisms"},{"id":"http://arxiv.org/abs/2508.17671v5","updated":"2025-12-24T02:19:21Z","published":"2025-08-25T05:08:49Z","title":"Consistent Opponent Modeling in Imperfect-Information Games","summary":"The goal of agents in multi-agent environments is to maximize total reward against the opposing agents that are encountered. Following a game-theoretic solution concept, such as Nash equilibrium, may obtain a strong performance in some settings; however, such approaches fail to capitalize on historical and observed data from repeated interactions against our opponents. Opponent modeling algorithms integrate machine learning techniques to exploit suboptimal opponents utilizing available data; however, the effectiveness of such approaches in imperfect-information games to date is quite limited. We show that existing opponent modeling approaches fail to satisfy a simple desirable property even against static opponents drawn from a known prior distribution; namely, they do not guarantee that the model approaches the opponent's true strategy even in the limit as the number of game iterations approaches infinity. We develop a new algorithm that is able to achieve this property and runs efficiently by solving a convex minimization problem based on the sequence-form game representation using projected gradient descent. The algorithm is guaranteed to efficiently converge to the opponent's true strategy under standard Bayesian identifiability and visitation assumptions, given observations from gameplay and possibly additional historical data if it is available.","authors":["Sam Ganzfried"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20135v2","updated":"2025-12-24T02:19:21Z","published":"2025-12-23T07:53:57Z","title":"MolAct: An Agentic RL Framework for Molecular Editing and Property Optimization","summary":"Molecular editing and optimization are multi-step problems that require iteratively improving properties while keeping molecules chemically valid and structurally similar. We frame both tasks as sequential, tool-guided decisions and introduce MolAct, an agentic reinforcement learning framework that employs a two-stage training paradigm: first building editing capability, then optimizing properties while reusing the learned editing behaviors. To the best of our knowledge, this is the first work to formalize molecular design as an Agentic Reinforcement Learning problem, where an LLM agent learns to interleave reasoning, tool-use, and molecular optimization. The framework enables agents to interact in multiple turns, invoking chemical tools for validity checking, property assessment, and similarity control, and leverages their feedback to refine subsequent edits. We instantiate the MolAct framework to train two model families: MolEditAgent for molecular editing tasks and MolOptAgent for molecular optimization tasks. In molecular editing, MolEditAgent-7B delivers 100, 95, and 98 valid add, delete, and substitute edits, outperforming strong closed \"thinking\" baselines such as DeepSeek-R1; MolEditAgent-3B approaches the performance of much larger open \"thinking\" models like Qwen3-32B-think. In molecular optimization, MolOptAgent-7B (trained on MolEditAgent-7B) surpasses the best closed \"thinking\" baseline (e.g., Claude 3.7) on LogP and remains competitive on solubility, while maintaining balanced performance across other objectives. These results highlight that treating molecular design as a multi-step, tool-augmented process is key to reliable and interpretable improvements.","authors":["Zhuo Yang","Yeyun Chen","Jiaqing Xie","Ben Gao","Shuaike Shen","Wanhao Liu","Liujia Yang","Beilun Wang","Tianfan Fu","Yuqiang Li"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2506.08602v2","updated":"2025-12-24T02:12:49Z","published":"2025-06-10T09:12:00Z","title":"WGLE:Backdoor-free and Multi-bit Black-box Watermarking for Graph Neural Networks","summary":"Graph Neural Networks (GNNs) are increasingly deployed in real-world applications, making ownership verification critical to protect their intellectual property against model theft. Fingerprinting and black-box watermarking are two main methods. However, the former relies on determining model similarity, which is computationally expensive and prone to ownership collisions after model post-processing. The latter embeds backdoors, exposing watermarked models to the risk of backdoor attacks. Moreover, both previous methods enable ownership verification but do not convey additional information about the copy model. If the owner has multiple models, each model requires a distinct trigger graph.\n  To address these challenges, this paper proposes WGLE, a novel black-box watermarking paradigm for GNNs that enables embedding the multi-bit string in GNN models without using backdoors. WGLE builds on a key insight we term Layer-wise Distance Difference on an Edge (LDDE), which quantifies the difference between the feature distance and the prediction distance of two connected nodes in a graph. By assigning unique LDDE values to the edges and employing the LDDE sequence as the watermark, WGLE supports multi-bit capacity without relying on backdoor mechanisms. We evaluate WGLE on six public datasets across six mainstream GNN architectures, and compare WGLE with state-of-the-art GNN watermarking and fingerprinting methods. WGLE achieves 100% ownership verification accuracy, with an average fidelity degradation of only 1.41%. Additionally, WGLE exhibits robust resilience against potential attacks. The code is available in the repository.","authors":["Tingzhi Li","Xuefeng Liu","Jing Lei","Xingang Zhang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.18871v3","updated":"2025-12-24T02:04:42Z","published":"2025-12-21T20:11:07Z","title":"Psychometric Validation of the Sophotechnic Mediation Scale and a New Understanding of the Development of GenAI Mastery: Lessons from 3,932 Adult Brazilian Workers","summary":"The rapid diffusion of generative artificial intelligence (GenAI) systems has introduced new forms of human-technology interaction, raising the question of whether sustained engagement gives rise to stable, internalized modes of cognition rather than merely transient efficiency gains. Grounded in the Cognitive Mediation Networks Theory, this study investigates Sophotechnic Mediation, a mode of thinking and acting associated with prolonged interaction with GenAI, and presents a comprehensive psychometric validation of the Sophotechnic Mediation Scale. Data were collected between 2023 and 2025 from independent cross-sectional samples totaling 3,932 adult workers from public and private organizations in the Metropolitan Region of Pernambuco, Brazil. Results indicate excellent internal consistency, a robust unidimensional structure, and measurement invariance across cohorts. Ordinal-robust confirmatory factor analyses and residual diagnostics show that elevated absolute fit indices reflect minor local dependencies rather than incorrect dimensionality. Distributional analyses reveal a time-evolving pattern characterized by a declining mass of non-adopters and convergence toward approximate Gaussianity among adopters, with model comparisons favoring a two-process hurdle model over a censored Gaussian specification. Sophotechnic Mediation is empirically distinct from Hypercultural mediation and is primarily driven by cumulative GenAI experience, with age moderating the rate of initial acquisition and the depth of later integration. Together, the findings support Sophotechnia as a coherent, measurable, and emergent mode of cognitive mediation associated with the ongoing GenAI revolution.","authors":["Bruno Campello de Souza"],"pdf_url":"","comment":"35 pages, 28 Manuscript, Portuguese and English Versions of the Instrument in Annex"},{"id":"http://arxiv.org/abs/2512.20884v1","updated":"2025-12-24T02:02:25Z","published":"2025-12-24T02:02:25Z","title":"The Silent Scholar Problem: A Probabilistic Framework for Breaking Epistemic Asymmetry in LLM Agents","summary":"Autonomous agents powered by LLMs and Retrieval-Augmented Generation (RAG) are proficient consumers of digital content but remain unidirectional, a limitation we term epistemic asymmetry. This isolation leads to redundant reasoning and stagnates collective intelligence. Current self-reflection frameworks remain largely heuristic and private, lacking a probabilistic foundation to quantify certainty or justify external interaction.To bridge this gap, we propose a formal probabilistic framework that provides agents with a non-altruistic motive for bidirectional knowledge exchange. We model an agent's belief in a proposition using a Beta-Bernoulli distribution with a forgetting factor ($γ$). This allows us to isolate epistemic uncertainty as the variance of belief, establishing a dual drive for interaction: A homeostatic motive: The need to maintain certainty against the temporal decay introduced by $γ$. An optimal learning strategy: Targeting points of maximum ambiguity ($\\mathbb{E}[θ]=0.5$) to maximize information gain. Under this framework, public contribution is reframed as optimal active learning: sharing solutions to elicit feedback is the most efficient method for an agent to reduce its own uncertainty. To ensure scalability, we introduce epistemic caching, which leverages the forgetting factor to dynamically prioritize resources for the active head of non-stationary knowledge distributions. Finally, we demonstrate how these accumulated belief states serve as verifiable reward signals for Reinforcement Learning from Human Feedback (RLHF) and high-quality data filters for Supervised Fine-Tuning (SFT). Simulation results validate that this uncertainty-driven strategy significantly outperforms random baselines in heterogeneous (Zipfian) environments, maintaining high adaptability to concept drift.","authors":["Zan-Kai Chong","Hiroyuki Ohsaki","Bryan Ng"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2507.01041v3","updated":"2025-12-24T01:45:18Z","published":"2025-06-23T07:14:04Z","title":"Fast AI Model Splitting over Edge Networks","summary":"Split learning (SL) has emerged as a computationally efficient approach for artificial intelligence (AI) model training, which can alleviate device-side computational workloads. However, complex AI model architectures pose high computational complexity to obtain the optimal model splitting. In this paper, we represent an arbitrary AI model as a directed acyclic graph (DAG), and then reformulate the optimal model splitting problem as a minimum s-t cut search problem. To solve the problem, we propose a fast DAG-based model splitting algorithm, which restructures the DAG to enable the optimal model splitting identification via a maximum flow method. Theoretical analysis indicates that the proposed algorithm is optimal. Furthermore, considering AI models with block structures, we propose a block-wise model splitting algorithm to reduce computational complexity. The algorithm abstracts each block, i.e., a component consisting of multiple layers, into a single vertex, thereby obtaining the optimal model splitting via a simplified DAG. Extensive experimental results demonstrate that the proposed algorithms can determine the optimal model splitting within milliseconds, as well as reduce training delay by 24.62%-38.95% in dynamic edge networks as compared to the state-of-the-art benchmarks.","authors":["Zuguang Li","Wen Wu","Shaohua Wu","Songge Zhang","Ye Wang"," Xuemin"," Shen"],"pdf_url":"","comment":"This version lacks sufficient detail in key technical parts, including the equivalence proof for the s-t cut transformation and the computational complexity analysis (Sections VI-D). We are withdrawing it to prepare a revised, more complete manuscript"},{"id":"http://arxiv.org/abs/2512.20136v2","updated":"2025-12-24T01:35:59Z","published":"2025-12-23T07:54:03Z","title":"M$^3$KG-RAG: Multi-hop Multimodal Knowledge Graph-enhanced Retrieval-Augmented Generation","summary":"Retrieval-Augmented Generation (RAG) has recently been extended to multimodal settings, connecting multimodal large language models (MLLMs) with vast corpora of external knowledge such as multimodal knowledge graphs (MMKGs). Despite their recent success, multimodal RAG in the audio-visual domain remains challenging due to 1) limited modality coverage and multi-hop connectivity of existing MMKGs, and 2) retrieval based solely on similarity in a shared multimodal embedding space, which fails to filter out off-topic or redundant knowledge. To address these limitations, we propose M$^3$KG-RAG, a Multi-hop Multimodal Knowledge Graph-enhanced RAG that retrieves query-aligned audio-visual knowledge from MMKGs, improving reasoning depth and answer faithfulness in MLLMs. Specifically, we devise a lightweight multi-agent pipeline to construct multi-hop MMKG (M$^3$KG), which contains context-enriched triplets of multimodal entities, enabling modality-wise retrieval based on input queries. Furthermore, we introduce GRASP (Grounded Retrieval And Selective Pruning), which ensures precise entity grounding to the query, evaluates answer-supporting relevance, and prunes redundant context to retain only knowledge essential for response generation. Extensive experiments across diverse multimodal benchmarks demonstrate that M$^3$KG-RAG significantly enhances MLLMs' multimodal reasoning and grounding over existing approaches.","authors":["Hyeongcheol Park","Jiyoung Seo","Jaewon Mun","Hogun Park","Wonmin Byeon","Sung June Kim","Hyeonsoo Im","JeungSub Lee","Sangpil Kim"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20866v1","updated":"2025-12-24T00:50:27Z","published":"2025-12-24T00:50:27Z","title":"Lightweight framework for underground pipeline recognition and spatial localization based on multi-view 2D GPR images","summary":"To address the issues of weak correlation between multi-view features, low recognition accuracy of small-scale targets, and insufficient robustness in complex scenarios in underground pipeline detection using 3D GPR, this paper proposes a 3D pipeline intelligent detection framework. First, based on a B/C/D-Scan three-view joint analysis strategy, a three-dimensional pipeline three-view feature evaluation method is established by cross-validating forward simulation results obtained using FDTD methods with actual measurement data. Second, the DCO-YOLO framework is proposed, which integrates DySample, CGLU, and OutlookAttention cross-dimensional correlation mechanisms into the original YOLOv11 algorithm, significantly improving the small-scale pipeline edge feature extraction capability. Furthermore, a 3D-DIoU spatial feature matching algorithm is proposed, which integrates three-dimensional geometric constraints and center distance penalty terms to achieve automated association of multi-view annotations. The three-view fusion strategy resolves inherent ambiguities in single-view detection. Experiments based on real urban underground pipeline data show that the proposed method achieves accuracy, recall, and mean average precision of 96.2%, 93.3%, and 96.7%, respectively, in complex multi-pipeline scenarios, which are 2.0%, 2.1%, and 0.9% higher than the baseline model. Ablation experiments validated the synergistic optimization effect of the dynamic feature enhancement module and Grad-CAM++ heatmap visualization demonstrated that the improved model significantly enhanced its ability to focus on pipeline geometric features. This study integrates deep learning optimization strategies with the physical characteristics of 3D GPR, offering an efficient and reliable novel technical framework for the intelligent recognition and localization of underground pipelines.","authors":["Haotian Lv","Chao Li","Jiangbo Dai","Yuhui Zhang","Zepeng Fan","Yiqiu Tan","Dawei Wang","Binglei Xie"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20861v1","updated":"2025-12-24T00:41:13Z","published":"2025-12-24T00:41:13Z","title":"Memory-Efficient Acceleration of Block Low-Rank Foundation Models on Resource Constrained GPUs","summary":"Recent advances in transformer-based foundation models have made them the default choice for many tasks, but their rapidly growing size makes fitting a full model on a single GPU increasingly difficult and their computational cost prohibitive. Block low-rank (BLR) compression techniques address this challenge by learning compact representations of weight matrices. While traditional low-rank (LR) methods often incur sharp accuracy drops, BLR approaches such as Monarch and BLAST can better capture the underlying structure, thus preserving accuracy while reducing computations and memory footprints. In this work, we use roofline analysis to show that, although BLR methods achieve theoretical savings and practical speedups for single-token inference, multi-token inference often becomes memory-bound in practice, increasing latency despite compiler-level optimizations in PyTorch. To address this, we introduce custom Triton kernels with partial fusion and memory layout optimizations for both Monarch and BLAST. On memory-constrained NVIDIA GPUs such as Jetson Orin Nano and A40, our kernels deliver up to $3.76\\times$ speedups and $3\\times$ model size compression over PyTorch dense baselines using CUDA backend and compiler-level optimizations, while supporting various models including Llama-7/1B, GPT2-S, DiT-XL/2, and ViT-B. Our code is available at https://github.com/pabillam/mem-efficient-blr .","authors":["Pierre Abillama","Changwoo Lee","Juechu Dong","David Blaauw","Dennis Sylvester","Hun-Seok Kim"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20856v1","updated":"2025-12-24T00:24:05Z","published":"2025-12-24T00:24:05Z","title":"NVIDIA Nemotron 3: Efficient and Open Intelligence","summary":"We introduce the Nemotron 3 family of models - Nano, Super, and Ultra. These models deliver strong agentic, reasoning, and conversational capabilities. The Nemotron 3 family uses a Mixture-of-Experts hybrid Mamba-Transformer architecture to provide best-in-class throughput and context lengths of up to 1M tokens. Super and Ultra models are trained with NVFP4 and incorporate LatentMoE, a novel approach that improves model quality. The two larger models also include MTP layers for faster text generation. All Nemotron 3 models are post-trained using multi-environment reinforcement learning enabling reasoning, multi-step tool use, and support granular reasoning budget control. Nano, the smallest model, outperforms comparable models in accuracy while remaining extremely cost-efficient for inference. Super is optimized for collaborative agents and high-volume workloads such as IT ticket automation. Ultra, the largest model, provides state-of-the-art accuracy and reasoning performance. Nano is released together with its technical report and this white paper, while Super and Ultra will follow in the coming months. We will openly release the model weights, pre- and post-training software, recipes, and all data for which we hold redistribution rights.","authors":[" NVIDIA"," :","Aaron Blakeman","Aaron Grattafiori","Aarti Basant","Abhibha Gupta","Abhinav Khattar","Adi Renduchintala","Aditya Vavre","Akanksha Shukla","Akhiad Bercovich","Aleksander Ficek","Aleksandr Shaposhnikov","Alex Kondratenko","Alexander Bukharin","Alexandre Milesi","Ali Taghibakhshi","Alisa Liu","Amelia Barton","Ameya Sunil Mahabaleshwarkar","Amir Klein","Amit Zuker","Amnon Geifman","Amy Shen","Anahita Bhiwandiwalla","Andrew Tao","Anjulie Agrusa","Ankur Verma","Ann Guan","Anubhav Mandarwal","Arham Mehta","Ashwath Aithal","Ashwin Poojary","Asif Ahamed","Asit Mishra","Asma Kuriparambil Thekkumpate","Ayush Dattagupta","Banghua Zhu","Bardiya Sadeghi","Barnaby Simkin","Ben Lanir","Benedikt Schifferer","Besmira Nushi","Bilal Kartal","Bita Darvish Rouhani","Boris Ginsburg","Brandon Norick","Brandon Soubasis","Branislav Kisacanin","Brian Yu","Bryan Catanzaro","Carlo del Mundo","Chantal Hwang","Charles Wang","Cheng-Ping Hsieh","Chenghao Zhang","Chenhan Yu","Chetan Mungekar","Chintan Patel","Chris Alexiuk","Christopher Parisien","Collin Neale","Cyril Meurillon","Damon Mosk-Aoyama","Dan Su","Dane Corneil","Daniel Afrimi","Daniel Lo","Daniel Rohrer","Daniel Serebrenik","Daria Gitman","Daria Levy","Darko Stosic","David Mosallanezhad","Deepak Narayanan","Dhruv Nathawani","Dima Rekesh","Dina Yared","Divyanshu Kakwani","Dong Ahn","Duncan Riach","Dusan Stosic","Edgar Minasyan","Edward Lin","Eileen Long","Eileen Peters Long","Elad Segal","Elena Lantz","Ellie Evans","Elliott Ning","Eric Chung","Eric Harper","Eric Tramel","Erick Galinkin","Erik Pounds","Evan Briones","Evelina Bakhturina","Evgeny Tsykunov","Faisal Ladhak","Fay Wang","Fei Jia","Felipe Soares","Feng Chen","Ferenc Galko","Frank Sun","Frankie Siino","Gal Hubara Agam","Ganesh Ajjanagadde","Gantavya Bhatt","Gargi Prasad","George Armstrong","Gerald Shen","Gorkem Batmaz","Grigor Nalbandyan","Haifeng Qian","Harsh Sharma","Hayley Ross","Helen Ngo","Herbert Hum","Herman Sahota","Hexin Wang","Himanshu Soni","Hiren Upadhyay","Huizi Mao","Huy C Nguyen","Huy Q Nguyen","Iain Cunningham","Ido Galil","Ido Shahaf","Igor Gitman","Ilya Loshchilov","Itamar Schen","Itay Levy","Ivan Moshkov","Izik Golan","Izzy Putterman","Jan Kautz","Jane Polak Scowcroft","Jared Casper","Jatin Mitra","Jeffrey Glick","Jenny Chen","Jesse Oliver","Jian Zhang","Jiaqi Zeng","Jie Lou","Jimmy Zhang","Jinhang Choi","Jining Huang","Joey Conway","Joey Guman","John Kamalu","Johnny Greco","Jonathan Cohen","Joseph Jennings","Joyjit Daw","Julien Veron Vialard","Junkeun Yi","Jupinder Parmar","Kai Xu","Kan Zhu","Kari Briski","Katherine Cheung","Katherine Luna","Keith Wyss","Keshav Santhanam","Kevin Shih","Kezhi Kong","Khushi Bhardwaj","Kirthi Shankar","Krishna C. Puvvada","Krzysztof Pawelec","Kumar Anik","Lawrence McAfee","Laya Sleiman","Leon Derczynski","Li Ding","Lizzie Wei","Lucas Liebenwein","Luis Vega","Maanu Grover","Maarten Van Segbroeck","Maer Rodrigues de Melo","Mahdi Nazemi","Makesh Narsimhan Sreedhar","Manoj Kilaru","Maor Ashkenazi","Marc Romeijn","Marcin Chochowski","Mark Cai","Markus Kliegl","Maryam Moosaei","Matt Kulka","Matvei Novikov","Mehrzad Samadi","Melissa Corpuz","Mengru Wang","Meredith Price","Michael Andersch","Michael Boone","Michael Evans","Miguel Martinez","Mikail Khona","Mike Chrzanowski","Minseok Lee","Mohammad Dabbah","Mohammad Shoeybi","Mostofa Patwary","Nabin Mulepati","Najeeb Nabwani","Natalie Hereth","Nave Assaf","Negar Habibi","Neta Zmora","Netanel Haber","Nicola Sessions","Nidhi Bhatia","Nikhil Jukar","Nikki Pope","Nikolai Ludwig","Nima Tajbakhsh","Nir Ailon","Nirmal Juluru","Nishant Sharma","Oleksii Hrinchuk","Oleksii Kuchaiev","Olivier Delalleau","Oluwatobi Olabiyi","Omer Ullman Argov","Omri Puny","Oren Tropp","Ouye Xie","Parth Chadha","Pasha Shamis","Paul Gibbons","Pavlo Molchanov","Pawel Morkisz","Peter Dykas","Peter Jin","Pinky Xu","Piotr Januszewski","Pranav Prashant Thombre","Prasoon Varshney","Pritam Gundecha","Przemek Tredak","Qing Miao","Qiyu Wan","Rabeeh Karimi Mahabadi","Rachit Garg","Ran El-Yaniv","Ran Zilberstein","Rasoul Shafipour","Rich Harang","Rick Izzo","Rima Shahbazyan","Rishabh Garg","Ritika Borkar","Ritu Gala","Riyad Islam","Robert Hesse","Roger Waleffe","Rohit Watve","Roi Koren","Ruoxi Zhang","Russell Hewett","Russell J. Hewett","Ryan Prenger","Ryan Timbrook","Sadegh Mahdavi","Sahil Modi","Samuel Kriman","Sangkug Lim","Sanjay Kariyappa","Sanjeev Satheesh","Saori Kaji","Satish Pasumarthi","Saurav Muralidharan","Sean Narentharen","Sean Narenthiran","Seonmyeong Bak","Sergey Kashirsky","Seth Poulos","Shahar Mor","Shanmugam Ramasamy","Shantanu Acharya","Shaona Ghosh","Sharath Turuvekere Sreenivas","Shelby Thomas","Shiqing Fan","Shreya Gopal","Shrimai Prabhumoye","Shubham Pachori","Shubham Toshniwal","Shuoyang Ding","Siddharth Singh","Simeng Sun","Smita Ithape","Somshubra Majumdar","Soumye Singhal","Stas Sergienko","Stefania Alborghetti","Stephen Ge","Sugam Dipak Devare","Sumeet Kumar Barua","Suseella Panguluri","Suyog Gupta","Sweta Priyadarshi","Syeda Nahida Akter","Tan Bui","Teodor-Dumitru Ene","Terry Kong","Thanh Do","Tijmen Blankevoort","Tim Moon","Tom Balough","Tomer Asida","Tomer Bar Natan","Tomer Ronen","Tugrul Konuk","Twinkle Vashishth","Udi Karpas","Ushnish De","Vahid Noorozi","Vahid Noroozi","Venkat Srinivasan","Venmugil Elango","Victor Cui","Vijay Korthikanti","Vinay Rao","Vitaly Kurin","Vitaly Lavrukhin","Vladimir Anisimov","Wanli Jiang","Wasi Uddin Ahmad","Wei Du","Wei Ping","Wenfei Zhou","Will Jennings","William Zhang","Wojciech Prazuch","Xiaowei Ren","Yashaswi Karnati","Yejin Choi","Yev Meyer","Yi-Fu Wu","Yian Zhang","Yigong Qin","Ying Lin","Yonatan Geifman","Yonggan Fu","Yoshi Subara","Yoshi Suhara","Yubo Gao","Zach Moshe","Zhen Dong","Zhongbo Zhu","Zihan Liu","Zijia Chen","Zijie Yan"],"pdf_url":"","comment":null}],"Computation and Language":[{"id":"http://arxiv.org/abs/2512.21336v1","updated":"2025-12-24T18:59:51Z","published":"2025-12-24T18:59:51Z","title":"Optimizing Decoding Paths in Masked Diffusion Models by Quantifying Uncertainty","summary":"Masked Diffusion Models (MDMs) offer flexible, non-autoregressive generation, but this freedom introduces a challenge: final output quality is highly sensitive to the decoding order. We are the first to formalize this issue, attributing the variability in output quality to the cumulative predictive uncertainty along a generative path. To quantify this uncertainty, we introduce Denoising Entropy, a computable metric that serves as an internal signal for evaluating generative process. Leveraging this metric, we propose two algorithms designed to optimize the decoding path: a post-hoc selection method and a real-time guidance strategy. Experiments demonstrate that our entropy-guided methods significantly improve generation quality, consistently boosting accuracy on challenging reasoning, planning, and code benchmarks. Our work establishes Denoising Entropy as a principled tool for understanding and controlling generation, effectively turning the uncertainty in MDMs from a liability into a key advantage for discovering high-quality solutions.","authors":["Ziyu Chen","Xinbei Jiang","Peng Sun","Tao Lin"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.21332v1","updated":"2025-12-24T18:59:01Z","published":"2025-12-24T18:59:01Z","title":"C2LLM Technical Report: A New Frontier in Code Retrieval via Adaptive Cross-Attention Pooling","summary":"We present C2LLM - Contrastive Code Large Language Models, a family of code embedding models in both 0.5B and 7B sizes. Building upon Qwen-2.5-Coder backbones, C2LLM adopts a Pooling by Multihead Attention (PMA) module for generating sequence embedding from token embeddings, effectively 1) utilizing the LLM's causal representations acquired during pretraining, while also 2) being able to aggregate information from all tokens in the sequence, breaking the information bottleneck in EOS-based sequence embeddings, and 3) supporting flexible adaptation of embedding dimension, serving as an alternative to MRL. Trained on three million publicly available data, C2LLM models set new records on MTEB-Code among models of similar sizes, with C2LLM-7B ranking 1st on the overall leaderboard.","authors":["Jin Qin","Zihan Liao","Ziyin Zhang","Hang Yu","Peng Di","Rui Wang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.21329v1","updated":"2025-12-24T18:58:04Z","published":"2025-12-24T18:58:04Z","title":"Your Reasoning Benchmark May Not Test Reasoning: Revealing Perception Bottleneck in Abstract Reasoning Benchmarks","summary":"Reasoning benchmarks such as the Abstraction and Reasoning Corpus (ARC) and ARC-AGI are widely used to assess progress in artificial intelligence and are often interpreted as probes of core, so-called ``fluid'' reasoning abilities. Despite their apparent simplicity for humans, these tasks remain challenging for frontier vision-language models (VLMs), a gap commonly attributed to deficiencies in machine reasoning. We challenge this interpretation and hypothesize that the gap arises primarily from limitations in visual perception rather than from shortcomings in inductive reasoning.\n  To verify this hypothesis, we introduce a two-stage experimental pipeline that explicitly separates perception and reasoning. In the perception stage, each image is independently converted into a natural-language description, while in the reasoning stage a model induces and applies rules using these descriptions. This design prevents leakage of cross-image inductive signals and isolates reasoning from perception bottlenecks. Across three ARC-style datasets, Mini-ARC, ACRE, and Bongard-LOGO, we show that the perception capability is the dominant factor underlying the observed performance gap by comparing the two-stage pipeline with against standard end-to-end one-stage evaluation. Manual inspection of reasoning traces in the VLM outputs further reveals that approximately 80 percent of model failures stem from perception errors. Together, these results demonstrate that ARC-style benchmarks conflate perceptual and reasoning challenges and that observed performance gaps may overstate deficiencies in machine reasoning. Our findings underscore the need for evaluation protocols that disentangle perception from reasoning when assessing progress in machine intelligence.","authors":["Xinhe Wang","Jin Huang","Xingjian Zhang","Tianhao Wang","Jiaqi W. Ma"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.21326v1","updated":"2025-12-24T18:54:37Z","published":"2025-12-24T18:54:37Z","title":"Measuring all the noises of LLM Evals","summary":"Separating signal from noise is central to experimental science. Applying well-established statistical method effectively to LLM evals requires consideration of their unique noise characteristics. We clearly define and measure three types of noise: prediction noise from generating different answers on a given question, data noise from sampling questions, and their combined total noise following the law of total variance. To emphasize relative comparisons and gain statistical power, we propose the all-pairs paired method, which applies the paired analysis to all pairs of LLMs and measures all the noise components based on millions of question-level predictions across many evals and settings. These measurements revealed clear patterns. First, each eval exhibits a characteristic and highly predictable total noise level across all model pairs. Second, paired prediction noise typically exceeds paired data noise, which means reducing prediction noise by averaging can significantly increase statistical power. These findings enable practitioners to assess significance without custom testing and to detect much smaller effects in controlled experiments.","authors":["Sida Wang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.21323v1","updated":"2025-12-24T18:46:55Z","published":"2025-12-24T18:46:55Z","title":"Parallel Token Prediction for Language Models","summary":"We propose Parallel Token Prediction (PTP), a universal framework for parallel sequence generation in language models. PTP jointly predicts multiple dependent tokens in a single transformer call by incorporating the sampling procedure into the model. This reduces the latency bottleneck of autoregressive decoding, and avoids the restrictive independence assumptions common in existing multi-token prediction methods. We prove that PTP can represent arbitrary autoregressive sequence distributions. PTP is trained either by distilling an existing model or through inverse autoregressive training without a teacher. Experimentally, we achieve state-of-the-art speculative decoding performance on Vicuna-7B by accepting over four tokens per step on Spec-Bench. The universality of our framework indicates that parallel generation of long sequences is feasible without loss of modeling power.","authors":["Felix Draxler","Justus Will","Farrin Marouf Sofian","Theofanis Karaletsos","Sameer Singh","Stephan Mandt"],"pdf_url":"","comment":"Preprint. Under review"},{"id":"http://arxiv.org/abs/2512.18859v2","updated":"2025-12-24T18:41:46Z","published":"2025-12-21T19:16:40Z","title":"Toward Human-Centered AI-Assisted Terminology Work","summary":"The rapid diffusion of generative artificial intelligence is transforming terminology work. While this technology promises gains in efficiency, its unstructured adoption risks weakening professional autonomy, amplifying bias, and eroding linguistic and conceptual diversity. This paper argues that a human-centered approach to artificial intelligence has become a necessity for terminology work. Building on research in artificial intelligence and translation studies, it proposes a human-centered framework that conceptualizes artificial intelligence as a means of amplifying the terminologist's capabilities, rather than replacing them. The framework is organized around three interrelated dimensions: the augmented terminologist, ethical AI, and human-centered design. Together, these dimensions emphasize the compatibility of high automation with strong human control, the central role of terminologists in bias mitigation, and the importance of designing AI tools and workflows around the needs, values, and well-being of the terminologist. The paper concludes by stressing that current choices in AI adoption will shape not only terminological practice, but also the preservation of accuracy, adequacy, and diversity in terminology and specialized knowledge.","authors":["Antonio San Martin"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17083v2","updated":"2025-12-24T18:05:57Z","published":"2025-12-18T21:29:43Z","title":"When F1 Fails: Granularity-Aware Evaluation for Dialogue Topic Segmentation","summary":"Dialogue topic segmentation supports summarization, retrieval, memory management, and conversational continuity. Despite decades of work, evaluation practice remains dominated by strict boundary matching and F1-based metrics. Modern large language model (LLM) based conversational systems increasingly rely on segmentation to manage conversation history beyond fixed context windows. In such systems, unstructured context accumulation degrades efficiency and coherence.\n  This paper introduces an evaluation framework that reports boundary density and segment alignment diagnostics (purity and coverage) alongside window-tolerant F1 (W-F1). By separating boundary scoring from boundary selection, we evaluate segmentation quality across density regimes rather than at a single operating point. Cross-dataset evaluation shows that reported performance differences often reflect annotation granularity mismatch rather than boundary placement quality alone.\n  We evaluate structurally distinct segmentation strategies across eight dialogue datasets spanning task-oriented, open-domain, meeting-style, and synthetic interactions. Boundary-based metrics are strongly coupled to boundary density: threshold sweeps produce larger W-F1 changes than switching between methods. These findings support viewing topic segmentation as a granularity selection problem rather than prediction of a single correct boundary set. This motivates separating boundary scoring from boundary selection for analyzing and tuning segmentation under varying annotation granularities.","authors":["Michael H. Coen"],"pdf_url":"","comment":"32 pages, 4 figures. Evaluation and methodology study on dialogue topic segmentation"},{"id":"http://arxiv.org/abs/2512.21280v1","updated":"2025-12-24T16:59:04Z","published":"2025-12-24T16:59:04Z","title":"SMART SLM: Structured Memory and Reasoning Transformer, A Small Language Model for Accurate Document Assistance","summary":"The user of Engineering Manuals (EM) finds it difficult to read EM s because they are long, have a dense format which includes written documents, step by step procedures, and standard parameter lists for engineering equipment. Off the shelf transformers, especially compact ones, treat this material as a flat stream of tokens. This approach leads to confident but incorrect numeric answers and forces the models to memorize separate facts inefficiently. SMART (Structured Memory and Reasoning Transformer) offers a different and practical solution to the above problem. SMART structures its processing by using a hierarchical approach, and is based upon three main job categories (1) A syntax-aware Fact Extractor (Grammarian) Tree LSTM which extracts facts as subject relation object relations from EM sentences (2) A compact indexed memory MANN (Memory Augmented Neural Network) that indexes these Rational Subject Relation Objects as 384 dimensional vectors that are associated with the source of the information, and (3) A 6 layer Transformer that learns to fuse the previously retrieved facts into its generated response. The entire SMART model utilizes 45.51M parameters, which is 64% less than GPT-2 (124M) and 69% less than BERT (133M), and it achieves a 21.3% higher accuracy than GPT-2, indicating that SMART fits the data better with the least amount of processing requirements. SMART employs dual modes of inference an indexed fast path for known documents (sub-second answer times) and an indexed dynamic path assisted by RAGs for new uploads (FAISS Top 20 results with memory severed at 64 slots). In real world deployment, this framework leads to more well supported results with reduced hallucinations than comparable small transformer models.","authors":["Divij Dudeja","Mayukha Pal"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.21257v1","updated":"2025-12-24T16:06:20Z","published":"2025-12-24T16:06:20Z","title":"ReaSeq: Unleashing World Knowledge via Reasoning for Sequential Modeling","summary":"Industrial recommender systems face two fundamental limitations under the log-driven paradigm: (1) knowledge poverty in ID-based item representations that causes brittle interest modeling under data sparsity, and (2) systemic blindness to beyond-log user interests that constrains model performance within platform boundaries. These limitations stem from an over-reliance on shallow interaction statistics and close-looped feedback while neglecting the rich world knowledge about product semantics and cross-domain behavioral patterns that Large Language Models have learned from vast corpora.\n  To address these challenges, we introduce ReaSeq, a reasoning-enhanced framework that leverages world knowledge in Large Language Models to address both limitations through explicit and implicit reasoning. Specifically, ReaSeq employs explicit Chain-of-Thought reasoning via multi-agent collaboration to distill structured product knowledge into semantically enriched item representations, and latent reasoning via Diffusion Large Language Models to infer plausible beyond-log behaviors. Deployed on Taobao's ranking system serving hundreds of millions of users, ReaSeq achieves substantial gains: >6.0% in IPV and CTR, >2.9% in Orders, and >2.5% in GMV, validating the effectiveness of world-knowledge-enhanced reasoning over purely log-driven approaches.","authors":["Chuan Wang","Gaoming Yang","Han Wu","Jiakai Tang","Jiahao Yu","Jian Wu","Jianwu Hu","Junjun Zheng","Shuwen Xiao","Yeqiu Yang","Yuning Jiang","Ahjol Nurlanbek","Binbin Cao","Bo Zheng","Fangmei Zhu","Gaoming Zhou","Huimin Yi","Huiping Chu","Jin Huang","Jinzhe Shan","Kenan Cui","Longbin Li","Silu Zhou","Wen Chen","Xia Ming","Xiang Gao","Xin Yao","Xingyu Wen","Yan Zhang","Yiwen Hu","Yulin Wang","Ziheng Bao","Zongyuan Wu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20491v2","updated":"2025-12-24T15:52:31Z","published":"2025-12-23T16:32:27Z","title":"Step-DeepResearch Technical Report","summary":"As LLMs shift toward autonomous agents, Deep Research has emerged as a pivotal metric. However, existing academic benchmarks like BrowseComp often fail to meet real-world demands for open-ended research, which requires robust skills in intent recognition, long-horizon decision-making, and cross-source verification. To address this, we introduce Step-DeepResearch, a cost-effective, end-to-end agent. We propose a Data Synthesis Strategy Based on Atomic Capabilities to reinforce planning and report writing, combined with a progressive training path from agentic mid-training to SFT and RL. Enhanced by a Checklist-style Judger, this approach significantly improves robustness. Furthermore, to bridge the evaluation gap in the Chinese domain, we establish ADR-Bench for realistic deep research scenarios. Experimental results show that Step-DeepResearch (32B) scores 61.4% on Scale AI Research Rubrics. On ADR-Bench, it significantly outperforms comparable models and rivals SOTA closed-source models like OpenAI and Gemini DeepResearch. These findings prove that refined training enables medium-sized models to achieve expert-level capabilities at industry-leading cost-efficiency.","authors":["Chen Hu","Haikuo Du","Heng Wang","Lin Lin","Mingrui Chen","Peng Liu","Ruihang Miao","Tianchi Yue","Wang You","Wei Ji","Wei Yuan","Wenjin Deng","Xiaojian Yuan","Xiaoyun Zhang","Xiangyu Liu","Xikai Liu","Yanming Xu","Yicheng Cao","Yifei Zhang","Yongyao Wang","Yubo Shu","Yurong Zhang","Yuxiang Zhang","Zheng Gong","Zhichao Chang","Binyan Li","Dan Ma","Furong Jia","Hongyuan Wang","Jiayu Liu","Jing Bai","Junlan Liu","Manjiao Liu","Na Wang","Qiuping Wu","Qinxin Du","Shiwei Li","Wen Sun","Yifeng Gong","Yonglin Chen","Yuling Zhao","Yuxuan Lin","Ziqi Ren","Zixuan Wang","Aihu Zhang","Brian Li","Buyun Ma","Kang An","Li Xie","Mingliang Li","Pan Li","Shidong Yang","Xi Chen","Xiaojia Liu","Yuchu Luo","Yuan Song","YuanHao Ding","Yuanwei Liang","Zexi Li","Zhaoning Zhang","Zixin Zhang","Binxing Jiao","Daxin Jiang","Jiansheng Chen","Jing Li","Xiangyu Zhang","Yibo Zhu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2505.00675v3","updated":"2025-12-24T15:24:04Z","published":"2025-05-01T17:31:33Z","title":"Rethinking Memory in LLM based Agents: Representations, Operations, and Emerging Topics","summary":"Memory is fundamental to large language model (LLM)-based agents, but existing surveys emphasize application-level use (e.g., personalized dialogue), while overlooking the atomic operations governing memory dynamics. This work categorizes memory into parametric (implicit in model weights) and contextual (explicit external data, structured/unstructured) forms, and defines six core operations: Consolidation, Updating, Indexing, Forgetting, Retrieval, and Condensation. Mapping these dimensions reveals four key research topics: long-term, long-context, parametric modification, and multi-source memory. The taxonomy provides a structured view of memory-related research, benchmarks, and tools, clarifying functional interactions in LLM-based agents and guiding future advancements. The datasets, papers, and tools are publicly available at https://github.com/Elvin-Yiming-Du/Survey_Memory_in_AI.","authors":["Yiming Du","Wenyu Huang","Danna Zheng","Zhaowei Wang","Sebastien Montella","Mirella Lapata","Kam-Fai Wong","Jeff Z. Pan"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2508.21010v2","updated":"2025-12-24T14:52:45Z","published":"2025-08-28T17:10:53Z","title":"ChainReaction: Causal Chain-Guided Reasoning for Modular and Explainable Causal-Why Video Question Answering","summary":"Existing Causal-Why Video Question Answering (VideoQA) models often struggle with higher-order reasoning, relying on opaque, monolithic pipelines that entangle video understanding, causal inference, and answer generation. These black-box approaches offer limited interpretability and tend to depend on shallow heuristics. We propose a novel, modular paradigm that explicitly decouples causal reasoning from answer generation, introducing natural language causal chains as interpretable intermediate representations. Inspired by human cognitive models, these structured cause-effect sequences bridge low-level video content with high-level causal reasoning, enabling transparent and logically coherent inference. Our two-stage architecture comprises a Causal Chain Extractor (CCE) that generates causal chains from video-question pairs, and a Causal Chain-Driven Answerer (CCDA) that derives answers grounded in these chains. To address the lack of annotated reasoning traces, we introduce a scalable method for generating accurate causal chains from existing datasets. We construct human verified causal chains for 46K samples. We also propose CauCo, a new evaluation metric for causality-oriented captioning. Experiments on three large-scale benchmarks demonstrate that our approach not only outperforms state-of-the-art models, but also yields substantial gains in explainability, user trust, and generalization -- positioning the CCE as a reusable causal reasoning engine across diverse domains. Project page: https://paritoshparmar.github.io/chainreaction/","authors":["Paritosh Parmar","Eric Peh","Basura Fernando"],"pdf_url":"","comment":"Project page: https://paritoshparmar.github.io/chainreaction/"},{"id":"http://arxiv.org/abs/2512.16378v2","updated":"2025-12-24T14:39:27Z","published":"2025-12-18T10:21:14Z","title":"Hearing to Translate: The Effectiveness of Speech Modality Integration into LLMs","summary":"As Large Language Models (LLMs) expand beyond text, integrating speech as a native modality has given rise to SpeechLLMs, which aim to translate spoken language directly, thereby bypassing traditional transcription-based pipelines. Whether this integration improves speech-to-text translation quality over established cascaded architectures, however, remains an open question. We present Hearing to Translate, the first comprehensive test suite rigorously benchmarking 5 state-of-the-art SpeechLLMs against 16 strong direct and cascade systems that couple leading speech foundation models (SFM), with multilingual LLMs. Our analysis spans 16 benchmarks, 13 language pairs, and 9 challenging conditions, including disfluent, noisy, and long-form speech. Across this extensive evaluation, we find that cascaded systems remain the most reliable overall, while current SpeechLLMs only match cascades in selected settings and SFMs lag behind both, highlighting that integrating an LLM, either within the model or in a pipeline, is essential for high-quality speech translation.","authors":["Sara Papi","Javier Garcia Gilabert","Zachary Hopton","Vilém Zouhar","Carlos Escolano","Gerard I. Gállego","Jorge Iranzo-Sánchez","Ahrii Kim","Dominik Macháček","Patricia Schmidtova","Maike Züfle"],"pdf_url":"","comment":"Project available at https://github.com/sarapapi/hearing2translate"},{"id":"http://arxiv.org/abs/2512.21204v1","updated":"2025-12-24T14:33:16Z","published":"2025-12-24T14:33:16Z","title":"SpidR-Adapt: A Universal Speech Representation Model for Few-Shot Adaptation","summary":"Human infants, with only a few hundred hours of speech exposure, acquire basic units of new languages, highlighting a striking efficiency gap compared to the data-hungry self-supervised speech models. To address this gap, this paper introduces SpidR-Adapt for rapid adaptation to new languages using minimal unlabeled data. We cast such low-resource speech representation learning as a meta-learning problem and construct a multi-task adaptive pre-training (MAdaPT) protocol which formulates the adaptation process as a bi-level optimization framework. To enable scalable meta-training under this framework, we propose a novel heuristic solution, first-order bi-level optimization (FOBLO), avoiding heavy computation costs. Finally, we stabilize meta-training by using a robust initialization through interleaved supervision which alternates self-supervised and supervised objectives. Empirically, SpidR-Adapt achieves rapid gains in phonemic discriminability (ABX) and spoken language modeling (sWUGGY, sBLIMP, tSC), improving over in-domain language models after training on less than 1h of target-language audio, over $100\\times$ more data-efficient than standard training. These findings highlight a practical, architecture-agnostic path toward biologically inspired, data-efficient representations. We open-source the training code and model checkpoints at https://github.com/facebookresearch/spidr-adapt.","authors":["Mahi Luthra","Jiayi Shen","Maxime Poli","Angelo Ortiz","Yosuke Higuchi","Youssef Benchekroun","Martin Gleize","Charles-Eric Saint-James","Dongyan Lin","Phillip Rust","Angel Villar","Surya Parimi","Vanessa Stark","Rashel Moritz","Juan Pino","Yann LeCun","Emmanuel Dupoux"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2510.11098v2","updated":"2025-12-24T13:27:20Z","published":"2025-10-13T07:45:52Z","title":"VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents","summary":"Recent advances in large audio language models (LALMs) have greatly enhanced multimodal conversational systems. However, existing benchmarks remain limited -- they are mainly English-centric, rely on synthetic speech, and lack comprehensive, discriminative evaluation across multiple dimensions. To address these gaps, we present Voice Chat Bot Bench (VCB Bench) -- a high-quality Chinese benchmark built entirely on real human speech. VCB Bench evaluates LALMs from three complementary perspectives: instruction following (including speech-level control beyond text commands), knowledge understanding (general knowledge, reasoning, and daily dialogue), and robustness (stability under perturbations in content, environment, and speaker traits). Experiments on representative LALMs reveal notable performance gaps and highlight future directions for improvement. VCB Bench provides a reproducible and fine-grained evaluation framework, offering standardized methodology and practical insights for advancing Chinese voice conversational models.","authors":["Jiliang Hu","Wenfu Wang","Zuchao Li","Chenxing Li","Yiyang Zhao","Hanzhao Li","Liqiang Zhang","Meng Yu","Dong Yu"],"pdf_url":"","comment":"20 pages, 5 figures"},{"id":"http://arxiv.org/abs/2503.12491v2","updated":"2025-12-24T13:21:40Z","published":"2025-03-16T12:49:44Z","title":"CAKE: Cascading and Adaptive KV Cache Eviction with Layer Preferences","summary":"Large language models (LLMs) excel at processing long sequences, boosting demand for key-value (KV) caching. While recent efforts to evict KV cache have alleviated the inference burden, they often fail to allocate resources rationally across layers with different attention patterns. In this paper, we introduce Cascading and Adaptive KV cache Eviction (CAKE), a novel approach that frames KV cache eviction as a \"cake-slicing problem.\" CAKE assesses layer-specific preferences by considering attention dynamics in both spatial and temporal dimensions, allocates rational cache size for layers accordingly, and manages memory constraints in a cascading manner. This approach enables a global view of cache allocation, adaptively distributing resources across diverse attention mechanisms while maintaining memory budgets. CAKE also employs a new eviction indicator that considers the shifting importance of tokens over time, addressing limitations in existing methods that overlook temporal dynamics. Comprehensive experiments on LongBench and NeedleBench show that CAKE maintains model performance with only 3.2% of the KV cache and consistently outperforms current baselines across various models and memory constraints, particularly in low-memory settings. Additionally, CAKE achieves over 10x speedup in decoding latency compared to full cache when processing contexts of 128K tokens with FlashAttention-2. Our code is available at https://github.com/antgroup/cakekv.","authors":["Ziran Qin","Yuchen Cao","Mingbao Lin","Wen Hu","Shixuan Fan","Ke Cheng","Weiyao Lin","Jianguo Li"],"pdf_url":"","comment":"Accepted by ICLR 2025"},{"id":"http://arxiv.org/abs/2512.20481v2","updated":"2025-12-24T12:51:11Z","published":"2025-12-23T16:16:42Z","title":"Coherence in the brain unfolds across separable temporal regimes","summary":"Coherence in language requires the brain to satisfy two competing temporal demands: gradual accumulation of meaning across extended context and rapid reconfiguration of representations at event boundaries. Despite their centrality to language and thought, how these processes are implemented in the human brain during naturalistic listening remains unclear. Here, we tested whether these two processes can be captured by annotation-free drift and shift signals and whether their neural expression dissociates across large-scale cortical systems. These signals were derived from a large language model (LLM) and formalized contextual drift and event shifts directly from the narrative input. To enable high-precision voxelwise encoding models with stable parameter estimates, we densely sampled one healthy adult across more than 7 hours of listening to thirteen crime stories while collecting ultra high-field (7T) BOLD data. We then modeled the feature-informed hemodynamic response using a regularized encoding framework validated on independent stories. Drift predictions were prevalent in default-mode network hubs, whereas shift predictions were evident bilaterally in the primary auditory cortex and language association cortex. Furthermore, activity in default-mode and parietal networks was best explained by a signal capturing how meaning accumulates and gradually fades over the course of the narrative. Together, these findings show that coherence during language comprehension is implemented through dissociable neural regimes of slow contextual integration and rapid event-driven reconfiguration, offering a mechanistic entry point for understanding disturbances of language coherence in psychiatric disorders.","authors":["Davide Stauba","Finn Rabe","Akhil Misra","Yves Pauli","Roya Hüppi","Ni Yang","Nils Lang","Lars Michels","Victoria Edkins","Sascha Frühholz","Iris Sommer","Wolfram Hinzen","Philipp Homan"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.21120v1","updated":"2025-12-24T11:39:00Z","published":"2025-12-24T11:39:00Z","title":"ClarifyMT-Bench: Benchmarking and Improving Multi-Turn Clarification for Conversational Large Language Models","summary":"Large language models (LLMs) are increasingly deployed as conversational assistants in open-domain, multi-turn settings, where users often provide incomplete or ambiguous information. However, existing LLM-focused clarification benchmarks primarily assume single-turn interactions or cooperative users, limiting their ability to evaluate clarification behavior in realistic settings. We introduce \\textbf{ClarifyMT-Bench}, a benchmark for multi-turn clarification grounded in a five-dimensional ambiguity taxonomy and a set of six behaviorally diverse simulated user personas. Through a hybrid LLM-human pipeline, we construct 6,120 multi-turn dialogues capturing diverse ambiguity sources and interaction patterns. Evaluating ten representative LLMs uncovers a consistent under-clarification bias: LLMs tend to answer prematurely, and performance degrades as dialogue depth increases. To mitigate this, we propose \\textbf{ClarifyAgent}, an agentic approach that decomposes clarification into perception, forecasting, tracking, and planning, substantially improving robustness across ambiguity conditions. ClarifyMT-Bench establishes a reproducible foundation for studying when LLMs should ask, when they should answer, and how to navigate ambiguity in real-world human-LLM interactions.","authors":["Sichun Luo","Yi Huang","Mukai Li","Shichang Meng","Fengyuan Liu","Zefa Hu","Junlan Feng","Qi Liu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2409.00162v2","updated":"2025-12-24T11:32:24Z","published":"2024-08-30T16:14:35Z","title":"Sequence to Sequence Reward Modeling: Improving RLHF by Language Feedback","summary":"Aligning the behavior of Large language models (LLMs) with human intentions and values remains a critical challenge. Reinforcement learning from human feedback (RLHF) aligns LLMs by training a reward model (RM) on human preferences and fine-tuning the LLMs to maximize RM feedback. Despite its effectiveness and popularity, RLHF is prone to biased local optimization. It means RM fails to provide feedback that accurately aligns with human preference, causing LLMs to explore unexpected generalizations, and failing to achieve alignment objectives. To mitigate this issue, we propose a novel \\textit{sequence-to-sequence (seq2seq) reward modeling} method. Its key insight is that learning from language feedback rather than scalar feedback improves RLHF without additional annotations. We replaced the reward modeling target from binary maximum likelihood estimation (MLE) with sequence MLE. This method enables richer and fine-grained language feedback without additional annotations, models, or training stages. Our experiments demonstrated its effectiveness, specifically, reducing the refusal-to-response paradigm in single-turn safety dialogues and the long-response bias in text summarization tasks. We provide further analysis that seq2seq RM improves RLHF performance across 2B and 7B LLMs on 3 NLP tasks, achieving an average win rate of 76.9\\%. We further show that seq2seq RM can still improve the performance of RLHF under out-of-distribution prompts.","authors":["Jiayi Zhou","Jiaming Ji","Juntao Dai","Dong Li","Yaodong Yang"],"pdf_url":"","comment":"7 pages"},{"id":"http://arxiv.org/abs/2512.21110v1","updated":"2025-12-24T11:15:57Z","published":"2025-12-24T11:15:57Z","title":"Beyond Context: Large Language Models Failure to Grasp Users Intent","summary":"Current Large Language Models (LLMs) safety approaches focus on explicitly harmful content while overlooking a critical vulnerability: the inability to understand context and recognize user intent. This creates exploitable vulnerabilities that malicious users can systematically leverage to circumvent safety mechanisms. We empirically evaluate multiple state-of-the-art LLMs, including ChatGPT, Claude, Gemini, and DeepSeek. Our analysis demonstrates the circumvention of reliable safety mechanisms through emotional framing, progressive revelation, and academic justification techniques. Notably, reasoning-enabled configurations amplified rather than mitigated the effectiveness of exploitation, increasing factual precision while failing to interrogate the underlying intent. The exception was Claude Opus 4.1, which prioritized intent detection over information provision in some use cases. This pattern reveals that current architectural designs create systematic vulnerabilities. These limitations require paradigmatic shifts toward contextual understanding and intent recognition as core safety capabilities rather than post-hoc protective mechanisms.","authors":["Ahmed M. Hussain","Salahuddin Salahuddin","Panos Papadimitratos"],"pdf_url":"","comment":"22 pages and 23 figures"},{"id":"http://arxiv.org/abs/2512.21107v1","updated":"2025-12-24T11:12:09Z","published":"2025-12-24T11:12:09Z","title":"Semi-Supervised Learning for Large Language Models Safety and Content Moderation","summary":"Safety for Large Language Models (LLMs) has been an ongoing research focus since their emergence and is even more relevant nowadays with the increasing capacity of those models. Currently, there are several guardrails in place for all public LLMs and multiple proposed datasets for training safety classifiers. However, training these safety classifiers relies on large quantities of labeled data, which can be problematic to acquire, prone to labeling errors, or often include synthetic data. To address these issues, we suggest a different approach: utilizing semi-supervised learning techniques, which leverage both labeled and unlabeled data, to improve the performance on the safety task. We analyze the improvements that these techniques can offer for both prompts given to Large Language Models and the responses to those requests. Moreover, since augmentation is the central part of semi-supervised algorithms, we demonstrate the importance of using task-specific augmentations, which significantly increase the performance when compared to general-purpose augmentation techniques.","authors":["Eduard Stefan Dinuta","Iustin Sirbu","Traian Rebedea"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.21106v1","updated":"2025-12-24T11:10:28Z","published":"2025-12-24T11:10:28Z","title":"Semantic Refinement with LLMs for Graph Representations","summary":"Graph-structured data exhibit substantial heterogeneity in where their predictive signals originate: in some domains, node-level semantics dominate, while in others, structural patterns play a central role. This structure-semantics heterogeneity implies that no graph learning model with a fixed inductive bias can generalize optimally across diverse graph domains. However, most existing methods address this challenge from the model side by incrementally injecting new inductive biases, which remains fundamentally limited given the open-ended diversity of real-world graphs. In this work, we take a data-centric perspective and treat node semantics as a task-adaptive variable. We propose a Data-Adaptive Semantic Refinement framework DAS for graph representation learning, which couples a fixed graph neural network (GNN) and a large language model (LLM) in a closed feedback loop. The GNN provides implicit supervisory signals to guide the semantic refinement of LLM, and the refined semantics are fed back to update the same graph learner. We evaluate our approach on both text-rich and text-free graphs. Results show consistent improvements on structure-dominated graphs while remaining competitive on semantics-rich graphs, demonstrating the effectiveness of data-centric semantic adaptation under structure-semantics heterogeneity.","authors":["Safal Thapaliya","Zehong Wang","Jiazheng Li","Ziming Li","Yanfang Ye","Chuxu Zhang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2511.14368v2","updated":"2025-12-24T08:17:05Z","published":"2025-11-18T11:18:08Z","title":"O3SLM: Open Weight, Open Data, and Open Vocabulary Sketch-Language Model","summary":"While Large Vision Language Models (LVLMs) are increasingly deployed in real-world applications, their ability to interpret abstract visual inputs remains limited. Specifically, they struggle to comprehend hand-drawn sketches, a modality that offers an intuitive means of expressing concepts that are difficult to describe textually. We identify the primary bottleneck as the absence of a large-scale dataset that jointly models sketches, photorealistic images, and corresponding natural language instructions. To address this, we present two key contributions: (1) a new, large-scale dataset of image-sketch-instruction triplets designed to facilitate both pretraining and instruction tuning, and (2) O3SLM, an LVLM trained on this dataset. Comprehensive evaluations on multiple sketch-based tasks: (a) object localization, (b) counting, (c) image retrieval i.e., (SBIR and fine-grained SBIR), and (d) visual question answering (VQA); while incorporating the three existing sketch datasets, namely QuickDraw!, Sketchy, and Tu Berlin, along with our generated SketchVCL dataset, show that O3SLM achieves state-of-the-art performance, substantially outperforming existing LVLMs in sketch comprehension and reasoning.","authors":["Rishi Gupta","Mukilan Karuppasamy","Shyam Marjit","Aditay Tripathi","Anirban Chakraborty"],"pdf_url":"","comment":"Accepted to AAAI 2026"},{"id":"http://arxiv.org/abs/2509.26226v2","updated":"2025-12-24T08:05:31Z","published":"2025-09-30T13:25:00Z","title":"Thinking-Free Policy Initialization Makes Distilled Reasoning Models More Effective and Efficient Reasoners","summary":"Reinforcement Learning with Verifiable Reward (RLVR) effectively solves complex tasks but demands extremely long context lengths during training, leading to substantial computational costs. While multi-stage training can partially mitigate this, starting with overly short contexts often causes irreversible performance degradation, ultimately failing to reduce overall training compute significantly. In this paper, we introduce **T**hinking-**F**ree **P**olicy **I**nitialization (**TFPI**), a simple yet effective adaptation to RLVR that bridges long Chain-of-Thought (CoT) distillation and standard RLVR. TFPI employs a simple *ThinkFree* operation, explicitly discarding the thinking content via a direct *</think>* append, to reduce token usage during inference. Training with *ThinkFree*-adapted inputs improves performance and lowers token consumption, even in the original slow-thinking mode. Extensive experiments across various benchmarks have shown that TFPI accelerates RL convergence, achieves a higher performance ceiling, and yields more token-efficient reasoning models without specialized rewards or complex training designs. With TFPI only, we train a 4B model to reach 89.0% accuracy on AIME24 and 65.5% on LiveCodeBench using less than 4K H20 hours.","authors":["Xin Xu","Cliveb AI","Kai Yang","Tianhao Chen","Yang Wang","Saiyong Yang","Can Yang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2511.17129v2","updated":"2025-12-24T07:50:00Z","published":"2025-11-21T10:45:44Z","title":"Learning to Compress: Unlocking the Potential of Large Language Models for Text Representation","summary":"Text representation plays a critical role in tasks like clustering, retrieval, and other downstream applications. With the emergence of large language models (LLMs), there is increasing interest in harnessing their capabilities for this purpose. However, most of the LLMs are inherently causal and optimized for next-token prediction, making them suboptimal for producing holistic representations. To address this, recent studies introduced pretext tasks to adapt LLMs for text representation. Most of these tasks, however, rely on token-level prediction objectives, such as the masked next-token prediction (MNTP) used in LLM2Vec. In this work, we explore the untapped potential of context compression as a pretext task for unsupervised adaptation of LLMs. During compression pre-training, the model learns to generate compact memory tokens, which substitute the whole context for downstream sequence prediction. Experiments demonstrate that a well-designed compression objective can significantly enhance LLM-based text representations, outperforming models trained with token-level pretext tasks. Further improvements through contrastive learning produce a strong representation model (LLM2Comp) that outperforms contemporary LLM-based text encoders on a wide range of tasks while being more sample-efficient, requiring significantly less training data. Code is available at https://github.com/longtaizi13579/LLM2Comp.","authors":["Yeqin Zhang","Yizheng Zhao","Chen Hu","Binxing Jiao","Daxin Jiang","Ruihang Miao","Cam-Tu Nguyen"],"pdf_url":"","comment":"Accepted by AAAI'26"},{"id":"http://arxiv.org/abs/2512.21017v1","updated":"2025-12-24T07:24:31Z","published":"2025-12-24T07:24:31Z","title":"Rethinking Supervised Fine-Tuning: Emphasizing Key Answer Tokens for Improved LLM Accuracy","summary":"With the rapid advancement of Large Language Models (LLMs), the Chain-of-Thought (CoT) component has become significant for complex reasoning tasks. However, in conventional Supervised Fine-Tuning (SFT), the model could allocate disproportionately more attention to CoT sequences with excessive length. This reduces focus on the much shorter but essential Key portion-the final answer, whose correctness directly determines task success and evaluation quality. To address this limitation, we propose SFTKey, a two-stage training scheme. In the first stage, conventional SFT is applied to ensure proper output format, while in the second stage, only the Key portion is fine-tuned to improve accuracy. Extensive experiments across multiple benchmarks and model families demonstrate that SFTKey achieves an average accuracy improvement exceeding 5\\% over conventional SFT, while preserving the ability to generate correct formats. Overall, this study advances LLM fine-tuning by explicitly balancing CoT learning with additional optimization on answer-relevant tokens.","authors":["Xiaofeng Shi","Qian Kou","Yuduo Li","Hua Zhou"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.21002v1","updated":"2025-12-24T06:57:35Z","published":"2025-12-24T06:57:35Z","title":"Distilling the Essence: Efficient Reasoning Distillation via Sequence Truncation","summary":"Distilling the reasoning capabilities from a large language model (LLM) to a smaller student model often involves training on substantial amounts of reasoning data. However, distillation over lengthy sequences with prompt (P), chain-of-thought (CoT), and answer (A) segments makes the process computationally expensive. In this work, we investigate how the allocation of supervision across different segments (P, CoT, A) affects student performance. Our analysis shows that selective knowledge distillation over only the CoT tokens can be effective when the prompt and answer information is encompassed by it. Building on this insight, we establish a truncation protocol to quantify computation-quality tradeoffs as a function of sequence length. We observe that training on only the first $50\\%$ of tokens of every training sequence can retain, on average, $\\approx94\\%$ of full-sequence performance on math benchmarks while reducing training time, memory usage, and FLOPs by about $50\\%$ each. These findings suggest that reasoning distillation benefits from prioritizing early reasoning tokens and provides a simple lever for computation-quality tradeoffs. Codes are available at https://github.com/weiruichen01/distilling-the-essence.","authors":["Wei-Rui Chen","Vignesh Kothapalli","Ata Fatahibaarzi","Hejian Sang","Shao Tang","Qingquan Song","Zhipeng Wang","Muhammad Abdul-Mageed"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.18748v2","updated":"2025-12-24T06:47:27Z","published":"2025-12-21T14:28:51Z","title":"Code2Doc: A Quality-First Curated Dataset for Code Documentation","summary":"The performance of automatic code documentation generation models depends critically on the quality of the training data used for supervision. However, most existing code documentation datasets are constructed through large scale scraping of public repositories with limited quality control. As a result, they often contain noisy documentation, extensive duplication, and increasing contamination from AI generated content. These issues weaken the supervision signal available to learning-based models and complicate evaluation.\n  We introduce Code2Doc, a quality-first curated dataset for function-level code documentation generation. Code2Doc consists of 13,358 high-quality function-documentation pairs extracted from widely used open-source repositories spanning five programming languages: Python, Java, TypeScript, JavaScript, and C++. The dataset is constructed using a four-stage curation pipeline that enforces documentation completeness and clarity, filters functions based on structural and complexity criteria, removes exact and near-duplicate code, and identifies documentation likely to be AI generated. Starting from 52,069 extracted candidates, only 25.6% satisfy all quality constraints.\n  We provide a detailed analysis of the resulting dataset, which achieves a mean documentation quality score of 6.93 out of 10. Overall, 86.9% of samples contain explicit type annotations, and only 2.9% are flagged as potentially AI generated. Baseline experiments show that fine-tuning a large language model on Code2Doc yields relative improvements of 29.47% in BLEU and 24.04% in ROUGE-L over zero shot performance, despite the modest dataset size. We release both the dataset and the full curation pipeline to support reproducible research on automatic code documentation generation.","authors":["Recep Kaan Karaman","Meftun Akarsu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20983v1","updated":"2025-12-24T06:17:21Z","published":"2025-12-24T06:17:21Z","title":"Automatic Replication of LLM Mistakes in Medical Conversations","summary":"Large language models (LLMs) are increasingly evaluated in clinical settings using multi-dimensional rubrics which quantify reasoning quality, safety, and patient-centeredness. Yet, replicating specific mistakes in other LLM models is not straightforward and often requires manual effort. We introduce MedMistake, an automatic pipeline that extracts mistakes LLMs make in patient-doctor conversations and converts them into a benchmark of single-shot QA pairs. Our pipeline (1) creates complex, conversational data between an LLM patient and LLM doctor, (2) runs an evaluation with a committee of 2 LLM judges across a variety of dimensions and (3) creates simplified single-shot QA scenarios from those mistakes. We release MedMistake-All, a dataset of 3,390 single-shot QA pairs where GPT-5 and Gemini 2.5 Pro are currently failing to answer correctly, as judged by two LLM judges. We used medical experts to validate a subset of 211/3390 questions (MedMistake-Bench), which we used to run a final evaluation of 12 frontier LLMs: Claude Opus 4.5, Claude Sonnet 4.5, DeepSeek-Chat, Gemini 2.5 Pro, Gemini 3 Pro, GPT-4o, GPT-5, GPT-5.1, GPT-5.2, Grok 4, Grok 4.1, Mistral Large. We found that GPT models, Claude and Grok obtained the best performance on MedMistake-Bench. We release both the doctor-validated benchmark (MedMistake-Bench), as well as the full dataset (MedMistake-All) at https://huggingface.co/datasets/TheLumos/MedicalMistakeBenchmark.","authors":["Oleksii Proniakin","Diego Fajardo","Ruslan Nazarenko","Razvan Marinescu"],"pdf_url":"","comment":"48 pages, 3 figures, 4 tables"},{"id":"http://arxiv.org/abs/1909.03716v3","updated":"2025-12-24T06:10:10Z","published":"2019-09-09T09:26:42Z","title":"Improving Neural Question Generation using World Knowledge","summary":"In this paper, we propose a method for incorporating world knowledge (linked entities and fine-grained entity types) into a neural question generation model. This world knowledge helps to encode additional information related to the entities present in the passage required to generate human-like questions. We evaluate our models on both SQuAD and MS MARCO to demonstrate the usefulness of the world knowledge features. The proposed world knowledge enriched question generation model is able to outperform the vanilla neural question generation model by 1.37 and 1.59 absolute BLEU 4 score on SQuAD and MS MARCO test dataset respectively.","authors":["Deepak Gupta","Kaheer Suleman","Mahmoud Adada","Andrew McNamara","Justin Harris"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20954v1","updated":"2025-12-24T05:25:17Z","published":"2025-12-24T05:25:17Z","title":"Reflection Pretraining Enables Token-Level Self-Correction in Biological Sequence Models","summary":"Chain-of-Thought (CoT) prompting has significantly advanced task-solving capabilities in natural language processing with large language models. Unlike standard prompting, CoT encourages the model to generate intermediate reasoning steps, non-answer tokens, that help guide the model toward more accurate final outputs. These intermediate steps enable more complex reasoning processes such as error correction, memory management, future planning, and self-reflection. However, applying CoT to non-natural language domains, such as protein and RNA language models, is not yet possible, primarily due to the limited expressiveness of their token spaces (e.g., amino acid tokens). In this work, we propose and define the concept of language expressiveness: the ability of a given language, using its tokens and grammar, to encode information. We show that the limited expressiveness of protein language severely restricts the applicability of CoT-style reasoning. To overcome this, we introduce reflection pretraining, for the first time in a biological sequence model, which enables the model to engage in intermediate reasoning through the generation of auxiliary \"thinking tokens\" beyond simple answer tokens. Theoretically, we demonstrate that our augmented token set significantly enhances biological language expressiveness, thereby improving the overall reasoning capacity of the model. Experimentally, our pretraining approach teaches protein models to self-correct and leads to substantial performance gains compared to standard pretraining.","authors":["Xiang Zhang","Jiaqi Wei","Yuejin Yang","Zijie Qiu","Yuhan Chen","Zhiqiang Gao","Muhammad Abdul-Mageed","Laks V. S. Lakshmanan","Wanli Ouyang","Chenyu You","Siqi Sun"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2510.04265v2","updated":"2025-12-24T05:18:52Z","published":"2025-10-05T16:14:03Z","title":"Don't Pass@k: A Bayesian Framework for Large Language Model Evaluation","summary":"Pass$@k$ is widely used to report performance for LLM reasoning, but it often yields unstable, misleading rankings, especially when the number of trials (samples) is limited and compute is constrained. We present a principled Bayesian evaluation framework that replaces Pass$@k$ and average accuracy over $N$ trials (avg$@N$) with posterior estimates of a model's underlying success probability and credible intervals, yielding stable rankings and a transparent decision rule for differences. Evaluation outcomes are modeled as categorical (not just 0/1) with a Dirichlet prior, giving closed-form expressions for the posterior mean and uncertainty of any weighted rubric and enabling the use of prior evidence when appropriate. Theoretically, under a uniform prior, the Bayesian posterior mean is order-equivalent to average accuracy (Pass$@1$), explaining its empirical robustness while adding principled uncertainty. Empirically, in simulations with known ground-truth success rates and on AIME'24/'25, HMMT'25, and BrUMO'25, the Bayesian/avg procedure achieves faster convergence and greater rank stability than Pass$@k$ and recent variants, enabling reliable comparisons at far smaller sample counts. The framework clarifies when observed gaps are statistically meaningful (non-overlapping credible intervals) versus noise, and it naturally extends to graded, rubric-based evaluations. Together, these results recommend replacing Pass$@k$ for LLM evaluation and ranking with a posterior-based, compute-efficient protocol that unifies binary and non-binary evaluation while making uncertainty explicit. Code is available at https://github.com/mohsenhariri/scorio","authors":["Mohsen Hariri","Amirhossein Samandar","Michael Hinczewski","Vipin Chaudhary"],"pdf_url":"","comment":"Code and simulations: https://github.com/mohsenhariri/scorio"},{"id":"http://arxiv.org/abs/2512.20950v1","updated":"2025-12-24T05:14:40Z","published":"2025-12-24T05:14:40Z","title":"MultiMind at SemEval-2025 Task 7: Crosslingual Fact-Checked Claim Retrieval via Multi-Source Alignment","summary":"This paper presents our system for SemEval-2025 Task 7: Multilingual and Crosslingual Fact-Checked Claim Retrieval. In an era where misinformation spreads rapidly, effective fact-checking is increasingly critical. We introduce TriAligner, a novel approach that leverages a dual-encoder architecture with contrastive learning and incorporates both native and English translations across different modalities. Our method effectively retrieves claims across multiple languages by learning the relative importance of different sources in alignment. To enhance robustness, we employ efficient data preprocessing and augmentation using large language models while incorporating hard negative sampling to improve representation learning. We evaluate our approach on monolingual and crosslingual benchmarks, demonstrating significant improvements in retrieval accuracy and fact-checking performance over baselines.","authors":["Mohammad Mahdi Abootorabi","Alireza Ghahramani Kure","Mohammadali Mohammadkhani","Sina Elahimanesh","Mohammad Ali Ali Panah"],"pdf_url":"","comment":"11 pages Published at the SemEval-2025 workshop"},{"id":"http://arxiv.org/abs/2512.20949v1","updated":"2025-12-24T05:10:19Z","published":"2025-12-24T05:10:19Z","title":"Neural Probe-Based Hallucination Detection for Large Language Models","summary":"Large language models(LLMs) excel at text generation and knowledge question-answering tasks, but they are prone to generating hallucinated content, severely limiting their application in high-risk domains. Current hallucination detection methods based on uncertainty estimation and external knowledge retrieval suffer from the limitation that they still produce erroneous content at high confidence levels and rely heavily on retrieval efficiency and knowledge coverage. In contrast, probe methods that leverage the model's hidden-layer states offer real-time and lightweight advantages. However, traditional linear probes struggle to capture nonlinear structures in deep semantic spaces.To overcome these limitations, we propose a neural network-based framework for token-level hallucination detection. By freezing language model parameters, we employ lightweight MLP probes to perform nonlinear modeling of high-level hidden states. A multi-objective joint loss function is designed to enhance detection stability and semantic disambiguity. Additionally, we establish a layer position-probe performance response model, using Bayesian optimization to automatically search for optimal probe insertion layers and achieve superior training results.Experimental results on LongFact, HealthBench, and TriviaQA demonstrate that MLP probes significantly outperform state-of-the-art methods in accuracy, recall, and detection capability under low false-positive conditions.","authors":["Shize Liang","Hongzhi Wang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20948v1","updated":"2025-12-24T05:07:07Z","published":"2025-12-24T05:07:07Z","title":"Foundation Model-based Evaluation of Neuropsychiatric Disorders: A Lifespan-Inclusive, Multi-Modal, and Multi-Lingual Study","summary":"Neuropsychiatric disorders, such as Alzheimer's disease (AD), depression, and autism spectrum disorder (ASD), are characterized by linguistic and acoustic abnormalities, offering potential biomarkers for early detection. Despite the promise of multi-modal approaches, challenges like multi-lingual generalization and the absence of a unified evaluation framework persist. To address these gaps, we propose FEND (Foundation model-based Evaluation of Neuropsychiatric Disorders), a comprehensive multi-modal framework integrating speech and text modalities for detecting AD, depression, and ASD across the lifespan. Leveraging 13 multi-lingual datasets spanning English, Chinese, Greek, French, and Dutch, we systematically evaluate multi-modal fusion performance. Our results show that multi-modal fusion excels in AD and depression detection but underperforms in ASD due to dataset heterogeneity. We also identify modality imbalance as a prevalent issue, where multi-modal fusion fails to surpass the best mono-modal models. Cross-corpus experiments reveal robust performance in task- and language-consistent scenarios but noticeable degradation in multi-lingual and task-heterogeneous settings. By providing extensive benchmarks and a detailed analysis of performance-influencing factors, FEND advances the field of automated, lifespan-inclusive, and multi-lingual neuropsychiatric disorder assessment. We encourage researchers to adopt the FEND framework for fair comparisons and reproducible research.","authors":["Zhongren Dong","Haotian Guo","Weixiang Xu","Huan Zhao","Zixing Zhang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20934v1","updated":"2025-12-24T04:30:21Z","published":"2025-12-24T04:30:21Z","title":"Transductive Visual Programming: Evolving Tool Libraries from Experience for Spatial Reasoning","summary":"Spatial reasoning in 3D scenes requires precise geometric calculations that challenge vision-language models. Visual programming addresses this by decomposing problems into steps calling specialized tools, yet existing methods rely on either fixed toolsets or speculative tool induction before solving problems, resulting in suboptimal programs and poor utilization of induced tools. We present Transductive Visual Programming (TVP), a novel framework that builds new tools from its own experience rather than speculation. TVP first solves problems using basic tools while accumulating experiential solutions into an Example Library, then abstracts recurring patterns from these programs into reusable higher-level tools for an evolving Tool Library. This allows TVP to tackle new problems with increasingly powerful tools learned from experience. On Omni3D-Bench, TVP achieves state-of-the-art performance, outperforming GPT-4o by 22% and the previous best visual programming system by 11%. Our transductively learned tools are used 5x more frequently as core program dependency than inductively created ones, demonstrating more effective tool discovery and reuse. The evolved tools also show strong generalization to unseen spatial tasks, achieving superior performance on benchmarks from SpatialScore-Hard collection without any testset-specific modification. Our work establishes experience-driven transductive tool creation as a powerful paradigm for building self-evolving visual programming agents that effectively tackle challenging spatial reasoning tasks. We release our code at https://transductive-visualprogram.github.io/.","authors":["Shengguang Wu","Xiaohan Wang","Yuhui Zhang","Hao Zhu","Serena Yeung-Levy"],"pdf_url":"","comment":"Project Website: https://transductive-visualprogram.github.io/"},{"id":"http://arxiv.org/abs/2512.20929v1","updated":"2025-12-24T04:19:20Z","published":"2025-12-24T04:19:20Z","title":"Decoding Predictive Inference in Visual Language Processing via Spatiotemporal Neural Coherence","summary":"Human language processing relies on the brain's capacity for predictive inference. We present a machine learning framework for decoding neural (EEG) responses to dynamic visual language stimuli in Deaf signers. Using coherence between neural signals and optical flow-derived motion features, we construct spatiotemporal representations of predictive neural dynamics. Through entropy-based feature selection, we identify frequency-specific neural signatures that differentiate interpretable linguistic input from linguistically disrupted (time-reversed) stimuli. Our results reveal distributed left-hemispheric and frontal low-frequency coherence as key features in language comprehension, with experience-dependent neural signatures correlating with age. This work demonstrates a novel multimodal approach for probing experience-driven generative models of perception in the brain.","authors":["Sean C. Borneman","Julia Krebs","Ronnie B. Wilbur","Evie A. Malaia"],"pdf_url":"","comment":"39th Conference on Neural Information Processing Systems (NeurIPS 2025) Workshop: Foundation Models for the Brain and Body"},{"id":"http://arxiv.org/abs/2512.15745v2","updated":"2025-12-24T03:46:46Z","published":"2025-12-10T09:26:18Z","title":"LLaDA2.0: Scaling Up Diffusion Language Models to 100B","summary":"This paper presents LLaDA2.0 -- a tuple of discrete diffusion large language models (dLLM) scaling up to 100B total parameters through systematic conversion from auto-regressive (AR) models -- establishing a new paradigm for frontier-scale deployment. Instead of costly training from scratch, LLaDA2.0 upholds knowledge inheritance, progressive adaption and efficiency-aware design principle, and seamless converts a pre-trained AR model into dLLM with a novel 3-phase block-level WSD based training scheme: progressive increasing block-size in block diffusion (warm-up), large-scale full-sequence diffusion (stable) and reverting back to compact-size block diffusion (decay). Along with post-training alignment with SFT and DPO, we obtain LLaDA2.0-mini (16B) and LLaDA2.0-flash (100B), two instruction-tuned Mixture-of-Experts (MoE) variants optimized for practical deployment. By preserving the advantages of parallel decoding, these models deliver superior performance and efficiency at the frontier scale. Both models were open-sourced.","authors":["Tiwei Bie","Maosong Cao","Kun Chen","Lun Du","Mingliang Gong","Zhuochen Gong","Yanmei Gu","Jiaqi Hu","Zenan Huang","Zhenzhong Lan","Chengxi Li","Chongxuan Li","Jianguo Li","Zehuan Li","Huabin Liu","Lin Liu","Guoshan Lu","Xiaocheng Lu","Yuxin Ma","Jianfeng Tan","Lanning Wei","Ji-Rong Wen","Yipeng Xing","Xiaolu Zhang","Junbo Zhao","Da Zheng","Jun Zhou","Junlin Zhou","Zhanchao Zhou","Liwang Zhu","Yihong Zhuang"],"pdf_url":"","comment":"19 pages"},{"id":"http://arxiv.org/abs/2512.00617v2","updated":"2025-12-24T03:42:55Z","published":"2025-11-29T20:16:11Z","title":"ART: Adaptive Response Tuning Framework -- A Multi-Agent Tournament-Based Approach to LLM Response Optimization","summary":"Large Language Models (LLMs) have demonstrated remarkable capabilities in natural language understanding and generation. However, single-model responses often exhibit inconsistencies, hallucinations, and varying quality across different query domains. This paper presents ART (Adaptive Response Tuning), a novel framework that employs tournament-style ELO ranking and multi-agent reasoning to systematically optimize LLM outputs. By enabling multiple LLM agents to compete, critique, and collaborate through structured tournament workflows, ART produces consensus responses that outperform individual model outputs. Our framework introduces configurable tournament parameters, dynamic agent selection, and multiple consensus fusion strategies. Experimental evaluations demonstrate significant improvements in response accuracy, coherence, and reliability compared to baseline single-model approaches. The ART framework provides a scalable, production-ready solution for applications requiring high-quality, vetted LLM responses, achieving an 8.4% improvement in overall quality metrics and R^2 values exceeding 0.96 in ELO rating convergence.","authors":["Omer Jauhar Khan"],"pdf_url":"","comment":"14 pages, 11 figures, 5 tables. IEEE conference-style paper with appendices"},{"id":"http://arxiv.org/abs/2512.20908v1","updated":"2025-12-24T03:19:05Z","published":"2025-12-24T03:19:05Z","title":"Where Did This Sentence Come From? Tracing Provenance in LLM Reasoning Distillation","summary":"Reasoning distillation has attracted increasing attention. It typically leverages a large teacher model to generate reasoning paths, which are then used to fine-tune a student model so that it mimics the teacher's behavior in training contexts. However, previous approaches have lacked a detailed analysis of the origins of the distilled model's capabilities. It remains unclear whether the student can maintain consistent behaviors with the teacher in novel test-time contexts, or whether it regresses to its original output patterns, raising concerns about the generalization of distillation models. To analyse this question, we introduce a cross-model Reasoning Distillation Provenance Tracing framework. For each action (e.g., a sentence) produced by the distilled model, we obtain the predictive probabilities assigned by the teacher, the original student, and the distilled model under the same context. By comparing these probabilities, we classify each action into different categories. By systematically disentangling the provenance of each action, we experimentally demonstrate that, in test-time contexts, the distilled model can indeed generate teacher-originated actions, which correlate with and plausibly explain observed performance on distilled model. Building on this analysis, we further propose a teacher-guided data selection method. Unlike prior approach that rely on heuristics, our method directly compares teacher-student divergences on the training data, providing a principled selection criterion. We validate the effectiveness of our approach across multiple representative teacher models and diverse student models. The results highlight the utility of our provenance-tracing framework and underscore its promise for reasoning distillation. We hope to share Reasoning Distillation Provenance Tracing and our insights into reasoning distillation with the community.","authors":["Kaiyuan Liu","Shaotian Yan","Rui Miao","Bing Wang","Chen Shen","Jun Zhang","Jieping Ye"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.14554v4","updated":"2025-12-24T02:46:01Z","published":"2025-12-16T16:28:32Z","title":"VLegal-Bench: Cognitively Grounded Benchmark for Vietnamese Legal Reasoning of Large Language Models","summary":"The rapid advancement of large language models (LLMs) has enabled new possibilities for applying artificial intelligence within the legal domain. Nonetheless, the complexity, hierarchical organization, and frequent revisions of Vietnamese legislation pose considerable challenges for evaluating how well these models interpret and utilize legal knowledge. To address this gap, the Vietnamese Legal Benchmark (VLegal-Bench) is introduced, the first comprehensive benchmark designed to systematically assess LLMs on Vietnamese legal tasks. Informed by Bloom's cognitive taxonomy, VLegal-Bench encompasses multiple levels of legal understanding through tasks designed to reflect practical usage scenarios. The benchmark comprises 10,450 samples generated through a rigorous annotation pipeline, where legal experts label and cross-validate each instance using our annotation system to ensure every sample is grounded in authoritative legal documents and mirrors real-world legal assistant workflows, including general legal questions and answers, retrieval-augmented generation, multi-step reasoning, and scenario-based problem solving tailored to Vietnamese law. By providing a standardized, transparent, and cognitively informed evaluation framework, VLegal-Bench establishes a solid foundation for assessing LLM performance in Vietnamese legal contexts and supports the development of more reliable, interpretable, and ethically aligned AI-assisted legal systems. To facilitate access and reproducibility, we provide a public landing page for this benchmark at https://vilegalbench.cmcai.vn/.","authors":["Nguyen Tien Dong","Minh-Anh Nguyen","Thanh Dat Hoang","Nguyen Tuan Ngoc","Dao Xuan Quang Minh","Phan Phi Hai","Nguyen Thi Ngoc Anh","Dang Van Tu","Binh Vu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2508.12140v2","updated":"2025-12-24T02:25:32Z","published":"2025-08-16T19:25:06Z","title":"Exploring Efficiency Frontiers of Thinking Budget in Medical Reasoning: Scaling Laws between Computational Resources and Reasoning Quality","summary":"This study presents the first comprehensive evaluation of thinking budget mechanisms in medical reasoning tasks, revealing fundamental scaling laws between computational resources and reasoning quality. We systematically evaluated two major model families, Qwen3 (1.7B to 235B parameters) and DeepSeek-R1 (1.5B to 70B parameters), across 15 medical datasets spanning diverse specialties and difficulty levels. Through controlled experiments with thinking budgets ranging from zero to unlimited tokens, we establish logarithmic scaling relationships where accuracy improvements follow a predictable pattern with both thinking budget and model size. Our findings identify three distinct efficiency regimes: high-efficiency (0 to 256 tokens) suitable for real-time applications, balanced (256 to 512 tokens) offering optimal cost-performance tradeoffs for routine clinical support, and high-accuracy (above 512 tokens) justified only for critical diagnostic tasks. Notably, smaller models demonstrate disproportionately larger benefits from extended thinking, with 15 to 20% improvements compared to 5 to 10% for larger models, suggesting a complementary relationship where thinking budget provides greater relative benefits for capacity-constrained models. Domain-specific patterns emerge clearly, with neurology and gastroenterology requiring significantly deeper reasoning processes than cardiovascular or respiratory medicine. The consistency between Qwen3 native thinking budget API and our proposed truncation method for DeepSeek-R1 validates the generalizability of thinking budget concepts across architectures. These results establish thinking budget control as a critical mechanism for optimizing medical AI systems, enabling dynamic resource allocation aligned with clinical needs while maintaining the transparency essential for healthcare deployment.","authors":["Ziqian Bi","Lu Chen","Junhao Song","Hongying Luo","Enze Ge","Junmin Huang","Tianyang Wang","Keyu Chen","Chia Xin Liang","Zihan Wei","Huafeng Liu","Chunjie Tian","Jibin Guan","Joe Yeong","Yongzhi Xu","Peng Wang","Xinyuan Song","Junfeng Hao"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2511.21701v2","updated":"2025-12-24T01:52:42Z","published":"2025-11-16T06:08:41Z","title":"47B Mixture-of-Experts Beats 671B Dense Models on Chinese Medical Examinations","summary":"The rapid advancement of large language models(LLMs) has prompted significant interest in their potential applications in medical domains. This paper presents a comprehensive benchmark evaluation of 27 state-of-the-art LLMs on Chinese medical examination questions, encompassing seven medical specialties across two professional levels. We introduce a robust evaluation framework that assesses model performance on 2,800 carefully curated questions from cardiovascular, gastroenterology, hematology, infectious diseases, nephrology, neurology, and respiratory medicine domains. Our dataset distinguishes between attending physician and senior physician difficulty levels, providing nuanced insights into model capabilities across varying complexity. Our empirical analysis reveals substantial performance variations among models, with Mixtral-8x7B achieving the highest overall accuracy of 74.25%, followed by DeepSeek-R1-671B at 64.07%. Notably, we observe no consistent correlation between model size and performance, as evidenced by the strong performance of smaller mixture-of-experts architectures. The evaluation demonstrates significant performance gaps between medical specialties, with models generally performing better on cardiovascular and neurology questions compared to gastroenterology and nephrology domains. Furthermore, our analysis indicates minimal performance degradation between attending and senior physician levels for top-performing models, suggesting robust generalization capabilities. This benchmark provides critical insights for the deployment of LLMs in medical education and clinical decision support systems, highlighting both the promise and current limitations of these technologies in specialized medical contexts.","authors":["Chiung-Yi Tseng","Danyang Zhang","Tianyang Wang","Hongying Luo","Lu Chen","Junming Huang","Jibin Guan","Junfeng Hao","Junhao Song","Xinyuan Song","Ziqian Bi"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20877v1","updated":"2025-12-24T01:36:50Z","published":"2025-12-24T01:36:50Z","title":"Architectural Trade-offs in Small Language Models Under Compute Constraints","summary":"We present a systematic empirical study of small language models under strict compute constraints, analyzing how architectural choices and training budget interact to determine performance. Starting from a linear next-token predictor, we progressively introduce nonlinearities, self-attention, and multi-layer transformer architectures, evaluating each on character-level modeling of Tiny Shakespeare and word-level modeling of Penn Treebank (PTB) and WikiText-2. We compare models using test negative log-likelihood (NLL), parameter count, and approximate training FLOPs to characterize accuracy-efficiency trade-offs. Our results show that attention-based models dominate MLPs in per-FLOP efficiency even at small scale, while increasing depth or context without sufficient optimization can degrade performance. We further examine rotary positional embeddings (RoPE), finding that architectural techniques successful in large language models do not necessarily transfer to small-model regimes.","authors":["Shivraj Singh Bhatti"],"pdf_url":"","comment":"15 pages, 11 images"},{"id":"http://arxiv.org/abs/2512.20136v2","updated":"2025-12-24T01:35:59Z","published":"2025-12-23T07:54:03Z","title":"M$^3$KG-RAG: Multi-hop Multimodal Knowledge Graph-enhanced Retrieval-Augmented Generation","summary":"Retrieval-Augmented Generation (RAG) has recently been extended to multimodal settings, connecting multimodal large language models (MLLMs) with vast corpora of external knowledge such as multimodal knowledge graphs (MMKGs). Despite their recent success, multimodal RAG in the audio-visual domain remains challenging due to 1) limited modality coverage and multi-hop connectivity of existing MMKGs, and 2) retrieval based solely on similarity in a shared multimodal embedding space, which fails to filter out off-topic or redundant knowledge. To address these limitations, we propose M$^3$KG-RAG, a Multi-hop Multimodal Knowledge Graph-enhanced RAG that retrieves query-aligned audio-visual knowledge from MMKGs, improving reasoning depth and answer faithfulness in MLLMs. Specifically, we devise a lightweight multi-agent pipeline to construct multi-hop MMKG (M$^3$KG), which contains context-enriched triplets of multimodal entities, enabling modality-wise retrieval based on input queries. Furthermore, we introduce GRASP (Grounded Retrieval And Selective Pruning), which ensures precise entity grounding to the query, evaluates answer-supporting relevance, and prunes redundant context to retain only knowledge essential for response generation. Extensive experiments across diverse multimodal benchmarks demonstrate that M$^3$KG-RAG significantly enhances MLLMs' multimodal reasoning and grounding over existing approaches.","authors":["Hyeongcheol Park","Jiyoung Seo","Jaewon Mun","Hogun Park","Wonmin Byeon","Sung June Kim","Hyeonsoo Im","JeungSub Lee","Sangpil Kim"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2509.01907v5","updated":"2025-12-24T01:10:58Z","published":"2025-09-02T03:01:23Z","title":"RSCC: A Large-Scale Remote Sensing Change Caption Dataset for Disaster Events","summary":"Remote sensing is critical for disaster monitoring, yet existing datasets lack temporal image pairs and detailed textual annotations. While single-snapshot imagery dominates current resources, it fails to capture dynamic disaster impacts over time. To address this gap, we introduce the Remote Sensing Change Caption (RSCC) dataset, a large-scale benchmark comprising 62,351 pre-/post-disaster image pairs (spanning earthquakes, floods, wildfires, and more) paired with rich, human-like change captions. By bridging the temporal and semantic divide in remote sensing data, RSCC enables robust training and evaluation of vision-language models for disaster-aware bi-temporal understanding. Our results highlight RSCC's ability to facilitate detailed disaster-related analysis, paving the way for more accurate, interpretable, and scalable vision-language applications in remote sensing. Code and dataset are available at https://github.com/Bili-Sakura/RSCC.","authors":["Zhenyuan Chen","Chenxi Wang","Ningyu Zhang","Feng Zhang"],"pdf_url":"","comment":"Accepted by NeurIPS 2025 Dataset and Benchmark Track"},{"id":"http://arxiv.org/abs/2512.20856v1","updated":"2025-12-24T00:24:05Z","published":"2025-12-24T00:24:05Z","title":"NVIDIA Nemotron 3: Efficient and Open Intelligence","summary":"We introduce the Nemotron 3 family of models - Nano, Super, and Ultra. These models deliver strong agentic, reasoning, and conversational capabilities. The Nemotron 3 family uses a Mixture-of-Experts hybrid Mamba-Transformer architecture to provide best-in-class throughput and context lengths of up to 1M tokens. Super and Ultra models are trained with NVFP4 and incorporate LatentMoE, a novel approach that improves model quality. The two larger models also include MTP layers for faster text generation. All Nemotron 3 models are post-trained using multi-environment reinforcement learning enabling reasoning, multi-step tool use, and support granular reasoning budget control. Nano, the smallest model, outperforms comparable models in accuracy while remaining extremely cost-efficient for inference. Super is optimized for collaborative agents and high-volume workloads such as IT ticket automation. Ultra, the largest model, provides state-of-the-art accuracy and reasoning performance. Nano is released together with its technical report and this white paper, while Super and Ultra will follow in the coming months. We will openly release the model weights, pre- and post-training software, recipes, and all data for which we hold redistribution rights.","authors":[" NVIDIA"," :","Aaron Blakeman","Aaron Grattafiori","Aarti Basant","Abhibha Gupta","Abhinav Khattar","Adi Renduchintala","Aditya Vavre","Akanksha Shukla","Akhiad Bercovich","Aleksander Ficek","Aleksandr Shaposhnikov","Alex Kondratenko","Alexander Bukharin","Alexandre Milesi","Ali Taghibakhshi","Alisa Liu","Amelia Barton","Ameya Sunil Mahabaleshwarkar","Amir Klein","Amit Zuker","Amnon Geifman","Amy Shen","Anahita Bhiwandiwalla","Andrew Tao","Anjulie Agrusa","Ankur Verma","Ann Guan","Anubhav Mandarwal","Arham Mehta","Ashwath Aithal","Ashwin Poojary","Asif Ahamed","Asit Mishra","Asma Kuriparambil Thekkumpate","Ayush Dattagupta","Banghua Zhu","Bardiya Sadeghi","Barnaby Simkin","Ben Lanir","Benedikt Schifferer","Besmira Nushi","Bilal Kartal","Bita Darvish Rouhani","Boris Ginsburg","Brandon Norick","Brandon Soubasis","Branislav Kisacanin","Brian Yu","Bryan Catanzaro","Carlo del Mundo","Chantal Hwang","Charles Wang","Cheng-Ping Hsieh","Chenghao Zhang","Chenhan Yu","Chetan Mungekar","Chintan Patel","Chris Alexiuk","Christopher Parisien","Collin Neale","Cyril Meurillon","Damon Mosk-Aoyama","Dan Su","Dane Corneil","Daniel Afrimi","Daniel Lo","Daniel Rohrer","Daniel Serebrenik","Daria Gitman","Daria Levy","Darko Stosic","David Mosallanezhad","Deepak Narayanan","Dhruv Nathawani","Dima Rekesh","Dina Yared","Divyanshu Kakwani","Dong Ahn","Duncan Riach","Dusan Stosic","Edgar Minasyan","Edward Lin","Eileen Long","Eileen Peters Long","Elad Segal","Elena Lantz","Ellie Evans","Elliott Ning","Eric Chung","Eric Harper","Eric Tramel","Erick Galinkin","Erik Pounds","Evan Briones","Evelina Bakhturina","Evgeny Tsykunov","Faisal Ladhak","Fay Wang","Fei Jia","Felipe Soares","Feng Chen","Ferenc Galko","Frank Sun","Frankie Siino","Gal Hubara Agam","Ganesh Ajjanagadde","Gantavya Bhatt","Gargi Prasad","George Armstrong","Gerald Shen","Gorkem Batmaz","Grigor Nalbandyan","Haifeng Qian","Harsh Sharma","Hayley Ross","Helen Ngo","Herbert Hum","Herman Sahota","Hexin Wang","Himanshu Soni","Hiren Upadhyay","Huizi Mao","Huy C Nguyen","Huy Q Nguyen","Iain Cunningham","Ido Galil","Ido Shahaf","Igor Gitman","Ilya Loshchilov","Itamar Schen","Itay Levy","Ivan Moshkov","Izik Golan","Izzy Putterman","Jan Kautz","Jane Polak Scowcroft","Jared Casper","Jatin Mitra","Jeffrey Glick","Jenny Chen","Jesse Oliver","Jian Zhang","Jiaqi Zeng","Jie Lou","Jimmy Zhang","Jinhang Choi","Jining Huang","Joey Conway","Joey Guman","John Kamalu","Johnny Greco","Jonathan Cohen","Joseph Jennings","Joyjit Daw","Julien Veron Vialard","Junkeun Yi","Jupinder Parmar","Kai Xu","Kan Zhu","Kari Briski","Katherine Cheung","Katherine Luna","Keith Wyss","Keshav Santhanam","Kevin Shih","Kezhi Kong","Khushi Bhardwaj","Kirthi Shankar","Krishna C. Puvvada","Krzysztof Pawelec","Kumar Anik","Lawrence McAfee","Laya Sleiman","Leon Derczynski","Li Ding","Lizzie Wei","Lucas Liebenwein","Luis Vega","Maanu Grover","Maarten Van Segbroeck","Maer Rodrigues de Melo","Mahdi Nazemi","Makesh Narsimhan Sreedhar","Manoj Kilaru","Maor Ashkenazi","Marc Romeijn","Marcin Chochowski","Mark Cai","Markus Kliegl","Maryam Moosaei","Matt Kulka","Matvei Novikov","Mehrzad Samadi","Melissa Corpuz","Mengru Wang","Meredith Price","Michael Andersch","Michael Boone","Michael Evans","Miguel Martinez","Mikail Khona","Mike Chrzanowski","Minseok Lee","Mohammad Dabbah","Mohammad Shoeybi","Mostofa Patwary","Nabin Mulepati","Najeeb Nabwani","Natalie Hereth","Nave Assaf","Negar Habibi","Neta Zmora","Netanel Haber","Nicola Sessions","Nidhi Bhatia","Nikhil Jukar","Nikki Pope","Nikolai Ludwig","Nima Tajbakhsh","Nir Ailon","Nirmal Juluru","Nishant Sharma","Oleksii Hrinchuk","Oleksii Kuchaiev","Olivier Delalleau","Oluwatobi Olabiyi","Omer Ullman Argov","Omri Puny","Oren Tropp","Ouye Xie","Parth Chadha","Pasha Shamis","Paul Gibbons","Pavlo Molchanov","Pawel Morkisz","Peter Dykas","Peter Jin","Pinky Xu","Piotr Januszewski","Pranav Prashant Thombre","Prasoon Varshney","Pritam Gundecha","Przemek Tredak","Qing Miao","Qiyu Wan","Rabeeh Karimi Mahabadi","Rachit Garg","Ran El-Yaniv","Ran Zilberstein","Rasoul Shafipour","Rich Harang","Rick Izzo","Rima Shahbazyan","Rishabh Garg","Ritika Borkar","Ritu Gala","Riyad Islam","Robert Hesse","Roger Waleffe","Rohit Watve","Roi Koren","Ruoxi Zhang","Russell Hewett","Russell J. Hewett","Ryan Prenger","Ryan Timbrook","Sadegh Mahdavi","Sahil Modi","Samuel Kriman","Sangkug Lim","Sanjay Kariyappa","Sanjeev Satheesh","Saori Kaji","Satish Pasumarthi","Saurav Muralidharan","Sean Narentharen","Sean Narenthiran","Seonmyeong Bak","Sergey Kashirsky","Seth Poulos","Shahar Mor","Shanmugam Ramasamy","Shantanu Acharya","Shaona Ghosh","Sharath Turuvekere Sreenivas","Shelby Thomas","Shiqing Fan","Shreya Gopal","Shrimai Prabhumoye","Shubham Pachori","Shubham Toshniwal","Shuoyang Ding","Siddharth Singh","Simeng Sun","Smita Ithape","Somshubra Majumdar","Soumye Singhal","Stas Sergienko","Stefania Alborghetti","Stephen Ge","Sugam Dipak Devare","Sumeet Kumar Barua","Suseella Panguluri","Suyog Gupta","Sweta Priyadarshi","Syeda Nahida Akter","Tan Bui","Teodor-Dumitru Ene","Terry Kong","Thanh Do","Tijmen Blankevoort","Tim Moon","Tom Balough","Tomer Asida","Tomer Bar Natan","Tomer Ronen","Tugrul Konuk","Twinkle Vashishth","Udi Karpas","Ushnish De","Vahid Noorozi","Vahid Noroozi","Venkat Srinivasan","Venmugil Elango","Victor Cui","Vijay Korthikanti","Vinay Rao","Vitaly Kurin","Vitaly Lavrukhin","Vladimir Anisimov","Wanli Jiang","Wasi Uddin Ahmad","Wei Du","Wei Ping","Wenfei Zhou","Will Jennings","William Zhang","Wojciech Prazuch","Xiaowei Ren","Yashaswi Karnati","Yejin Choi","Yev Meyer","Yi-Fu Wu","Yian Zhang","Yigong Qin","Ying Lin","Yonatan Geifman","Yonggan Fu","Yoshi Subara","Yoshi Suhara","Yubo Gao","Zach Moshe","Zhen Dong","Zhongbo Zhu","Zihan Liu","Zijia Chen","Zijie Yan"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20854v1","updated":"2025-12-24T00:16:31Z","published":"2025-12-24T00:16:31Z","title":"How important is Recall for Measuring Retrieval Quality?","summary":"In realistic retrieval settings with large and evolving knowledge bases, the total number of documents relevant to a query is typically unknown, and recall cannot be computed. In this paper, we evaluate several established strategies for handling this limitation by measuring the correlation between retrieval quality metrics and LLM-based judgments of response quality, where responses are generated from the retrieved documents. We conduct experiments across multiple datasets with a relatively low number of relevant documents (2-15). We also introduce a simple retrieval quality measure that performs well without requiring knowledge of the total number of relevant documents.","authors":["Shelly Schwartz","Oleg Vasilyev","Randy Sawaya"],"pdf_url":"","comment":null}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2512.21338v1","updated":"2025-12-24T18:59:58Z","published":"2025-12-24T18:59:58Z","title":"HiStream: Efficient High-Resolution Video Generation via Redundancy-Eliminated Streaming","summary":"High-resolution video generation, while crucial for digital media and film, is computationally bottlenecked by the quadratic complexity of diffusion models, making practical inference infeasible. To address this, we introduce HiStream, an efficient autoregressive framework that systematically reduces redundancy across three axes: i) Spatial Compression: denoising at low resolution before refining at high resolution with cached features; ii) Temporal Compression: a chunk-by-chunk strategy with a fixed-size anchor cache, ensuring stable inference speed; and iii) Timestep Compression: applying fewer denoising steps to subsequent, cache-conditioned chunks. On 1080p benchmarks, our primary HiStream model (i+ii) achieves state-of-the-art visual quality while demonstrating up to 76.2x faster denoising compared to the Wan2.1 baseline and negligible quality loss. Our faster variant, HiStream+, applies all three optimizations (i+ii+iii), achieving a 107.5x acceleration over the baseline, offering a compelling trade-off between speed and quality, thereby making high-resolution video generation both practical and scalable.","authors":["Haonan Qiu","Shikun Liu","Zijian Zhou","Zhaochong An","Weiming Ren","Zhiheng Liu","Jonas Schult","Sen He","Shoufa Chen","Yuren Cong","Tao Xiang","Ziwei Liu","Juan-Manuel Perez-Rua"],"pdf_url":"","comment":"Project Page: http://haonanqiu.com/projects/HiStream.html"},{"id":"http://arxiv.org/abs/2512.21337v1","updated":"2025-12-24T18:59:54Z","published":"2025-12-24T18:59:54Z","title":"Beyond Memorization: A Multi-Modal Ordinal Regression Benchmark to Expose Popularity Bias in Vision-Language Models","summary":"We expose a significant popularity bias in state-of-the-art vision-language models (VLMs), which achieve up to 34% higher accuracy on famous buildings compared to ordinary ones, indicating a reliance on memorization over generalizable understanding. To systematically investigate this, we introduce the largest open benchmark for this task: the YearGuessr dataset, a collection of 55,546 building images with multi-modal attributes from 157 countries, annotated with continuous ordinal labels of their construction year (1001-2024), GPS data, and page-view counts as a proxy for popularity. Using this dataset, we frame the construction year prediction task as ordinal regression and introduce popularity-aware interval accuracy metrics to quantify this bias. Our resulting benchmark of 30+ models, including our YearCLIP model, confirms that VLMs excel on popular, memorized items but struggle significantly with unrecognized subjects, exposing a critical flaw in their reasoning capabilities. Project page: https://sytwu.github.io/BeyondMemo/","authors":["Li-Zhong Szu-Tu","Ting-Lin Wu","Chia-Jui Chang","He Syu","Yu-Lun Liu"],"pdf_url":"","comment":"Project page: https://sytwu.github.io/BeyondMemo/"},{"id":"http://arxiv.org/abs/2512.21334v1","updated":"2025-12-24T18:59:36Z","published":"2025-12-24T18:59:36Z","title":"Streaming Video Instruction Tuning","summary":"We present Streamo, a real-time streaming video LLM that serves as a general-purpose interactive assistant. Unlike existing online video models that focus narrowly on question answering or captioning, Streamo performs a broad spectrum of streaming video tasks, including real-time narration, action understanding, event captioning, temporal event grounding, and time-sensitive question answering. To develop such versatility, we construct Streamo-Instruct-465K, a large-scale instruction-following dataset tailored for streaming video understanding. The dataset covers diverse temporal contexts and multi-task supervision, enabling unified training across heterogeneous streaming tasks. After training end-to-end on the instruction-following dataset through a streamlined pipeline, Streamo exhibits strong temporal reasoning, responsive interaction, and broad generalization across a variety of streaming benchmarks. Extensive experiments show that Streamo bridges the gap between offline video perception models and real-time multimodal assistants, making a step toward unified, intelligent video understanding in continuous video streams.","authors":["Jiaer Xia","Peixian Chen","Mengdan Zhang","Xing Sun","Kaiyang Zhou"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.21333v1","updated":"2025-12-24T18:59:05Z","published":"2025-12-24T18:59:05Z","title":"Fast SAM2 with Text-Driven Token Pruning","summary":"Segment Anything Model 2 (SAM2), a vision foundation model has significantly advanced in prompt-driven video object segmentation, yet their practical deployment remains limited by the high computational and memory cost of processing dense visual tokens across time. The SAM2 pipelines typically propagate all visual tokens produced by the image encoder through downstream temporal reasoning modules, regardless of their relevance to the target object, resulting in reduced scalability due to quadratic memory attention overhead. In this work, we introduce a text-guided token pruning framework that improves inference efficiency by selectively reducing token density prior to temporal propagation, without modifying the underlying segmentation architecture. Operating after visual encoding and before memory based propagation, our method ranks tokens using a lightweight routing mechanism that integrates local visual context, semantic relevance derived from object-centric textual descriptions (either user-provided or automatically generated), and uncertainty cues that help preserve ambiguous or boundary critical regions. By retaining only the most informative tokens for downstream processing, the proposed approach reduces redundant computation while maintaining segmentation fidelity. Extensive experiments across multiple challenging video segmentation benchmarks demonstrate that post-encoder token pruning provides a practical and effective pathway to efficient, prompt-aware video segmentation, achieving up to 42.50 percent faster inference and 37.41 percent lower GPU memory usage compared to the unpruned baseline SAM2, while preserving competitive J and F performance. These results highlight the potential of early token selection to improve the scalability of transformer-based video segmentation systems for real-time and resource-constrained applications.","authors":["Avilasha Mandal","Chaoning Zhang","Fachrina Dewi Puspitasari","Xudong Wang","Jiaquan Zhang","Caiyan Qin","Guoqing Wang","Yang Yang","Heng Tao Shen"],"pdf_url":"","comment":"28 pages, 9 figures"},{"id":"http://arxiv.org/abs/2512.21331v1","updated":"2025-12-24T18:58:16Z","published":"2025-12-24T18:58:16Z","title":"TICON: A Slide-Level Tile Contextualizer for Histopathology Representation Learning","summary":"The interpretation of small tiles in large whole slide images (WSI) often needs a larger image context. We introduce TICON, a transformer-based tile representation contextualizer that produces rich, contextualized embeddings for ''any'' application in computational pathology. Standard tile encoder-based pipelines, which extract embeddings of tiles stripped from their context, fail to model the rich slide-level information essential for both local and global tasks. Furthermore, different tile-encoders excel at different downstream tasks. Therefore, a unified model is needed to contextualize embeddings derived from ''any'' tile-level foundation model. TICON addresses this need with a single, shared encoder, pretrained using a masked modeling objective to simultaneously unify and contextualize representations from diverse tile-level pathology foundation models. Our experiments demonstrate that TICON-contextualized embeddings significantly improve performance across many different tasks, establishing new state-of-the-art results on tile-level benchmarks (i.e., HEST-Bench, THUNDER, CATCH) and slide-level benchmarks (i.e., Patho-Bench). Finally, we pretrain an aggregator on TICON to form a slide-level foundation model, using only 11K WSIs, outperforming SoTA slide-level foundation models pretrained with up to 350K WSIs.","authors":["Varun Belagali","Saarthak Kapse","Pierre Marza","Srijan Das","Zilinghan Li","Sofiène Boutaj","Pushpak Pati","Srikar Yellapragada","Tarak Nath Nandi","Ravi K Madduri","Joel Saltz","Prateek Prasanna","Stergios Christodoulidis Maria Vakalopoulou","Dimitris Samaras"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2511.12870v2","updated":"2025-12-24T18:29:21Z","published":"2025-11-17T02:00:22Z","title":"View-aware Cross-modal Distillation for Multi-view Action Recognition","summary":"The widespread use of multi-sensor systems has increased research in multi-view action recognition. While existing approaches in multi-view setups with fully overlapping sensors benefit from consistent view coverage, partially overlapping settings where actions are visible in only a subset of views remain underexplored. This challenge becomes more severe in real-world scenarios, as many systems provide only limited input modalities and rely on sequence-level annotations instead of dense frame-level labels. In this study, we propose View-aware Cross-modal Knowledge Distillation (ViCoKD), a framework that distills knowledge from a fully supervised multi-modal teacher to a modality- and annotation-limited student. ViCoKD employs a cross-modal adapter with cross-modal attention, allowing the student to exploit multi-modal correlations while operating with incomplete modalities. Moreover, we propose a View-aware Consistency module to address view misalignment, where the same action may appear differently or only partially across viewpoints. It enforces prediction alignment when the action is co-visible across views, guided by human-detection masks and confidence-weighted Jensen-Shannon divergence between their predicted class distributions. Experiments on the real-world MultiSensor-Home dataset show that ViCoKD consistently outperforms competitive distillation methods across multiple backbones and environments, delivering significant gains and surpassing the teacher model under limited conditions.","authors":["Trung Thanh Nguyen","Yasutomo Kawanishi","Vijay John","Takahiro Komamizu","Ichiro Ide"],"pdf_url":"","comment":"IEEE/CVF Winter Conference on Applications of Computer Vision (WACV) 2026"},{"id":"http://arxiv.org/abs/2512.21315v1","updated":"2025-12-24T18:21:01Z","published":"2025-12-24T18:21:01Z","title":"Does the Data Processing Inequality Reflect Practice? On the Utility of Low-Level Tasks","summary":"The data processing inequality is an information-theoretic principle stating that the information content of a signal cannot be increased by processing the observations. In particular, it suggests that there is no benefit in enhancing the signal or encoding it before addressing a classification problem. This assertion can be proven to be true for the case of the optimal Bayes classifier. However, in practice, it is common to perform \"low-level\" tasks before \"high-level\" downstream tasks despite the overwhelming capabilities of modern deep neural networks. In this paper, we aim to understand when and why low-level processing can be beneficial for classification. We present a comprehensive theoretical study of a binary classification setup, where we consider a classifier that is tightly connected to the optimal Bayes classifier and converges to it as the number of training samples increases. We prove that for any finite number of training samples, there exists a pre-classification processing that improves the classification accuracy. We also explore the effect of class separation, training set size, and class balance on the relative gain from this procedure. We support our theory with an empirical investigation of the theoretical setup. Finally, we conduct an empirical study where we investigate the effect of denoising and encoding on the performance of practical deep classifiers on benchmark datasets. Specifically, we vary the size and class distribution of the training set, and the noise level, and demonstrate trends that are consistent with our theoretical results.","authors":["Roy Turgeman","Tom Tirer"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.21302v1","updated":"2025-12-24T17:40:42Z","published":"2025-12-24T17:40:42Z","title":"AndroidLens: Long-latency Evaluation with Nested Sub-targets for Android GUI Agents","summary":"Graphical user interface (GUI) agents can substantially improve productivity by automating frequently executed long-latency tasks on mobile devices. However, existing evaluation benchmarks are still constrained to limited applications, simple tasks, and coarse-grained metrics. To address this, we introduce AndroidLens, a challenging evaluation framework for mobile GUI agents, comprising 571 long-latency tasks in both Chinese and English environments, each requiring an average of more than 26 steps to complete. The framework features: (1) tasks derived from real-world user scenarios across 38 domains, covering complex types such as multi-constraint, multi-goal, and domain-specific tasks; (2) static evaluation that preserves real-world anomalies and allows multiple valid paths to reduce bias; and (3) dynamic evaluation that employs a milestone-based scheme for fine-grained progress measurement via Average Task Progress (ATP). Our evaluation indicates that even the best models reach only a 12.7% task success rate and 50.47% ATP. We also underscore key challenges in real-world environments, including environmental anomalies, adaptive exploration, and long-term memory retention.","authors":["Yue Cao","Yingyao Wang","Pi Bu","Jingxuan Xing","Wei Jiang","Zekun Zhu","Junpeng Ma","Sashuai Zhou","Tong Lu","Jun Song","Yu Cheng","Yuning Jiang","Bo Zheng"],"pdf_url":"","comment":"23 pages, 13 figures, 8 tables"},{"id":"http://arxiv.org/abs/2512.21287v1","updated":"2025-12-24T17:10:37Z","published":"2025-12-24T17:10:37Z","title":"Post-Processing Mask-Based Table Segmentation for Structural Coordinate Extraction","summary":"Structured data extraction from tables plays a crucial role in document image analysis for scanned documents and digital archives. Although many methods have been proposed to detect table structures and extract cell contents, accurately identifying table segment boundaries (rows and columns) remains challenging, particularly in low-resolution or noisy images. In many real-world scenarios, table data are incomplete or degraded, limiting the adaptability of transformer-based methods to noisy inputs. Mask-based edge detection techniques have shown greater robustness under such conditions, as their sensitivity can be adjusted through threshold tuning; however, existing approaches typically apply masks directly to images, leading to noise sensitivity, resolution loss, or high computational cost. This paper proposes a novel multi-scale signal-processing method for detecting table edges from table masks. Row and column transitions are modeled as one-dimensional signals and processed using Gaussian convolution with progressively increasing variances, followed by statistical thresholding to suppress noise while preserving stable structural edges. Detected signal peaks are mapped back to image coordinates to obtain accurate segment boundaries. Experimental results show that applying the proposed approach to column edge detection improves Cell-Aware Segmentation Accuracy (CASA) a layout-aware metric evaluating both textual correctness and correct cell placement from 67% to 76% on the PubLayNet-1M benchmark when using TableNet with PyTesseract OCR. The method is robust to resolution variations through zero-padding and scaling strategies and produces optimized structured tabular outputs suitable for downstream analysis.","authors":["Suren Bandara"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.21284v1","updated":"2025-12-24T17:05:09Z","published":"2025-12-24T17:05:09Z","title":"Surgical Scene Segmentation using a Spike-Driven Video Transformer with Real-Time Potential","summary":"Modern surgical systems increasingly rely on intelligent scene understanding to provide timely situational awareness for enhanced intra-operative safety. Within this pipeline, surgical scene segmentation plays a central role in accurately perceiving operative events. Although recent deep learning models, particularly large-scale foundation models, achieve remarkable segmentation accuracy, their substantial computational demands and power consumption hinder real-time deployment in resource-constrained surgical environments. To address this limitation, we explore the emerging SNN as a promising paradigm for highly efficient surgical intelligence. However, their performance is still constrained by the scarcity of labeled surgical data and the inherently sparse nature of surgical video representations. To this end, we propose \\textit{SpikeSurgSeg}, the first spike-driven video Transformer framework tailored for surgical scene segmentation with real-time potential on non-GPU platforms. To address the limited availability of surgical annotations, we introduce a surgical-scene masked autoencoding pretraining strategy for SNNs that enables robust spatiotemporal representation learning via layer-wise tube masking. Building on this pretrained backbone, we further adopt a lightweight spike-driven segmentation head that produces temporally consistent predictions while preserving the low-latency characteristics of SNNs. Extensive experiments on EndoVis18 and our in-house SurgBleed dataset demonstrate that SpikeSurgSeg achieves mIoU comparable to SOTA ANN-based models while reducing inference latency by at least $8\\times$. Notably, it delivers over $20\\times$ acceleration relative to most foundation-model baselines, underscoring its potential for time-critical surgical scene segmentation.","authors":["Shihao Zou","Jingjing Li","Wei Ji","Jincai Huang","Kai Wang","Guo Dan","Weixin Si","Yi Pan"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.21276v1","updated":"2025-12-24T16:46:04Z","published":"2025-12-24T16:46:04Z","title":"GriDiT: Factorized Grid-Based Diffusion for Efficient Long Image Sequence Generation","summary":"Modern deep learning methods typically treat image sequences as large tensors of sequentially stacked frames. However, is this straightforward representation ideal given the current state-of-the-art (SoTA)? In this work, we address this question in the context of generative models and aim to devise a more effective way of modeling image sequence data. Observing the inefficiencies and bottlenecks of current SoTA image sequence generation methods, we showcase that rather than working with large tensors, we can improve the generation process by factorizing it into first generating the coarse sequence at low resolution and then refining the individual frames at high resolution. We train a generative model solely on grid images comprising subsampled frames. Yet, we learn to generate image sequences, using the strong self-attention mechanism of the Diffusion Transformer (DiT) to capture correlations between frames. In effect, our formulation extends a 2D image generator to operate as a low-resolution 3D image-sequence generator without introducing any architectural modifications. Subsequently, we super-resolve each frame individually to add the sequence-independent high-resolution details. This approach offers several advantages and can overcome key limitations of the SoTA in this domain. Compared to existing image sequence generation models, our method achieves superior synthesis quality and improved coherence across sequences. It also delivers high-fidelity generation of arbitrary-length sequences and increased efficiency in inference time and training data usage. Furthermore, our straightforward formulation enables our method to generalize effectively across diverse data domains, which typically require additional priors and supervision to model in a generative context. Our method consistently outperforms SoTA in quality and inference speed (at least twice-as-fast) across datasets.","authors":["Snehal Singh Tomar","Alexandros Graikos","Arjun Krishna","Dimitris Samaras","Klaus Mueller"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.19823v2","updated":"2025-12-24T16:32:32Z","published":"2025-12-22T19:29:57Z","title":"Learning to Refocus with Video Diffusion Models","summary":"Focus is a cornerstone of photography, yet autofocus systems often fail to capture the intended subject, and users frequently wish to adjust focus after capture. We introduce a novel method for realistic post-capture refocusing using video diffusion models. From a single defocused image, our approach generates a perceptually accurate focal stack, represented as a video sequence, enabling interactive refocusing and unlocking a range of downstream applications. We release a large-scale focal stack dataset acquired under diverse real-world smartphone conditions to support this work and future research. Our method consistently outperforms existing approaches in both perceptual quality and robustness across challenging scenarios, paving the way for more advanced focus-editing capabilities in everyday photography. Code and data are available at https://learn2refocus.github.io","authors":["SaiKiran Tedla","Zhoutong Zhang","Xuaner Zhang","Shumian Xin"],"pdf_url":"","comment":"Code and data are available at https://learn2refocus.github.io . SIGGRAPH Asia 2025, Dec. 2025"},{"id":"http://arxiv.org/abs/2512.21268v1","updated":"2025-12-24T16:24:18Z","published":"2025-12-24T16:24:18Z","title":"ACD: Direct Conditional Control for Video Diffusion Models via Attention Supervision","summary":"Controllability is a fundamental requirement in video synthesis, where accurate alignment with conditioning signals is essential. Existing classifier-free guidance methods typically achieve conditioning indirectly by modeling the joint distribution of data and conditions, which often results in limited controllability over the specified conditions. Classifier-based guidance enforces conditions through an external classifier, but the model may exploit this mechanism to raise the classifier score without genuinely satisfying the intended condition, resulting in adversarial artifacts and limited effective controllability. In this paper, we propose Attention-Conditional Diffusion (ACD), a novel framework for direct conditional control in video diffusion models via attention supervision. By aligning the model's attention maps with external control signals, ACD achieves better controllability. To support this, we introduce a sparse 3D-aware object layout as an efficient conditioning signal, along with a dedicated Layout ControlNet and an automated annotation pipeline for scalable layout integration. Extensive experiments on benchmark video generation datasets demonstrate that ACD delivers superior alignment with conditioning inputs while preserving temporal coherence and visual fidelity, establishing an effective paradigm for conditional video synthesis.","authors":["Weiqi Li","Zehao Zhang","Liang Lin","Guangrun Wang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.21264v1","updated":"2025-12-24T16:16:09Z","published":"2025-12-24T16:16:09Z","title":"AnyAD: Unified Any-Modality Anomaly Detection in Incomplete Multi-Sequence MRI","summary":"Reliable anomaly detection in brain MRI remains challenging due to the scarcity of annotated abnormal cases and the frequent absence of key imaging modalities in real clinical workflows. Existing single-class or multi-class anomaly detection (AD) models typically rely on fixed modality configurations, require repetitive training, or fail to generalize to unseen modality combinations, limiting their clinical scalability. In this work, we present a unified Any-Modality AD framework that performs robust anomaly detection and localization under arbitrary MRI modality availability. The framework integrates a dual-pathway DINOv2 encoder with a feature distribution alignment mechanism that statistically aligns incomplete-modality features with full-modality representations, enabling stable inference even with severe modality dropout. To further enhance semantic consistency, we introduce an Intrinsic Normal Prototypes (INPs) extractor and an INP-guided decoder that reconstruct only normal anatomical patterns while naturally amplifying abnormal deviations. Through randomized modality masking and indirect feature completion during training, the model learns to adapt to all modality configurations without re-training. Extensive experiments on BraTS2018, MU-Glioma-Post, and Pretreat-MetsToBrain-Masks demonstrate that our approach consistently surpasses state-of-the-art industrial and medical AD baselines across 7 modality combinations, achieving superior generalization. This study establishes a scalable paradigm for multimodal medical AD under real-world, imperfect modality conditions. Our source code is available at https://github.com/wuchangw/AnyAD.","authors":["Changwei Wu","Yifei Chen","Yuxin Du","Mingxuan Liu","Jinying Zong","Beining Wu","Jie Dong","Feiwei Qin","Yunkang Cao","Qiyuan Tian"],"pdf_url":"","comment":"15 pages, 8 figures"},{"id":"http://arxiv.org/abs/2512.21252v1","updated":"2025-12-24T16:00:15Z","published":"2025-12-24T16:00:15Z","title":"DreaMontage: Arbitrary Frame-Guided One-Shot Video Generation","summary":"The \"one-shot\" technique represents a distinct and sophisticated aesthetic in filmmaking. However, its practical realization is often hindered by prohibitive costs and complex real-world constraints. Although emerging video generation models offer a virtual alternative, existing approaches typically rely on naive clip concatenation, which frequently fails to maintain visual smoothness and temporal coherence. In this paper, we introduce DreaMontage, a comprehensive framework designed for arbitrary frame-guided generation, capable of synthesizing seamless, expressive, and long-duration one-shot videos from diverse user-provided inputs. To achieve this, we address the challenge through three primary dimensions. (i) We integrate a lightweight intermediate-conditioning mechanism into the DiT architecture. By employing an Adaptive Tuning strategy that effectively leverages base training data, we unlock robust arbitrary-frame control capabilities. (ii) To enhance visual fidelity and cinematic expressiveness, we curate a high-quality dataset and implement a Visual Expression SFT stage. In addressing critical issues such as subject motion rationality and transition smoothness, we apply a Tailored DPO scheme, which significantly improves the success rate and usability of the generated content. (iii) To facilitate the production of extended sequences, we design a Segment-wise Auto-Regressive (SAR) inference strategy that operates in a memory-efficient manner. Extensive experiments demonstrate that our approach achieves visually striking and seamlessly coherent one-shot effects while maintaining computational efficiency, empowering users to transform fragmented visual materials into vivid, cohesive one-shot cinematic experiences.","authors":["Jiawei Liu","Junqiao Li","Jiangfan Deng","Gen Li","Siyu Zhou","Zetao Fang","Shanshan Lao","Zengde Deng","Jianing Zhu","Tingting Ma","Jiayi Li","Yunqiu Wang","Qian He","Xinglong Wu"],"pdf_url":"","comment":"Project Page: https://dreamontage.github.io/DreaMontage/"},{"id":"http://arxiv.org/abs/2512.17864v2","updated":"2025-12-24T15:38:23Z","published":"2025-12-19T18:11:15Z","title":"Interpretable Plant Leaf Disease Detection Using Attention-Enhanced CNN","summary":"Plant diseases pose a significant threat to global food security, necessitating accurate and interpretable disease detection methods. This study introduces an interpretable attention-guided Convolutional Neural Network (CNN), CBAM-VGG16, for plant leaf disease detection. By integrating Convolution Block Attention Module (CBAM) at each convolutional stage, the model enhances feature extraction and disease localization. Trained on five diverse plant disease datasets, our approach outperforms recent techniques, achieving high accuracy (up to 98.87%) and demonstrating robust generalization. Here, we show the effectiveness of our method through comprehensive evaluation and interpretability analysis using CBAM attention maps, Grad-CAM, Grad-CAM++, and Layer-wise Relevance Propagation (LRP). This study advances the application of explainable AI in agricultural diagnostics, offering a transparent and reliable system for smart farming. The code of our proposed work is available at https://github.com/BS0111/PlantAttentionCBAM.","authors":["Balram Singh","Ram Prakash Sharma","Somnath Dey"],"pdf_url":"","comment":"27 pages, 12 figures"},{"id":"http://arxiv.org/abs/2512.21241v1","updated":"2025-12-24T15:35:03Z","published":"2025-12-24T15:35:03Z","title":"Improving the Convergence Rate of Ray Search Optimization for Query-Efficient Hard-Label Attacks","summary":"In hard-label black-box adversarial attacks, where only the top-1 predicted label is accessible, the prohibitive query complexity poses a major obstacle to practical deployment. In this paper, we focus on optimizing a representative class of attacks that search for the optimal ray direction yielding the minimum $\\ell_2$-norm perturbation required to move a benign image into the adversarial region. Inspired by Nesterov's Accelerated Gradient (NAG), we propose a momentum-based algorithm, ARS-OPT, which proactively estimates the gradient with respect to a future ray direction inferred from accumulated momentum. We provide a theoretical analysis of its convergence behavior, showing that ARS-OPT enables more accurate directional updates and achieves faster, more stable optimization. To further accelerate convergence, we incorporate surrogate-model priors into ARS-OPT's gradient estimation, resulting in PARS-OPT with enhanced performance. The superiority of our approach is supported by theoretical guarantees under standard assumptions. Extensive experiments on ImageNet and CIFAR-10 demonstrate that our method surpasses 13 state-of-the-art approaches in query efficiency.","authors":["Xinjie Xu","Shuyu Cheng","Dongwei Xu","Qi Xuan","Chen Ma"],"pdf_url":"","comment":"Published at AAAI 2026 (Oral). This version corresponds to the conference proceedings; v2 will include the appendix"},{"id":"http://arxiv.org/abs/2512.21237v1","updated":"2025-12-24T15:26:11Z","published":"2025-12-24T15:26:11Z","title":"SegMo: Segment-aligned Text to 3D Human Motion Generation","summary":"Generating 3D human motions from textual descriptions is an important research problem with broad applications in video games, virtual reality, and augmented reality. Recent methods align the textual description with human motion at the sequence level, neglecting the internal semantic structure of modalities. However, both motion descriptions and motion sequences can be naturally decomposed into smaller and semantically coherent segments, which can serve as atomic alignment units to achieve finer-grained correspondence. Motivated by this, we propose SegMo, a novel Segment-aligned text-conditioned human Motion generation framework to achieve fine-grained text-motion alignment. Our framework consists of three modules: (1) Text Segment Extraction, which decomposes complex textual descriptions into temporally ordered phrases, each representing a simple atomic action; (2) Motion Segment Extraction, which partitions complete motion sequences into corresponding motion segments; and (3) Fine-grained Text-Motion Alignment, which aligns text and motion segments with contrastive learning. Extensive experiments demonstrate that SegMo improves the strong baseline on two widely used datasets, achieving an improved TOP 1 score of 0.553 on the HumanML3D test set. Moreover, thanks to the learned shared embedding space for text and motion segments, SegMo can also be applied to retrieval-style tasks such as motion grounding and motion-to-text retrieval.","authors":["Bowen Dang","Lin Wu","Xiaohang Yang","Zheng Yuan","Zhixiang Chen"],"pdf_url":"","comment":"The IEEE/CVF Winter Conference on Applications of Computer Vision 2026"},{"id":"http://arxiv.org/abs/2507.21968v2","updated":"2025-12-24T15:24:31Z","published":"2025-07-29T16:16:17Z","title":"Knowledge Augmentation via Synthetic Data: A Framework for Real-World ECG Image Classification","summary":"In real-world clinical practice, electrocardiograms (ECGs) are often captured and shared as photographs. However, publicly available ECG data, and thus most related research, relies on digital signals. This has led to a disconnect in which computer assisted interpretation of ECG cannot easily be applied to ECG images. The emergence of high-fidelity synthetic data generators has introduced practical alternatives by producing realistic, photo-like, ECG images derived from the digital signal that could help narrow this divide.\n  To address this, we propose a novel knowledge augmentation framework that uses synthetic data generated from multiple sources to provide generalisable and accurate interpretation of ECG photographs. Our framework features two key contributions. First, we introduce a robust pre-processing pipeline designed to remove background artifacts and reduces visual differences between images. Second, we implement a two-stage training strategy: a Morphology Learning Stage, where the model captures broad morphological features from visually different, scan-like synthetic data, followed by a Task-Specific Adaptation Stage, where the model is fine-tuned on the photo-like target data.\n  We tested the model on the British Heart Foundation Challenge dataset, to classify five common ECG findings: myocardial infarction (MI), atrial fibrillation, hypertrophy, conduction disturbance, and ST/T changes. Our approach, built upon the ConvNeXt backbone, outperforms a single-source training baseline and achieved \\textbf{1st} place in the challenge with an macro-AUROC of \\textbf{0.9677}. These results suggest that incorporating morphology learning from heterogeneous sources offers a more robust and generalizable paradigm than conventional single-source training.","authors":["Xiaoyu Wang","Ramesh Nadarajah","Zhiqiang Zhang","David Wong"],"pdf_url":"","comment":"10 pages, 6 figures"},{"id":"http://arxiv.org/abs/2512.21221v1","updated":"2025-12-24T15:02:33Z","published":"2025-12-24T15:02:33Z","title":"Leveraging Lightweight Entity Extraction for Scalable Event-Based Image Retrieval","summary":"Retrieving images from natural language descriptions is a core task at the intersection of computer vision and natural language processing, with wide-ranging applications in search engines, media archiving, and digital content management. However, real-world image-text retrieval remains challenging due to vague or context-dependent queries, linguistic variability, and the need for scalable solutions. In this work, we propose a lightweight two-stage retrieval pipeline that leverages event-centric entity extraction to incorporate temporal and contextual signals from real-world captions. The first stage performs efficient candidate filtering using BM25 based on salient entities, while the second stage applies BEiT-3 models to capture deep multimodal semantics and rerank the results. Evaluated on the OpenEvents v1 benchmark, our method achieves a mean average precision of 0.559, substantially outperforming prior baselines. These results highlight the effectiveness of combining event-guided filtering with long-text vision-language modeling for accurate and efficient retrieval in complex, real-world scenarios. Our code is available at https://github.com/PhamPhuHoa-23/Event-Based-Image-Retrieval","authors":["Dao Sy Duy Minh","Huynh Trung Kiet","Nguyen Lam Phu Quy","Phu-Hoa Pham","Tran Chi Nguyen"],"pdf_url":"","comment":"System description paper for EVENTA Grand Challenge Track 2 at ACM Multimedia 2025 (MM '25). Ranked 4th place. 6 pages, 1 figure, 2 tables"},{"id":"http://arxiv.org/abs/2512.21220v1","updated":"2025-12-24T15:01:26Z","published":"2025-12-24T15:01:26Z","title":"RoboSafe: Safeguarding Embodied Agents via Executable Safety Logic","summary":"Embodied agents powered by vision-language models (VLMs) are increasingly capable of executing complex real-world tasks, yet they remain vulnerable to hazardous instructions that may trigger unsafe behaviors. Runtime safety guardrails, which intercept hazardous actions during task execution, offer a promising solution due to their flexibility. However, existing defenses often rely on static rule filters or prompt-level control, which struggle to address implicit risks arising in dynamic, temporally dependent, and context-rich environments. To address this, we propose RoboSafe, a hybrid reasoning runtime safeguard for embodied agents through executable predicate-based safety logic. RoboSafe integrates two complementary reasoning processes on a Hybrid Long-Short Safety Memory. We first propose a Backward Reflective Reasoning module that continuously revisits recent trajectories in short-term memory to infer temporal safety predicates and proactively triggers replanning when violations are detected. We then propose a Forward Predictive Reasoning module that anticipates upcoming risks by generating context-aware safety predicates from the long-term safety memory and the agent's multimodal observations. Together, these components form an adaptive, verifiable safety logic that is both interpretable and executable as code. Extensive experiments across multiple agents demonstrate that RoboSafe substantially reduces hazardous actions (-36.8% risk occurrence) compared with leading baselines, while maintaining near-original task performance. Real-world evaluations on physical robotic arms further confirm its practicality. Code will be released upon acceptance.","authors":["Le Wang","Zonghao Ying","Xiao Yang","Quanchen Zou","Zhenfei Yin","Tianlin Li","Jian Yang","Yaodong Yang","Aishan Liu","Xianglong Liu"],"pdf_url":"","comment":"11 pages, 6 figures"},{"id":"http://arxiv.org/abs/2512.21218v1","updated":"2025-12-24T14:59:49Z","published":"2025-12-24T14:59:49Z","title":"Latent Implicit Visual Reasoning","summary":"While Large Multimodal Models (LMMs) have made significant progress, they remain largely text-centric, relying on language as their core reasoning modality. As a result, they are limited in their ability to handle reasoning tasks that are predominantly visual. Recent approaches have sought to address this by supervising intermediate visual steps with helper images, depth maps, or image crops. However, these strategies impose restrictive priors on what \"useful\" visual abstractions look like, add heavy annotation costs, and struggle to generalize across tasks. To address this critical limitation, we propose a task-agnostic mechanism that trains LMMs to discover and use visual reasoning tokens without explicit supervision. These tokens attend globally and re-encode the image in a task-adaptive way, enabling the model to extract relevant visual information without hand-crafted supervision. Our approach outperforms direct fine-tuning and achieves state-of-the-art results on a diverse range of vision-centric tasks -- including those where intermediate abstractions are hard to specify -- while also generalizing to multi-task instruction tuning.","authors":["Kelvin Li","Chuyi Shang","Leonid Karlinsky","Rogerio Feris","Trevor Darrell","Roei Herzig"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2510.23117v4","updated":"2025-12-24T14:58:32Z","published":"2025-10-27T08:38:17Z","title":"Seeing Structural Failure Before it Happens: An Image-Based Physics-Informed Neural Network (PINN) for Spaghetti Bridge Load Prediction","summary":"Physics Informed Neural Networks (PINNs) are gaining attention for their ability to embed physical laws into deep learning models, which is particularly useful in structural engineering tasks with limited data. This paper aims to explore the use of PINNs to predict the weight of small scale spaghetti bridges, a task relevant to understanding load limits and potential failure modes in simplified structural models. Our proposed framework incorporates physics-based constraints to the prediction model for improved performance. In addition to standard PINNs, we introduce a novel architecture named Physics Informed Kolmogorov Arnold Network (PIKAN), which blends universal function approximation theory with physical insights. The structural parameters provided as input to the model are collected either manually or through computer vision methods. Our dataset includes 15 real bridges, augmented to 100 samples, and our best model achieves an $R^2$ score of 0.9603 and a mean absolute error (MAE) of 10.50 units. From applied perspective, we also provide a web based interface for parameter entry and prediction. These results show that PINNs can offer reliable estimates of structural weight, even with limited data, and may help inform early stage failure analysis in lightweight bridge designs.\n  The complete data and code are available at https://github.com/OmerJauhar/PINNS-For-Spaghetti-Bridges.","authors":["Omer Jauhar Khan","Sudais Khan","Hafeez Anwar","Shahzeb Khan","Shams Ul Arifeen","Farman Ullah"],"pdf_url":"","comment":"14 pages, 21 figures. Preprint"},{"id":"http://arxiv.org/abs/2508.21010v2","updated":"2025-12-24T14:52:45Z","published":"2025-08-28T17:10:53Z","title":"ChainReaction: Causal Chain-Guided Reasoning for Modular and Explainable Causal-Why Video Question Answering","summary":"Existing Causal-Why Video Question Answering (VideoQA) models often struggle with higher-order reasoning, relying on opaque, monolithic pipelines that entangle video understanding, causal inference, and answer generation. These black-box approaches offer limited interpretability and tend to depend on shallow heuristics. We propose a novel, modular paradigm that explicitly decouples causal reasoning from answer generation, introducing natural language causal chains as interpretable intermediate representations. Inspired by human cognitive models, these structured cause-effect sequences bridge low-level video content with high-level causal reasoning, enabling transparent and logically coherent inference. Our two-stage architecture comprises a Causal Chain Extractor (CCE) that generates causal chains from video-question pairs, and a Causal Chain-Driven Answerer (CCDA) that derives answers grounded in these chains. To address the lack of annotated reasoning traces, we introduce a scalable method for generating accurate causal chains from existing datasets. We construct human verified causal chains for 46K samples. We also propose CauCo, a new evaluation metric for causality-oriented captioning. Experiments on three large-scale benchmarks demonstrate that our approach not only outperforms state-of-the-art models, but also yields substantial gains in explainability, user trust, and generalization -- positioning the CCE as a reusable causal reasoning engine across diverse domains. Project page: https://paritoshparmar.github.io/chainreaction/","authors":["Paritosh Parmar","Eric Peh","Basura Fernando"],"pdf_url":"","comment":"Project page: https://paritoshparmar.github.io/chainreaction/"},{"id":"http://arxiv.org/abs/2512.21209v1","updated":"2025-12-24T14:44:51Z","published":"2025-12-24T14:44:51Z","title":"Human Motion Estimation with Everyday Wearables","summary":"While on-body device-based human motion estimation is crucial for applications such as XR interaction, existing methods often suffer from poor wearability, expensive hardware, and cumbersome calibration, which hinder their adoption in daily life. To address these challenges, we present EveryWear, a lightweight and practical human motion capture approach based entirely on everyday wearables: a smartphone, smartwatch, earbuds, and smart glasses equipped with one forward-facing and two downward-facing cameras, requiring no explicit calibration before use. We introduce Ego-Elec, a 9-hour real-world dataset covering 56 daily activities across 17 diverse indoor and outdoor environments, with ground-truth 3D annotations provided by the motion capture (MoCap), to facilitate robust research and benchmarking in this direction. Our approach employs a multimodal teacher-student framework that integrates visual cues from egocentric cameras with inertial signals from consumer devices. By training directly on real-world data rather than synthetic data, our model effectively eliminates the sim-to-real gap that constrains prior work. Experiments demonstrate that our method outperforms baseline models, validating its effectiveness for practical full-body motion estimation.","authors":["Siqi Zhu","Yixuan Li","Junfu Li","Qi Wu","Zan Wang","Haozhe Ma","Wei Liang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.21201v1","updated":"2025-12-24T14:28:17Z","published":"2025-12-24T14:28:17Z","title":"Schrödinger's Navigator: Imagining an Ensemble of Futures for Zero-Shot Object Navigation","summary":"Zero-shot object navigation (ZSON) requires a robot to locate a target object in a previously unseen environment without relying on pre-built maps or task-specific training. However, existing ZSON methods often struggle in realistic and cluttered environments, particularly when the scene contains heavy occlusions, unknown risks, or dynamically moving target objects. To address these challenges, we propose \\textbf{Schrödinger's Navigator}, a navigation framework inspired by Schrödinger's thought experiment on uncertainty. The framework treats unobserved space as a set of plausible future worlds and reasons over them before acting. Conditioned on egocentric visual inputs and three candidate trajectories, a trajectory-conditioned 3D world model imagines future observations along each path. This enables the agent to see beyond occlusions and anticipate risks in unseen regions without requiring extra detours or dense global mapping. The imagined 3D observations are fused into the navigation map and used to update a value map. These updates guide the policy toward trajectories that avoid occlusions, reduce exposure to uncertain space, and better track moving targets. Experiments on a Go2 quadruped robot across three challenging scenarios, including severe static occlusions, unknown risks, and dynamically moving targets, show that Schrödinger's Navigator consistently outperforms strong ZSON baselines in self-localization, object localization, and overall Success Rate in occlusion-heavy environments. These results demonstrate the effectiveness of trajectory-conditioned 3D imagination in enabling robust zero-shot object navigation.","authors":["Yu He","Da Huang","Zhenyang Liu","Zixiao Gu","Qiang Sun","Guangnan Ye","Yanwei Fu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.21194v1","updated":"2025-12-24T14:18:38Z","published":"2025-12-24T14:18:38Z","title":"VisRes Bench: On Evaluating the Visual Reasoning Capabilities of VLMs","summary":"Vision-Language Models (VLMs) have achieved remarkable progress across tasks such as visual question answering and image captioning. Yet, the extent to which these models perform visual reasoning as opposed to relying on linguistic priors remains unclear. To address this, we introduce VisRes Bench, a benchmark designed to study visual reasoning in naturalistic settings without contextual language supervision. Analyzing model behavior across three levels of complexity, we uncover clear limitations in perceptual and relational visual reasoning capacities. VisRes isolates distinct reasoning abilities across its levels. Level 1 probes perceptual completion and global image matching under perturbations such as blur, texture changes, occlusion, and rotation; Level 2 tests rule-based inference over a single attribute (e.g., color, count, orientation); and Level 3 targets compositional reasoning that requires integrating multiple visual attributes. Across more than 19,000 controlled task images, we find that state-of-the-art VLMs perform near random under subtle perceptual perturbations, revealing limited abstraction beyond pattern recognition. We conclude by discussing how VisRes provides a unified framework for advancing abstract visual reasoning in multimodal research.","authors":["Brigitta Malagurski Törtei","Yasser Dahou","Ngoc Dung Huynh","Wamiq Reyaz Para","Phúc H. Lê Khac","Ankit Singh","Sofian Chaybouti","Sanath Narayan"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.21185v1","updated":"2025-12-24T14:08:38Z","published":"2025-12-24T14:08:38Z","title":"UltraShape 1.0: High-Fidelity 3D Shape Generation via Scalable Geometric Refinement","summary":"In this report, we introduce UltraShape 1.0, a scalable 3D diffusion framework for high-fidelity 3D geometry generation. The proposed approach adopts a two-stage generation pipeline: a coarse global structure is first synthesized and then refined to produce detailed, high-quality geometry. To support reliable 3D generation, we develop a comprehensive data processing pipeline that includes a novel watertight processing method and high-quality data filtering. This pipeline improves the geometric quality of publicly available 3D datasets by removing low-quality samples, filling holes, and thickening thin structures, while preserving fine-grained geometric details. To enable fine-grained geometry refinement, we decouple spatial localization from geometric detail synthesis in the diffusion process. We achieve this by performing voxel-based refinement at fixed spatial locations, where voxel queries derived from coarse geometry provide explicit positional anchors encoded via RoPE, allowing the diffusion model to focus on synthesizing local geometric details within a reduced, structured solution space. Our model is trained exclusively on publicly available 3D datasets, achieving strong geometric quality despite limited training resources. Extensive evaluations demonstrate that UltraShape 1.0 performs competitively with existing open-source methods in both data processing quality and geometry generation. All code and trained models will be released to support future research.","authors":["Tanghui Jia","Dongyu Yan","Dehao Hao","Yang Li","Kaiyi Zhang","Xianyi He","Lanjiong Li","Jinnan Chen","Lutao Jiang","Qishen Yin","Long Quan","Ying-Cong Chen","Li Yuan"],"pdf_url":"","comment":"14 pages, 10 figures, Technical Report,"},{"id":"http://arxiv.org/abs/2512.21183v1","updated":"2025-12-24T14:07:04Z","published":"2025-12-24T14:07:04Z","title":"Towards Arbitrary Motion Completing via Hierarchical Continuous Representation","summary":"Physical motions are inherently continuous, and higher camera frame rates typically contribute to improved smoothness and temporal coherence. For the first time, we explore continuous representations of human motion sequences, featuring the ability to interpolate, inbetween, and even extrapolate any input motion sequences at arbitrary frame rates. To achieve this, we propose a novel parametric activation-induced hierarchical implicit representation framework, referred to as NAME, based on Implicit Neural Representations (INRs). Our method introduces a hierarchical temporal encoding mechanism that extracts features from motion sequences at multiple temporal scales, enabling effective capture of intricate temporal patterns. Additionally, we integrate a custom parametric activation function, powered by Fourier transformations, into the MLP-based decoder to enhance the expressiveness of the continuous representation. This parametric formulation significantly augments the model's ability to represent complex motion behaviors with high accuracy. Extensive evaluations across several benchmark datasets demonstrate the effectiveness and robustness of our proposed approach.","authors":["Chenghao Xu","Guangtao Lyu","Qi Liu","Jiexi Yan","Muli Yang","Cheng Deng"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.21180v1","updated":"2025-12-24T13:59:43Z","published":"2025-12-24T13:59:43Z","title":"Equivariant Multiscale Learned Invertible Reconstruction for Cone Beam CT: From Simulated to Real Data","summary":"Cone Beam CT (CBCT) is an important imaging modality nowadays, however lower image quality of CBCT compared to more conventional Computed Tomography (CT) remains a limiting factor in CBCT applications. Deep learning reconstruction methods are a promising alternative to classical analytical and iterative reconstruction methods, but applying such methods to CBCT is often difficult due to the lack of ground truth data, memory limitations and the need for fast inference at clinically-relevant resolutions. In this work we propose LIRE++, an end-to-end rotationally-equivariant multiscale learned invertible primal-dual scheme for fast and memory-efficient CBCT reconstruction. Memory optimizations and multiscale reconstruction allow for fast training and inference, while rotational equivariance improves parameter efficiency. LIRE++ was trained on simulated projection data from a fast quasi-Monte Carlo CBCT projection simulator that we developed as well. Evaluated on synthetic data, LIRE++ gave an average improvement of 1 dB in Peak Signal-to-Noise Ratio over alternative deep learning baselines. On real clinical data, LIRE++ improved the average Mean Absolute Error between the reconstruction and the corresponding planning CT by 10 Hounsfield Units with respect to current proprietary state-of-the-art hybrid deep-learning/iterative method.","authors":["Nikita Moriakov","Efstratios Gavves","Jonathan H. Mason","Carmen Seller-Oria","Jonas Teuwen","Jan-Jakob Sonke"],"pdf_url":"","comment":"29 pages. arXiv admin note: substantial text overlap with arXiv:2401.11256"},{"id":"http://arxiv.org/abs/2512.02789v3","updated":"2025-12-24T13:57:31Z","published":"2025-12-02T14:04:30Z","title":"TrackNetV5: Residual-Driven Spatio-Temporal Refinement and Motion Direction Decoupling for Fast Object Tracking","summary":"The TrackNet series has established a strong baseline for fast-moving small object tracking in sports. However, existing iterations face significant limitations: V1-V3 struggle with occlusions due to a reliance on purely visual cues, while TrackNetV4, despite introducing motion inputs, suffers from directional ambiguity as its absolute difference method discards motion polarity. To overcome these bottlenecks, we propose TrackNetV5, a robust architecture integrating two novel mechanisms. First, to recover lost directional priors, we introduce the Motion Direction Decoupling (MDD) module. Unlike V4, MDD decomposes temporal dynamics into signed polarity fields, explicitly encoding both movement occurrence and trajectory direction. Second, we propose the Residual-Driven Spatio-Temporal Refinement (R-STR) head. Operating on a coarse-to-fine paradigm, this Transformer-based module leverages factorized spatio-temporal contexts to estimate a corrective residual, effectively recovering occluded targets. Extensive experiments on the TrackNetV2 dataset demonstrate that TrackNetV5 achieves a new state-of-the-art F1-score of 0.9859 and an accuracy of 0.9733, significantly outperforming previous versions. Notably, this performance leap is achieved with a marginal 3.7% increase in FLOPs compared to V4, maintaining real-time inference capabilities while delivering superior tracking precision.","authors":["Haonan Tang","Yanjun Chen","Lezhi Jiang","Qianfei Li","Xinyu Guo"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.21174v1","updated":"2025-12-24T13:48:22Z","published":"2025-12-24T13:48:22Z","title":"A Turn Toward Better Alignment: Few-Shot Generative Adaptation with Equivariant Feature Rotation","summary":"Few-shot image generation aims to effectively adapt a source generative model to a target domain using very few training images. Most existing approaches introduce consistency constraints-typically through instance-level or distribution-level loss functions-to directly align the distribution patterns of source and target domains within their respective latent spaces. However, these strategies often fall short: overly strict constraints can amplify the negative effects of the domain gap, leading to distorted or uninformative content, while overly relaxed constraints may fail to leverage the source domain effectively. This limitation primarily stems from the inherent discrepancy in the underlying distribution structures of the source and target domains. The scarcity of target samples further compounds this issue by hindering accurate estimation of the target domain's distribution. To overcome these limitations, we propose Equivariant Feature Rotation (EFR), a novel adaptation strategy that aligns source and target domains at two complementary levels within a self-rotated proxy feature space. Specifically, we perform adaptive rotations within a parameterized Lie Group to transform both source and target features into an equivariant proxy space, where alignment is conducted. These learnable rotation matrices serve to bridge the domain gap by preserving intra-domain structural information without distortion, while the alignment optimization facilitates effective knowledge transfer from the source to the target domain. Comprehensive experiments on a variety of commonly used datasets demonstrate that our method significantly enhances the generative performance within the targeted domain.","authors":["Chenghao Xu","Qi Liu","Jiexi Yan","Muli Yang","Cheng Deng"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2505.11998v3","updated":"2025-12-24T13:44:53Z","published":"2025-05-17T13:19:01Z","title":"Parameter Efficient Continual Learning with Dynamic Low-Rank Adaptation","summary":"Catastrophic forgetting has remained a critical challenge for deep neural networks in Continual Learning (CL) as it undermines consolidated knowledge when learning new tasks. Parameter efficient fine tuning CL techniques are gaining traction for their effectiveness in addressing catastrophic forgetting with a lightweight training schedule while avoiding degradation of consolidated knowledge in pre-trained models. However, low rank adapters (LoRA) in these approaches are highly sensitive to rank selection which can lead to sub-optimal resource allocation and performance. To this end, we introduce PEARL, a rehearsal-free CL framework that entails dynamic rank allocation for LoRA components during CL training. Specifically, PEARL leverages reference task weights and adaptively determines the rank of task-specific LoRA components based on the current tasks' proximity to reference task weights in parameter space. To demonstrate the versatility of PEARL, we evaluate it across three vision architectures (ResNet, Separable Convolutional Network and Vision Transformer) and a multitude of CL scenarios, and show that PEARL outperforms all considered baselines by a large margin.","authors":["Prashant Shivaram Bhat","Shakib Yazdani","Elahe Arani","Bahram Zonooz"],"pdf_url":"","comment":"27 pages, 5 figures"},{"id":"http://arxiv.org/abs/2510.16416v3","updated":"2025-12-24T13:40:37Z","published":"2025-10-18T09:22:40Z","title":"SSL4RL: Revisiting Self-supervised Learning as Intrinsic Reward for Visual-Language Reasoning","summary":"Vision-language models (VLMs) have shown remarkable abilities by integrating large language models with visual inputs. However, they often fail to utilize visual evidence adequately, either depending on linguistic priors in vision-centric tasks or resorting to textual shortcuts during reasoning. Although reinforcement learning (RL) can align models with desired behaviors, its application to VLMs has been hindered by the lack of scalable and reliable reward mechanisms. To overcome this challenge, we propose SSL4RL, a novel framework that leverages self-supervised learning (SSL) tasks as a source of verifiable rewards for RL-based fine-tuning. Our approach reformulates SSL objectives-such as predicting image rotation or reconstructing masked patches-into dense, automatic reward signals, eliminating the need for human preference data or unreliable AI evaluators. Experiments show that SSL4RL substantially improves performance on both vision-centric and vision-language reasoning benchmarks. Furthermore, through systematic ablations, we identify key factors-such as task difficulty, model scale, and semantic alignment with the target domain-that influence the effectiveness of SSL4RL tasks, offering new design principles for future work. We also demonstrate the framework's generality by applying it to graph learning, where it yields significant gains. SSL4RL establishes a versatile and effective paradigm for aligning multimodal models using verifiable, self-supervised objectives.","authors":["Xiaojun Guo","Runyu Zhou","Yifei Wang","Qi Zhang","Chenheng Zhang","Stefanie Jegelka","Xiaohan Wang","Jiajun Chai","Guojun Yin","Wei Lin","Yisen Wang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2506.00474v2","updated":"2025-12-24T13:19:04Z","published":"2025-05-31T08:45:02Z","title":"A European Multi-Center Breast Cancer MRI Dataset","summary":"Early detection of breast cancer is critical for improving patient outcomes. While mammography remains the primary screening modality, magnetic resonance imaging (MRI) is increasingly recommended as a supplemental tool for women with dense breast tissue and those at elevated risk. However, the acquisition and interpretation of multiparametric breast MRI are time-consuming and require specialized expertise, limiting scalability in clinical practice. Artificial intelligence (AI) methods have shown promise in supporting breast MRI interpretation, but their development is hindered by the limited availability of large, diverse, and publicly accessible datasets. To address this gap, we present a publicly available, multi-center breast MRI dataset collected across six clinical institutions in five European countries. The dataset comprises 741 examinations from women undergoing screening or diagnostic breast MRI and includes malignant, benign, and non-lesion cases. Data were acquired using heterogeneous scanners, field strengths, and acquisition protocols, reflecting real-world clinical variability. In addition, we report baseline benchmark experiments using a transformer-based model to illustrate potential use cases of the dataset and to provide reference performance for future methodological comparisons.","authors":["Gustav Müller-Franzes","Lorena Escudero Sánchez","Nicholas Payne","Alexandra Athanasiou","Michael Kalogeropoulos","Aitor Lopez","Alfredo Miguel Soro Busto","Julia Camps Herrero","Nika Rasoolzadeh","Tianyu Zhang","Ritse Mann","Debora Jutz","Maike Bode","Christiane Kuhl","Yuan Gao","Wouter Veldhuis","Oliver Lester Saldanha","JieFu Zhu","Jakob Nikolas Kather","Daniel Truhn","Fiona J. Gilbert"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2412.06244v4","updated":"2025-12-24T13:11:11Z","published":"2024-12-09T06:34:23Z","title":"Unbiased Region-Language Alignment for Open-Vocabulary Dense Prediction","summary":"Pre-trained vision-language models (VLMs), such as CLIP, have demonstrated impressive zero-shot recognition capability, but still underperform in dense prediction tasks. Self-distillation recently is emerging as a promising approach for fine-tuning VLMs to better adapt to local regions without requiring extensive annotations. However, previous state-of-the-art approaches often suffer from significant `foreground bias', where models tend to wrongly identify background regions as foreground objects. To alleviate this issue, we propose DenseVLM, a framework designed to learn unbiased region-language alignment from powerful pre-trained VLM representations. To alleviate this issue, we propose DenseVLM, a framework designed to learn unbiased region-language alignment from powerful pre-trained VLM representations. DenseVLM leverages the pre-trained VLM to retrieve categories for unlabeled regions and then decouples the interference between foreground and background features. We show that DenseVLM can directly replace the original VLM in open-vocabulary object detection and image segmentation methods, leading to notable performance improvements. Furthermore, it exhibits promising zero-shot scalability when training on more extensive and diverse datasets. Our code is available at https://github.com/HVision-NKU/DenseVLM.","authors":["Yunheng Li","Yuxuan Li","Quansheng Zeng","Wenhai Wang","Qibin Hou","Ming-Ming Cheng"],"pdf_url":"","comment":"Accepted at ICCV 2025. The code is available at https://github.com/HVision-NKU/DenseVLM"},{"id":"http://arxiv.org/abs/2512.21150v1","updated":"2025-12-24T12:36:57Z","published":"2025-12-24T12:36:57Z","title":"ORCA: Object Recognition and Comprehension for Archiving Marine Species","summary":"Marine visual understanding is essential for monitoring and protecting marine ecosystems, enabling automatic and scalable biological surveys. However, progress is hindered by limited training data and the lack of a systematic task formulation that aligns domain-specific marine challenges with well-defined computer vision tasks, thereby limiting effective model application. To address this gap, we present ORCA, a multi-modal benchmark for marine research comprising 14,647 images from 478 species, with 42,217 bounding box annotations and 22,321 expert-verified instance captions. The dataset provides fine-grained visual and textual annotations that capture morphology-oriented attributes across diverse marine species. To catalyze methodological advances, we evaluate 18 state-of-the-art models on three tasks: object detection (closed-set and open-vocabulary), instance captioning, and visual grounding. Results highlight key challenges, including species diversity, morphological overlap, and specialized domain demands, underscoring the difficulty of marine understanding. ORCA thus establishes a comprehensive benchmark to advance research in marine domain. Project Page: http://orca.hkustvgd.com/.","authors":["Yuk-Kwan Wong","Haixin Liang","Zeyu Ma","Yiwei Chen","Ziqiang Zheng","Rinaldi Gotama","Pascal Sebastian","Lauren D. Sparks","Sai-Kit Yeung"],"pdf_url":"","comment":"Accepted by The IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), 2026"},{"id":"http://arxiv.org/abs/2512.15249v2","updated":"2025-12-24T12:33:48Z","published":"2025-12-17T09:47:29Z","title":"Intersectional Fairness in Vision-Language Models for Medical Image Disease Classification","summary":"Medical artificial intelligence (AI) systems, particularly multimodal vision-language models (VLM), often exhibit intersectional biases where models are systematically less confident in diagnosing marginalised patient subgroups. Such bias can lead to higher rates of inaccurate and missed diagnoses due to demographically skewed data and divergent distributions of diagnostic certainty. Current fairness interventions frequently fail to address these gaps or compromise overall diagnostic performance to achieve statistical parity among the subgroups. In this study, we developed Cross-Modal Alignment Consistency (CMAC-MMD), a training framework that standardises diagnostic certainty across intersectional patient subgroups. Unlike traditional debiasing methods, this approach equalises the model's decision confidence without requiring sensitive demographic data during clinical inference. We evaluated this approach using 10,015 skin lesion images (HAM10000) with external validation on 12,000 images (BCN20000), and 10,000 fundus images for glaucoma detection (Harvard-FairVLMed), stratifying performance by intersectional age, gender, and race attributes. In the dermatology cohort, the proposed method reduced the overall intersectional missed diagnosis gap (difference in True Positive Rate, $Δ$TPR) from 0.50 to 0.26 while improving the overall Area Under the Curve (AUC) from 0.94 to 0.97 compared to standard training. Similarly, for glaucoma screening, the method reduced $Δ$TPR from 0.41 to 0.31, achieving a better AUC of 0.72 (vs. 0.71 baseline). This establishes a scalable framework for developing high-stakes clinical decision support systems that are both accurate and can perform equitably across diverse patient subgroups, ensuring reliable performance without increasing privacy risks.","authors":["Yupeng Zhang","Adam G. Dunn","Usman Naseem","Jinman Kim"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.21135v1","updated":"2025-12-24T12:06:26Z","published":"2025-12-24T12:06:26Z","title":"TGC-Net: A Structure-Aware and Semantically-Aligned Framework for Text-Guided Medical Image Segmentation","summary":"Text-guided medical segmentation enhances segmentation accuracy by utilizing clinical reports as auxiliary information. However, existing methods typically rely on unaligned image and text encoders, which necessitate complex interaction modules for multimodal fusion. While CLIP provides a pre-aligned multimodal feature space, its direct application to medical imaging is limited by three main issues: insufficient preservation of fine-grained anatomical structures, inadequate modeling of complex clinical descriptions, and domain-specific semantic misalignment. To tackle these challenges, we propose TGC-Net, a CLIP-based framework focusing on parameter-efficient, task-specific adaptations. Specifically, it incorporates a Semantic-Structural Synergy Encoder (SSE) that augments CLIP's ViT with a CNN branch for multi-scale structural refinement, a Domain-Augmented Text Encoder (DATE) that injects large-language-model-derived medical knowledge, and a Vision-Language Calibration Module (VLCM) that refines cross-modal correspondence in a unified feature space. Experiments on five datasets across chest X-ray and thoracic CT modalities demonstrate that TGC-Net achieves state-of-the-art performance with substantially fewer trainable parameters, including notable Dice gains on challenging benchmarks.","authors":["Gaoren Lin","Huangxuan Zhao","Yuan Xiong","Lefei Zhang","Bo Du","Wentao Zhu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.21126v1","updated":"2025-12-24T11:57:50Z","published":"2025-12-24T11:57:50Z","title":"MarineEval: Assessing the Marine Intelligence of Vision-Language Models","summary":"We have witnessed promising progress led by large language models (LLMs) and further vision language models (VLMs) in handling various queries as a general-purpose assistant. VLMs, as a bridge to connect the visual world and language corpus, receive both visual content and various text-only user instructions to generate corresponding responses. Though great success has been achieved by VLMs in various fields, in this work, we ask whether the existing VLMs can act as domain experts, accurately answering marine questions, which require significant domain expertise and address special domain challenges/requirements. To comprehensively evaluate the effectiveness and explore the boundary of existing VLMs, we construct the first large-scale marine VLM dataset and benchmark called MarineEval, with 2,000 image-based question-answering pairs. During our dataset construction, we ensure the diversity and coverage of the constructed data: 7 task dimensions and 20 capacity dimensions. The domain requirements are specially integrated into the data construction and further verified by the corresponding marine domain experts. We comprehensively benchmark 17 existing VLMs on our MarineEval and also investigate the limitations of existing models in answering marine research questions. The experimental results reveal that existing VLMs cannot effectively answer the domain-specific questions, and there is still a large room for further performance improvements. We hope our new benchmark and observations will facilitate future research. Project Page: http://marineeval.hkustvgd.com/","authors":["YuK-Kwan Wong","Tuan-An To","Jipeng Zhang","Ziqiang Zheng","Sai-Kit Yeung"],"pdf_url":"","comment":"Accepted by The IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), 2026"},{"id":"http://arxiv.org/abs/2511.20325v2","updated":"2025-12-24T11:56:26Z","published":"2025-11-25T13:57:24Z","title":"AD-R1: Closed-Loop Reinforcement Learning for End-to-End Autonomous Driving with Impartial World Models","summary":"End-to-end models for autonomous driving hold the promise of learning complex behaviors directly from sensor data, but face critical challenges in safety and handling long-tail events. Reinforcement Learning (RL) offers a promising path to overcome these limitations, yet its success in autonomous driving has been elusive. We identify a fundamental flaw hindering this progress: a deep seated optimistic bias in the world models used for RL. To address this, we introduce a framework for post-training policy refinement built around an Impartial World Model. Our primary contribution is to teach this model to be honest about danger. We achieve this with a novel data synthesis pipeline, Counterfactual Synthesis, which systematically generates a rich curriculum of plausible collisions and off-road events. This transforms the model from a passive scene completer into a veridical forecaster that remains faithful to the causal link between actions and outcomes. We then integrate this Impartial World Model into our closed-loop RL framework, where it serves as an internal critic. During refinement, the agent queries the critic to ``dream\" of the outcomes for candidate actions. We demonstrate through extensive experiments, including on a new Risk Foreseeing Benchmark, that our model significantly outperforms baselines in predicting failures. Consequently, when used as a critic, it enables a substantial reduction in safety violations in challenging simulations, proving that teaching a model to dream of danger is a critical step towards building truly safe and intelligent autonomous agents.","authors":["Tianyi Yan","Tao Tang","Xingtai Gui","Yongkang Li","Jiasen Zhesng","Weiyao Huang","Lingdong Kong","Wencheng Han","Xia Zhou","Xueyang Zhang","Yifei Zhan","Kun Zhan","Cheng-zhong Xu","Jianbing Shen"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20619v2","updated":"2025-12-24T11:39:25Z","published":"2025-12-23T18:59:56Z","title":"SemanticGen: Video Generation in Semantic Space","summary":"State-of-the-art video generative models typically learn the distribution of video latents in the VAE space and map them to pixels using a VAE decoder. While this approach can generate high-quality videos, it suffers from slow convergence and is computationally expensive when generating long videos. In this paper, we introduce SemanticGen, a novel solution to address these limitations by generating videos in the semantic space. Our main insight is that, due to the inherent redundancy in videos, the generation process should begin in a compact, high-level semantic space for global planning, followed by the addition of high-frequency details, rather than directly modeling a vast set of low-level video tokens using bi-directional attention. SemanticGen adopts a two-stage generation process. In the first stage, a diffusion model generates compact semantic video features, which define the global layout of the video. In the second stage, another diffusion model generates VAE latents conditioned on these semantic features to produce the final output. We observe that generation in the semantic space leads to faster convergence compared to the VAE latent space. Our method is also effective and computationally efficient when extended to long video generation. Extensive experiments demonstrate that SemanticGen produces high-quality videos and outperforms state-of-the-art approaches and strong baselines.","authors":["Jianhong Bai","Xiaoshi Wu","Xintao Wang","Xiao Fu","Yuanxing Zhang","Qinghe Wang","Xiaoyu Shi","Menghan Xia","Zuozhu Liu","Haoji Hu","Pengfei Wan","Kun Gai"],"pdf_url":"","comment":"Project page: https://jianhongbai.github.io/SemanticGen/"},{"id":"http://arxiv.org/abs/2512.21118v1","updated":"2025-12-24T11:34:44Z","published":"2025-12-24T11:34:44Z","title":"STLDM: Spatio-Temporal Latent Diffusion Model for Precipitation Nowcasting","summary":"Precipitation nowcasting is a critical spatio-temporal prediction task for society to prevent severe damage owing to extreme weather events. Despite the advances in this field, the complex and stochastic nature of this task still poses challenges to existing approaches. Specifically, deterministic models tend to produce blurry predictions while generative models often struggle with poor accuracy. In this paper, we present a simple yet effective model architecture termed STLDM, a diffusion-based model that learns the latent representation from end to end alongside both the Variational Autoencoder and the conditioning network. STLDM decomposes this task into two stages: a deterministic forecasting stage handled by the conditioning network, and an enhancement stage performed by the latent diffusion model. Experimental results on multiple radar datasets demonstrate that STLDM achieves superior performance compared to the state of the art, while also improving inference efficiency. The code is available in https://github.com/sqfoo/stldm_official.","authors":["Shi Quan Foo","Chi-Ho Wong","Zhihan Gao","Dit-Yan Yeung","Ka-Hing Wong","Wai-Kin Wong"],"pdf_url":"","comment":"Accepted by TMLR. Camera-ready submission"},{"id":"http://arxiv.org/abs/2512.21104v1","updated":"2025-12-24T11:06:26Z","published":"2025-12-24T11:06:26Z","title":"FreeInpaint: Tuning-free Prompt Alignment and Visual Rationality Enhancement in Image Inpainting","summary":"Text-guided image inpainting endeavors to generate new content within specified regions of images using textual prompts from users. The primary challenge is to accurately align the inpainted areas with the user-provided prompts while maintaining a high degree of visual fidelity. While existing inpainting methods have produced visually convincing results by leveraging the pre-trained text-to-image diffusion models, they still struggle to uphold both prompt alignment and visual rationality simultaneously. In this work, we introduce FreeInpaint, a plug-and-play tuning-free approach that directly optimizes the diffusion latents on the fly during inference to improve the faithfulness of the generated images. Technically, we introduce a prior-guided noise optimization method that steers model attention towards valid inpainting regions by optimizing the initial noise. Furthermore, we meticulously design a composite guidance objective tailored specifically for the inpainting task. This objective efficiently directs the denoising process, enhancing prompt alignment and visual rationality by optimizing intermediate latents at each step. Through extensive experiments involving various inpainting diffusion models and evaluation metrics, we demonstrate the effectiveness and robustness of our proposed FreeInpaint.","authors":["Chao Gong","Dong Li","Yingwei Pan","Jingjing Chen","Ting Yao","Tao Mei"],"pdf_url":"","comment":"Accepted by AAAI 2026"},{"id":"http://arxiv.org/abs/2512.11831v3","updated":"2025-12-24T10:53:25Z","published":"2025-12-03T09:28:29Z","title":"On the Design of One-step Diffusion via Shortcutting Flow Paths","summary":"Recent advances in few-step diffusion models have demonstrated their efficiency and effectiveness by shortcutting the probabilistic paths of diffusion models, especially in training one-step diffusion models from scratch (\\emph{a.k.a.} shortcut models). However, their theoretical derivation and practical implementation are often closely coupled, which obscures the design space. To address this, we propose a common design framework for representative shortcut models. This framework provides theoretical justification for their validity and disentangles concrete component-level choices, thereby enabling systematic identification of improvements. With our proposed improvements, the resulting one-step model achieves a new state-of-the-art FID50k of 2.85 on ImageNet-256x256 under the classifier-free guidance setting with one step generation, and further reaches FID50k of 2.53 with 2x training steps. Remarkably, the model requires no pre-training, distillation, or curriculum learning. We believe our work lowers the barrier to component-level innovation in shortcut models and facilitates principled exploration of their design space.","authors":["Haitao Lin","Peiyan Hu","Minsi Ren","Zhifeng Gao","Zhi-Ming Ma","Guolin ke","Tailin Wu","Stan Z. Li"],"pdf_url":"","comment":"10 pages of main body, conference paper"},{"id":"http://arxiv.org/abs/2512.21099v1","updated":"2025-12-24T10:50:04Z","published":"2025-12-24T10:50:04Z","title":"TexAvatars : Hybrid Texel-3D Representations for Stable Rigging of Photorealistic Gaussian Head Avatars","summary":"Constructing drivable and photorealistic 3D head avatars has become a central task in AR/XR, enabling immersive and expressive user experiences. With the emergence of high-fidelity and efficient representations such as 3D Gaussians, recent works have pushed toward ultra-detailed head avatars. Existing approaches typically fall into two categories: rule-based analytic rigging or neural network-based deformation fields. While effective in constrained settings, both approaches often fail to generalize to unseen expressions and poses, particularly in extreme reenactment scenarios. Other methods constrain Gaussians to the global texel space of 3DMMs to reduce rendering complexity. However, these texel-based avatars tend to underutilize the underlying mesh structure. They apply minimal analytic deformation and rely heavily on neural regressors and heuristic regularization in UV space, which weakens geometric consistency and limits extrapolation to complex, out-of-distribution deformations. To address these limitations, we introduce TexAvatars, a hybrid avatar representation that combines the explicit geometric grounding of analytic rigging with the spatial continuity of texel space. Our approach predicts local geometric attributes in UV space via CNNs, but drives 3D deformation through mesh-aware Jacobians, enabling smooth and semantically meaningful transitions across triangle boundaries. This hybrid design separates semantic modeling from geometric control, resulting in improved generalization, interpretability, and stability. Furthermore, TexAvatars captures fine-grained expression effects, including muscle-induced wrinkles, glabellar lines, and realistic mouth cavity geometry, with high fidelity. Our method achieves state-of-the-art performance under extreme pose and expression variations, demonstrating strong generalization in challenging head reenactment settings.","authors":["Jaeseong Lee","Junyeong Ahn","Taewoong Kang","Jaegul Choo"],"pdf_url":"","comment":"3DV 2026, Project page with videos: https://summertight.github.io/TexAvatars/"},{"id":"http://arxiv.org/abs/2512.21095v1","updated":"2025-12-24T10:35:21Z","published":"2025-12-24T10:35:21Z","title":"UniRec-0.1B: Unified Text and Formula Recognition with 0.1B Parameters","summary":"Text and formulas constitute the core informational components of many documents. Accurately and efficiently recognizing both is crucial for developing robust and generalizable document parsing systems. Recently, vision-language models (VLMs) have achieved impressive unified recognition of text and formulas. However, they are large-sized and computationally demanding, restricting their usage in many applications. In this paper, we propose UniRec-0.1B, a unified recognition model with only 0.1B parameters. It is capable of performing text and formula recognition at multiple levels, including characters, words, lines, paragraphs, and documents. To implement this task, we first establish UniRec40M, a large-scale dataset comprises 40 million text, formula and their mix samples, enabling the training of a powerful yet lightweight model. Secondly, we identify two challenges when building such a lightweight but unified expert model. They are: structural variability across hierarchies and semantic entanglement between textual and formulaic content. To tackle these, we introduce a hierarchical supervision training that explicitly guides structural comprehension, and a semantic-decoupled tokenizer that separates text and formula representations. Finally, we develop a comprehensive evaluation benchmark covering Chinese and English documents from multiple domains and with multiple levels. Experimental results on this and public benchmarks demonstrate that UniRec-0.1B outperforms both general-purpose VLMs and leading document parsing expert models, while achieving a 2-9$\\times$ speedup, validating its effectiveness and efficiency. Codebase and Dataset: https://github.com/Topdu/OpenOCR.","authors":["Yongkun Du","Zhineng Chen","Yazhen Xie","Weikang Baiand Hao Feng","Wei Shi","Yuchen Su","Can Huang","Yu-Gang Jiang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.21094v1","updated":"2025-12-24T10:30:35Z","published":"2025-12-24T10:30:35Z","title":"T2AV-Compass: Towards Unified Evaluation for Text-to-Audio-Video Generation","summary":"Text-to-Audio-Video (T2AV) generation aims to synthesize temporally coherent video and semantically synchronized audio from natural language, yet its evaluation remains fragmented, often relying on unimodal metrics or narrowly scoped benchmarks that fail to capture cross-modal alignment, instruction following, and perceptual realism under complex prompts. To address this limitation, we present T2AV-Compass, a unified benchmark for comprehensive evaluation of T2AV systems, consisting of 500 diverse and complex prompts constructed via a taxonomy-driven pipeline to ensure semantic richness and physical plausibility. Besides, T2AV-Compass introduces a dual-level evaluation framework that integrates objective signal-level metrics for video quality, audio quality, and cross-modal alignment with a subjective MLLM-as-a-Judge protocol for instruction following and realism assessment. Extensive evaluation of 11 representative T2AVsystems reveals that even the strongest models fall substantially short of human-level realism and cross-modal consistency, with persistent failures in audio realism, fine-grained synchronization, instruction following, etc. These results indicate significant improvement room for future models and highlight the value of T2AV-Compass as a challenging and diagnostic testbed for advancing text-to-audio-video generation.","authors":["Zhe Cao","Tao Wang","Jiaming Wang","Yanghai Wang","Yuanxing Zhang","Jialu Chen","Miao Deng","Jiahao Wang","Yubin Guo","Chenxi Liao","Yize Zhang","Zhaoxiang Zhang","Jiaheng Liu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2210.13327v2","updated":"2025-12-24T10:04:30Z","published":"2022-10-24T15:28:43Z","title":"Deep Kronecker Network","summary":"We propose Deep Kronecker Network (DKN), a novel framework designed for analyzing medical imaging data, such as MRI, fMRI, CT, etc. Medical imaging data is different from general images in at least two aspects: i) sample size is usually much more limited, ii) model interpretation is more of a concern compared to outcome prediction. Due to its unique nature, general methods, such as convolutional neural network (CNN), are difficult to be directly applied. As such, we propose DKN, that is able to i) adapt to low sample size limitation, ii) provide desired model interpretation, and iii) achieve the prediction power as CNN. The DKN is general in the sense that it not only works for both matrix and (high-order) tensor represented image data, but also could be applied to both discrete and continuous outcomes. The DKN is built on a Kronecker product structure and implicitly imposes a piecewise smooth property on coefficients. Moreover, the Kronecker structure can be written into a convolutional form, so DKN also resembles a CNN, particularly, a fully convolutional network (FCN). Furthermore, we prove that with an alternating minimization algorithm, the solutions of DKN are guaranteed to converge to the truth geometrically even if the objective function is highly nonconvex. Interestingly, the DKN is also highly connected to the tensor regression framework proposed by Zhou et al. (2010), where a CANDECOMP/PARAFAC (CP) low-rank structure is imposed on tensor coefficients. Finally, we conduct both classification and regression analyses using real MRI data from the Alzheimer's Disease Neuroimaging Initiative (ADNI) to demonstrate the effectiveness of DKN.","authors":["Long Feng","Guang Yang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.21083v1","updated":"2025-12-24T09:58:30Z","published":"2025-12-24T09:58:30Z","title":"Hierarchical Modeling Approach to Fast and Accurate Table Recognition","summary":"The extraction and use of diverse knowledge from numerous documents is a pressing challenge in intelligent information retrieval. Documents contain elements that require different recognition methods. Table recognition typically consists of three subtasks, namely table structure, cell position and cell content recognition. Recent models have achieved excellent recognition with a combination of multi-task learning, local attention, and mutual learning. However, their effectiveness has not been fully explained, and they require a long period of time for inference. This paper presents a novel multi-task model that utilizes non-causal attention to capture the entire table structure, and a parallel inference algorithm for faster cell content inference. The superiority is demonstrated both visually and statistically on two large public datasets.","authors":["Takaya Kawakatsu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.21078v1","updated":"2025-12-24T09:55:16Z","published":"2025-12-24T09:55:16Z","title":"UniPR-3D: Towards Universal Visual Place Recognition with Visual Geometry Grounded Transformer","summary":"Visual Place Recognition (VPR) has been traditionally formulated as a single-image retrieval task. Using multiple views offers clear advantages, yet this setting remains relatively underexplored and existing methods often struggle to generalize across diverse environments. In this work we introduce UniPR-3D, the first VPR architecture that effectively integrates information from multiple views. UniPR-3D builds on a VGGT backbone capable of encoding multi-view 3D representations, which we adapt by designing feature aggregators and fine-tune for the place recognition task. To construct our descriptor, we jointly leverage the 3D tokens and intermediate 2D tokens produced by VGGT. Based on their distinct characteristics, we design dedicated aggregation modules for 2D and 3D features, allowing our descriptor to capture fine-grained texture cues while also reasoning across viewpoints. To further enhance generalization, we incorporate both single- and multi-frame aggregation schemes, along with a variable-length sequence retrieval strategy. Our experiments show that UniPR-3D sets a new state of the art, outperforming both single- and multi-view baselines and highlighting the effectiveness of geometry-grounded tokens for VPR. Our code and models will be made publicly available on Github https://github.com/dtc111111/UniPR-3D.","authors":["Tianchen Deng","Xun Chen","Ziming Li","Hongming Shen","Danwei Wang","Javier Civera","Hesheng Wang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.21065v1","updated":"2025-12-24T09:16:42Z","published":"2025-12-24T09:16:42Z","title":"Language-Guided Grasp Detection with Coarse-to-Fine Learning for Robotic Manipulation","summary":"Grasping is one of the most fundamental challenging capabilities in robotic manipulation, especially in unstructured, cluttered, and semantically diverse environments. Recent researches have increasingly explored language-guided manipulation, where robots not only perceive the scene but also interpret task-relevant natural language instructions. However, existing language-conditioned grasping methods typically rely on shallow fusion strategies, leading to limited semantic grounding and weak alignment between linguistic intent and visual grasp reasoning.In this work, we propose Language-Guided Grasp Detection (LGGD) with a coarse-to-fine learning paradigm for robotic manipulation. LGGD leverages CLIP-based visual and textual embeddings within a hierarchical cross-modal fusion pipeline, progressively injecting linguistic cues into the visual feature reconstruction process. This design enables fine-grained visual-semantic alignment and improves the feasibility of the predicted grasps with respect to task instructions. In addition, we introduce a language-conditioned dynamic convolution head (LDCH) that mixes multiple convolution experts based on sentence-level features, enabling instruction-adaptive coarse mask and grasp predictions. A final refinement module further enhances grasp consistency and robustness in complex scenes.Experiments on the OCID-VLG and Grasp-Anything++ datasets show that LGGD surpasses existing language-guided grasping methods, exhibiting strong generalization to unseen objects and diverse language queries. Moreover, deployment on a real robotic platform demonstrates the practical effectiveness of our approach in executing accurate, instruction-conditioned grasp actions. The code will be released publicly upon acceptance.","authors":["Zebin Jiang","Tianle Jin","Xiangtong Yao","Alois Knoll","Hu Cao"],"pdf_url":"","comment":"Submitted to IEEE Journal"},{"id":"http://arxiv.org/abs/2512.21064v1","updated":"2025-12-24T09:10:04Z","published":"2025-12-24T09:10:04Z","title":"Multimodal Skeleton-Based Action Representation Learning via Decomposition and Composition","summary":"Multimodal human action understanding is a significant problem in computer vision, with the central challenge being the effective utilization of the complementarity among diverse modalities while maintaining model efficiency. However, most existing methods rely on simple late fusion to enhance performance, which results in substantial computational overhead. Although early fusion with a shared backbone for all modalities is efficient, it struggles to achieve excellent performance. To address the dilemma of balancing efficiency and effectiveness, we introduce a self-supervised multimodal skeleton-based action representation learning framework, named Decomposition and Composition. The Decomposition strategy meticulously decomposes the fused multimodal features into distinct unimodal features, subsequently aligning them with their respective ground truth unimodal counterparts. On the other hand, the Composition strategy integrates multiple unimodal features, leveraging them as self-supervised guidance to enhance the learning of multimodal representations. Extensive experiments on the NTU RGB+D 60, NTU RGB+D 120, and PKU-MMD II datasets demonstrate that the proposed method strikes an excellent balance between computational cost and model performance.","authors":["Hongsong Wang","Heng Fei","Bingxuan Dai","Jie Gui"],"pdf_url":"","comment":"Accepted by Machine Intelligence Research (Journal Impact Factor 8.7, 2024)"},{"id":"http://arxiv.org/abs/2512.21058v1","updated":"2025-12-24T08:52:08Z","published":"2025-12-24T08:52:08Z","title":"Beyond Pixel Simulation: Pathology Image Generation via Diagnostic Semantic Tokens and Prototype Control","summary":"In computational pathology, understanding and generation have evolved along disparate paths: advanced understanding models already exhibit diagnostic-level competence, whereas generative models largely simulate pixels. Progress remains hindered by three coupled factors: the scarcity of large, high-quality image-text corpora; the lack of precise, fine-grained semantic control, which forces reliance on non-semantic cues; and terminological heterogeneity, where diverse phrasings for the same diagnostic concept impede reliable text conditioning. We introduce UniPath, a semantics-driven pathology image generation framework that leverages mature diagnostic understanding to enable controllable generation. UniPath implements Multi-Stream Control: a Raw-Text stream; a High-Level Semantics stream that uses learnable queries to a frozen pathology MLLM to distill paraphrase-robust Diagnostic Semantic Tokens and to expand prompts into diagnosis-aware attribute bundles; and a Prototype stream that affords component-level morphological control via a prototype bank. On the data front, we curate a 2.65M image-text corpus and a finely annotated, high-quality 68K subset to alleviate data scarcity. For a comprehensive assessment, we establish a four-tier evaluation hierarchy tailored to pathology. Extensive experiments demonstrate UniPath's SOTA performance, including a Patho-FID of 80.9 (51% better than the second-best) and fine-grained semantic control achieving 98.7% of the real-image. The meticulously curated datasets, complete source code, and pre-trained model weights developed in this study will be made openly accessible to the public.","authors":["Minghao Han","YiChen Liu","Yizhou Liu","Zizhi Chen","Jingqun Tang","Xuecheng Wu","Dingkang Yang","Lihua Zhang"],"pdf_url":"","comment":"32 pages, 17 figures, and 6 tables"},{"id":"http://arxiv.org/abs/2505.18736v2","updated":"2025-12-24T08:49:27Z","published":"2025-05-24T15:14:45Z","title":"Rethinking Direct Preference Optimization in Diffusion Models","summary":"Aligning text-to-image (T2I) diffusion models with human preferences has emerged as a critical research challenge. While recent advances in this area have extended preference optimization techniques from large language models (LLMs) to the diffusion setting, they often struggle with limited exploration. In this work, we propose a novel and orthogonal approach to enhancing diffusion-based preference optimization. First, we introduce a stable reference model update strategy that relaxes the frozen reference model, encouraging exploration while maintaining a stable optimization anchor through reference model regularization. Second, we present a timestep-aware training strategy that mitigates the reward scale imbalance problem across timesteps. Our method can be integrated into various preference optimization algorithms. Experimental results show that our approach improves the performance of state-of-the-art methods on human preference evaluation benchmarks. The code is available at the Github: https://github.com/kaist-cvml/RethinkingDPO_Diffusion_Models.","authors":["Junyong Kang","Seohyun Lim","Kyungjune Baek","Hyunjung Shim"],"pdf_url":"","comment":"Accepted by SPIGM@NeurIPS 2025 and AAAI-26 (Oral)"},{"id":"http://arxiv.org/abs/2512.21054v1","updated":"2025-12-24T08:44:58Z","published":"2025-12-24T08:44:58Z","title":"DexAvatar: 3D Sign Language Reconstruction with Hand and Body Pose Priors","summary":"The trend in sign language generation is centered around data-driven generative methods that require vast amounts of precise 2D and 3D human pose data to achieve an acceptable generation quality. However, currently, most sign language datasets are video-based and limited to automatically reconstructed 2D human poses (i.e., keypoints) and lack accurate 3D information. Furthermore, existing state-of-the-art for automatic 3D human pose estimation from sign language videos is prone to self-occlusion, noise, and motion blur effects, resulting in poor reconstruction quality. In response to this, we introduce DexAvatar, a novel framework to reconstruct bio-mechanically accurate fine-grained hand articulations and body movements from in-the-wild monocular sign language videos, guided by learned 3D hand and body priors. DexAvatar achieves strong performance in the SGNify motion capture dataset, the only benchmark available for this task, reaching an improvement of 35.11% in the estimation of body and hand poses compared to the state-of-the-art. The official website of this work is: https://github.com/kaustesseract/DexAvatar.","authors":["Kaustubh Kundu","Hrishav Bakul Barua","Lucy Robertson-Bell","Zhixi Cai","Kalin Stefanov"],"pdf_url":"","comment":"Accepted in WACV 2026"},{"id":"http://arxiv.org/abs/2512.21053v1","updated":"2025-12-24T08:40:57Z","published":"2025-12-24T08:40:57Z","title":"Optical Flow-Guided 6DoF Object Pose Tracking with an Event Camera","summary":"Object pose tracking is one of the pivotal technologies in multimedia, attracting ever-growing attention in recent years. Existing methods employing traditional cameras encounter numerous challenges such as motion blur, sensor noise, partial occlusion, and changing lighting conditions. The emerging bio-inspired sensors, particularly event cameras, possess advantages such as high dynamic range and low latency, which hold the potential to address the aforementioned challenges. In this work, we present an optical flow-guided 6DoF object pose tracking method with an event camera. A 2D-3D hybrid feature extraction strategy is firstly utilized to detect corners and edges from events and object models, which characterizes object motion precisely. Then, we search for the optical flow of corners by maximizing the event-associated probability within a spatio-temporal window, and establish the correlation between corners and edges guided by optical flow. Furthermore, by minimizing the distances between corners and edges, the 6DoF object pose is iteratively optimized to achieve continuous pose tracking. Experimental results of both simulated and real events demonstrate that our methods outperform event-based state-of-the-art methods in terms of both accuracy and robustness.","authors":["Zibin Liu","Banglei Guan","Yang Shang","Shunkun Liang","Zhenbao Yu","Qifeng Yu"],"pdf_url":"","comment":"9 pages, 5 figures. In Proceedings of the 32nd ACM International Conference on Multimedia (MM '24)"},{"id":"http://arxiv.org/abs/2512.21050v1","updated":"2025-12-24T08:31:44Z","published":"2025-12-24T08:31:44Z","title":"Matrix Completion Via Reweighted Logarithmic Norm Minimization","summary":"Low-rank matrix completion (LRMC) has demonstrated remarkable success in a wide range of applications. To address the NP-hard nature of the rank minimization problem, the nuclear norm is commonly used as a convex and computationally tractable surrogate for the rank function. However, this approach often yields suboptimal solutions due to the excessive shrinkage of singular values. In this letter, we propose a novel reweighted logarithmic norm as a more effective nonconvex surrogate, which provides a closer approximation than many existing alternatives. We efficiently solve the resulting optimization problem by employing the alternating direction method of multipliers (ADMM). Experimental results on image inpainting demonstrate that the proposed method achieves superior performance compared to state-of-the-art LRMC approaches, both in terms of visual quality and quantitative metrics.","authors":["Zhijie Wang","Liangtian He","Qinghua Zhang","Jifei Miao","Liang-Jian Deng","Jun Liu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2511.14368v2","updated":"2025-12-24T08:17:05Z","published":"2025-11-18T11:18:08Z","title":"O3SLM: Open Weight, Open Data, and Open Vocabulary Sketch-Language Model","summary":"While Large Vision Language Models (LVLMs) are increasingly deployed in real-world applications, their ability to interpret abstract visual inputs remains limited. Specifically, they struggle to comprehend hand-drawn sketches, a modality that offers an intuitive means of expressing concepts that are difficult to describe textually. We identify the primary bottleneck as the absence of a large-scale dataset that jointly models sketches, photorealistic images, and corresponding natural language instructions. To address this, we present two key contributions: (1) a new, large-scale dataset of image-sketch-instruction triplets designed to facilitate both pretraining and instruction tuning, and (2) O3SLM, an LVLM trained on this dataset. Comprehensive evaluations on multiple sketch-based tasks: (a) object localization, (b) counting, (c) image retrieval i.e., (SBIR and fine-grained SBIR), and (d) visual question answering (VQA); while incorporating the three existing sketch datasets, namely QuickDraw!, Sketchy, and Tu Berlin, along with our generated SketchVCL dataset, show that O3SLM achieves state-of-the-art performance, substantially outperforming existing LVLMs in sketch comprehension and reasoning.","authors":["Rishi Gupta","Mukilan Karuppasamy","Shyam Marjit","Aditay Tripathi","Anirban Chakraborty"],"pdf_url":"","comment":"Accepted to AAAI 2026"},{"id":"http://arxiv.org/abs/2512.21040v1","updated":"2025-12-24T08:07:39Z","published":"2025-12-24T08:07:39Z","title":"A Large-Depth-Range Layer-Based Hologram Dataset for Machine Learning-Based 3D Computer-Generated Holography","summary":"Machine learning-based computer-generated holography (ML-CGH) has advanced rapidly in recent years, yet progress is constrained by the limited availability of high-quality, large-scale hologram datasets. To address this, we present KOREATECH-CGH, a publicly available dataset comprising 6,000 pairs of RGB-D images and complex holograms across resolutions ranging from 256*256 to 2048*2048, with depth ranges extending to the theoretical limits of the angular spectrum method for wide 3D scene coverage. To improve hologram quality at large depth ranges, we introduce amplitude projection, a post-processing technique that replaces amplitude components of hologram wavefields at each depth layer while preserving phase. This approach enhances reconstruction fidelity, achieving 27.01 dB PSNR and 0.87 SSIM, surpassing a recent optimized silhouette-masking layer-based method by 2.03 dB and 0.04 SSIM, respectively. We further validate the utility of KOREATECH-CGH through experiments on hologram generation and super-resolution using state-of-the-art ML models, confirming its applicability for training and evaluating next-generation ML-CGH systems.","authors":["Jaehong Lee","You Chan No","YoungWoo Kim","Duksu Kim"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.21038v1","updated":"2025-12-24T08:06:17Z","published":"2025-12-24T08:06:17Z","title":"Next-Scale Prediction: A Self-Supervised Approach for Real-World Image Denoising","summary":"Self-supervised real-world image denoising remains a fundamental challenge, arising from the antagonistic trade-off between decorrelating spatially structured noise and preserving high-frequency details. Existing blind-spot network (BSN) methods rely on pixel-shuffle downsampling (PD) to decorrelate noise, but aggressive downsampling fragments fine structures, while milder downsampling fails to remove correlated noise. To address this, we introduce Next-Scale Prediction (NSP), a novel self-supervised paradigm that decouples noise decorrelation from detail preservation. NSP constructs cross-scale training pairs, where BSN takes low-resolution, fully decorrelated sub-images as input to predict high-resolution targets that retain fine details. As a by-product, NSP naturally supports super-resolution of noisy images without retraining or modification. Extensive experiments demonstrate that NSP achieves state-of-the-art self-supervised denoising performance on real-world benchmarks, significantly alleviating the long-standing conflict between noise decorrelation and detail preservation.","authors":["Yiwen Shan","Haiyu Zhao","Peng Hu","Xi Peng","Yuanbiao Gou"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.21032v1","updated":"2025-12-24T07:55:54Z","published":"2025-12-24T07:55:54Z","title":"Multi-Attribute guided Thermal Face Image Translation based on Latent Diffusion Model","summary":"Modern surveillance systems increasingly rely on multi-wavelength sensors and deep neural networks to recognize faces in infrared images captured at night. However, most facial recognition models are trained on visible light datasets, leading to substantial performance degradation on infrared inputs due to significant domain shifts. Early feature-based methods for infrared face recognition proved ineffective, prompting researchers to adopt generative approaches that convert infrared images into visible light images for improved recognition. This paradigm, known as Heterogeneous Face Recognition (HFR), faces challenges such as model and modality discrepancies, leading to distortion and feature loss in generated images. To address these limitations, this paper introduces a novel latent diffusion-based model designed to generate high-quality visible face images from thermal inputs while preserving critical identity features. A multi-attribute classifier is incorporated to extract key facial attributes from visible images, mitigating feature loss during infrared-to-visible image restoration. Additionally, we propose the Self-attn Mamba module, which enhances global modeling of cross-modal features and significantly improves inference speed. Experimental results on two benchmark datasets demonstrate the superiority of our approach, achieving state-of-the-art performance in both image quality and identity preservation.","authors":["Mingshu Cai","Osamu Yoshie","Yuya Ieiri"],"pdf_url":"","comment":"Accepted by 2025 IEEE International Joint Conference on Biometrics (IJCB 2025)"},{"id":"http://arxiv.org/abs/2512.12284v3","updated":"2025-12-24T07:46:59Z","published":"2025-12-13T11:02:04Z","title":"V-Rex: Real-Time Streaming Video LLM Acceleration via Dynamic KV Cache Retrieval","summary":"Streaming video large language models (LLMs) are increasingly used for real-time multimodal tasks such as video captioning, question answering, conversational agents, and augmented reality. However, these models face fundamental memory and computational challenges because their key-value (KV) caches grow substantially with continuous streaming video input. This process requires an iterative prefill stage, which is a unique feature of streaming video LLMs. Due to its iterative prefill stage, it suffers from significant limitations, including extensive computation, substantial data transfer, and degradation in accuracy. Crucially, this issue is exacerbated for edge deployment, which is the primary target for these models.\n  In this work, we propose V-Rex, the first software-hardware co-designed accelerator that comprehensively addresses both algorithmic and hardware bottlenecks in streaming video LLM inference. At its core, V-Rex introduces ReSV, a training-free dynamic KV cache retrieval algorithm. ReSV exploits temporal and spatial similarity-based token clustering to reduce excessive KV cache memory across video frames. To fully realize these algorithmic benefits, V-Rex offers a compact, low-latency hardware accelerator with a dynamic KV cache retrieval engine (DRE), featuring bit-level and early-exit based computing units. V-Rex achieves unprecedented real-time of 3.9-8.3 FPS and energy-efficient streaming video LLM inference on edge deployment with negligible accuracy loss. While DRE only accounts for 2.2% power and 2.0% area, the system delivers 1.9-19.7x speedup and 3.1-18.5x energy efficiency improvements over AGX Orin GPU. This work is the first to comprehensively tackle KV cache retrieval across algorithms and hardware, enabling real-time streaming video LLM inference on resource-constrained edge devices.","authors":["Donghyuk Kim","Sejeong Yang","Wonjin Shin","Joo-Young Kim"],"pdf_url":"","comment":"14 pages, 20 figures, conference, accepted by HPCA 2026"},{"id":"http://arxiv.org/abs/2512.19022v2","updated":"2025-12-24T07:36:25Z","published":"2025-12-22T04:30:11Z","title":"Steering Vision-Language Pre-trained Models for Incremental Face Presentation Attack Detection","summary":"Face Presentation Attack Detection (PAD) demands incremental learning (IL) to combat evolving spoofing tactics and domains. Privacy regulations, however, forbid retaining past data, necessitating rehearsal-free IL (RF-IL). Vision-Language Pre-trained (VLP) models, with their prompt-tunable cross-modal representations, enable efficient adaptation to new spoofing styles and domains. Capitalizing on this strength, we propose \\textbf{SVLP-IL}, a VLP-based RF-IL framework that balances stability and plasticity via \\textit{Multi-Aspect Prompting} (MAP) and \\textit{Selective Elastic Weight Consolidation} (SEWC). MAP isolates domain dependencies, enhances distribution-shift sensitivity, and mitigates forgetting by jointly exploiting universal and domain-specific cues. SEWC selectively preserves critical weights from previous tasks, retaining essential knowledge while allowing flexibility for new adaptations. Comprehensive experiments across multiple PAD benchmarks show that SVLP-IL significantly reduces catastrophic forgetting and enhances performance on unseen domains. SVLP-IL offers a privacy-compliant, practical solution for robust lifelong PAD deployment in RF-IL settings.","authors":["Haoze Li","Jie Zhang","Guoying Zhao","Stephen Lin","Shiguang Shan"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.21019v1","updated":"2025-12-24T07:26:06Z","published":"2025-12-24T07:26:06Z","title":"Efficient and Robust Video Defense Framework against 3D-field Personalized Talking Face","summary":"State-of-the-art 3D-field video-referenced Talking Face Generation (TFG) methods synthesize high-fidelity personalized talking-face videos in real time by modeling 3D geometry and appearance from reference portrait video. This capability raises significant privacy concerns regarding malicious misuse of personal portraits. However, no efficient defense framework exists to protect such videos against 3D-field TFG methods. While image-based defenses could apply per-frame 2D perturbations, they incur prohibitive computational costs, severe video quality degradation, failing to disrupt 3D information for video protection. To address this, we propose a novel and efficient video defense framework against 3D-field TFG methods, which protects portrait video by perturbing the 3D information acquisition process while maintain high-fidelity video quality. Specifically, our method introduces: (1) a similarity-guided parameter sharing mechanism for computational efficiency, and (2) a multi-scale dual-domain attention module to jointly optimize spatial-frequency perturbations. Extensive experiments demonstrate that our proposed framework exhibits strong defense capability and achieves a 47x acceleration over the fastest baseline while maintaining high fidelity. Moreover, it remains robust against scaling operations and state-of-the-art purification attacks, and the effectiveness of our design choices is further validated through ablation studies. Our project is available at https://github.com/Richen7418/VDF.","authors":["Rui-qing Sun","Xingshan Yao","Tian Lan","Hui-Yang Zhao","Jia-Ling Shi","Chen-Hao Cui","Zhijing Wu","Chen Yang","Xian-Ling Mao"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.21015v1","updated":"2025-12-24T07:21:59Z","published":"2025-12-24T07:21:59Z","title":"FluencyVE: Marrying Temporal-Aware Mamba with Bypass Attention for Video Editing","summary":"Large-scale text-to-image diffusion models have achieved unprecedented success in image generation and editing. However, extending this success to video editing remains challenging. Recent video editing efforts have adapted pretrained text-to-image models by adding temporal attention mechanisms to handle video tasks. Unfortunately, these methods continue to suffer from temporal inconsistency issues and high computational overheads. In this study, we propose FluencyVE, which is a simple yet effective one-shot video editing approach. FluencyVE integrates the linear time-series module, Mamba, into a video editing model based on pretrained Stable Diffusion models, replacing the temporal attention layer. This enables global frame-level attention while reducing the computational costs. In addition, we employ low-rank approximation matrices to replace the query and key weight matrices in the causal attention, and use a weighted averaging technique during training to update the attention scores. This approach significantly preserves the generative power of the text-to-image model while effectively reducing the computational burden. Experiments and analyses demonstrate promising results in editing various attributes, subjects, and locations in real-world videos.","authors":["Mingshu Cai","Yixuan Li","Osamu Yoshie","Yuya Ieiri"],"pdf_url":"","comment":"Accepted by IEEE Transactions on Multimedia (TMM)"},{"id":"http://arxiv.org/abs/2512.21011v1","updated":"2025-12-24T07:15:33Z","published":"2025-12-24T07:15:33Z","title":"Granular-ball Guided Masking: Structure-aware Data Augmentation","summary":"Deep learning models have achieved remarkable success in computer vision, but they still rely heavily on large-scale labeled data and tend to overfit when data are limited or distributions shift. Data augmentation, particularly mask-based information dropping, can enhance robustness by forcing models to explore complementary cues; however, existing approaches often lack structural awareness and may discard essential semantics. We propose Granular-ball Guided Masking (GBGM), a structure-aware augmentation strategy guided by Granular-ball Computing (GBC). GBGM adaptively preserves semantically rich, structurally important regions while suppressing redundant areas through a coarse-to-fine hierarchical masking process, producing augmentations that are both representative and discriminative. Extensive experiments on multiple benchmarks demonstrate consistent improvements in classification accuracy and masked image reconstruction, confirming the effectiveness and broad applicability of the proposed method. Simple and model-agnostic, it integrates seamlessly into CNNs and Vision Transformers and provides a new paradigm for structure-aware data augmentation.","authors":["Shuyin Xia","Fan Chen","Dawei Dai","Meng Yang","Junwei Han","Xinbo Gao","Guoyin Wang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2502.09080v4","updated":"2025-12-24T07:13:27Z","published":"2025-02-13T08:54:04Z","title":"BevSplat: Resolving Height Ambiguity via Feature-Based Gaussian Primitives for Weakly-Supervised Cross-View Localization","summary":"This paper addresses the problem of weakly supervised cross-view localization, where the goal is to estimate the pose of a ground camera relative to a satellite image with noisy ground truth annotations. A common approach to bridge the cross-view domain gap for pose estimation is Bird's-Eye View (BEV) synthesis. However, existing methods struggle with height ambiguity due to the lack of depth information in ground images and satellite height maps. Previous solutions either assume a flat ground plane or rely on complex models, such as cross-view transformers. We propose BevSplat, a novel method that resolves height ambiguity by using feature-based Gaussian primitives. Each pixel in the ground image is represented by a 3D Gaussian with semantic and spatial features, which are synthesized into a BEV feature map for relative pose estimation. Additionally, to address challenges with panoramic query images, we introduce an icosphere-based supervision strategy for the Gaussian primitives. We validate our method on the widely used KITTI and VIGOR datasets, which include both pinhole and panoramic query images. Experimental results show that BevSplat significantly improves localization accuracy over prior approaches.","authors":["Qiwei Wang","Shaoxun Wu","Yujiao Shi"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17514v2","updated":"2025-12-24T07:10:56Z","published":"2025-12-19T12:30:29Z","title":"Foundation Model Priors Enhance Object Focus in Feature Space for Source-Free Object Detection","summary":"Current state-of-the-art approaches in Source-Free Object Detection (SFOD) typically rely on Mean-Teacher self-labeling. However, domain shift often reduces the detector's ability to maintain strong object-focused representations, causing high-confidence activations over background clutter. This weak object focus results in unreliable pseudo-labels from the detection head. While prior works mainly refine these pseudo-labels, they overlook the underlying need to strengthen the feature space itself. We propose FALCON-SFOD (Foundation-Aligned Learning with Clutter suppression and Noise robustness), a framework designed to enhance object-focused adaptation under domain shift. It consists of two complementary components. SPAR (Spatial Prior-Aware Regularization) leverages the generalization strength of vision foundation models to regularize the detector's feature space. Using class-agnostic binary masks derived from OV-SAM, SPAR promotes structured and foreground-focused activations by guiding the network toward object regions. IRPL (Imbalance-aware Noise Robust Pseudo-Labeling) complements SPAR by promoting balanced and noise-tolerant learning under severe foreground-background imbalance. Guided by a theoretical analysis that connects these designs to tighter localization and classification error bounds, FALCON-SFOD achieves competitive performance across SFOD benchmarks.","authors":["Sairam VCR","Rishabh Lalla","Aveen Dayal","Tejal Kulkarni","Anuj Lalla","Vineeth N Balasubramanian","Muhammad Haris Khan"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2504.18127v2","updated":"2025-12-24T07:07:31Z","published":"2025-04-25T07:23:13Z","title":"Towards Arbitrary-Scale Spacecraft Image Super-Resolution via Salient Region-Guidance","summary":"Spacecraft image super-resolution seeks to enhance low-resolution spacecraft images into high-resolution ones. Although existing arbitrary-scale super-resolution methods perform well on general images, they tend to overlook the difference in features between the spacecraft core region and the large black space background, introducing irrelevant noise. In this paper, we propose a salient region-guided spacecraft image arbitrary-scale super-resolution network (SGSASR), which uses features from the spacecraft core salient regions to guide latent modulation and achieve arbitrary-scale super-resolution. Specifically, we design a spacecraft core region recognition block (SCRRB) that identifies the core salient regions in spacecraft images using a pre-trained saliency detection model. Furthermore, we present an adaptive-weighted feature fusion enhancement mechanism (AFFEM) to selectively aggregate the spacecraft core region features with general image features by dynamic weight parameter to enhance the response of the core salient regions. Experimental results demonstrate that the proposed SGSASR outperforms state-of-the-art approaches.","authors":["Jingfan Yang","Hu Gao","Ying Zhang","Depeng Dang"],"pdf_url":"","comment":"Accepted by Pattern Recognition"},{"id":"http://arxiv.org/abs/2512.21004v1","updated":"2025-12-24T07:07:08Z","published":"2025-12-24T07:07:08Z","title":"Learning from Next-Frame Prediction: Autoregressive Video Modeling Encodes Effective Representations","summary":"Recent advances in pretraining general foundation models have significantly improved performance across diverse downstream tasks. While autoregressive (AR) generative models like GPT have revolutionized NLP, most visual generative pretraining methods still rely on BERT-style masked modeling, which often disregards the temporal information essential for video analysis. The few existing autoregressive visual pretraining methods suffer from issues such as inaccurate semantic localization and poor generation quality, leading to poor semantics. In this work, we propose NExT-Vid, a novel autoregressive visual generative pretraining framework that utilizes masked next-frame prediction to jointly model images and videos. NExT-Vid introduces a context-isolated autoregressive predictor to decouple semantic representation from target decoding, and a conditioned flow-matching decoder to enhance generation quality and diversity. Through context-isolated flow-matching pretraining, our approach achieves strong representations. Extensive experiments on large-scale pretrained models demonstrate that our proposed method consistently outperforms previous generative pretraining methods for visual representation learning via attentive probing in downstream classification.","authors":["Jinghan Li","Yang Jin","Hao Jiang","Yadong Mu","Yang Song","Kun Xu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.21003v1","updated":"2025-12-24T06:59:29Z","published":"2025-12-24T06:59:29Z","title":"MVInverse: Feed-forward Multi-view Inverse Rendering in Seconds","summary":"Multi-view inverse rendering aims to recover geometry, materials, and illumination consistently across multiple viewpoints. When applied to multi-view images, existing single-view approaches often ignore cross-view relationships, leading to inconsistent results. In contrast, multi-view optimization methods rely on slow differentiable rendering and per-scene refinement, making them computationally expensive and hard to scale. To address these limitations, we introduce a feed-forward multi-view inverse rendering framework that directly predicts spatially varying albedo, metallic, roughness, diffuse shading, and surface normals from sequences of RGB images. By alternating attention across views, our model captures both intra-view long-range lighting interactions and inter-view material consistency, enabling coherent scene-level reasoning within a single forward pass. Due to the scarcity of real-world training data, models trained on existing synthetic datasets often struggle to generalize to real-world scenes. To overcome this limitation, we propose a consistency-based finetuning strategy that leverages unlabeled real-world videos to enhance both multi-view coherence and robustness under in-the-wild conditions. Extensive experiments on benchmark datasets demonstrate that our method achieves state-of-the-art performance in terms of multi-view consistency, material and normal estimation quality, and generalization to real-world imagery.","authors":["Xiangzuo Wu","Chengwei Ren","Jun Zhou","Xiu Li","Yuan Liu"],"pdf_url":"","comment":"21 pages, 17 figures, 5 tables"},{"id":"http://arxiv.org/abs/2511.20446v2","updated":"2025-12-24T06:50:09Z","published":"2025-11-25T16:17:23Z","title":"Learning to Generate Human-Human-Object Interactions from Textual Descriptions","summary":"The way humans interact with each other, including interpersonal distances, spatial configuration, and motion, varies significantly across different situations. To enable machines to understand such complex, context-dependent behaviors, it is essential to model multiple people in relation to the surrounding scene context. In this paper, we present a novel research problem to model the correlations between two people engaged in a shared interaction involving an object. We refer to this formulation as Human-Human-Object Interactions (HHOIs). To overcome the lack of dedicated datasets for HHOIs, we present a newly captured HHOIs dataset and a method to synthesize HHOI data by leveraging image generative models. As an intermediary, we obtain individual human-object interaction (HOIs) and human-human interaction (HHIs) from the HHOIs, and with these data, we train an text-to-HOI and text-to-HHI model using score-based diffusion model. Finally, we present a unified generative framework that integrates the two individual model, capable of synthesizing complete HHOIs in a single advanced sampling process. Our method extends HHOI generation to multi-human settings, enabling interactions involving more than two individuals. Experimental results show that our method generates realistic HHOIs conditioned on textual descriptions, outperforming previous approaches that focus only on single-human HOIs. Furthermore, we introduce multi-human motion generation involving objects as an application of our framework.","authors":["Jeonghyeon Na","Sangwon Baik","Inhee Lee","Junyoung Lee","Hanbyul Joo"],"pdf_url":"","comment":"Project Page: https://tlb-miss.github.io/hhoi/"},{"id":"http://arxiv.org/abs/2512.20988v1","updated":"2025-12-24T06:30:42Z","published":"2025-12-24T06:30:42Z","title":"PUFM++: Point Cloud Upsampling via Enhanced Flow Matching","summary":"Recent advances in generative modeling have demonstrated strong promise for high-quality point cloud upsampling. In this work, we present PUFM++, an enhanced flow-matching framework for reconstructing dense and accurate point clouds from sparse, noisy, and partial observations. PUFM++ improves flow matching along three key axes: (i) geometric fidelity, (ii) robustness to imperfect input, and (iii) consistency with downstream surface-based tasks. We introduce a two-stage flow-matching strategy that first learns a direct, straight-path flow from sparse inputs to dense targets, and then refines it using noise-perturbed samples to approximate the terminal marginal distribution better. To accelerate and stabilize inference, we propose a data-driven adaptive time scheduler that improves sampling efficiency based on interpolation behavior. We further impose on-manifold constraints during sampling to ensure that generated points remain aligned with the underlying surface. Finally, we incorporate a recurrent interface network~(RIN) to strengthen hierarchical feature interactions and boost reconstruction quality. Extensive experiments on synthetic benchmarks and real-world scans show that PUFM++ sets a new state of the art in point cloud upsampling, delivering superior visual fidelity and quantitative accuracy across a wide range of tasks. Code and pretrained models are publicly available at https://github.com/Holmes-Alan/Enhanced_PUFM.","authors":["Zhi-Song Liu","Chenhang He","Roland Maier","Andreas Rupp"],"pdf_url":"","comment":"21 pages, 15 figures"},{"id":"http://arxiv.org/abs/2512.20980v1","updated":"2025-12-24T06:14:55Z","published":"2025-12-24T06:14:55Z","title":"X-ray Insights Unleashed: Pioneering the Enhancement of Multi-Label Long-Tail Data","summary":"Long-tailed pulmonary anomalies in chest radiography present formidable diagnostic challenges. Despite the recent strides in diffusion-based methods for enhancing the representation of tailed lesions, the paucity of rare lesion exemplars curtails the generative capabilities of these approaches, thereby leaving the diagnostic precision less than optimal. In this paper, we propose a novel data synthesis pipeline designed to augment tail lesions utilizing a copious supply of conventional normal X-rays. Specifically, a sufficient quantity of normal samples is amassed to train a diffusion model capable of generating normal X-ray images. This pre-trained diffusion model is subsequently utilized to inpaint the head lesions present in the diseased X-rays, thereby preserving the tail classes as augmented training data. Additionally, we propose the integration of a Large Language Model Knowledge Guidance (LKG) module alongside a Progressive Incremental Learning (PIL) strategy to stabilize the inpainting fine-tuning process. Comprehensive evaluations conducted on the public lung datasets MIMIC and CheXpert demonstrate that the proposed method sets a new benchmark in performance.","authors":["Xinquan Yang","Jinheng Xie","Yawen Huang","Yuexiang Li","Huimin Huang","Hao Zheng","Xian Wu","Yefeng Zheng","Linlin Shen"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20976v1","updated":"2025-12-24T06:08:50Z","published":"2025-12-24T06:08:50Z","title":"XGrid-Mapping: Explicit Implicit Hybrid Grid Submaps for Efficient Incremental Neural LiDAR Mapping","summary":"Large-scale incremental mapping is fundamental to the development of robust and reliable autonomous systems, as it underpins incremental environmental understanding with sequential inputs for navigation and decision-making. LiDAR is widely used for this purpose due to its accuracy and robustness. Recently, neural LiDAR mapping has shown impressive performance; however, most approaches rely on dense implicit representations and underutilize geometric structure, while existing voxel-guided methods struggle to achieve real-time performance. To address these challenges, we propose XGrid-Mapping, a hybrid grid framework that jointly exploits explicit and implicit representations for efficient neural LiDAR mapping. Specifically, the strategy combines a sparse grid, providing geometric priors and structural guidance, with an implicit dense grid that enriches scene representation. By coupling the VDB structure with a submap-based organization, the framework reduces computational load and enables efficient incremental mapping on a large scale. To mitigate discontinuities across submaps, we introduce a distillation-based overlap alignment strategy, in which preceding submaps supervise subsequent ones to ensure consistency in overlapping regions. To further enhance robustness and sampling efficiency, we incorporate a dynamic removal module. Extensive experiments show that our approach delivers superior mapping quality while overcoming the efficiency limitations of voxel-guided methods, thereby outperforming existing state-of-the-art mapping methods.","authors":["Zeqing Song","Zhongmiao Yan","Junyuan Deng","Songpengcheng Xia","Xiang Mu","Jingyi Xu","Qi Wu","Ling Pei"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20975v1","updated":"2025-12-24T06:04:58Z","published":"2025-12-24T06:04:58Z","title":"SPOT!: Map-Guided LLM Agent for Unsupervised Multi-CCTV Dynamic Object Tracking","summary":"CCTV-based vehicle tracking systems face structural limitations in continuously connecting the trajectories of the same vehicle across multiple camera environments. In particular, blind spots occur due to the intervals between CCTVs and limited Fields of View (FOV), which leads to object ID switching and trajectory loss, thereby reducing the reliability of real-time path prediction. This paper proposes SPOT (Spatial Prediction Over Trajectories), a map-guided LLM agent capable of tracking vehicles even in blind spots of multi-CCTV environments without prior training. The proposed method represents road structures (Waypoints) and CCTV placement information as documents based on 2D spatial coordinates and organizes them through chunking techniques to enable real-time querying and inference. Furthermore, it transforms the vehicle's position into the actual world coordinate system using the relative position and FOV information of objects observed in CCTV images. By combining map spatial information with the vehicle's moving direction, speed, and driving patterns, a beam search is performed at the intersection level to derive candidate CCTV locations where the vehicle is most likely to enter after the blind spot. Experimental results based on the CARLA simulator in a virtual city environment confirmed that the proposed method accurately predicts the next appearing CCTV even in blind spot sections, maintaining continuous vehicle trajectories more effectively than existing techniques.","authors":["Yujin Noh","Inho Jake Park","Chigon Hwang"],"pdf_url":"","comment":"33 pages, 27figures"},{"id":"http://arxiv.org/abs/2512.20963v1","updated":"2025-12-24T05:40:40Z","published":"2025-12-24T05:40:40Z","title":"Generalization of Diffusion Models Arises with a Balanced Representation Space","summary":"Diffusion models excel at generating high-quality, diverse samples, yet they risk memorizing training data when overfit to the training objective. We analyze the distinctions between memorization and generalization in diffusion models through the lens of representation learning. By investigating a two-layer ReLU denoising autoencoder (DAE), we prove that (i) memorization corresponds to the model storing raw training samples in the learned weights for encoding and decoding, yielding localized \"spiky\" representations, whereas (ii) generalization arises when the model captures local data statistics, producing \"balanced\" representations. Furthermore, we validate these theoretical findings on real-world unconditional and text-to-image diffusion models, demonstrating that the same representation structures emerge in deep generative models with significant practical implications. Building on these insights, we propose a representation-based method for detecting memorization and a training-free editing technique that allows precise control via representation steering. Together, our results highlight that learning good representations is central to novel and meaningful generative modeling.","authors":["Zekai Zhang","Xiao Li","Xiang Li","Lianghe Shi","Meng Wu","Molei Tao","Qing Qu"],"pdf_url":"","comment":"40 pages, 19 figures. The first two authors contributed equally"},{"id":"http://arxiv.org/abs/2512.19512v2","updated":"2025-12-24T05:32:41Z","published":"2025-12-22T16:06:36Z","title":"Anatomy-R1: Enhancing Anatomy Reasoning in Multimodal Large Language Models via Anatomical Similarity Curriculum and Group Diversity Augmentation","summary":"Multimodal Large Language Models (MLLMs) have achieved impressive progress in natural image reasoning, yet their potential in medical imaging remains underexplored, especially in clinical anatomical surgical images. Anatomy understanding tasks demand precise understanding and clinically coherent answers, which are difficult to achieve due to the complexity of medical data and the scarcity of high-quality expert annotations. These challenges limit the effectiveness of conventional Supervised Fine-Tuning (SFT) strategies. While recent work has demonstrated that Group Relative Policy Optimization (GRPO) can enhance reasoning in MLLMs without relying on large amounts of data, we find two weaknesses that hinder GRPO's reasoning performance in anatomy recognition: 1) knowledge cannot be effectively shared between different anatomical structures, resulting in uneven information gain and preventing the model from converging, and 2) the model quickly converges to a single reasoning path, suppressing the exploration of diverse strategies. To overcome these challenges, we propose two novel methods. First, we implement a progressive learning strategy called Anatomical Similarity Curriculum Learning by controlling question difficulty via the similarity of answer choices, enabling the model to master complex problems incrementally. Second, we utilize question augmentation referred to as Group Diversity Question Augmentation to expand the model's search space for difficult queries, mitigating the tendency to produce uniform responses. Comprehensive experiments on the SGG-VQA and OmniMedVQA benchmarks show our method achieves a significant improvement across the two benchmarks, demonstrating its effectiveness in enhancing the medical reasoning capabilities of MLLMs. The code can be found in https://github.com/tomato996/Anatomy-R1","authors":["Ziyang Song","Zelin Zang","Zuyao Chen","Xusheng Liang","Dong Yi","Jinlin Wu","Hongbin Liu","Jiebo Luo","Zhen. Lei"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2511.11662v2","updated":"2025-12-24T05:18:32Z","published":"2025-11-11T09:56:35Z","title":"AGENet: Adaptive Edge-aware Geodesic Distance Learning for Few-Shot Medical Image Segmentation","summary":"Medical image segmentation requires large annotated datasets, creating a significant bottleneck for clinical applications. While few-shot segmentation methods can learn from minimal examples, existing approaches demonstrate suboptimal performance in precise boundary delineation for medical images, particularly when anatomically similar regions appear without sufficient spatial context. We propose AGENet (Adaptive Geodesic Edge-aware Network), a novel framework that incorporates spatial relationships through edge-aware geodesic distance learning. Our key insight is that medical structures follow predictable geometric patterns that can guide prototype extraction even with limited training data. Unlike methods relying on complex architectural components or heavy neural networks, our approach leverages computationally lightweight geometric modeling. The framework combines three main components: (1) An edge-aware geodesic distance learning module that respects anatomical boundaries through iterative Fast Marching refinement, (2) adaptive prototype extraction that captures both global structure and local boundary details via spatially-weighted aggregation, and (3) adaptive parameter learning that automatically adjusts to different organ characteristics. Extensive experiments across diverse medical imaging datasets demonstrate improvements over state-of-the-art methods. Notably, our method reduces boundary errors compared to existing approaches while maintaining computational efficiency, making it highly suitable for clinical applications requiring precise segmentation with limited annotated data.","authors":["Ziyuan Gao"],"pdf_url":"","comment":"Accepted for publication in WACV 2026 (Round 2)"},{"id":"http://arxiv.org/abs/2511.06433v2","updated":"2025-12-24T04:54:47Z","published":"2025-11-09T16:02:13Z","title":"Diagnose Like A REAL Pathologist: An Uncertainty-Focused Approach for Trustworthy Multi-Resolution Multiple Instance Learning","summary":"With the increasing demand for histopathological specimen examination and diagnostic reporting, Multiple Instance Learning (MIL) has received heightened research focus as a viable solution for AI-centric diagnostic aid. Recently, to improve its performance and make it work more like a pathologist, several MIL approaches based on the use of multiple-resolution images have been proposed, delivering often higher performance than those that use single-resolution images. Despite impressive recent developments of multiple-resolution MIL, previous approaches only focus on improving performance, thereby lacking research on well-calibrated MIL that clinical experts can rely on for trustworthy diagnostic results. In this study, we propose Uncertainty-Focused Calibrated MIL (UFC-MIL), which more closely mimics the pathologists' examination behaviors while providing calibrated diagnostic predictions, using multiple images with different resolutions. UFC-MIL includes a novel patch-wise loss that learns the latent patterns of instances and expresses their uncertainty for classification. Also, the attention-based architecture with a neighbor patch aggregation module collects features for the classifier. In addition, aggregated predictions are calibrated through patch-level uncertainty without requiring multiple iterative inferences, which is a key practical advantage. Against challenging public datasets, UFC-MIL shows superior performance in model calibration while achieving classification accuracy comparable to that of state-of-the-art methods.","authors":["Sungrae Hong","Sol Lee","Jisu Shin","Jiwon Jeong","Mun Yong Yi"],"pdf_url":"","comment":"Accepted by IEEE/CVF Winter Conference on Applications of Computer Vision (WACV) 2026"},{"id":"http://arxiv.org/abs/2406.16087v8","updated":"2025-12-24T04:53:14Z","published":"2024-06-23T12:02:17Z","title":"Imperative Learning: A Self-supervised Neuro-Symbolic Learning Framework for Robot Autonomy","summary":"Data-driven methods such as reinforcement and imitation learning have achieved remarkable success in robot autonomy. However, their data-centric nature still hinders them from generalizing well to ever-changing environments. Moreover, labeling data for robotic tasks is often impractical and expensive. To overcome these challenges, we introduce a new self-supervised neuro-symbolic (NeSy) computational framework, imperative learning (IL), for robot autonomy, leveraging the generalization abilities of symbolic reasoning. The framework of IL consists of three primary components: a neural module, a reasoning engine, and a memory system. We formulate IL as a special bilevel optimization (BLO), which enables reciprocal learning over the three modules. This overcomes the label-intensive obstacles associated with data-driven approaches and takes advantage of symbolic reasoning concerning logical reasoning, physical principles, geometric analysis, etc. We discuss several optimization techniques for IL and verify their effectiveness in five distinct robot autonomy tasks including path planning, rule induction, optimal control, visual odometry, and multi-robot routing. Through various experiments, we show that IL can significantly enhance robot autonomy capabilities and we anticipate that it will catalyze further research across diverse domains.","authors":["Chen Wang","Kaiyi Ji","Junyi Geng","Zhongqiang Ren","Taimeng Fu","Fan Yang","Yifan Guo","Haonan He","Xiangyu Chen","Zitong Zhan","Qiwei Du","Shaoshu Su","Bowen Li","Yuheng Qiu","Yi Du","Qihang Li","Yifan Yang","Xiao Lin","Zhipeng Zhao"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20937v1","updated":"2025-12-24T04:41:04Z","published":"2025-12-24T04:41:04Z","title":"Beyond Artifacts: Real-Centric Envelope Modeling for Reliable AI-Generated Image Detection","summary":"The rapid progress of generative models has intensified the need for reliable and robust detection under real-world conditions. However, existing detectors often overfit to generator-specific artifacts and remain highly sensitive to real-world degradations. As generative architectures evolve and images undergo multi-round cross-platform sharing and post-processing (chain degradations), these artifact cues become obsolete and harder to detect. To address this, we propose Real-centric Envelope Modeling (REM), a new paradigm that shifts detection from learning generator artifacts to modeling the robust distribution of real images. REM introduces feature-level perturbations in self-reconstruction to generate near-real samples, and employs an envelope estimator with cross-domain consistency to learn a boundary enclosing the real image manifold. We further build RealChain, a comprehensive benchmark covering both open-source and commercial generators with simulated real-world degradation. Across eight benchmark evaluations, REM achieves an average improvement of 7.5% over state-of-the-art methods, and notably maintains exceptional generalization on the severely degraded RealChain benchmark, establishing a solid foundation for synthetic image detection under real-world conditions. The code and the RealChain benchmark will be made publicly available upon acceptance of the paper.","authors":["Ruiqi Liu","Yi Han","Zhengbo Zhang","Liwei Yao","Zhiyuan Yan","Jialiang Shen","ZhiJin Chen","Boyi Sun","Lubin Weng","Jing Dong","Yan Wang","Shu Wu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20936v1","updated":"2025-12-24T04:39:45Z","published":"2025-12-24T04:39:45Z","title":"Reasoning-Driven Amodal Completion: Collaborative Agents and Perceptual Evaluation","summary":"Amodal completion, the task of inferring invisible object parts, faces significant challenges in maintaining semantic consistency and structural integrity. Prior progressive approaches are inherently limited by inference instability and error accumulation. To tackle these limitations, we present a Collaborative Multi-Agent Reasoning Framework that explicitly decouples Semantic Planning from Visual Synthesis. By employing specialized agents for upfront reasoning, our method generates a structured, explicit plan before pixel generation, enabling visually and semantically coherent single-pass synthesis. We integrate this framework with two critical mechanisms: (1) a self-correcting Verification Agent that employs Chain-of-Thought reasoning to rectify visible region segmentation and identify residual occluders strictly within the Semantic Planning phase, and (2) a Diverse Hypothesis Generator that addresses the ambiguity of invisible regions by offering diverse, plausible semantic interpretations, surpassing the limited pixel-level variations of standard random seed sampling. Furthermore, addressing the limitations of traditional metrics in assessing inferred invisible content, we introduce the MAC-Score (MLLM Amodal Completion Score), a novel human-aligned evaluation metric. Validated against human judgment and ground truth, these metrics establish a robust standard for assessing structural completeness and semantic consistency with visible context. Extensive experiments demonstrate that our framework significantly outperforms state-of-the-art methods across multiple datasets. Our project is available at: https://fanhongxing.github.io/remac-page.","authors":["Hongxing Fan","Shuyu Zhao","Jiayang Ao","Lu Sheng"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20934v1","updated":"2025-12-24T04:30:21Z","published":"2025-12-24T04:30:21Z","title":"Transductive Visual Programming: Evolving Tool Libraries from Experience for Spatial Reasoning","summary":"Spatial reasoning in 3D scenes requires precise geometric calculations that challenge vision-language models. Visual programming addresses this by decomposing problems into steps calling specialized tools, yet existing methods rely on either fixed toolsets or speculative tool induction before solving problems, resulting in suboptimal programs and poor utilization of induced tools. We present Transductive Visual Programming (TVP), a novel framework that builds new tools from its own experience rather than speculation. TVP first solves problems using basic tools while accumulating experiential solutions into an Example Library, then abstracts recurring patterns from these programs into reusable higher-level tools for an evolving Tool Library. This allows TVP to tackle new problems with increasingly powerful tools learned from experience. On Omni3D-Bench, TVP achieves state-of-the-art performance, outperforming GPT-4o by 22% and the previous best visual programming system by 11%. Our transductively learned tools are used 5x more frequently as core program dependency than inductively created ones, demonstrating more effective tool discovery and reuse. The evolved tools also show strong generalization to unseen spatial tasks, achieving superior performance on benchmarks from SpatialScore-Hard collection without any testset-specific modification. Our work establishes experience-driven transductive tool creation as a powerful paradigm for building self-evolving visual programming agents that effectively tackle challenging spatial reasoning tasks. We release our code at https://transductive-visualprogram.github.io/.","authors":["Shengguang Wu","Xiaohan Wang","Yuhui Zhang","Hao Zhu","Serena Yeung-Levy"],"pdf_url":"","comment":"Project Website: https://transductive-visualprogram.github.io/"},{"id":"http://arxiv.org/abs/2512.20927v1","updated":"2025-12-24T04:16:18Z","published":"2025-12-24T04:16:18Z","title":"Quantile Rendering: Efficiently Embedding High-dimensional Feature on 3D Gaussian Splatting","summary":"Recent advancements in computer vision have successfully extended Open-vocabulary segmentation (OVS) to the 3D domain by leveraging 3D Gaussian Splatting (3D-GS). Despite this progress, efficiently rendering the high-dimensional features required for open-vocabulary queries poses a significant challenge. Existing methods employ codebooks or feature compression, causing information loss, thereby degrading segmentation quality. To address this limitation, we introduce Quantile Rendering (Q-Render), a novel rendering strategy for 3D Gaussians that efficiently handles high-dimensional features while maintaining high fidelity. Unlike conventional volume rendering, which densely samples all 3D Gaussians intersecting each ray, Q-Render sparsely samples only those with dominant influence along the ray. By integrating Q-Render into a generalizable 3D neural network, we also propose Gaussian Splatting Network (GS-Net), which predicts Gaussian features in a generalizable manner. Extensive experiments on ScanNet and LeRF demonstrate that our framework outperforms state-of-the-art methods, while enabling real-time rendering with an approximate ~43.7x speedup on 512-D feature maps. Code will be made publicly available.","authors":["Yoonwoo Jeong","Cheng Sun","Frank Wang","Minsu Cho","Jaesung Choe"],"pdf_url":"","comment":"Will be updated"},{"id":"http://arxiv.org/abs/2512.20921v1","updated":"2025-12-24T03:57:21Z","published":"2025-12-24T03:57:21Z","title":"Self-supervised Multiplex Consensus Mamba for General Image Fusion","summary":"Image fusion integrates complementary information from different modalities to generate high-quality fused images, thereby enhancing downstream tasks such as object detection and semantic segmentation. Unlike task-specific techniques that primarily focus on consolidating inter-modal information, general image fusion needs to address a wide range of tasks while improving performance without increasing complexity. To achieve this, we propose SMC-Mamba, a Self-supervised Multiplex Consensus Mamba framework for general image fusion. Specifically, the Modality-Agnostic Feature Enhancement (MAFE) module preserves fine details through adaptive gating and enhances global representations via spatial-channel and frequency-rotational scanning. The Multiplex Consensus Cross-modal Mamba (MCCM) module enables dynamic collaboration among experts, reaching a consensus to efficiently integrate complementary information from multiple modalities. The cross-modal scanning within MCCM further strengthens feature interactions across modalities, facilitating seamless integration of critical information from both sources. Additionally, we introduce a Bi-level Self-supervised Contrastive Learning Loss (BSCL), which preserves high-frequency information without increasing computational overhead while simultaneously boosting performance in downstream tasks. Extensive experiments demonstrate that our approach outperforms state-of-the-art (SOTA) image fusion algorithms in tasks such as infrared-visible, medical, multi-focus, and multi-exposure fusion, as well as downstream visual tasks.","authors":["Yingying Wang","Rongjin Zhuang","Hui Zheng","Xuanhua He","Ke Cao","Xiaotong Tu","Xinghao Ding"],"pdf_url":"","comment":"Accepted by AAAI 2026, 9 pages, 4 figures"},{"id":"http://arxiv.org/abs/2512.15713v2","updated":"2025-12-24T03:37:34Z","published":"2025-12-17T18:59:55Z","title":"DiffusionVL: Translating Any Autoregressive Models into Diffusion Vision Language Models","summary":"In recent multimodal research, the diffusion paradigm has emerged as a promising alternative to the autoregressive paradigm (AR), owing to its unique decoding advantages. However, due to the capability limitations of the base diffusion language model, the performance of the diffusion vision language model (dVLM) still lags significantly behind that of mainstream models. This leads to a simple yet fundamental question: Is it possible to construct dVLMs based on existing powerful AR models? In response, we propose DiffusionVL, a dVLM family that could be translated from any powerful AR models. Through simple fine-tuning, we successfully adapt AR pre-trained models into the diffusion paradigm. This approach yields two key observations: (1) The paradigm shift from AR-based multimodal models to diffusion is remarkably effective. (2) Direct conversion of an AR language model to a dVLM is also feasible, achieving performance competitive with LLaVA-style visual-instruction-tuning. Further, we introduce a block-decoding design into dVLMs that supports arbitrary-length generation and KV cache reuse, achieving a significant inference speedup. We conduct a large number of experiments. Despite training with less than 5% of the data required by prior methods, DiffusionVL achieves a comprehensive performance improvement-a 34.4% gain on the MMMU-Pro (vision) bench and 37.5% gain on the MME (Cog.) bench-alongside a 2x inference speedup. The model and code are released at https://github.com/hustvl/DiffusionVL.","authors":["Lunbin Zeng","Jingfeng Yao","Bencheng Liao","Hongyuan Tao","Wenyu Liu","Xinggang Wang"],"pdf_url":"","comment":"11 pages, 5 figures, conference or other essential info"},{"id":"http://arxiv.org/abs/2512.20907v1","updated":"2025-12-24T03:18:51Z","published":"2025-12-24T03:18:51Z","title":"PanoGrounder: Bridging 2D and 3D with Panoramic Scene Representations for VLM-based 3D Visual Grounding","summary":"3D Visual Grounding (3DVG) is a critical bridge from vision-language perception to robotics, requiring both language understanding and 3D scene reasoning. Traditional supervised models leverage explicit 3D geometry but exhibit limited generalization, owing to the scarcity of 3D vision-language datasets and the limited reasoning capabilities compared to modern vision-language models (VLMs). We propose PanoGrounder, a generalizable 3DVG framework that couples multi-modal panoramic representation with pretrained 2D VLMs for strong vision-language reasoning. Panoramic renderings, augmented with 3D semantic and geometric features, serve as an intermediate representation between 2D and 3D, and offer two major benefits: (i) they can be directly fed to VLMs with minimal adaptation and (ii) they retain long-range object-to-object relations thanks to their 360-degree field of view. We devise a three-stage pipeline that places a compact set of panoramic viewpoints considering the scene layout and geometry, grounds a text query on each panoramic rendering with a VLM, and fuses per-view predictions into a single 3D bounding box via lifting. Our approach achieves state-of-the-art results on ScanRefer and Nr3D, and demonstrates superior generalization to unseen 3D datasets and text rephrasings.","authors":["Seongmin Jung","Seongho Choi","Gunwoo Jeon","Minsu Cho","Jongwoo Lim"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.05209v3","updated":"2025-12-24T03:04:00Z","published":"2025-12-04T19:25:48Z","title":"DEAR: Dataset for Evaluating the Aesthetics of Rendering","summary":"Traditional Image Quality Assessment~(IQA) focuses on quantifying technical degradations such as noise, blur, or compression artifacts, using both full-reference and no-reference objective metrics. However, evaluation of rendering aesthetics, a growing domain relevant to photographic editing, content creation, and AI-generated imagery, remains underexplored due to the lack of datasets that reflect the inherently subjective nature of style preference. In this work, a novel benchmark dataset designed to model human aesthetic judgments of image rendering styles is introduced: the Dataset for Evaluating the Aesthetics of Rendering (DEAR). Built upon the MIT-Adobe FiveK dataset, DEAR incorporates pairwise human preference scores collected via large-scale crowdsourcing, with each image pair evaluated by 25 distinct human evaluators with a total of 13,648 of them participating overall. These annotations capture nuanced, context-sensitive aesthetic preferences, enabling the development and evaluation of models that go beyond traditional distortion-based IQA, focusing on a new task: Evaluation of Aesthetics of Rendering (EAR). The data collection pipeline is described, human voting patterns are analyzed, and multiple use cases are outlined, including style preference prediction, aesthetic benchmarking, and personalized aesthetic modeling. To the best of the authors' knowledge, DEAR is the first dataset to systematically address image aesthetics of rendering assessment grounded in subjective human preferences. A subset of 100 images with markup for them is published on HuggingFace (huggingface.co/datasets/vsevolodpl/DEAR).","authors":["Vsevolod Plohotnuk","Artyom Panshin","Nikola Banić","Simone Bianco","Michael Freeman","Egor Ershov"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20901v1","updated":"2025-12-24T02:59:01Z","published":"2025-12-24T02:59:01Z","title":"Benchmarking and Enhancing VLM for Compressed Image Understanding","summary":"With the rapid development of Vision-Language Models (VLMs) and the growing demand for their applications, efficient compression of the image inputs has become increasingly important. Existing VLMs predominantly digest and understand high-bitrate compressed images, while their ability to interpret low-bitrate compressed images has yet to be explored by far. In this paper, we introduce the first comprehensive benchmark to evaluate the ability of VLM against compressed images, varying existing widely used image codecs and diverse set of tasks, encompassing over one million compressed images in our benchmark. Next, we analyse the source of performance gap, by categorising the gap from a) the information loss during compression and b) generalisation failure of VLM. We visualize these gaps with concrete examples and identify that for compressed images, only the generalization gap can be mitigated. Finally, we propose a universal VLM adaptor to enhance model performance on images compressed by existing codecs. Consequently, we demonstrate that a single adaptor can improve VLM performance across images with varying codecs and bitrates by 10%-30%. We believe that our benchmark and enhancement method provide valuable insights and contribute toward bridging the gap between VLMs and compressed images.","authors":["Zifu Zhang","Tongda Xu","Siqi Li","Shengxi Li","Yue Zhang","Mai Xu","Yan Wang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20898v1","updated":"2025-12-24T02:47:22Z","published":"2025-12-24T02:47:22Z","title":"DGSAN: Dual-Graph Spatiotemporal Attention Network for Pulmonary Nodule Malignancy Prediction","summary":"Lung cancer continues to be the leading cause of cancer-related deaths globally. Early detection and diagnosis of pulmonary nodules are essential for improving patient survival rates. Although previous research has integrated multimodal and multi-temporal information, outperforming single modality and single time point, the fusion methods are limited to inefficient vector concatenation and simple mutual attention, highlighting the need for more effective multimodal information fusion. To address these challenges, we introduce a Dual-Graph Spatiotemporal Attention Network, which leverages temporal variations and multimodal data to enhance the accuracy of predictions. Our methodology involves developing a Global-Local Feature Encoder to better capture the local, global, and fused characteristics of pulmonary nodules. Additionally, a Dual-Graph Construction method organizes multimodal features into inter-modal and intra-modal graphs. Furthermore, a Hierarchical Cross-Modal Graph Fusion Module is introduced to refine feature integration. We also compiled a novel multimodal dataset named the NLST-cmst dataset as a comprehensive source of support for related research. Our extensive experiments, conducted on both the NLST-cmst and curated CSTL-derived datasets, demonstrate that our DGSAN significantly outperforms state-of-the-art methods in classifying pulmonary nodules with exceptional computational efficiency.","authors":["Xiao Yu","Zhaojie Fang","Guanyu Zhou","Yin Shen","Huoling Luo","Ye Li","Ahmed Elazab","Xiang Wan","Ruiquan Ge","Changmiao Wang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20610v2","updated":"2025-12-24T02:38:59Z","published":"2025-12-23T18:57:53Z","title":"FedPOD: the deployable units of training for federated learning","summary":"This paper proposes FedPOD, which ranked first in the 2024 Federated Tumor Segmentation (FeTS) Challenge, for optimizing learning efficiency and communication cost in federated learning among multiple clients. Inspired by FedPIDAvg, we define a round-wise task for FedPOD to enhance training efficiency. FedPIDAvg achieved performance improvement by incorporating the training loss reduction for prediction entropy as weights using differential terms. Furthermore, by modeling data distribution with a Poisson distribution and using a PID controller, it reduced communication costs even in skewed data distribution. However, excluding participants classified as outliers based on the Poisson distribution can limit data utilization. Additionally, PID controller requires the same participants to be maintained throughout the federated learning process as it uses previous rounds' learning information in the current round. In our approach, FedPOD addresses these issues by including participants excluded as outliers, eliminating dependency on previous rounds' learning information, and applying a method for calculating validation loss at each round. In this challenge, FedPOD presents comparable performance to FedPIDAvg in metrics of Dice score, 0.78, 0.71 and 0.72 for WT, ET and TC in average, and projected convergence score, 0.74 in average. Furthermore, the concept of FedPOD draws inspiration from Kubernetes' smallest computing unit, POD, designed to be compatible with Kubernetes auto-scaling. Extending round-wise tasks of FedPOD to POD units allows flexible design by applying scale-out similar to Kubernetes' auto-scaling. This work demonstrated the potentials of FedPOD to enhance federated learning by improving efficiency, flexibility, and performance in metrics.","authors":["Daewoon Kim","Si Young Yie","Jae Sung Lee"],"pdf_url":"","comment":"12 pages, 12 figures, MICCAI"},{"id":"http://arxiv.org/abs/2512.20892v1","updated":"2025-12-24T02:30:23Z","published":"2025-12-24T02:30:23Z","title":"Beyond Weight Adaptation: Feature-Space Domain Injection for Cross-Modal Ship Re-Identification","summary":"Cross-Modality Ship Re-Identification (CMS Re-ID) is critical for achieving all-day and all-weather maritime target tracking, yet it is fundamentally challenged by significant modality discrepancies. Mainstream solutions typically rely on explicit modality alignment strategies; however, this paradigm heavily depends on constructing large-scale paired datasets for pre-training. To address this, grounded in the Platonic Representation Hypothesis, we explore the potential of Vision Foundation Models (VFMs) in bridging modality gaps. Recognizing the suboptimal performance of existing generic Parameter-Efficient Fine-Tuning (PEFT) methods that operate within the weight space, particularly on limited-capacity models, we shift the optimization perspective to the feature space and propose a novel PEFT strategy termed Domain Representation Injection (DRI). Specifically, while keeping the VFM fully frozen to maximize the preservation of general knowledge, we design a lightweight, learnable Offset Encoder to extract domain-specific representations rich in modality and identity attributes from raw inputs. Guided by the contextual information of intermediate features at different layers, a Modulator adaptively transforms these representations. Subsequently, they are injected into the intermediate layers via additive fusion, dynamically reshaping the feature distribution to adapt to the downstream task without altering the VFM's pre-trained weights. Extensive experimental results demonstrate the superiority of our method, achieving State-of-the-Art (SOTA) performance with minimal trainable parameters. For instance, on the HOSS-ReID dataset, we attain 57.9\\% and 60.5\\% mAP using only 1.54M and 7.05M parameters, respectively. The code is available at https://github.com/TingfengXian/DRI.","authors":["Tingfeng Xian","Wenlve Zhou","Zhiheng Zhou","Zhelin Li"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2508.04236v3","updated":"2025-12-24T01:40:55Z","published":"2025-08-06T09:18:45Z","title":"PIS3R: Very Large Parallax Image Stitching via Deep 3D Reconstruction","summary":"Image stitching aim to align two images taken from different viewpoints into one seamless, wider image. However, when the 3D scene contains depth variations and the camera baseline is significant, noticeable parallax occurs-meaning the relative positions of scene elements differ substantially between views. Most existing stitching methods struggle to handle such images with large parallax effectively. To address this challenge, in this paper, we propose an image stitching solution called PIS3R that is robust to very large parallax based on the novel concept of deep 3D reconstruction. First, we apply visual geometry grounded transformer to two input images with very large parallax to obtain both intrinsic and extrinsic parameters, as well as the dense 3D scene reconstruction. Subsequently, we reproject reconstructed dense point cloud onto a designated reference view using the recovered camera parameters, achieving pixel-wise alignment and generating an initial stitched image. Finally, to further address potential artifacts such as holes or noise in the initial stitching, we propose a point-conditioned image diffusion module to obtain the refined result.Compared with existing methods, our solution is very large parallax tolerant and also provides results that fully preserve the geometric integrity of all pixels in the 3D photogrammetric context, enabling direct applicability to downstream 3D vision tasks such as SfM. Experimental results demonstrate that the proposed algorithm provides accurate stitching results for images with very large parallax, and outperforms the existing methods qualitatively and quantitatively.","authors":["Muhua Zhu","Xinhao Jin","Chengbo Wang","Yongcong Zhang","Yifei Xue","Tie Ji","Yizhen Lao"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20871v1","updated":"2025-12-24T01:21:25Z","published":"2025-12-24T01:21:25Z","title":"NeRV360: Neural Representation for 360-Degree Videos with a Viewport Decoder","summary":"Implicit neural representations for videos (NeRV) have shown strong potential for video compression. However, applying NeRV to high-resolution 360-degree videos causes high memory usage and slow decoding, making real-time applications impractical. We propose NeRV360, an end-to-end framework that decodes only the user-selected viewport instead of reconstructing the entire panoramic frame. Unlike conventional pipelines, NeRV360 integrates viewport extraction into decoding and introduces a spatial-temporal affine transform module for conditional decoding based on viewpoint and time. Experiments on 6K-resolution videos show that NeRV360 achieves a 7-fold reduction in memory consumption and a 2.5-fold increase in decoding speed compared to HNeRV, a representative prior work, while delivering better image quality in terms of objective metrics.","authors":["Daichi Arai","Kyohei Unno","Yasuko Sugito","Yuichi Kusakabe"],"pdf_url":"","comment":"2026 IIEEJ International Conference on Image Electronics and Visual Computing (IEVC)"},{"id":"http://arxiv.org/abs/2504.11467v3","updated":"2025-12-24T01:11:05Z","published":"2025-04-10T12:55:08Z","title":"A Multicore and Edge TPU-Accelerated Multimodal TinyML System for Livestock Behavior Recognition","summary":"The advancement of technology has revolutionized the agricultural industry, transitioning it from labor-intensive farming practices to automated, AI-powered management systems. In recent years, more intelligent livestock monitoring solutions have been proposed to enhance farming efficiency and productivity. This work presents a novel approach to animal activity recognition and movement tracking, leveraging tiny machine learning (TinyML) techniques, wireless communication framework, and microcontroller platforms to develop an efficient, cost-effective livestock sensing system. It collects and fuses accelerometer data and vision inputs to build a multimodal network for three tasks: image classification, object detection, and behavior recognition. The system is deployed and evaluated on commercial microcontrollers for real-time inference using embedded applications, demonstrating up to 270$\\times$ model size reduction, less than 80ms response latency, and on-par performance comparable to existing methods. The incorporation of the wireless communication technique allows for seamless data transmission between devices, benefiting use cases in remote locations with poor Internet connectivity. This work delivers a robust, scalable IoT-edge livestock monitoring solution adaptable to diverse farming needs, offering flexibility for future extensions.","authors":["Qianxue Zhang","Eiman Kanjo"],"pdf_url":"","comment":"12 pages, 10 figures"},{"id":"http://arxiv.org/abs/2509.01907v5","updated":"2025-12-24T01:10:58Z","published":"2025-09-02T03:01:23Z","title":"RSCC: A Large-Scale Remote Sensing Change Caption Dataset for Disaster Events","summary":"Remote sensing is critical for disaster monitoring, yet existing datasets lack temporal image pairs and detailed textual annotations. While single-snapshot imagery dominates current resources, it fails to capture dynamic disaster impacts over time. To address this gap, we introduce the Remote Sensing Change Caption (RSCC) dataset, a large-scale benchmark comprising 62,351 pre-/post-disaster image pairs (spanning earthquakes, floods, wildfires, and more) paired with rich, human-like change captions. By bridging the temporal and semantic divide in remote sensing data, RSCC enables robust training and evaluation of vision-language models for disaster-aware bi-temporal understanding. Our results highlight RSCC's ability to facilitate detailed disaster-related analysis, paving the way for more accurate, interpretable, and scalable vision-language applications in remote sensing. Code and dataset are available at https://github.com/Bili-Sakura/RSCC.","authors":["Zhenyuan Chen","Chenxi Wang","Ningyu Zhang","Feng Zhang"],"pdf_url":"","comment":"Accepted by NeurIPS 2025 Dataset and Benchmark Track"},{"id":"http://arxiv.org/abs/2512.20866v1","updated":"2025-12-24T00:50:27Z","published":"2025-12-24T00:50:27Z","title":"Lightweight framework for underground pipeline recognition and spatial localization based on multi-view 2D GPR images","summary":"To address the issues of weak correlation between multi-view features, low recognition accuracy of small-scale targets, and insufficient robustness in complex scenarios in underground pipeline detection using 3D GPR, this paper proposes a 3D pipeline intelligent detection framework. First, based on a B/C/D-Scan three-view joint analysis strategy, a three-dimensional pipeline three-view feature evaluation method is established by cross-validating forward simulation results obtained using FDTD methods with actual measurement data. Second, the DCO-YOLO framework is proposed, which integrates DySample, CGLU, and OutlookAttention cross-dimensional correlation mechanisms into the original YOLOv11 algorithm, significantly improving the small-scale pipeline edge feature extraction capability. Furthermore, a 3D-DIoU spatial feature matching algorithm is proposed, which integrates three-dimensional geometric constraints and center distance penalty terms to achieve automated association of multi-view annotations. The three-view fusion strategy resolves inherent ambiguities in single-view detection. Experiments based on real urban underground pipeline data show that the proposed method achieves accuracy, recall, and mean average precision of 96.2%, 93.3%, and 96.7%, respectively, in complex multi-pipeline scenarios, which are 2.0%, 2.1%, and 0.9% higher than the baseline model. Ablation experiments validated the synergistic optimization effect of the dynamic feature enhancement module and Grad-CAM++ heatmap visualization demonstrated that the improved model significantly enhanced its ability to focus on pipeline geometric features. This study integrates deep learning optimization strategies with the physical characteristics of 3D GPR, offering an efficient and reliable novel technical framework for the intelligent recognition and localization of underground pipelines.","authors":["Haotian Lv","Chao Li","Jiangbo Dai","Yuhui Zhang","Zepeng Fan","Yiqiu Tan","Dawei Wang","Binglei Xie"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20858v1","updated":"2025-12-24T00:33:59Z","published":"2025-12-24T00:33:59Z","title":"ALIVE: An Avatar-Lecture Interactive Video Engine with Content-Aware Retrieval for Real-Time Interaction","summary":"Traditional lecture videos offer flexibility but lack mechanisms for real-time clarification, forcing learners to search externally when confusion arises. Recent advances in large language models and neural avatars provide new opportunities for interactive learning, yet existing systems typically lack lecture awareness, rely on cloud-based services, or fail to integrate retrieval and avatar-delivered explanations in a unified, privacy-preserving pipeline.\n  We present ALIVE, an Avatar-Lecture Interactive Video Engine that transforms passive lecture viewing into a dynamic, real-time learning experience. ALIVE operates fully on local hardware and integrates (1) Avatar-delivered lecture generated through ASR transcription, LLM refinement, and neural talking-head synthesis; (2) A content-aware retrieval mechanism that combines semantic similarity with timestamp alignment to surface contextually relevant lecture segments; and (3) Real-time multimodal interaction, enabling students to pause the lecture, ask questions through text or voice, and receive grounded explanations either as text or as avatar-delivered responses.\n  To maintain responsiveness, ALIVE employs lightweight embedding models, FAISS-based retrieval, and segmented avatar synthesis with progressive preloading. We demonstrate the system on a complete medical imaging course, evaluate its retrieval accuracy, latency characteristics, and user experience, and show that ALIVE provides accurate, content-aware, and engaging real-time support.\n  ALIVE illustrates how multimodal AI-when combined with content-aware retrieval and local deployment-can significantly enhance the pedagogical value of recorded lectures, offering an extensible pathway toward next-generation interactive learning environments.","authors":["Md Zabirul Islam","Md Motaleb Hossen Manik","Ge Wang"],"pdf_url":"","comment":null}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2512.10688v5","updated":"2025-12-24T17:12:02Z","published":"2025-12-11T14:35:13Z","title":"Rethinking Popularity Bias in Collaborative Filtering via Analytical Vector Decomposition","summary":"Popularity bias fundamentally undermines the personalization capabilities of collaborative filtering (CF) models, causing them to disproportionately recommend popular items while neglecting users' genuine preferences for niche content. While existing approaches treat this as an external confounding factor, we reveal that popularity bias is an intrinsic geometric artifact of Bayesian Pairwise Ranking (BPR) optimization in CF models. Through rigorous mathematical analysis, we prove that BPR systematically organizes item embeddings along a dominant \"popularity direction\" where embedding magnitudes directly correlate with interaction frequency. This geometric distortion forces user embeddings to simultaneously handle two conflicting tasks-expressing genuine preference and calibrating against global popularity-trapping them in suboptimal configurations that favor popular items regardless of individual tastes. We propose Directional Decomposition and Correction (DDC), a universally applicable framework that surgically corrects this embedding geometry through asymmetric directional updates. DDC guides positive interactions along personalized preference directions while steering negative interactions away from the global popularity direction, disentangling preference from popularity at the geometric source. Extensive experiments across multiple BPR-based architectures demonstrate that DDC significantly outperforms state-of-the-art debiasing methods, reducing training loss to less than 5% of heavily-tuned baselines while achieving superior recommendation quality and fairness. Code is available in https://github.com/LingFeng-Liu-AI/DDC.","authors":["Lingfeng Liu","Yixin Song","Dazhong Shen","Bing Yin","Hao Li","Yanyong Zhang","Chao Wang"],"pdf_url":"","comment":"Accepted by SIGKDD 2026(First Cycle)"},{"id":"http://arxiv.org/abs/2512.21257v1","updated":"2025-12-24T16:06:20Z","published":"2025-12-24T16:06:20Z","title":"ReaSeq: Unleashing World Knowledge via Reasoning for Sequential Modeling","summary":"Industrial recommender systems face two fundamental limitations under the log-driven paradigm: (1) knowledge poverty in ID-based item representations that causes brittle interest modeling under data sparsity, and (2) systemic blindness to beyond-log user interests that constrains model performance within platform boundaries. These limitations stem from an over-reliance on shallow interaction statistics and close-looped feedback while neglecting the rich world knowledge about product semantics and cross-domain behavioral patterns that Large Language Models have learned from vast corpora.\n  To address these challenges, we introduce ReaSeq, a reasoning-enhanced framework that leverages world knowledge in Large Language Models to address both limitations through explicit and implicit reasoning. Specifically, ReaSeq employs explicit Chain-of-Thought reasoning via multi-agent collaboration to distill structured product knowledge into semantically enriched item representations, and latent reasoning via Diffusion Large Language Models to infer plausible beyond-log behaviors. Deployed on Taobao's ranking system serving hundreds of millions of users, ReaSeq achieves substantial gains: >6.0% in IPV and CTR, >2.9% in Orders, and >2.5% in GMV, validating the effectiveness of world-knowledge-enhanced reasoning over purely log-driven approaches.","authors":["Chuan Wang","Gaoming Yang","Han Wu","Jiakai Tang","Jiahao Yu","Jian Wu","Jianwu Hu","Junjun Zheng","Shuwen Xiao","Yeqiu Yang","Yuning Jiang","Ahjol Nurlanbek","Binbin Cao","Bo Zheng","Fangmei Zhu","Gaoming Zhou","Huimin Yi","Huiping Chu","Jin Huang","Jinzhe Shan","Kenan Cui","Longbin Li","Silu Zhou","Wen Chen","Xia Ming","Xiang Gao","Xin Yao","Xingyu Wen","Yan Zhang","Yiwen Hu","Yulin Wang","Ziheng Bao","Zongyuan Wu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.21120v1","updated":"2025-12-24T11:39:00Z","published":"2025-12-24T11:39:00Z","title":"ClarifyMT-Bench: Benchmarking and Improving Multi-Turn Clarification for Conversational Large Language Models","summary":"Large language models (LLMs) are increasingly deployed as conversational assistants in open-domain, multi-turn settings, where users often provide incomplete or ambiguous information. However, existing LLM-focused clarification benchmarks primarily assume single-turn interactions or cooperative users, limiting their ability to evaluate clarification behavior in realistic settings. We introduce \\textbf{ClarifyMT-Bench}, a benchmark for multi-turn clarification grounded in a five-dimensional ambiguity taxonomy and a set of six behaviorally diverse simulated user personas. Through a hybrid LLM-human pipeline, we construct 6,120 multi-turn dialogues capturing diverse ambiguity sources and interaction patterns. Evaluating ten representative LLMs uncovers a consistent under-clarification bias: LLMs tend to answer prematurely, and performance degrades as dialogue depth increases. To mitigate this, we propose \\textbf{ClarifyAgent}, an agentic approach that decomposes clarification into perception, forecasting, tracking, and planning, substantially improving robustness across ambiguity conditions. ClarifyMT-Bench establishes a reproducible foundation for studying when LLMs should ask, when they should answer, and how to navigate ambiguity in real-world human-LLM interactions.","authors":["Sichun Luo","Yi Huang","Mukai Li","Shichang Meng","Fengyuan Liu","Zefa Hu","Junlan Feng","Qi Liu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.21076v1","updated":"2025-12-24T09:49:56Z","published":"2025-12-24T09:49:56Z","title":"Blurb-Refined Inference from Crowdsourced Book Reviews using Hierarchical Genre Mining with Dual-Path Graph Convolutions","summary":"Accurate book genre classification is fundamental to digital library organization, content discovery, and personalized recommendation. Existing approaches typically model genre prediction as a flat, single-label task, ignoring hierarchical genre structure and relying heavily on noisy, subjective user reviews, which often degrade classification reliability. We propose HiGeMine, a two-phase hierarchical genre mining framework that robustly integrates user reviews with authoritative book blurbs. In the first phase, HiGeMine employs a zero-shot semantic alignment strategy to filter reviews, retaining only those semantically consistent with the corresponding blurb, thereby mitigating noise, bias, and irrelevance. In the second phase, we introduce a dual-path, two-level graph-based classification architecture: a coarse-grained Level-1 binary classifier distinguishes fiction from non-fiction, followed by Level-2 multi-label classifiers for fine-grained genre prediction. Inter-genre dependencies are explicitly modeled using a label co-occurrence graph, while contextual representations are derived from pretrained language models applied to the filtered textual content. To facilitate systematic evaluation, we curate a new hierarchical book genre dataset. Extensive experiments demonstrate that HiGeMine consistently outperformed strong baselines across hierarchical genre classification tasks. The proposed framework offers a principled and effective solution for leveraging both structured and unstructured textual data in hierarchical book genre analysis.","authors":["Suraj Kumar","Utsav Kumar Nareti","Soumi Chattopadhyay","Chandranath Adak","Prolay Mallick"],"pdf_url":"","comment":"10 pages, 4 figures, 3 tables"},{"id":"http://arxiv.org/abs/2512.21039v1","updated":"2025-12-24T08:06:52Z","published":"2025-12-24T08:06:52Z","title":"Agentic Multi-Persona Framework for Evidence-Aware Fake News Detection","summary":"The rapid proliferation of online misinformation poses significant risks to public trust, policy, and safety, necessitating reliable automated fake news detection. Existing methods often struggle with multimodal content, domain generalization, and explainability. We propose AMPEND-LS, an agentic multi-persona evidence-grounded framework with LLM-SLM synergy for multimodal fake news detection. AMPEND-LS integrates textual, visual, and contextual signals through a structured reasoning pipeline powered by LLMs, augmented with reverse image search, knowledge graph paths, and persuasion strategy analysis. To improve reliability, we introduce a credibility fusion mechanism combining semantic similarity, domain trustworthiness, and temporal context, and a complementary SLM classifier to mitigate LLM uncertainty and hallucinations. Extensive experiments across three benchmark datasets demonstrate that AMPEND-LS consistently outperformed state-of-the-art baselines in accuracy, F1 score, and robustness. Qualitative case studies further highlight its transparent reasoning and resilience against evolving misinformation. This work advances the development of adaptive, explainable, and evidence-aware systems for safeguarding online information integrity.","authors":["Roopa Bukke","Soumya Pandey","Suraj Kumar","Soumi Chattopadhyay","Chandranath Adak"],"pdf_url":"","comment":"12 pages, 8 tables, 2 figures"},{"id":"http://arxiv.org/abs/2512.21021v1","updated":"2025-12-24T07:35:17Z","published":"2025-12-24T07:35:17Z","title":"Towards Better Search with Domain-Aware Text Embeddings for C2C Marketplaces","summary":"Consumer-to-consumer (C2C) marketplaces pose distinct retrieval challenges: short, ambiguous queries; noisy, user-generated listings; and strict production constraints. This paper reports our experiment to build a domain-aware Japanese text-embedding approach to improve the quality of search at Mercari, Japan's largest C2C marketplace. We experimented with fine-tuning on purchase-driven query-title pairs, using role-specific prefixes to model query-item asymmetry. To meet production constraints, we apply Matryoshka Representation Learning to obtain compact, truncation-robust embeddings. Offline evaluation on historical search logs shows consistent gains over a strong generic encoder, with particularly large improvements when replacing PCA compression with Matryoshka truncation. A manual assessment further highlights better handling of proper nouns, marketplace-specific semantics, and term-importance alignment. Additionally, an initial online A/B test demonstrates statistically significant improvements in revenue per user and search-flow efficiency, with transaction frequency maintained. Results show that domain-aware embeddings improve relevance and efficiency at scale and form a practical foundation for richer LLM-era search experiences.","authors":["Andre Rusli","Miao Cao","Shoma Ishimoto","Sho Akiyama","Max Frenzel"],"pdf_url":"","comment":"5 pages, AAAI 2026 Workshop on New Frontiers in Information Retrieval"},{"id":"http://arxiv.org/abs/2512.20950v1","updated":"2025-12-24T05:14:40Z","published":"2025-12-24T05:14:40Z","title":"MultiMind at SemEval-2025 Task 7: Crosslingual Fact-Checked Claim Retrieval via Multi-Source Alignment","summary":"This paper presents our system for SemEval-2025 Task 7: Multilingual and Crosslingual Fact-Checked Claim Retrieval. In an era where misinformation spreads rapidly, effective fact-checking is increasingly critical. We introduce TriAligner, a novel approach that leverages a dual-encoder architecture with contrastive learning and incorporates both native and English translations across different modalities. Our method effectively retrieves claims across multiple languages by learning the relative importance of different sources in alignment. To enhance robustness, we employ efficient data preprocessing and augmentation using large language models while incorporating hard negative sampling to improve representation learning. We evaluate our approach on monolingual and crosslingual benchmarks, demonstrating significant improvements in retrieval accuracy and fact-checking performance over baselines.","authors":["Mohammad Mahdi Abootorabi","Alireza Ghahramani Kure","Mohammadali Mohammadkhani","Sina Elahimanesh","Mohammad Ali Ali Panah"],"pdf_url":"","comment":"11 pages Published at the SemEval-2025 workshop"},{"id":"http://arxiv.org/abs/2512.20916v1","updated":"2025-12-24T03:44:25Z","published":"2025-12-24T03:44:25Z","title":"MMSRARec: Summarization and Retrieval Augumented Sequential Recommendation Based on Multimodal Large Language Model","summary":"Recent advancements in Multimodal Large Language Models (MLLMs) have demonstrated significant potential in recommendation systems. However, the effective application of MLLMs to multimodal sequential recommendation remains unexplored: A) Existing methods primarily leverage the multimodal semantic understanding capabilities of pre-trained MLLMs to generate item embeddings or semantic IDs, thereby enhancing traditional recommendation models. These approaches generate item representations that exhibit limited interpretability, and pose challenges when transferring to language model-based recommendation systems. B) Other approaches convert user behavior sequence into image-text pairs and perform recommendation through multiple MLLM inference, incurring prohibitive computational and time costs. C) Current MLLM-based recommendation systems generally neglect the integration of collaborative signals. To address these limitations while balancing recommendation performance, interpretability, and computational cost, this paper proposes MultiModal Summarization-and-Retrieval-Augmented Sequential Recommendation. Specifically, we first employ MLLM to summarize items into concise keywords and fine-tune the model using rewards that incorporate summary length, information loss, and reconstruction difficulty, thereby enabling adaptive adjustment of the summarization policy. Inspired by retrieval-augmented generation, we then transform collaborative signals into corresponding keywords and integrate them as supplementary context. Finally, we apply supervised fine-tuning with multi-task learning to align the MLLM with the multimodal sequential recommendation. Extensive evaluations on common recommendation datasets demonstrate the effectiveness of MMSRARec, showcasing its capability to efficiently and interpretably understand user behavior histories and item information for accurate recommendations.","authors":["Haoyu Wang","Yitong Wang","Jining Wang"],"pdf_url":"","comment":"Under Review"},{"id":"http://arxiv.org/abs/2512.20896v1","updated":"2025-12-24T02:44:25Z","published":"2025-12-24T02:44:25Z","title":"Accurate and Diverse Recommendations via Propensity-Weighted Linear Autoencoders","summary":"In real-world recommender systems, user-item interactions are Missing Not At Random (MNAR), as interactions with popular items are more frequently observed than those with less popular ones. Missing observations shift recommendations toward frequently interacted items, which reduces the diversity of the recommendation list. To alleviate this problem, Inverse Propensity Scoring (IPS) is widely used and commonly models propensities based on a power-law function of item interaction frequency. However, we found that such power-law-based correction overly penalizes popular items and harms their recommendation performance. We address this issue by redefining the propensity score to allow broader item recommendation without excessively penalizing popular items. The proposed score is formulated by applying a sigmoid function to the logarithm of the item observation frequency, maintaining the simplicity of power-law scoring while allowing for more flexible adjustment. Furthermore, we incorporate the redefined propensity score into a linear autoencoder model, which tends to favor popular items, and evaluate its effectiveness. Experimental results revealed that our method substantially improves the diversity of items in the recommendation list without sacrificing recommendation accuracy.","authors":["Kazuma Onishi","Katsuhiko Hayashi","Hidetaka Kamigaito"],"pdf_url":"","comment":"Published in the proceedings of SIGIR-AP'25"},{"id":"http://arxiv.org/abs/2512.20854v1","updated":"2025-12-24T00:16:31Z","published":"2025-12-24T00:16:31Z","title":"How important is Recall for Measuring Retrieval Quality?","summary":"In realistic retrieval settings with large and evolving knowledge bases, the total number of documents relevant to a query is typically unknown, and recall cannot be computed. In this paper, we evaluate several established strategies for handling this limitation by measuring the correlation between retrieval quality metrics and LLM-based judgments of response quality, where responses are generated from the retrieved documents. We conduct experiments across multiple datasets with a relatively low number of relevant documents (2-15). We also introduce a simple retrieval quality measure that performs well without requiring knowledge of the total number of relevant documents.","authors":["Shelly Schwartz","Oleg Vasilyev","Randy Sawaya"],"pdf_url":"","comment":null}],"Machine Learning":[{"id":"http://arxiv.org/abs/2512.21336v1","updated":"2025-12-24T18:59:51Z","published":"2025-12-24T18:59:51Z","title":"Optimizing Decoding Paths in Masked Diffusion Models by Quantifying Uncertainty","summary":"Masked Diffusion Models (MDMs) offer flexible, non-autoregressive generation, but this freedom introduces a challenge: final output quality is highly sensitive to the decoding order. We are the first to formalize this issue, attributing the variability in output quality to the cumulative predictive uncertainty along a generative path. To quantify this uncertainty, we introduce Denoising Entropy, a computable metric that serves as an internal signal for evaluating generative process. Leveraging this metric, we propose two algorithms designed to optimize the decoding path: a post-hoc selection method and a real-time guidance strategy. Experiments demonstrate that our entropy-guided methods significantly improve generation quality, consistently boosting accuracy on challenging reasoning, planning, and code benchmarks. Our work establishes Denoising Entropy as a principled tool for understanding and controlling generation, effectively turning the uncertainty in MDMs from a liability into a key advantage for discovering high-quality solutions.","authors":["Ziyu Chen","Xinbei Jiang","Peng Sun","Tao Lin"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.21335v1","updated":"2025-12-24T18:59:47Z","published":"2025-12-24T18:59:47Z","title":"Autonomous Uncertainty Quantification for Computational Point-of-care Sensors","summary":"Computational point-of-care (POC) sensors enable rapid, low-cost, and accessible diagnostics in emergency, remote and resource-limited areas that lack access to centralized medical facilities. These systems can utilize neural network-based algorithms to accurately infer a diagnosis from the signals generated by rapid diagnostic tests or sensors. However, neural network-based diagnostic models are subject to hallucinations and can produce erroneous predictions, posing a risk of misdiagnosis and inaccurate clinical decisions. To address this challenge, here we present an autonomous uncertainty quantification technique developed for POC diagnostics. As our testbed, we used a paper-based, computational vertical flow assay (xVFA) platform developed for rapid POC diagnosis of Lyme disease, the most prevalent tick-borne disease globally. The xVFA platform integrates a disposable paper-based assay, a handheld optical reader and a neural network-based inference algorithm, providing rapid and cost-effective Lyme disease diagnostics in under 20 min using only 20 uL of patient serum. By incorporating a Monte Carlo dropout (MCDO)-based uncertainty quantification approach into the diagnostics pipeline, we identified and excluded erroneous predictions with high uncertainty, significantly improving the sensitivity and reliability of the xVFA in an autonomous manner, without access to the ground truth diagnostic information of patients. Blinded testing using new patient samples demonstrated an increase in diagnostic sensitivity from 88.2% to 95.7%, indicating the effectiveness of MCDO-based uncertainty quantification in enhancing the robustness of neural network-driven computational POC sensing systems.","authors":["Artem Goncharov","Rajesh Ghosh","Hyou-Arm Joung","Dino Di Carlo","Aydogan Ozcan"],"pdf_url":"","comment":"18 Pages, 5 Figures"},{"id":"http://arxiv.org/abs/2512.21326v1","updated":"2025-12-24T18:54:37Z","published":"2025-12-24T18:54:37Z","title":"Measuring all the noises of LLM Evals","summary":"Separating signal from noise is central to experimental science. Applying well-established statistical method effectively to LLM evals requires consideration of their unique noise characteristics. We clearly define and measure three types of noise: prediction noise from generating different answers on a given question, data noise from sampling questions, and their combined total noise following the law of total variance. To emphasize relative comparisons and gain statistical power, we propose the all-pairs paired method, which applies the paired analysis to all pairs of LLMs and measures all the noise components based on millions of question-level predictions across many evals and settings. These measurements revealed clear patterns. First, each eval exhibits a characteristic and highly predictable total noise level across all model pairs. Second, paired prediction noise typically exceeds paired data noise, which means reducing prediction noise by averaging can significantly increase statistical power. These findings enable practitioners to assess significance without custom testing and to detect much smaller effects in controlled experiments.","authors":["Sida Wang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.21323v1","updated":"2025-12-24T18:46:55Z","published":"2025-12-24T18:46:55Z","title":"Parallel Token Prediction for Language Models","summary":"We propose Parallel Token Prediction (PTP), a universal framework for parallel sequence generation in language models. PTP jointly predicts multiple dependent tokens in a single transformer call by incorporating the sampling procedure into the model. This reduces the latency bottleneck of autoregressive decoding, and avoids the restrictive independence assumptions common in existing multi-token prediction methods. We prove that PTP can represent arbitrary autoregressive sequence distributions. PTP is trained either by distilling an existing model or through inverse autoregressive training without a teacher. Experimentally, we achieve state-of-the-art speculative decoding performance on Vicuna-7B by accepting over four tokens per step on Spec-Bench. The universality of our framework indicates that parallel generation of long sequences is feasible without loss of modeling power.","authors":["Felix Draxler","Justus Will","Farrin Marouf Sofian","Theofanis Karaletsos","Sameer Singh","Stephan Mandt"],"pdf_url":"","comment":"Preprint. Under review"},{"id":"http://arxiv.org/abs/2511.15172v3","updated":"2025-12-24T18:38:44Z","published":"2025-11-19T06:51:03Z","title":"Complex variational autoencoders admit Kähler structure","summary":"It has been discovered that latent-Euclidean variational autoencoders (VAEs) admit, in various capacities, Riemannian structure. We adapt these arguments but for complex VAEs with a complex latent stage. We show that complex VAEs reveal to some level Kähler geometric structure. Our methods will be tailored for decoder geometry. We derive the Fisher information metric in the complex case under a latent complex Gaussian with trivial relation matrix. It is well known from statistical information theory that the Fisher information coincides with the Hessian of the Kullback-Leibler (KL) divergence. Thus, the metric Kähler potential relation is exactly achieved under relative entropy. We propose a Kähler potential derivative of complex Gaussian mixtures that acts as a rough proxy to the Fisher information metric while still being faithful to the underlying Kähler geometry. Computation of the metric via this potential is efficient, and through our potential, valid as a plurisubharmonic (PSH) function, large scale computational burden of automatic differentiation is displaced to small scale. Our methods leverage the law of total covariance to bridge behavior between our potential and the Fisher metric. We show that we can regularize the latent space with decoder geometry, and that we can sample in accordance with a weighted complex volume element. We demonstrate these strategies, at the exchange of sample variation, yield consistently smoother representations and fewer semantic outliers.","authors":["Andrew Gracyk"],"pdf_url":"","comment":"Corrections and improvements"},{"id":"http://arxiv.org/abs/2512.21319v1","updated":"2025-12-24T18:37:59Z","published":"2025-12-24T18:37:59Z","title":"Variationally correct operator learning: Reduced basis neural operator with a posteriori error estimation","summary":"Minimizing PDE-residual losses is a common strategy to promote physical consistency in neural operators. However, standard formulations often lack variational correctness, meaning that small residuals do not guarantee small solution errors due to the use of non-compliant norms or ad hoc penalty terms for boundary conditions. This work develops a variationally correct operator learning framework by constructing first-order system least-squares (FOSLS) objectives whose values are provably equivalent to the solution error in PDE-induced norms. We demonstrate this framework on stationary diffusion and linear elasticity, incorporating mixed Dirichlet-Neumann boundary conditions via variational lifts to preserve norm equivalence without inconsistent penalties. To ensure the function space conformity required by the FOSLS loss, we propose a Reduced Basis Neural Operator (RBNO). The RBNO predicts coefficients for a pre-computed, conforming reduced basis, thereby ensuring variational stability by design while enabling efficient training. We provide a rigorous convergence analysis that bounds the total error by the sum of finite element discretization bias, reduced basis truncation error, neural network approximation error, and statistical estimation errors arising from finite sampling and optimization. Numerical benchmarks validate these theoretical bounds and demonstrate that the proposed approach achieves superior accuracy in PDE-compliant norms compared to standard baselines, while the residual loss serves as a reliable, computable a posteriori error estimator.","authors":["Yuan Qiu","Wolfgang Dahmen","Peng Chen"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.21315v1","updated":"2025-12-24T18:21:01Z","published":"2025-12-24T18:21:01Z","title":"Does the Data Processing Inequality Reflect Practice? On the Utility of Low-Level Tasks","summary":"The data processing inequality is an information-theoretic principle stating that the information content of a signal cannot be increased by processing the observations. In particular, it suggests that there is no benefit in enhancing the signal or encoding it before addressing a classification problem. This assertion can be proven to be true for the case of the optimal Bayes classifier. However, in practice, it is common to perform \"low-level\" tasks before \"high-level\" downstream tasks despite the overwhelming capabilities of modern deep neural networks. In this paper, we aim to understand when and why low-level processing can be beneficial for classification. We present a comprehensive theoretical study of a binary classification setup, where we consider a classifier that is tightly connected to the optimal Bayes classifier and converges to it as the number of training samples increases. We prove that for any finite number of training samples, there exists a pre-classification processing that improves the classification accuracy. We also explore the effect of class separation, training set size, and class balance on the relative gain from this procedure. We support our theory with an empirical investigation of the theoretical setup. Finally, we conduct an empirical study where we investigate the effect of denoising and encoding on the performance of practical deep classifiers on benchmark datasets. Specifically, we vary the size and class distribution of the training set, and the noise level, and demonstrate trends that are consistent with our theoretical results.","authors":["Roy Turgeman","Tom Tirer"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.21311v1","updated":"2025-12-24T18:14:02Z","published":"2025-12-24T18:14:02Z","title":"Learning to Solve PDEs on Neural Shape Representations","summary":"Solving partial differential equations (PDEs) on shapes underpins many shape analysis and engineering tasks; yet, prevailing PDE solvers operate on polygonal/triangle meshes while modern 3D assets increasingly live as neural representations. This mismatch leaves no suitable method to solve surface PDEs directly within the neural domain, forcing explicit mesh extraction or per-instance residual training, preventing end-to-end workflows. We present a novel, mesh-free formulation that learns a local update operator conditioned on neural (local) shape attributes, enabling surface PDEs to be solved directly where the (neural) data lives. The operator integrates naturally with prevalent neural surface representations, is trained once on a single representative shape, and generalizes across shape and topology variations, enabling accurate, fast inference without explicit meshing or per-instance optimization while preserving differentiability. Across analytic benchmarks (heat equation and Poisson solve on sphere) and real neural assets across different representations, our method slightly outperforms CPM while remaining reasonably close to FEM, and, to our knowledge, delivers the first end-to-end pipeline that solves surface PDEs on both neural and classical surface representations. Code will be released on acceptance.","authors":["Lilian Welschinger","Yilin Liu","Zican Wang","Niloy Mitra"],"pdf_url":"","comment":"Article webpage link: https://welschinger.github.io/Learning-to-Solve-PDEs-on-Neural-Shape-Representations/"},{"id":"http://arxiv.org/abs/2510.22293v2","updated":"2025-12-24T18:06:18Z","published":"2025-10-25T13:36:18Z","title":"Predicting Metabolic Dysfunction-Associated Steatotic Liver Disease using Machine Learning Methods","summary":"Background: Metabolic Dysfunction-Associated Steatotic Liver Disease (MASLD) affects ~33% of U.S. adults and is the most common chronic liver disease. Although often asymptomatic, progression can lead to cirrhosis. Early detection is important, as lifestyle interventions can prevent disease progression. We developed a fair, rigorous, and reproducible MASLD prediction model and compared it to prior methods using a large electronic health record database.\n  Methods: We evaluated LASSO logistic regression, random forest, XGBoost, and a neural network for MASLD prediction using clinical feature subsets, including the top 10 SHAP-ranked features. To reduce disparities in true positive rates across racial and ethnic subgroups, we applied an equal opportunity postprocessing method.\n  Results: This study included 59,492 patients in the training data, 24,198 in the validating data, and 25,188 in the testing data. The LASSO logistic regression model with the top 10 features was selected for its interpretability and comparable performance. Before fairness adjustment, the model achieved AUROC of 0.84, accuracy of 78%, sensitivity of 72%, specificity of 79%, and F1-score of 0.617. After equal opportunity postprocessing, accuracy modestly increased to 81% and specificity to 94%, while sensitivity decreased to 41% and F1-score to 0.515, reflecting the fairness trade-off.\n  Conclusions: We developed the MASER prediction model (MASLD Static EHR Risk Prediction), a LASSO logistic regression model which achieved competitive performance for MASLD prediction (AUROC 0.836, accuracy 77.6%), comparable to previously reported ensemble and tree-based models. Overall, this approach demonstrates that interpretable models can achieve a balance of predictive performance and fairness in diverse patient populations.","authors":["Mary E. An","Paul Griffin","Jonathan G. Stine","Ramakrishna Balakrishnan","Soundar Kumara"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2110.03155v8","updated":"2025-12-24T17:53:45Z","published":"2021-10-07T03:14:46Z","title":"Intrinsic Benefits of Categorical Distributional Loss: Uncertainty-aware Regularized Exploration in Reinforcement Learning","summary":"The remarkable empirical performance of distributional reinforcement learning (RL) has garnered increasing attention to understanding its theoretical advantages over classical RL. By decomposing the categorical distributional loss commonly employed in distributional RL, we find that the potential superiority of distributional RL can be attributed to a derived distribution-matching entropy regularization. This less-studied entropy regularization aims to capture additional knowledge of return distribution beyond only its expectation, contributing to an augmented reward signal in policy optimization. In contrast to the vanilla entropy regularization in MaxEnt RL, which explicitly encourages exploration by promoting diverse actions, the novel entropy regularization derived from categorical distributional loss implicitly updates policies to align the learned policy with (estimated) environmental uncertainty. Finally, extensive experiments verify the significance of this uncertainty-aware regularization from distributional RL on the empirical benefits over classical RL. Our study offers an innovative exploration perspective to explain the intrinsic benefits of distributional learning in RL.","authors":["Ke Sun","Yingnan Zhao","Enze Shi","Yafei Wang","Xiaodong Yan","Bei Jiang","Linglong Kong"],"pdf_url":"","comment":"NeurIPS 2025; Previous Version in ICML Workshop: Exploration in AI Today (EXAIT) 2025"},{"id":"http://arxiv.org/abs/2512.21301v1","updated":"2025-12-24T17:39:37Z","published":"2025-12-24T17:39:37Z","title":"Transcriptome-Conditioned Personalized De Novo Drug Generation for AML Using Metaheuristic Assembly and Target-Driven Filtering","summary":"Acute Myeloid Leukemia (AML) remains a clinical challenge due to its extreme molecular heterogeneity and high relapse rates. While precision medicine has introduced mutation-specific therapies, many patients still lack effective, personalized options. This paper presents a novel, end-to-end computational framework that bridges the gap between patient-specific transcriptomics and de novo drug discovery. By analyzing bulk RNA sequencing data from the TCGA-LAML cohort, the study utilized Weighted Gene Co-expression Network Analysis (WGCNA) to prioritize 20 high-value biomarkers, including metabolic transporters like HK3 and immune-modulatory receptors such as SIGLEC9. The physical structures of these targets were modeled using AlphaFold3, and druggable hotspots were quantitatively mapped via the DOGSiteScorer engine. Then developed a novel, reaction-first evolutionary metaheuristic algorithm as well as multi-objective optimization programming that assembles novel ligands from fragment libraries, guided by spatial alignment to these identified hotspots. The generative model produced structurally unique chemical entities with a strong bias toward drug-like space, as evidenced by QED scores peaking between 0.5 and 0.7. Validation through ADMET profiling and SwissDock molecular docking identified high-confidence candidates, such as Ligand L1, which achieved a binding free energy of -6.571 kcal/mol against the A08A96 biomarker. These results demonstrate that integrating systems biology with metaheuristic molecular assembly can produce pharmacologically viable, patient tailored leads, offering a scalable blueprint for precision oncology in AML and beyond","authors":["Abdullah G. Elafifi","Basma Mamdouh","Mariam Hanafy","Muhammed Alaa Eldin","Yosef Khaled","Nesma Mohamed El-Gelany","Tarek H. M. Abou-El-Enien"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2506.06489v4","updated":"2025-12-24T17:26:35Z","published":"2025-06-06T19:29:13Z","title":"Alternating Gradient Flows: A Theory of Feature Learning in Two-layer Neural Networks","summary":"What features neural networks learn, and how, remains an open question. In this paper, we introduce Alternating Gradient Flows (AGF), an algorithmic framework that describes the dynamics of feature learning in two-layer networks trained from small initialization. Prior works have shown that gradient flow in this regime exhibits a staircase-like loss curve, alternating between plateaus where neurons slowly align to useful directions and sharp drops where neurons rapidly grow in norm. AGF approximates this behavior as an alternating two-step process: maximizing a utility function over dormant neurons and minimizing a cost function over active ones. AGF begins with all neurons dormant. At each iteration, a dormant neuron activates, triggering the acquisition of a feature and a drop in the loss. AGF quantifies the order, timing, and magnitude of these drops, matching experiments across several commonly studied architectures. We show that AGF unifies and extends existing saddle-to-saddle analyses in fully connected linear networks and attention-only linear transformers, where the learned features are singular modes and principal components, respectively. In diagonal linear networks, we prove AGF converges to gradient flow in the limit of vanishing initialization. Applying AGF to quadratic networks trained to perform modular addition, we give the first complete characterization of the training dynamics, revealing that networks learn Fourier features in decreasing order of coefficient magnitude. Altogether, AGF offers a promising step towards understanding feature learning in neural networks.","authors":["Daniel Kunin","Giovanni Luca Marchetti","Feng Chen","Dhruva Karkada","James B. Simon","Michael R. DeWeese","Surya Ganguli","Nina Miolane"],"pdf_url":"","comment":"40 pages, 8 figures, NeurIPS 2025"},{"id":"http://arxiv.org/abs/2502.06096v4","updated":"2025-12-24T17:17:29Z","published":"2025-02-10T02:01:30Z","title":"Post-detection inference for sequential changepoint localization","summary":"This paper addresses a fundamental but largely unexplored challenge in sequential changepoint analysis: conducting inference following a detected change. We develop a very general framework to construct confidence sets for the unknown changepoint using only the data observed up to a data-dependent stopping time at which an arbitrary sequential detection algorithm declares a change. Our framework is nonparametric, making no assumption on the composite post-change class, the observation space, or the sequential detection procedure used, and is non-asymptotically valid. We also extend it to handle composite pre-change classes under a suitable assumption, and also derive confidence sets for the change magnitude in parametric settings. We provide theoretical guarantees on the width of our confidence intervals. Extensive simulations demonstrate that the produced sets have reasonable size, and slightly conservative coverage. In summary, we present the first general method for sequential changepoint localization, which is theoretically sound and broadly applicable in practice.","authors":["Aytijhya Saha","Aaditya Ramdas"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.21288v1","updated":"2025-12-24T17:10:44Z","published":"2025-12-24T17:10:44Z","title":"Model Merging via Multi-Teacher Knowledge Distillation","summary":"Model merging has emerged as a lightweight alternative to joint multi-task learning (MTL), yet the generalization properties of merged models remain largely unexplored. Establishing such theoretical guarantees is non-trivial, as the merging process typically forbids access to the original training data and involves combining fine-tuned models trained on fundamentally heterogeneous data distributions. Without a principled understanding of these dynamics, current methods often rely on heuristics to approximate the optimal combination of parameters. This dependence is most critical in coefficient scaling, the weighting factors that modulate the magnitude of each fine-tuned model's contribution to the shared parameter. However, without a principled objective to guide their selection, these methods lead to brittle performance and are highly sensitive to scaling initialization. We address this gap by (i) establishing a novel flatness-aware PAC-Bayes generalization bound specifically for the model merging setting. This analysis introduces a \"cross-task heterogeneity\" term that formally captures the mismatch between diverse fine-tuned model priors and the target multi-task distributions. Guided by this theoretical insight, (ii) we frame model merging as multi-teacher knowledge distillation on scarce, unlabeled data. We formally demonstrate that minimizing the student-teacher Kullback-Leibler divergence directly tightens the upper bound on the merged model's excess risk. Guided by the flatness-aware bound derived, (iii) we operationalize this objective via SAMerging, a method that employs Sharpness-Aware Minimization (SAM) to find flat minima. Empirically, SAMerging establishes a new state of the art across vision and NLP benchmarks, achieving remarkable performance. The code is available at https://github.com/arshandalili/SAMerging.","authors":["Seyed Arshan Dalili","Mehrdad Mahdavi"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2410.11957v2","updated":"2025-12-24T16:49:53Z","published":"2024-10-15T18:00:13Z","title":"Agnostic Process Tomography","summary":"Characterizing a quantum system by learning its state or evolution is a fundamental problem in quantum physics and learning theory with a myriad of applications. Recently, as a new approach to this problem, the task of agnostic state tomography was defined, in which one aims to approximate an arbitrary quantum state by a simpler one in a given class. Generalizing this notion to quantum processes, we initiate the study of agnostic process tomography: given query access to an unknown quantum channel $Φ$ and a known concept class $\\mathcal{C}$ of channels, output a quantum channel that approximates $Φ$ as well as any channel in the concept class $\\mathcal{C}$, up to some error. In this work, we propose several natural applications for this new task in quantum machine learning, quantum metrology, classical simulation, and error mitigation. In addition, we give efficient agnostic process tomography algorithms for a wide variety of concept classes, including Pauli strings, Pauli channels, quantum junta channels, low-degree channels, and a class of channels produced by $\\mathsf{QAC}^0$ circuits. The main technical tool we use is Pauli spectrum analysis of operators and superoperators. We also prove that, using ancilla qubits, any agnostic state tomography algorithm can be extended to one solving agnostic process tomography for a compatible concept class of unitaries, immediately giving us efficient agnostic learning algorithms for Clifford circuits, Clifford circuits with few T gates, and circuits consisting of a tensor product of single-qubit gates. Together, our results provide insight into the conditions and new algorithms necessary to extend the learnability of a concept class from the standard tomographic setting to the agnostic one.","authors":["Chirag Wadhwa","Laura Lewis","Elham Kashefi","Mina Doosti"],"pdf_url":"","comment":"11+52 pages, 2 figures, 1 table. v2: Minor improvements and edits"},{"id":"http://arxiv.org/abs/2512.21243v1","updated":"2025-12-24T15:36:21Z","published":"2025-12-24T15:36:21Z","title":"LookPlanGraph: Embodied Instruction Following Method with VLM Graph Augmentation","summary":"Methods that use Large Language Models (LLM) as planners for embodied instruction following tasks have become widespread. To successfully complete tasks, the LLM must be grounded in the environment in which the robot operates. One solution is to use a scene graph that contains all the necessary information. Modern methods rely on prebuilt scene graphs and assume that all task-relevant information is available at the start of planning. However, these approaches do not account for changes in the environment that may occur between the graph construction and the task execution. We propose LookPlanGraph - a method that leverages a scene graph composed of static assets and object priors. During plan execution, LookPlanGraph continuously updates the graph with relevant objects, either by verifying existing priors or discovering new entities. This is achieved by processing the agents egocentric camera view using a Vision Language Model. We conducted experiments with changed object positions VirtualHome and OmniGibson simulated environments, demonstrating that LookPlanGraph outperforms methods based on predefined static scene graphs. To demonstrate the practical applicability of our approach, we also conducted experiments in a real-world setting. Additionally, we introduce the GraSIF (Graph Scenes for Instruction Following) dataset with automated validation framework, comprising 514 tasks drawn from SayPlan Office, BEHAVIOR-1K, and VirtualHome RobotHow. Project page available at https://lookplangraph.github.io .","authors":["Anatoly O. Onishchenko","Alexey K. Kovalev","Aleksandr I. Panov"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.21241v1","updated":"2025-12-24T15:35:03Z","published":"2025-12-24T15:35:03Z","title":"Improving the Convergence Rate of Ray Search Optimization for Query-Efficient Hard-Label Attacks","summary":"In hard-label black-box adversarial attacks, where only the top-1 predicted label is accessible, the prohibitive query complexity poses a major obstacle to practical deployment. In this paper, we focus on optimizing a representative class of attacks that search for the optimal ray direction yielding the minimum $\\ell_2$-norm perturbation required to move a benign image into the adversarial region. Inspired by Nesterov's Accelerated Gradient (NAG), we propose a momentum-based algorithm, ARS-OPT, which proactively estimates the gradient with respect to a future ray direction inferred from accumulated momentum. We provide a theoretical analysis of its convergence behavior, showing that ARS-OPT enables more accurate directional updates and achieves faster, more stable optimization. To further accelerate convergence, we incorporate surrogate-model priors into ARS-OPT's gradient estimation, resulting in PARS-OPT with enhanced performance. The superiority of our approach is supported by theoretical guarantees under standard assumptions. Extensive experiments on ImageNet and CIFAR-10 demonstrate that our method surpasses 13 state-of-the-art approaches in query efficiency.","authors":["Xinjie Xu","Shuyu Cheng","Dongwei Xu","Qi Xuan","Chen Ma"],"pdf_url":"","comment":"Published at AAAI 2026 (Oral). This version corresponds to the conference proceedings; v2 will include the appendix"},{"id":"http://arxiv.org/abs/2507.04716v2","updated":"2025-12-24T15:30:39Z","published":"2025-07-07T07:14:42Z","title":"Optimal Model Selection for Conformalized Robust Optimization","summary":"In decision-making under uncertainty, Contextual Robust Optimization (CRO) provides reliability by minimizing the worst-case decision loss over a prediction set. While recent advances use conformal prediction to construct prediction sets for machine learning models, the downstream decisions critically depend on model selection. This paper introduces novel model selection frameworks for CRO that unify robustness control with decision risk minimization. We first propose Conformalized Robust Optimization with Model Selection (CROMS), a framework that selects the model to approximately minimize the averaged decision risk in CRO solutions. Given the target robustness level 1-α, we present a computationally efficient algorithm called E-CROMS, which achieves asymptotic robustness control and decision optimality. To correct the control bias in finite samples, we further develop two algorithms: F-CROMS, which ensures a 1-αrobustness but requires searching the label space; and J-CROMS, which offers lower computational cost while achieving a 1-2αrobustness. Furthermore, we extend the CROMS framework to the individualized setting, where model selection is performed by minimizing the conditional decision risk given the covariates of the test data. This framework advances conformal prediction methodology by enabling covariate-aware model selection. Numerical results demonstrate significant improvements in decision efficiency across diverse synthetic and real-world applications, outperforming baseline approaches.","authors":["Yajie Bao","Yang Hu","Haojie Ren","Peng Zhao","Changliang Zou"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.21238v1","updated":"2025-12-24T15:29:54Z","published":"2025-12-24T15:29:54Z","title":"Assessing the Software Security Comprehension of Large Language Models","summary":"Large language models (LLMs) are increasingly used in software development, but their level of software security expertise remains unclear. This work systematically evaluates the security comprehension of five leading LLMs: GPT-4o-Mini, GPT-5-Mini, Gemini-2.5-Flash, Llama-3.1, and Qwen-2.5, using Blooms Taxonomy as a framework. We assess six cognitive dimensions: remembering, understanding, applying, analyzing, evaluating, and creating. Our methodology integrates diverse datasets, including curated multiple-choice questions, vulnerable code snippets (SALLM), course assessments from an Introduction to Software Security course, real-world case studies (XBOW), and project-based creation tasks from a Secure Software Engineering course. Results show that while LLMs perform well on lower-level cognitive tasks such as recalling facts and identifying known vulnerabilities, their performance degrades significantly on higher-order tasks that require reasoning, architectural evaluation, and secure system creation. Beyond reporting aggregate accuracy, we introduce a software security knowledge boundary that identifies the highest cognitive level at which a model consistently maintains reliable performance. In addition, we identify 51 recurring misconception patterns exhibited by LLMs across Blooms levels.","authors":["Mohammed Latif Siddiq","Natalie Sekerak","Antonio Karam","Maria Leal","Arvin Islam-Gomes","Joanna C. S. Santos"],"pdf_url":"","comment":"Submitted to Empirical Software Engineering (EMSE) journal"},{"id":"http://arxiv.org/abs/2512.20399v2","updated":"2025-12-24T15:28:58Z","published":"2025-12-23T14:40:08Z","title":"GeoTransolver: Learning Physics on Irregular Domains Using Multi-scale Geometry Aware Physics Attention Transformer","summary":"We present GeoTransolver, a Multiscale Geometry-Aware Physics Attention Transformer for CAE that replaces standard attention with GALE, coupling physics-aware self-attention on learned state slices with cross-attention to a shared geometry/global/boundary-condition context computed from multi-scale ball queries (inspired by DoMINO) and reused in every block. Implemented and released in NVIDIA PhysicsNeMo, GeoTransolver persistently projects geometry, global and boundary condition parameters into physical state spaces to anchor latent computations to domain structure and operating regimes. We benchmark GeoTransolver on DrivAerML, Luminary SHIFT-SUV, and Luminary SHIFT-Wing, comparing against Domino, Transolver (as released in PhysicsNeMo), and literature-reported AB-UPT, and evaluate drag/lift R2 and Relative L1 errors for field variables. GeoTransolver delivers better accuracy, improved robustness to geometry/regime shifts, and favorable data efficiency; we include ablations on DrivAerML and qualitative results such as contour plots and design trends for the best GeoTransolver models. By unifying multiscale geometry-aware context with physics-based attention in a scalable transformer, GeoTransolver advances operator learning for high-fidelity surrogate modeling across complex, irregular domains and non-linear physical regimes.","authors":["Corey Adams","Rishikesh Ranade","Ram Cherukuri","Sanjay Choudhry"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.21231v1","updated":"2025-12-24T15:15:18Z","published":"2025-12-24T15:15:18Z","title":"MiST: Understanding the Role of Mid-Stage Scientific Training in Developing Chemical Reasoning Models","summary":"Large Language Models can develop reasoning capabilities through online fine-tuning with rule-based rewards. However, recent studies reveal a critical constraint: reinforcement learning succeeds only when the base model already assigns non-negligible probability to correct answers -- a property we term 'latent solvability'. This work investigates the emergence of chemical reasoning capabilities and what these prerequisites mean for chemistry. We identify two necessary conditions for RL-based chemical reasoning: 1) Symbolic competence, and 2) Latent chemical knowledge. We propose mid-stage scientific training (MiST): a set of mid-stage training techniques to satisfy these, including data-mixing with SMILES/CIF-aware pre-processing, continued pre-training on 2.9B tokens, and supervised fine-tuning on 1B tokens. These steps raise the latent-solvability score on 3B and 7B models by up to 1.8x, and enable RL to lift top-1 accuracy from 10.9 to 63.9% on organic reaction naming, and from 40.6 to 67.4% on inorganic material generation. Similar results are observed for other challenging chemical tasks, while producing interpretable reasoning traces. Our results define clear prerequisites for chemical reasoning training and highlight the broader role of mid-stage training in unlocking reasoning capabilities.","authors":["Andres M Bran","Tong Xie","Shai Pranesh","Jeffrey Meng","Xuan Vu Nguyen","Jeremy Goumaz","David Ming Segura","Ruizhi Xu","Dongzhan Zhou","Wenjie Zhang","Bram Hoex","Philippe Schwaller"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2510.23117v4","updated":"2025-12-24T14:58:32Z","published":"2025-10-27T08:38:17Z","title":"Seeing Structural Failure Before it Happens: An Image-Based Physics-Informed Neural Network (PINN) for Spaghetti Bridge Load Prediction","summary":"Physics Informed Neural Networks (PINNs) are gaining attention for their ability to embed physical laws into deep learning models, which is particularly useful in structural engineering tasks with limited data. This paper aims to explore the use of PINNs to predict the weight of small scale spaghetti bridges, a task relevant to understanding load limits and potential failure modes in simplified structural models. Our proposed framework incorporates physics-based constraints to the prediction model for improved performance. In addition to standard PINNs, we introduce a novel architecture named Physics Informed Kolmogorov Arnold Network (PIKAN), which blends universal function approximation theory with physical insights. The structural parameters provided as input to the model are collected either manually or through computer vision methods. Our dataset includes 15 real bridges, augmented to 100 samples, and our best model achieves an $R^2$ score of 0.9603 and a mean absolute error (MAE) of 10.50 units. From applied perspective, we also provide a web based interface for parameter entry and prediction. These results show that PINNs can offer reliable estimates of structural weight, even with limited data, and may help inform early stage failure analysis in lightweight bridge designs.\n  The complete data and code are available at https://github.com/OmerJauhar/PINNS-For-Spaghetti-Bridges.","authors":["Omer Jauhar Khan","Sudais Khan","Hafeez Anwar","Shahzeb Khan","Shams Ul Arifeen","Farman Ullah"],"pdf_url":"","comment":"14 pages, 21 figures. Preprint"},{"id":"http://arxiv.org/abs/2507.13704v3","updated":"2025-12-24T14:56:07Z","published":"2025-07-18T07:12:19Z","title":"A study of EHVI vs fixed scalarization for molecule design","summary":"Multi-objective Bayesian optimization (MOBO) provides a principled framework for navigating trade-offs in molecular design. However, its empirical advantages over scalarized alternatives remain underexplored. We benchmark a simple Pareto-based MOBO strategy - Expected Hypervolume Improvement (EHVI) - against a simple fixed-weight scalarized baseline using Expected Improvement (EI), under a tightly controlled setup with identical Gaussian Process surrogates and molecular representations. Across three molecular optimization tasks, EHVI consistently outperforms scalarized EI in terms of Pareto front coverage, convergence speed, and chemical diversity. While scalarization encompasses flexible variants - including random or adaptive schemes - our results show that even strong deterministic instantiations can underperform in low-data regimes. These findings offer concrete evidence for the practical advantages of Pareto-aware acquisition in de novo molecular optimization, especially when evaluation budgets are limited and trade-offs are nontrivial.","authors":["Anabel Yong","Austin Tripp","Layla Hosseini-Gerami","Brooks Paige"],"pdf_url":"","comment":"Accepted to NeurIPS AI4Science Workshop 2025"},{"id":"http://arxiv.org/abs/2508.21010v2","updated":"2025-12-24T14:52:45Z","published":"2025-08-28T17:10:53Z","title":"ChainReaction: Causal Chain-Guided Reasoning for Modular and Explainable Causal-Why Video Question Answering","summary":"Existing Causal-Why Video Question Answering (VideoQA) models often struggle with higher-order reasoning, relying on opaque, monolithic pipelines that entangle video understanding, causal inference, and answer generation. These black-box approaches offer limited interpretability and tend to depend on shallow heuristics. We propose a novel, modular paradigm that explicitly decouples causal reasoning from answer generation, introducing natural language causal chains as interpretable intermediate representations. Inspired by human cognitive models, these structured cause-effect sequences bridge low-level video content with high-level causal reasoning, enabling transparent and logically coherent inference. Our two-stage architecture comprises a Causal Chain Extractor (CCE) that generates causal chains from video-question pairs, and a Causal Chain-Driven Answerer (CCDA) that derives answers grounded in these chains. To address the lack of annotated reasoning traces, we introduce a scalable method for generating accurate causal chains from existing datasets. We construct human verified causal chains for 46K samples. We also propose CauCo, a new evaluation metric for causality-oriented captioning. Experiments on three large-scale benchmarks demonstrate that our approach not only outperforms state-of-the-art models, but also yields substantial gains in explainability, user trust, and generalization -- positioning the CCE as a reusable causal reasoning engine across diverse domains. Project page: https://paritoshparmar.github.io/chainreaction/","authors":["Paritosh Parmar","Eric Peh","Basura Fernando"],"pdf_url":"","comment":"Project page: https://paritoshparmar.github.io/chainreaction/"},{"id":"http://arxiv.org/abs/2512.21211v1","updated":"2025-12-24T14:51:12Z","published":"2025-12-24T14:51:12Z","title":"Causal-driven attribution (CDA): Estimating channel influence without user-level data","summary":"Attribution modelling lies at the heart of marketing effectiveness, yet most existing approaches depend on user-level path data, which are increasingly inaccessible due to privacy regulations and platform restrictions. This paper introduces a Causal-Driven Attribution (CDA) framework that infers channel influence using only aggregated impression-level data, avoiding any reliance on user identifiers or click-path tracking. CDA integrates temporal causal discovery (using PCMCI) with causal effect estimation via a Structural Causal Model to recover directional channel relationships and quantify their contributions to conversions. Using large-scale synthetic data designed to replicate real marketing dynamics, we show that CDA achieves an average relative RMSE of 9.50% when given the true causal graph, and 24.23% when using the predicted graph, demonstrating strong accuracy under correct structure and meaningful signal recovery even under structural uncertainty. CDA captures cross-channel interdependencies while providing interpretable, privacy-preserving attribution insights, offering a scalable and future-proof alternative to traditional path-based models.","authors":["Georgios Filippou","Boi Mai Quach","Diana Lenghel","Arthur White","Ashish Kumar Jha"],"pdf_url":"","comment":"42 pages, 8 figures, submitted initially to the journal of the academy of marketing science on 24th Dec 2025"},{"id":"http://arxiv.org/abs/2512.21208v1","updated":"2025-12-24T14:43:59Z","published":"2025-12-24T14:43:59Z","title":"Analytic and Variational Stability of Deep Learning Systems","summary":"We propose a unified analytic and variational framework for studying stability in deep learning systems viewed as coupled representation-parameter dynamics. The central object is the Learning Stability Profile, which tracks the infinitesimal response of representations, parameters, and update mechanisms to perturbations along the learning trajectory. We prove a Fundamental Analytic Stability Theorem showing that uniform boundedness of these stability signatures is equivalent, up to norm equivalence, to the existence of a Lyapunov-type energy that dissipates along the learning flow. In smooth regimes, the framework yields explicit stability exponents linking spectral norms, activation regularity, step sizes, and learning rates to contractivity of the learning dynamics. Classical spectral stability results for feedforward networks, a discrete CFL-type condition for residual architectures, and parametric and temporal stability laws for stochastic gradient methods arise as direct consequences. The theory extends to non-smooth learning systems, including ReLU networks, proximal and projected updates, and stochastic subgradient flows, by replacing classical derivatives with Clarke generalized derivatives and smooth energies with variational Lyapunov functionals. The resulting framework provides a unified dynamical description of stability across architectures and optimization methods, clarifying how architectural and algorithmic choices jointly govern robustness and sensitivity to perturbations. It also provides a foundation for further extensions to continuous-time limits and geometric formulations of learning dynamics.","authors":["Ronald Katende"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2505.11998v3","updated":"2025-12-24T13:44:53Z","published":"2025-05-17T13:19:01Z","title":"Parameter Efficient Continual Learning with Dynamic Low-Rank Adaptation","summary":"Catastrophic forgetting has remained a critical challenge for deep neural networks in Continual Learning (CL) as it undermines consolidated knowledge when learning new tasks. Parameter efficient fine tuning CL techniques are gaining traction for their effectiveness in addressing catastrophic forgetting with a lightweight training schedule while avoiding degradation of consolidated knowledge in pre-trained models. However, low rank adapters (LoRA) in these approaches are highly sensitive to rank selection which can lead to sub-optimal resource allocation and performance. To this end, we introduce PEARL, a rehearsal-free CL framework that entails dynamic rank allocation for LoRA components during CL training. Specifically, PEARL leverages reference task weights and adaptively determines the rank of task-specific LoRA components based on the current tasks' proximity to reference task weights in parameter space. To demonstrate the versatility of PEARL, we evaluate it across three vision architectures (ResNet, Separable Convolutional Network and Vision Transformer) and a multitude of CL scenarios, and show that PEARL outperforms all considered baselines by a large margin.","authors":["Prashant Shivaram Bhat","Shakib Yazdani","Elahe Arani","Bahram Zonooz"],"pdf_url":"","comment":"27 pages, 5 figures"},{"id":"http://arxiv.org/abs/2512.21170v1","updated":"2025-12-24T13:39:11Z","published":"2025-12-24T13:39:11Z","title":"A Unified Framework for EEG Seizure Detection Using Universum-Integrated Generalized Eigenvalues Proximal Support Vector Machine","summary":"The paper presents novel Universum-enhanced classifiers: the Universum Generalized Eigenvalue Proximal Support Vector Machine (U-GEPSVM) and the Improved U-GEPSVM (IU-GEPSVM) for EEG signal classification. Using the computational efficiency of generalized eigenvalue decomposition and the generalization benefits of Universum learning, the proposed models address critical challenges in EEG analysis: non-stationarity, low signal-to-noise ratio, and limited labeled data. U-GEPSVM extends the GEPSVM framework by incorporating Universum constraints through a ratio-based objective function, while IU-GEPSVM enhances stability through a weighted difference-based formulation that provides independent control over class separation and Universum alignment. The models are evaluated on the Bonn University EEG dataset across two binary classification tasks: (O vs S)-healthy (eyes closed) vs seizure, and (Z vs S)-healthy (eyes open) vs seizure. IU-GEPSVM achieves peak accuracies of 85% (O vs S) and 80% (Z vs S), with mean accuracies of 81.29% and 77.57% respectively, outperforming baseline methods.","authors":["Yogesh Kumar","Vrushank Ahire","M. A. Ganaie"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.04332v3","updated":"2025-12-24T13:37:40Z","published":"2025-12-03T23:45:07Z","title":"Data-regularized Reinforcement Learning for Diffusion Models at Scale","summary":"Aligning generative diffusion models with human preferences via reinforcement learning (RL) is critical yet challenging. Most existing algorithms are often vulnerable to reward hacking, such as quality degradation, over-stylization, or reduced diversity. Our analysis demonstrates that this can be attributed to the inherent limitations of their regularization, which provides unreliable penalties. We introduce Data-regularized Diffusion Reinforcement Learning (DDRL), a novel framework that uses the forward KL divergence to anchor the policy to an off-policy data distribution. Theoretically, DDRL enables robust, unbiased integration of RL with standard diffusion training. Empirically, this translates into a simple yet effective algorithm that combines reward maximization with diffusion loss minimization. With over a million GPU hours of experiments and ten thousand double-blind human evaluations, we demonstrate on high-resolution video generation tasks that DDRL significantly improves rewards while alleviating the reward hacking seen in baselines, achieving the highest human preference and establishing a robust and scalable paradigm for diffusion post-training.","authors":["Haotian Ye","Kaiwen Zheng","Jiashu Xu","Puheng Li","Huayu Chen","Jiaqi Han","Sheng Liu","Qinsheng Zhang","Hanzi Mao","Zekun Hao","Prithvijit Chattopadhyay","Dinghao Yang","Liang Feng","Maosheng Liao","Junjie Bai","Ming-Yu Liu","James Zou","Stefano Ermon"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.21166v1","updated":"2025-12-24T13:31:34Z","published":"2025-12-24T13:31:34Z","title":"A Community-Enhanced Graph Representation Model for Link Prediction","summary":"Although Graph Neural Networks (GNNs) have become the dominant approach for graph representation learning, their performance on link prediction tasks does not always surpass that of traditional heuristic methods such as Common Neighbors and Jaccard Coefficient. This is mainly because existing GNNs tend to focus on learning local node representations, making it difficult to effectively capture structural relationships between node pairs. Furthermore, excessive reliance on local neighborhood information can lead to over-smoothing. Prior studies have shown that introducing global structural encoding can partially alleviate this issue. To address these limitations, we propose a Community-Enhanced Link Prediction (CELP) framework that incorporates community structure to jointly model local and global graph topology. Specifically, CELP enhances the graph via community-aware, confidence-guided edge completion and pruning, while integrating multi-scale structural features to achieve more accurate link prediction. Experimental results across multiple benchmark datasets demonstrate that CELP achieves superior performance, validating the crucial role of community structure in improving link prediction accuracy.","authors":["Lei Wang","Darong Lai"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2511.12867v2","updated":"2025-12-24T13:31:20Z","published":"2025-11-17T01:41:14Z","title":"Bootstrapping LLMs via Preference-Based Policy Optimization","summary":"Bootstrapping large language models (LLMs) through preference-based policy optimization offers a promising direction for aligning model behavior with human preferences without relying on extensive manual annotations. In this work, we propose a novel preference-based policy optimization (PbPO) framework that formulates the learning process as a min-max game between the main policy and a reward model (RM). The RM is constrained within a confidence set derived from preference data to ensure reliable exploitation. Our iterative online algorithm actively collects preference data through guided exploration of the evolving policy, enabling continual self-improvement of both the policy and the RM. We provide theoretical guarantees for our method, establishing high-probability regret bounds for both settings with sequence-level RM and token-level RM, demonstrating its effectiveness in bootstrapping LLMs. Extensive experiments on five benchmarks show that our approach consistently outperforms existing state-of-the-art preference optimization techniques.","authors":["Chen Jia"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.21165v1","updated":"2025-12-24T13:25:36Z","published":"2025-12-24T13:25:36Z","title":"BALLAST: Bandit-Assisted Learning for Latency-Aware Stable Timeouts in Raft","summary":"Randomized election timeouts are a simple and effective liveness heuristic for Raft, but they become brittle under long-tail latency, jitter, and partition recovery, where repeated split votes can inflate unavailability. This paper presents BALLAST, a lightweight online adaptation mechanism that replaces static timeout heuristics with contextual bandits. BALLAST selects from a discrete set of timeout \"arms\" using efficient linear contextual bandits (LinUCB variants), and augments learning with safe exploration to cap risk during unstable periods. We evaluate BALLAST on a reproducible discrete-event simulation with long-tail delay, loss, correlated bursts, node heterogeneity, and partition/recovery turbulence. Across challenging WAN regimes, BALLAST substantially reduces recovery time and unwritable time compared to standard randomized timeouts and common heuristics, while remaining competitive on stable LAN/WAN settings.","authors":["Qizhi Wang"],"pdf_url":"","comment":"15 pages, 22 tables, 11 figures"},{"id":"http://arxiv.org/abs/2512.21153v1","updated":"2025-12-24T12:45:36Z","published":"2025-12-24T12:45:36Z","title":"ElfCore: A 28nm Neural Processor Enabling Dynamic Structured Sparse Training and Online Self-Supervised Learning with Activity-Dependent Weight Update","summary":"In this paper, we present ElfCore, a 28nm digital spiking neural network processor tailored for event-driven sensory signal processing. ElfCore is the first to efficiently integrate: (1) a local online self-supervised learning engine that enables multi-layer temporal learning without labeled inputs; (2) a dynamic structured sparse training engine that supports high-accuracy sparse-to-sparse learning; and (3) an activity-dependent sparse weight update mechanism that selectively updates weights based solely on input activity and network dynamics. Demonstrated on tasks including gesture recognition, speech, and biomedical signal processing, ElfCore outperforms state-of-the-art solutions with up to 16X lower power consumption, 3.8X reduced on-chip memory requirements, and 5.9X greater network capacity efficiency.","authors":["Zhe Su","Giacomo Indiveri"],"pdf_url":"","comment":"This paper has been published in the proceedings of the 2025 IEEE European Solid-State Electronics Research Conference (ESSERC)"},{"id":"http://arxiv.org/abs/2512.21152v1","updated":"2025-12-24T12:43:40Z","published":"2025-12-24T12:43:40Z","title":"MODE: Multi-Objective Adaptive Coreset Selection","summary":"We present Mode(Multi-Objective adaptive Data Efficiency), a framework that dynamically combines coreset selection strategies based on their evolving contribution to model performance. Unlike static methods, \\mode adapts selection criteria to training phases: emphasizing class balance early, diversity during representation learning, and uncertainty at convergence. We show that MODE achieves (1-1/e)-approximation with O(n \\log n) complexity and demonstrates competitive accuracy while providing interpretable insights into data utility evolution. Experiments show \\mode reduces memory requirements","authors":["Tanmoy Mukherjee","Pierre Marquis","Zied Bouraoui"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.21132v1","updated":"2025-12-24T12:02:00Z","published":"2025-12-24T12:02:00Z","title":"AutoBaxBuilder: Bootstrapping Code Security Benchmarking","summary":"As LLMs see wide adoption in software engineering, the reliable assessment of the correctness and security of LLM-generated code is crucial. Notably, prior work has demonstrated that security is often overlooked, exposing that LLMs are prone to generating code with security vulnerabilities. These insights were enabled by specialized benchmarks, crafted through significant manual effort by security experts. However, relying on manually-crafted benchmarks is insufficient in the long term, because benchmarks (i) naturally end up contaminating training data, (ii) must extend to new tasks to provide a more complete picture, and (iii) must increase in difficulty to challenge more capable LLMs. In this work, we address these challenges and present AutoBaxBuilder, a framework that generates tasks and tests for code security benchmarking from scratch. We introduce a robust pipeline with fine-grained plausibility checks, leveraging the code understanding capabilities of LLMs to construct functionality tests and end-to-end security-probing exploits. To confirm the quality of the generated benchmark, we conduct both a qualitative analysis and perform quantitative experiments, comparing it against tasks constructed by human experts. We use AutoBaxBuilder to construct entirely new tasks and release them to the public as AutoBaxBench, together with a thorough evaluation of the security capabilities of LLMs on these tasks. We find that a new task can be generated in under 2 hours, costing less than USD 10.","authors":["Tobias von Arx","Niels Mündler","Mark Vero","Maximilian Baader","Martin Vechev"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2505.23383v3","updated":"2025-12-24T11:43:45Z","published":"2025-05-29T12:04:07Z","title":"Automated Modeling Method for Pathloss Model Discovery","summary":"Modeling propagation is the cornerstone for designing and optimizing next-generation wireless systems, with a particular emphasis on 5G and beyond era. Traditional modeling methods have long relied on statistic-based techniques to characterize propagation behavior across different environments. With the expansion of wireless communication systems, there is a growing demand for methods that guarantee the accuracy and interpretability of modeling. Artificial intelligence (AI)-based techniques, in particular, are increasingly being adopted to overcome this challenge, although the interpretability is not assured with most of these methods. Inspired by recent advancements in AI, this paper proposes a novel approach that accelerates the discovery of path loss models while maintaining interpretability. The proposed method automates the formulation, evaluation, and refinement of the model, facilitating the discovery of the model. We examine two techniques: one based on Deep Symbolic Regression, offering full interpretability, and the second based on Kolmogorov-Arnold Networks, providing two levels of interpretability. Both approaches are evaluated on two synthetic and two real-world datasets. Our results show that Kolmogorov-Arnold Networks achieve the coefficient of determination value R^2 close to 1 with minimal prediction error, while Deep Symbolic Regression generates compact models with moderate accuracy. Moreover, on the selected examples, we demonstrate that automated methods outperform traditional methods, achieving up to 75% reduction in prediction errors, offering accurate and explainable solutions with potential to increase the efficiency of discovering next-generation path loss models.","authors":["Ahmad Anaqreh","Shih-Kai Chou","Mihael Mohorčič","Thomas Lagkas","Carolina Fortuna"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.21118v1","updated":"2025-12-24T11:34:44Z","published":"2025-12-24T11:34:44Z","title":"STLDM: Spatio-Temporal Latent Diffusion Model for Precipitation Nowcasting","summary":"Precipitation nowcasting is a critical spatio-temporal prediction task for society to prevent severe damage owing to extreme weather events. Despite the advances in this field, the complex and stochastic nature of this task still poses challenges to existing approaches. Specifically, deterministic models tend to produce blurry predictions while generative models often struggle with poor accuracy. In this paper, we present a simple yet effective model architecture termed STLDM, a diffusion-based model that learns the latent representation from end to end alongside both the Variational Autoencoder and the conditioning network. STLDM decomposes this task into two stages: a deterministic forecasting stage handled by the conditioning network, and an enhancement stage performed by the latent diffusion model. Experimental results on multiple radar datasets demonstrate that STLDM achieves superior performance compared to the state of the art, while also improving inference efficiency. The code is available in https://github.com/sqfoo/stldm_official.","authors":["Shi Quan Foo","Chi-Ho Wong","Zhihan Gao","Dit-Yan Yeung","Ka-Hing Wong","Wai-Kin Wong"],"pdf_url":"","comment":"Accepted by TMLR. Camera-ready submission"},{"id":"http://arxiv.org/abs/2512.21113v1","updated":"2025-12-24T11:21:07Z","published":"2025-12-24T11:21:07Z","title":"A Mechanistic Analysis of Transformers for Dynamical Systems","summary":"Transformers are increasingly adopted for modeling and forecasting time-series, yet their internal mechanisms remain poorly understood from a dynamical systems perspective. In contrast to classical autoregressive and state-space models, which benefit from well-established theoretical foundations, Transformer architectures are typically treated as black boxes. This gap becomes particularly relevant as attention-based models are considered for general-purpose or zero-shot forecasting across diverse dynamical regimes. In this work, we do not propose a new forecasting model, but instead investigate the representational capabilities and limitations of single-layer Transformers when applied to dynamical data. Building on a dynamical systems perspective we interpret causal self-attention as a linear, history-dependent recurrence and analyze how it processes temporal information. Through a series of linear and nonlinear case studies, we identify distinct operational regimes. For linear systems, we show that the convexity constraint imposed by softmax attention fundamentally restricts the class of dynamics that can be represented, leading to oversmoothing in oscillatory settings. For nonlinear systems under partial observability, attention instead acts as an adaptive delay-embedding mechanism, enabling effective state reconstruction when sufficient temporal context and latent dimensionality are available. These results help bridge empirical observations with classical dynamical systems theory, providing insight into when and why Transformers succeed or fail as models of dynamical systems.","authors":["Gregory Duthé","Nikolaos Evangelou","Wei Liu","Ioannis G. Kevrekidis","Eleni Chatzi"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.21107v1","updated":"2025-12-24T11:12:09Z","published":"2025-12-24T11:12:09Z","title":"Semi-Supervised Learning for Large Language Models Safety and Content Moderation","summary":"Safety for Large Language Models (LLMs) has been an ongoing research focus since their emergence and is even more relevant nowadays with the increasing capacity of those models. Currently, there are several guardrails in place for all public LLMs and multiple proposed datasets for training safety classifiers. However, training these safety classifiers relies on large quantities of labeled data, which can be problematic to acquire, prone to labeling errors, or often include synthetic data. To address these issues, we suggest a different approach: utilizing semi-supervised learning techniques, which leverage both labeled and unlabeled data, to improve the performance on the safety task. We analyze the improvements that these techniques can offer for both prompts given to Large Language Models and the responses to those requests. Moreover, since augmentation is the central part of semi-supervised algorithms, we demonstrate the importance of using task-specific augmentations, which significantly increase the performance when compared to general-purpose augmentation techniques.","authors":["Eduard Stefan Dinuta","Iustin Sirbu","Traian Rebedea"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.21106v1","updated":"2025-12-24T11:10:28Z","published":"2025-12-24T11:10:28Z","title":"Semantic Refinement with LLMs for Graph Representations","summary":"Graph-structured data exhibit substantial heterogeneity in where their predictive signals originate: in some domains, node-level semantics dominate, while in others, structural patterns play a central role. This structure-semantics heterogeneity implies that no graph learning model with a fixed inductive bias can generalize optimally across diverse graph domains. However, most existing methods address this challenge from the model side by incrementally injecting new inductive biases, which remains fundamentally limited given the open-ended diversity of real-world graphs. In this work, we take a data-centric perspective and treat node semantics as a task-adaptive variable. We propose a Data-Adaptive Semantic Refinement framework DAS for graph representation learning, which couples a fixed graph neural network (GNN) and a large language model (LLM) in a closed feedback loop. The GNN provides implicit supervisory signals to guide the semantic refinement of LLM, and the refined semantics are fed back to update the same graph learner. We evaluate our approach on both text-rich and text-free graphs. Results show consistent improvements on structure-dominated graphs while remaining competitive on semantics-rich graphs, demonstrating the effectiveness of data-centric semantic adaptation under structure-semantics heterogeneity.","authors":["Safal Thapaliya","Zehong Wang","Jiazheng Li","Ziming Li","Yanfang Ye","Chuxu Zhang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.21102v1","updated":"2025-12-24T11:02:03Z","published":"2025-12-24T11:02:03Z","title":"Shared Representation Learning for High-Dimensional Multi-Task Forecasting under Resource Contention in Cloud-Native Backends","summary":"This study proposes a unified forecasting framework for high-dimensional multi-task time series to meet the prediction demands of cloud native backend systems operating under highly dynamic loads, coupled metrics, and parallel tasks. The method builds a shared encoding structure to represent diverse monitoring indicators in a unified manner and employs a state fusion mechanism to capture trend changes and local disturbances across different time scales. A cross-task structural propagation module is introduced to model potential dependencies among nodes, enabling the model to understand complex structural patterns formed by resource contention, link interactions, and changes in service topology. To enhance adaptability to non-stationary behaviors, the framework incorporates a dynamic adjustment mechanism that automatically regulates internal feature flows according to system state changes, ensuring stable predictions in the presence of sudden load shifts, topology drift, and resource jitter. The experimental evaluation compares multiple models across various metrics and verifies the effectiveness of the framework through analyses of hyperparameter sensitivity, environmental sensitivity, and data sensitivity. The results show that the proposed method achieves superior performance on several error metrics and provides more accurate representations of future states under different operating conditions. Overall, the unified forecasting framework offers reliable predictive capability for high-dimensional, multi-task, and strongly dynamic environments in cloud native systems and provides essential technical support for intelligent backend management.","authors":["Zixiao Huang","Jixiao Yang","Sijia Li","Chi Zhang","Jinyu Chen","Chengda Xu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.11831v3","updated":"2025-12-24T10:53:25Z","published":"2025-12-03T09:28:29Z","title":"On the Design of One-step Diffusion via Shortcutting Flow Paths","summary":"Recent advances in few-step diffusion models have demonstrated their efficiency and effectiveness by shortcutting the probabilistic paths of diffusion models, especially in training one-step diffusion models from scratch (\\emph{a.k.a.} shortcut models). However, their theoretical derivation and practical implementation are often closely coupled, which obscures the design space. To address this, we propose a common design framework for representative shortcut models. This framework provides theoretical justification for their validity and disentangles concrete component-level choices, thereby enabling systematic identification of improvements. With our proposed improvements, the resulting one-step model achieves a new state-of-the-art FID50k of 2.85 on ImageNet-256x256 under the classifier-free guidance setting with one step generation, and further reaches FID50k of 2.53 with 2x training steps. Remarkably, the model requires no pre-training, distillation, or curriculum learning. We believe our work lowers the barrier to component-level innovation in shortcut models and facilitates principled exploration of their design space.","authors":["Haitao Lin","Peiyan Hu","Minsi Ren","Zhifeng Gao","Zhi-Ming Ma","Guolin ke","Tailin Wu","Stan Z. Li"],"pdf_url":"","comment":"10 pages of main body, conference paper"},{"id":"http://arxiv.org/abs/2502.11609v3","updated":"2025-12-24T10:22:07Z","published":"2025-02-17T09:52:19Z","title":"Exploiting Task Relationships in Continual Learning via Transferability-Aware Task Embeddings","summary":"Continual learning (CL) has been a critical topic in contemporary deep neural network applications, where higher levels of both forward and backward transfer are desirable for an effective CL performance. Existing CL strategies primarily focus on task models, either by regularizing model updates or by separating task-specific and shared components, while often overlooking the potential of leveraging inter-task relationships to enhance transfer. To address this gap, we propose a transferability-aware task embedding, termed H-embedding, and construct a hypernet framework under its guidance to learn task-conditioned model weights for CL tasks. Specifically, H-embedding is derived from an information theoretic measure of transferability and is designed to be online and easy to compute. Our method is also characterized by notable practicality, requiring only the storage of a low-dimensional task embedding per task and supporting efficient end-to-end training. Extensive evaluations on benchmarks including CIFAR-100, ImageNet-R, and DomainNet show that our framework performs prominently compared to various baseline and SOTA approaches, demonstrating strong potential in capturing and utilizing intrinsic task relationships. Our code is publicly available at https://github.com/viki760/H-embedding-Guided-Hypernet.","authors":["Yanru Wu","Jianning Wang","Xiangyu Chen","Enming Zhang","Yang Tan","Hanbing Liu","Yang Li"],"pdf_url":"","comment":"28 pages, 5 figures, accepted by NeurIPS 2025"},{"id":"http://arxiv.org/abs/2210.13327v2","updated":"2025-12-24T10:04:30Z","published":"2022-10-24T15:28:43Z","title":"Deep Kronecker Network","summary":"We propose Deep Kronecker Network (DKN), a novel framework designed for analyzing medical imaging data, such as MRI, fMRI, CT, etc. Medical imaging data is different from general images in at least two aspects: i) sample size is usually much more limited, ii) model interpretation is more of a concern compared to outcome prediction. Due to its unique nature, general methods, such as convolutional neural network (CNN), are difficult to be directly applied. As such, we propose DKN, that is able to i) adapt to low sample size limitation, ii) provide desired model interpretation, and iii) achieve the prediction power as CNN. The DKN is general in the sense that it not only works for both matrix and (high-order) tensor represented image data, but also could be applied to both discrete and continuous outcomes. The DKN is built on a Kronecker product structure and implicitly imposes a piecewise smooth property on coefficients. Moreover, the Kronecker structure can be written into a convolutional form, so DKN also resembles a CNN, particularly, a fully convolutional network (FCN). Furthermore, we prove that with an alternating minimization algorithm, the solutions of DKN are guaranteed to converge to the truth geometrically even if the objective function is highly nonconvex. Interestingly, the DKN is also highly connected to the tensor regression framework proposed by Zhou et al. (2010), where a CANDECOMP/PARAFAC (CP) low-rank structure is imposed on tensor coefficients. Finally, we conduct both classification and regression analyses using real MRI data from the Alzheimer's Disease Neuroimaging Initiative (ADNI) to demonstrate the effectiveness of DKN.","authors":["Long Feng","Guang Yang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.21083v1","updated":"2025-12-24T09:58:30Z","published":"2025-12-24T09:58:30Z","title":"Hierarchical Modeling Approach to Fast and Accurate Table Recognition","summary":"The extraction and use of diverse knowledge from numerous documents is a pressing challenge in intelligent information retrieval. Documents contain elements that require different recognition methods. Table recognition typically consists of three subtasks, namely table structure, cell position and cell content recognition. Recent models have achieved excellent recognition with a combination of multi-task learning, local attention, and mutual learning. However, their effectiveness has not been fully explained, and they require a long period of time for inference. This paper presents a novel multi-task model that utilizes non-causal attention to capture the entire table structure, and a parallel inference algorithm for faster cell content inference. The superiority is demonstrated both visually and statistically on two large public datasets.","authors":["Takaya Kawakatsu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.21081v1","updated":"2025-12-24T09:56:28Z","published":"2025-12-24T09:56:28Z","title":"Dyna-Style Reinforcement Learning Modeling and Control of Non-linear Dynamics","summary":"Controlling systems with complex, nonlinear dynamics poses a significant challenge, particularly in achieving efficient and robust control. In this paper, we propose a Dyna-Style Reinforcement Learning control framework that integrates Sparse Identification of Nonlinear Dynamics (SINDy) with Twin Delayed Deep Deterministic Policy Gradient (TD3) reinforcement learning. SINDy is used to identify a data-driven model of the system, capturing its key dynamics without requiring an explicit physical model. This identified model is used to generate synthetic rollouts that are periodically injected into the reinforcement learning replay buffer during training on the real environment, enabling efficient policy learning with limited data available. By leveraging this hybrid approach, we mitigate the sample inefficiency of traditional model-free reinforcement learning methods while ensuring accurate control of nonlinear systems. To demonstrate the effectiveness of this framework, we apply it to a bi-rotor system as a case study, evaluating its performance in stabilization and trajectory tracking. The results show that our SINDy-TD3 approach achieves superior accuracy and robustness compared to direct reinforcement learning techniques, highlighting the potential of combining data-driven modeling with reinforcement learning for complex dynamical systems.","authors":["Karim Abdelsalam","Zeyad Gamal","Ayman El-Badawy"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.21080v1","updated":"2025-12-24T09:56:00Z","published":"2025-12-24T09:56:00Z","title":"LLM Personas as a Substitute for Field Experiments in Method Benchmarking","summary":"Field experiments (A/B tests) are often the most credible benchmark for methods in societal systems, but their cost and latency create a major bottleneck for iterative method development. LLM-based persona simulation offers a cheap synthetic alternative, yet it is unclear whether replacing humans with personas preserves the benchmark interface that adaptive methods optimize against. We prove an if-and-only-if characterization: when (i) methods observe only the aggregate outcome (aggregate-only observation) and (ii) evaluation depends only on the submitted artifact and not on the algorithm's identity or provenance (algorithm-blind evaluation), swapping humans for personas is just panel change from the method's point of view, indistinguishable from changing the evaluation population (e.g., New York to Jakarta). Furthermore, we move from validity to usefulness: we define an information-theoretic discriminability of the induced aggregate channel and show that making persona benchmarking as decision-relevant as a field experiment is fundamentally a sample-size question, yielding explicit bounds on the number of independent persona evaluations required to reliably distinguish meaningfully different methods at a chosen resolution.","authors":["Enoch Hyunwook Kang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.21076v1","updated":"2025-12-24T09:49:56Z","published":"2025-12-24T09:49:56Z","title":"Blurb-Refined Inference from Crowdsourced Book Reviews using Hierarchical Genre Mining with Dual-Path Graph Convolutions","summary":"Accurate book genre classification is fundamental to digital library organization, content discovery, and personalized recommendation. Existing approaches typically model genre prediction as a flat, single-label task, ignoring hierarchical genre structure and relying heavily on noisy, subjective user reviews, which often degrade classification reliability. We propose HiGeMine, a two-phase hierarchical genre mining framework that robustly integrates user reviews with authoritative book blurbs. In the first phase, HiGeMine employs a zero-shot semantic alignment strategy to filter reviews, retaining only those semantically consistent with the corresponding blurb, thereby mitigating noise, bias, and irrelevance. In the second phase, we introduce a dual-path, two-level graph-based classification architecture: a coarse-grained Level-1 binary classifier distinguishes fiction from non-fiction, followed by Level-2 multi-label classifiers for fine-grained genre prediction. Inter-genre dependencies are explicitly modeled using a label co-occurrence graph, while contextual representations are derived from pretrained language models applied to the filtered textual content. To facilitate systematic evaluation, we curate a new hierarchical book genre dataset. Extensive experiments demonstrate that HiGeMine consistently outperformed strong baselines across hierarchical genre classification tasks. The proposed framework offers a principled and effective solution for leveraging both structured and unstructured textual data in hierarchical book genre analysis.","authors":["Suraj Kumar","Utsav Kumar Nareti","Soumi Chattopadhyay","Chandranath Adak","Prolay Mallick"],"pdf_url":"","comment":"10 pages, 4 figures, 3 tables"},{"id":"http://arxiv.org/abs/2512.21075v1","updated":"2025-12-24T09:39:04Z","published":"2025-12-24T09:39:04Z","title":"Understanding Scaling Laws in Deep Neural Networks via Feature Learning Dynamics","summary":"The empirical success of deep learning is often attributed to scaling laws that predict consistent gains as model, data, and compute grow; however, large models can exhibit training instability and diminishing returns, suggesting that scaling laws describe what success looks like but not when and why scaling succeeds or fails. A central obstacle is the lack of a rigorous understanding of feature learning at large depth. While muP characterizes feature-learning dynamics in the infinite-width limit and enables hyperparameter transfer across width, its depth extension (depth-muP) breaks down for residual blocks with more than one internal layer. We derive Neural Feature Dynamics (NFD) for ResNets with single-layer residual blocks, characterizing feature learning via a coupled forward-backward stochastic system in the joint infinite-width and infinite-depth limit. In this regime, NFD identifies when scaling-law trends persist and explains diminishing returns. It also reveals a vanishing mechanism induced by the 1/sqrt(depth) residual scaling under which the gradient-independence assumption (GIA), known to fail during training at finite depth, becomes provably valid again at infinite depth, yielding an analytically tractable regime for end-to-end feature learning. Motivated by this insight, we study two-layer residual blocks and show that the same mechanism causes feature-learning collapse in the first internal layer at large depth, providing a structural explanation for the empirical failure of depth-muP. Based on this diagnosis, we propose a depth-aware learning-rate correction that counteracts the collapse and empirically restores depth-wise hyperparameter transfer, yielding stronger performance in deeper ResNets.","authors":["Zihan Yao","Ruoyu Wu","Tianxiang Gao"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2511.14317v6","updated":"2025-12-24T09:06:26Z","published":"2025-11-18T10:21:07Z","title":"Intervention Efficiency and Perturbation Validation Framework: Capacity-Aware and Robust Clinical Model Selection under the Rashomon Effect","summary":"In clinical machine learning, the coexistence of multiple models with comparable performance (a manifestation of the Rashomon Effect) poses fundamental challenges for trustworthy deployment and evaluation. Small, imbalanced, and noisy datasets, coupled with high-dimensional and weakly identified clinical features, amplify this multiplicity and make conventional validation schemes unreliable. As a result, selecting among equally performing models becomes uncertain, particularly when resource constraints and operational priorities are not considered by conventional metrics like F1 score. To address these issues, we propose two complementary tools for robust model assessment and selection: Intervention Efficiency (IE) and the Perturbation Validation Framework (PVF). IE is a capacity-aware metric that quantifies how efficiently a model identifies actionable true positives when only limited interventions are feasible, thereby linking predictive performance with clinical utility. PVF introduces a structured approach to assess the stability of models under data perturbations, identifying models whose performance remains most invariant across noisy or shifted validation sets. Empirical results on synthetic and real-world healthcare datasets show that using these tools facilitates the selection of models that generalize more robustly and align with capacity constraints, offering a new direction for tackling the Rashomon Effect in clinical settings.","authors":["Yuwen Zhang","Viet Tran","Paul Weng"],"pdf_url":"","comment":"Accepted to the Workshop on Navigating Model Uncertainty and the Rashomon Effect: From Theory and Tools to Applications and Impact (AAAI 2026)"},{"id":"http://arxiv.org/abs/2504.04973v3","updated":"2025-12-24T08:54:34Z","published":"2025-04-07T11:58:19Z","title":"Ensuring Safety in an Uncertain Environment: Constrained MDPs via Stochastic Thresholds","summary":"This paper studies constrained Markov decision processes (CMDPs) with constraints against stochastic thresholds, aiming at safety of reinforcement learning in unknown and uncertain environments. We leverage a Growing-Window estimator sampling from interactions with the uncertain environment to estimate the thresholds, based on which we design Stochastic Pessimistic-Optimistic Thresholding (SPOT), a novel model-based primal-dual algorithm for multiple constraints against stochastic thresholds. SPOT enables reinforcement learning under both pessimistic and optimistic threshold settings. We prove that our algorithm achieves sublinear regret and constraint violation; i.e., a reward regret of $\\tilde{\\mathcal{O}}(\\sqrt{T})$ while allowing an $\\tilde{\\mathcal{O}}(\\sqrt{T})$ constraint violation over $T$ episodes. The theoretical guarantees show that our algorithm achieves performance comparable to that of an approach relying on fixed and clear thresholds. To the best of our knowledge, SPOT is the first reinforcement learning algorithm that realises theoretical guaranteed performance in an uncertain environment where even thresholds are unknown.","authors":["Qian Zuo","Fengxiang He"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2509.22358v3","updated":"2025-12-24T08:45:34Z","published":"2025-09-26T13:53:56Z","title":"Stochastic activations","summary":"We introduce stochastic activations. This novel strategy randomly selects between several non-linear functions in the feed-forward layer of a large language model. In particular, we choose between SILU or RELU depending on a Bernoulli draw. This strategy circumvents the optimization problem associated with RELU, namely, the constant shape for negative inputs that prevents the gradient flow. We leverage this strategy in two ways:\n  (1) We use stochastic activations during pre-training and fine-tune the model with RELU, which is used at inference time to provide sparse latent vectors. This reduces the inference FLOPs and translates into a significant speedup on CPU and GPU. This leads to better results than training from scratch with the RELU activation function.\n  (2) We evaluate stochastic activations for sequence generation. This strategy performs reasonably well: it has higher diversity and has only slightly inferior performance to the best deterministic non-linearity, SILU, combined with temperature sampling. This provides an alternative way to increase the diversity of generated text.","authors":["Maria Lomeli","Matthijs Douze","Gergely Szilvasy","Loic Cabannes","Jade Copet","Sainbayar Sukhbaatar","Jason Weston","Gabriel Synnaeve","Pierre-Emmanuel Mazaré","Hervé Jégou"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.21054v1","updated":"2025-12-24T08:44:58Z","published":"2025-12-24T08:44:58Z","title":"DexAvatar: 3D Sign Language Reconstruction with Hand and Body Pose Priors","summary":"The trend in sign language generation is centered around data-driven generative methods that require vast amounts of precise 2D and 3D human pose data to achieve an acceptable generation quality. However, currently, most sign language datasets are video-based and limited to automatically reconstructed 2D human poses (i.e., keypoints) and lack accurate 3D information. Furthermore, existing state-of-the-art for automatic 3D human pose estimation from sign language videos is prone to self-occlusion, noise, and motion blur effects, resulting in poor reconstruction quality. In response to this, we introduce DexAvatar, a novel framework to reconstruct bio-mechanically accurate fine-grained hand articulations and body movements from in-the-wild monocular sign language videos, guided by learned 3D hand and body priors. DexAvatar achieves strong performance in the SGNify motion capture dataset, the only benchmark available for this task, reaching an improvement of 35.11% in the estimation of body and hand poses compared to the state-of-the-art. The official website of this work is: https://github.com/kaustesseract/DexAvatar.","authors":["Kaustubh Kundu","Hrishav Bakul Barua","Lucy Robertson-Bell","Zhixi Cai","Kalin Stefanov"],"pdf_url":"","comment":"Accepted in WACV 2026"},{"id":"http://arxiv.org/abs/2512.19735v2","updated":"2025-12-24T08:34:41Z","published":"2025-12-17T12:29:53Z","title":"Case Prompting to Mitigate Large Language Model Bias for ICU Mortality Prediction","summary":"Accurate mortality risk prediction for intensive care unit (ICU) patients is essential for clinical decision-making. Although large language models (LLMs) show promise in predicting outcomes from structured medical data, their predictions may exhibit demographic biases related to sex, age, and race, limiting their trustworthy use in clinical practice. Existing debiasing methods often reduce predictive performance, making it difficult to jointly optimize fairness and accuracy. In this study, we systematically examine bias in LLM-based ICU mortality prediction and propose a training-free, clinically adaptive prompting framework to simultaneously improve fairness and performance. We first develop a multi-dimensional bias assessment scheme for comprehensive model diagnosis. Building on this analysis, we introduce CAse Prompting (CAP), a novel prompting framework that integrates conventional debiasing prompts with case-based reasoning. CAP guides the model to learn from similar historical misprediction cases and their correct outcomes, enabling correction of biased reasoning patterns. Experiments on the MIMIC-IV dataset show that CAP substantially improves both predictive accuracy and fairness. CAP increases AUROC from 0.806 to 0.873 and AUPRC from 0.497 to 0.694, while reducing sex- and race-related disparities by over 90%. Feature reliance analysis further indicates highly consistent attention patterns across demographic groups, with similarity scores exceeding 0.98. These results demonstrate that LLMs exhibit measurable bias in ICU mortality prediction, and that a carefully designed prompting framework can effectively co-optimize fairness and performance without retraining, offering a transferable paradigm for equitable clinical decision support.","authors":["Gangxiong Zhang","Yongchao Long","Yong Zhang","Yuxi Zhou","Shenda Hong"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/1912.03896v4","updated":"2025-12-24T08:34:27Z","published":"2019-12-09T08:24:29Z","title":"Explicit Group Sparse Projection with Applications to Deep Learning and NMF","summary":"We design a new sparse projection method for a set of vectors that guarantees a desired average sparsity level measured leveraging the popular Hoyer measure (an affine function of the ratio of the $\\ell_1$ and $\\ell_2$ norms). Existing approaches either project each vector individually or require the use of a regularization parameter which implicitly maps to the average $\\ell_0$-measure of sparsity. Instead, in our approach we set the sparsity level for the whole set explicitly and simultaneously project a group of vectors with the sparsity level of each vector tuned automatically. We show that the computational complexity of our projection operator is linear in the size of the problem. Additionally, we propose a generalization of this projection by replacing the $\\ell_1$ norm by its weighted version. We showcase the efficacy of our approach in both supervised and unsupervised learning tasks on image datasets including CIFAR10 and ImageNet. In deep neural network pruning, the sparse models produced by our method on ResNet50 have significantly higher accuracies at corresponding sparsity values compared to existing competitors. In nonnegative matrix factorization, our approach yields competitive reconstruction errors against state-of-the-art algorithms.","authors":["Riyasat Ohib","Nicolas Gillis","Niccolò Dalmasso","Sameena Shah","Vamsi K. Potluru","Sergey Plis"],"pdf_url":"","comment":"20 pages, 10 figures; major revisions; affiliation corrected, grant added"},{"id":"http://arxiv.org/abs/2512.20605v2","updated":"2025-12-24T08:32:45Z","published":"2025-12-23T18:51:50Z","title":"Emergent temporal abstractions in autoregressive models enable hierarchical reinforcement learning","summary":"Large-scale autoregressive models pretrained on next-token prediction and finetuned with reinforcement learning (RL) have achieved unprecedented success on many problem domains. During RL, these models explore by generating new outputs, one token at a time. However, sampling actions token-by-token can result in highly inefficient learning, particularly when rewards are sparse. Here, we show that it is possible to overcome this problem by acting and exploring within the internal representations of an autoregressive model. Specifically, to discover temporally-abstract actions, we introduce a higher-order, non-causal sequence model whose outputs control the residual stream activations of a base autoregressive model. On grid world and MuJoCo-based tasks with hierarchical structure, we find that the higher-order model learns to compress long activation sequence chunks onto internal controllers. Critically, each controller executes a sequence of behaviorally meaningful actions that unfold over long timescales and are accompanied with a learned termination condition, such that composing multiple controllers over time leads to efficient exploration on novel tasks. We show that direct internal controller reinforcement, a process we term \"internal RL\", enables learning from sparse rewards in cases where standard RL finetuning fails. Our results demonstrate the benefits of latent action generation and reinforcement in autoregressive models, suggesting internal RL as a promising avenue for realizing hierarchical RL within foundation models.","authors":["Seijin Kobayashi","Yanick Schimpf","Maximilian Schlegel","Angelika Steger","Maciej Wolczyk","Johannes von Oswald","Nino Scherrer","Kaitlin Maile","Guillaume Lajoie","Blake A. Richards","Rif A. Saurous","James Manyika","Blaise Agüera y Arcas","Alexander Meulemans","João Sacramento"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.21048v1","updated":"2025-12-24T08:29:28Z","published":"2025-12-24T08:29:28Z","title":"zkFL-Health: Blockchain-Enabled Zero-Knowledge Federated Learning for Medical AI Privacy","summary":"Healthcare AI needs large, diverse datasets, yet strict privacy and governance constraints prevent raw data sharing across institutions. Federated learning (FL) mitigates this by training where data reside and exchanging only model updates, but practical deployments still face two core risks: (1) privacy leakage via gradients or updates (membership inference, gradient inversion) and (2) trust in the aggregator, a single point of failure that can drop, alter, or inject contributions undetected. We present zkFL-Health, an architecture that combines FL with zero-knowledge proofs (ZKPs) and Trusted Execution Environments (TEEs) to deliver privacy-preserving, verifiably correct collaborative training for medical AI. Clients locally train and commit their updates; the aggregator operates within a TEE to compute the global update and produces a succinct ZK proof (via Halo2/Nova) that it used exactly the committed inputs and the correct aggregation rule, without revealing any client update to the host. Verifier nodes validate the proof and record cryptographic commitments on-chain, providing an immutable audit trail and removing the need to trust any single party. We outline system and threat models tailored to healthcare, the zkFL-Health protocol, security/privacy guarantees, and a performance evaluation plan spanning accuracy, privacy risk, latency, and cost. This framework enables multi-institutional medical AI with strong confidentiality, integrity, and auditability, key properties for clinical adoption and regulatory compliance.","authors":["Savvy Sharma","George Petrovic","Sarthak Kaushik"],"pdf_url":"","comment":"10 pages, 1 figure, 5 tables"},{"id":"http://arxiv.org/abs/2510.00915v3","updated":"2025-12-24T08:25:26Z","published":"2025-10-01T13:56:44Z","title":"Reinforcement Learning with Verifiable yet Noisy Rewards under Imperfect Verifiers","summary":"Reinforcement Learning with Verifiable Rewards (RLVR) replaces costly human labeling with automated verifiers. To reduce verifier hacking, many RLVR systems binarize rewards to $\\{0,1\\}$, but imperfect verifiers inevitably introduce \\emph{false negatives} (rejecting correct answers) and \\emph{false positives} (accepting incorrect ones). We formalize verifier unreliability as a stochastic reward channel with asymmetric noise rates $ρ_0$ and $ρ_1$ -- the FP rate and the FN rate, respectively. From this abstraction we derive two lightweight corrections: (i) a \\emph{backward} correction that yields an unbiased surrogate reward and thus an unbiased policy-gradient estimator in expectation, and (ii) a \\emph{forward} correction that reweights score-function terms so the expected update aligns with the clean gradient direction and requires only the FN rate. We implement both as lightweight hooks in a group relative policy optimization pipeline, both corrections improve RLVR for math reasoning under synthetic and real verifier noise, with the forward variant being more stable under heavier noise. Finally, an appeals mechanism with a lightweight LLM verifier estimates the FN rate online and further improves performance.","authors":["Xin-Qiang Cai","Wei Wang","Feng Liu","Tongliang Liu","Gang Niu","Masashi Sugiyama"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2511.14368v2","updated":"2025-12-24T08:17:05Z","published":"2025-11-18T11:18:08Z","title":"O3SLM: Open Weight, Open Data, and Open Vocabulary Sketch-Language Model","summary":"While Large Vision Language Models (LVLMs) are increasingly deployed in real-world applications, their ability to interpret abstract visual inputs remains limited. Specifically, they struggle to comprehend hand-drawn sketches, a modality that offers an intuitive means of expressing concepts that are difficult to describe textually. We identify the primary bottleneck as the absence of a large-scale dataset that jointly models sketches, photorealistic images, and corresponding natural language instructions. To address this, we present two key contributions: (1) a new, large-scale dataset of image-sketch-instruction triplets designed to facilitate both pretraining and instruction tuning, and (2) O3SLM, an LVLM trained on this dataset. Comprehensive evaluations on multiple sketch-based tasks: (a) object localization, (b) counting, (c) image retrieval i.e., (SBIR and fine-grained SBIR), and (d) visual question answering (VQA); while incorporating the three existing sketch datasets, namely QuickDraw!, Sketchy, and Tu Berlin, along with our generated SketchVCL dataset, show that O3SLM achieves state-of-the-art performance, substantially outperforming existing LVLMs in sketch comprehension and reasoning.","authors":["Rishi Gupta","Mukilan Karuppasamy","Shyam Marjit","Aditay Tripathi","Anirban Chakraborty"],"pdf_url":"","comment":"Accepted to AAAI 2026"},{"id":"http://arxiv.org/abs/2512.21039v1","updated":"2025-12-24T08:06:52Z","published":"2025-12-24T08:06:52Z","title":"Agentic Multi-Persona Framework for Evidence-Aware Fake News Detection","summary":"The rapid proliferation of online misinformation poses significant risks to public trust, policy, and safety, necessitating reliable automated fake news detection. Existing methods often struggle with multimodal content, domain generalization, and explainability. We propose AMPEND-LS, an agentic multi-persona evidence-grounded framework with LLM-SLM synergy for multimodal fake news detection. AMPEND-LS integrates textual, visual, and contextual signals through a structured reasoning pipeline powered by LLMs, augmented with reverse image search, knowledge graph paths, and persuasion strategy analysis. To improve reliability, we introduce a credibility fusion mechanism combining semantic similarity, domain trustworthiness, and temporal context, and a complementary SLM classifier to mitigate LLM uncertainty and hallucinations. Extensive experiments across three benchmark datasets demonstrate that AMPEND-LS consistently outperformed state-of-the-art baselines in accuracy, F1 score, and robustness. Qualitative case studies further highlight its transparent reasoning and resilience against evolving misinformation. This work advances the development of adaptive, explainable, and evidence-aware systems for safeguarding online information integrity.","authors":["Roopa Bukke","Soumya Pandey","Suraj Kumar","Soumi Chattopadhyay","Chandranath Adak"],"pdf_url":"","comment":"12 pages, 8 tables, 2 figures"},{"id":"http://arxiv.org/abs/2509.26226v2","updated":"2025-12-24T08:05:31Z","published":"2025-09-30T13:25:00Z","title":"Thinking-Free Policy Initialization Makes Distilled Reasoning Models More Effective and Efficient Reasoners","summary":"Reinforcement Learning with Verifiable Reward (RLVR) effectively solves complex tasks but demands extremely long context lengths during training, leading to substantial computational costs. While multi-stage training can partially mitigate this, starting with overly short contexts often causes irreversible performance degradation, ultimately failing to reduce overall training compute significantly. In this paper, we introduce **T**hinking-**F**ree **P**olicy **I**nitialization (**TFPI**), a simple yet effective adaptation to RLVR that bridges long Chain-of-Thought (CoT) distillation and standard RLVR. TFPI employs a simple *ThinkFree* operation, explicitly discarding the thinking content via a direct *</think>* append, to reduce token usage during inference. Training with *ThinkFree*-adapted inputs improves performance and lowers token consumption, even in the original slow-thinking mode. Extensive experiments across various benchmarks have shown that TFPI accelerates RL convergence, achieves a higher performance ceiling, and yields more token-efficient reasoning models without specialized rewards or complex training designs. With TFPI only, we train a 4B model to reach 89.0% accuracy on AIME24 and 65.5% on LiveCodeBench using less than 4K H20 hours.","authors":["Xin Xu","Cliveb AI","Kai Yang","Tianhao Chen","Yang Wang","Saiyong Yang","Can Yang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.21029v1","updated":"2025-12-24T07:52:23Z","published":"2025-12-24T07:52:23Z","title":"Critical Points of Degenerate Metrics on Algebraic Varieties: A Tale of Overparametrization","summary":"We study the critical points over an algebraic variety of an optimization problem defined by a quadratic objective that is degenerate. This scenario arises in machine learning when the dataset size is small with respect to the model, and is typically referred to as overparametrization. Our main result relates the degenerate optimization problem to a nondegenerate one via a projection. In the highly-degenerate regime, we find that a central role is played by the ramification locus of the projection. Additionally, we provide tools for counting the number of critical points over projective varieties, and discuss specific cases arising from deep learning. Our work bridges tools from algebraic geometry with ideas from machine learning, and it extends the line of literature around the Euclidean distance degree to the degenerate setting.","authors":["Giovanni Luca Marchetti","Erin Connelly","Paul Breiding","Kathlén Kohn"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.21021v1","updated":"2025-12-24T07:35:17Z","published":"2025-12-24T07:35:17Z","title":"Towards Better Search with Domain-Aware Text Embeddings for C2C Marketplaces","summary":"Consumer-to-consumer (C2C) marketplaces pose distinct retrieval challenges: short, ambiguous queries; noisy, user-generated listings; and strict production constraints. This paper reports our experiment to build a domain-aware Japanese text-embedding approach to improve the quality of search at Mercari, Japan's largest C2C marketplace. We experimented with fine-tuning on purchase-driven query-title pairs, using role-specific prefixes to model query-item asymmetry. To meet production constraints, we apply Matryoshka Representation Learning to obtain compact, truncation-robust embeddings. Offline evaluation on historical search logs shows consistent gains over a strong generic encoder, with particularly large improvements when replacing PCA compression with Matryoshka truncation. A manual assessment further highlights better handling of proper nouns, marketplace-specific semantics, and term-importance alignment. Additionally, an initial online A/B test demonstrates statistically significant improvements in revenue per user and search-flow efficiency, with transaction frequency maintained. Results show that domain-aware embeddings improve relevance and efficiency at scale and form a practical foundation for richer LLM-era search experiences.","authors":["Andre Rusli","Miao Cao","Shoma Ishimoto","Sho Akiyama","Max Frenzel"],"pdf_url":"","comment":"5 pages, AAAI 2026 Workshop on New Frontiers in Information Retrieval"},{"id":"http://arxiv.org/abs/2512.21020v1","updated":"2025-12-24T07:34:20Z","published":"2025-12-24T07:34:20Z","title":"Enhancing diffusion models with Gaussianization preprocessing","summary":"Diffusion models are a class of generative models that have demonstrated remarkable success in tasks such as image generation. However, one of the bottlenecks of these models is slow sampling due to the delay before the onset of trajectory bifurcation, at which point substantial reconstruction begins. This issue degrades generation quality, especially in the early stages. Our primary objective is to mitigate bifurcation-related issues by preprocessing the training data to enhance reconstruction quality, particularly for small-scale network architectures. Specifically, we propose applying Gaussianization preprocessing to the training data to make the target distribution more closely resemble an independent Gaussian distribution, which serves as the initial density of the reconstruction process. This preprocessing step simplifies the model's task of learning the target distribution, thereby improving generation quality even in the early stages of reconstruction with small networks. The proposed method is, in principle, applicable to a broad range of generative tasks, enabling more stable and efficient sampling processes.","authors":["Li Cunzhi","Louis Kang","Hideaki Shimazaki"],"pdf_url":"","comment":"17 pages, 9 figures"},{"id":"http://arxiv.org/abs/2511.03928v3","updated":"2025-12-24T07:28:22Z","published":"2025-11-06T00:09:33Z","title":"SynQuE: Estimating Synthetic Dataset Quality Without Annotations","summary":"We introduce and formalize the Synthetic Dataset Quality Estimation (SynQuE) problem: ranking synthetic datasets by their expected real-world task performance using only limited unannotated real data. This addresses a critical and open challenge where data is scarce due to collection costs or privacy constraints. We establish the first comprehensive benchmarks for this problem by introducing and evaluating proxy metrics that choose synthetic data for training to maximize task performance on real data. We introduce the first proxy metrics for SynQuE by adapting distribution and diversity-based distance measures to our context via embedding models. To address the shortcomings of these metrics on complex planning tasks, we propose LENS, a novel proxy that leverages large language model reasoning. Our results show that SynQuE proxies correlate with real task performance across diverse tasks, including sentiment analysis, Text2SQL, web navigation, and image classification, with LENS consistently outperforming others on complex tasks by capturing nuanced characteristics. For instance, on text-to-SQL parsing, training on the top-3 synthetic datasets selected via SynQuE proxies can raise accuracy from 30.4% to 38.4 (+8.1)% on average compared to selecting data indiscriminately. This work establishes SynQuE as a practical framework for synthetic data selection under real-data scarcity and motivates future research on foundation model-based data characterization and fine-grained data selection.","authors":["Arthur Chen","Victor Zhong"],"pdf_url":"","comment":"Our code and dataset are available here: https://github.com/r2llab/SynQuE"},{"id":"http://arxiv.org/abs/2512.21010v1","updated":"2025-12-24T07:14:31Z","published":"2025-12-24T07:14:31Z","title":"LLM Swiss Round: Aggregating Multi-Benchmark Performance via Competitive Swiss-System Dynamics","summary":"The rapid proliferation of Large Language Models (LLMs) and diverse specialized benchmarks necessitates a shift from fragmented, task-specific metrics to a holistic, competitive ranking system that effectively aggregates performance across multiple ability dimensions. Primarily using static scoring, current evaluation methods are fundamentally limited. They struggle to determine the proper mix ratio across diverse benchmarks, and critically, they fail to capture a model's dynamic competitive fitness or its vulnerability when confronted with sequential, high-stakes tasks. To address this, we introduce the novel Competitive Swiss-System Dynamics (CSD) framework. CSD simulates a multi-round, sequential contest where models are dynamically paired across a curated sequence of benchmarks based on their accumulated win-loss record. And Monte Carlo Simulation ($N=100,000$ iterations) is used to approximate the statistically robust Expected Win Score ($E[S_m]$), which eliminates the noise of random pairing and early-round luck. Furthermore, we implement a Failure Sensitivity Analysis by parameterizing the per-round elimination quantity ($T_k$), which allows us to profile models based on their risk appetite--distinguishing between robust generalists and aggressive specialists. We demonstrate that CSD provides a more nuanced and context-aware ranking than traditional aggregate scoring and static pairwise models, representing a vital step towards risk-informed, next-generation LLM evaluation.","authors":["Jiashuo Liu","Jiayun Wu","Chunjie Wu","Jingkai Liu","Zaiyuan Wang","Huan Zhou","Wenhao Huang","Hongseok Namkoong"],"pdf_url":"","comment":"18 pages"},{"id":"http://arxiv.org/abs/2512.21005v1","updated":"2025-12-24T07:10:17Z","published":"2025-12-24T07:10:17Z","title":"Learning from Neighbors with PHIBP: Predicting Infectious Disease Dynamics in Data-Sparse Environments","summary":"Modeling sparse count data, which arise across numerous scientific fields, presents significant statistical challenges. This chapter addresses these challenges in the context of infectious disease prediction, with a focus on predicting outbreaks in geographic regions that have historically reported zero cases. To this end, we present the detailed computational framework and experimental application of the Poisson Hierarchical Indian Buffet Process (PHIBP), with demonstrated success in handling sparse count data in microbiome and ecological studies. The PHIBP's architecture, grounded in the concept of absolute abundance, systematically borrows statistical strength from related regions and circumvents the known sensitivities of relative-rate methods to zero counts. Through a series of experiments on infectious disease data, we show that this principled approach provides a robust foundation for generating coherent predictive distributions and for the effective use of comparative measures such as alpha and beta diversity. The chapter's emphasis on algorithmic implementation and experimental results confirms that this unified framework delivers both accurate outbreak predictions and meaningful epidemiological insights in data-sparse settings.","authors":["Edwin Fong","Lancelot F. James","Juho Lee"],"pdf_url":"","comment":"Draft Book chapter on AMMI methods -- Application of PHIBP arXiv:2502.01919 to Infectious Disease Detection with suggested extensions using the developments in arXiv:2508.18668"},{"id":"http://arxiv.org/abs/2404.19557v5","updated":"2025-12-24T06:59:27Z","published":"2024-04-30T13:39:26Z","title":"Neural Dynamic Data Valuation: A Stochastic Optimal Control Approach","summary":"Data valuation has become a cornerstone of the modern data economy, where datasets function as tradable intellectual assets that drive decision-making, model training, and market transactions. Despite substantial progress, existing valuation methods remain limited by high computational cost, weak fairness guarantees, and poor interpretability, which hinder their deployment in large-scale, high-stakes applications. This paper introduces Neural Dynamic Data Valuation (NDDV), a new framework that formulates data valuation as a stochastic optimal control problem to capture the dynamic evolution of data utility over time. Unlike static combinatorial approaches, NDDV models data interactions through continuous trajectories that reflect both individual and collective learning dynamics.","authors":["Zhangyong Liang","Ji Zhang","Xin Wang","Pengfei Zhang","Zhao Li"],"pdf_url":"","comment":"14 pages, 10 figures"},{"id":"http://arxiv.org/abs/2512.21000v1","updated":"2025-12-24T06:55:43Z","published":"2025-12-24T06:55:43Z","title":"CoSeNet: A Novel Approach for Optimal Segmentation of Correlation Matrices","summary":"In this paper, we propose a novel approach for the optimal identification of correlated segments in noisy correlation matrices. The proposed model is known as CoSeNet (Correlation Seg-mentation Network) and is based on a four-layer algorithmic architecture that includes several processing layers: input, formatting, re-scaling, and segmentation layer. The proposed model can effectively identify correlated segments in such matrices, better than previous approaches for similar problems. Internally, the proposed model utilizes an overlapping technique and uses pre-trained Machine Learning (ML) algorithms, which makes it robust and generalizable. CoSeNet approach also includes a method that optimizes the parameters of the re-scaling layer using a heuristic algorithm and fitness based on a Window Difference-based metric. The output of the model is a binary noise-free matrix representing optimal segmentation as well as its seg-mentation points and can be used in a variety of applications, obtaining compromise solutions between efficiency, memory, and speed of the proposed deployment model.","authors":["Alberto. Palomo-Alonso","David Casillas-Perez","Silvia Jimenez-Fernandez","Antonio Portilla-Figueras","Sancho Salcedo-Sanz"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2507.11783v3","updated":"2025-12-24T06:45:54Z","published":"2025-07-15T22:52:44Z","title":"EEG Foundation Models: A Critical Review of Current Progress and Future Directions","summary":"Premise. Patterns of electrical brain activity recorded via electroencephalography (EEG) offer immense value for scientific and clinical investigations. The inability of supervised EEG encoders to learn robust EEG patterns and their over-reliance on expensive signal annotations have sparked a transition towards general-purpose self-supervised EEG encoders, i.e., EEG foundation models (EEG-FMs), for robust and scalable EEG feature extraction. However, the real-world readiness of early EEG-FMs and the rubrics for long-term research progress remain unclear. Objective. In this work, we conduct a review of ten early EEG-FMs to capture common trends and identify key directions for future development of EEG-FMs. Methods. We comparatively analyze each EEG-FM using three fundamental pillars of foundation modeling, namely the representation of input data, self-supervised modeling, and the evaluation strategy. Based on this analysis, we present a critical synthesis of EEG-FM methodology, empirical findings, and outstanding research gaps. Results. We find that most EEG-FMs adopt a sequence-based modeling scheme that relies on transformer-based backbones and the reconstruction of masked temporal EEG sequences for self-supervision. However, model evaluations remain heterogeneous and largely limited, making it challenging to assess their practical off-the-shelf utility. In addition to adopting standardized and realistic evaluations, future work should demonstrate more substantial scaling effects and make principled and trustworthy choices throughout the EEG representation learning pipeline. Significance. Our review indicates that the development of benchmarks, software tools, technical methodologies, and applications in collaboration with domain experts may advance the translational utility and real-world adoption of EEG-FMs.","authors":["Gayal Kuruppu","Neeraj Wagh","Vaclav Kremen","Sandipan Pati","Gregory Worrell","Yogatheesan Varatharajah"],"pdf_url":"","comment":"22 pages (main), 5 figures (main), 4 tables (main + supplement)"},{"id":"http://arxiv.org/abs/2308.08427v2","updated":"2025-12-24T06:35:42Z","published":"2023-08-16T15:17:57Z","title":"Eliciting Risk Aversion with Inverse Reinforcement Learning via Interactive Questioning","summary":"We investigate a framework for robo-advisors to estimate non-expert clients' risk aversion using adaptive binary-choice questionnaires. We model risk aversion using cost functions and spectral risk measures in a static setting. We prove the finite-sample identifiability and, for properly designed questions, obtain a convergence rate of $\\sqrt{N}$ up to a logarithmic factor, where $N$ is the number of questions. We introduce the notion of distinguishing power and demonstrate, through simulated experiments, that designing questions by maximizing distinguishing power achieves satisfactory accuracy in learning risk aversion with fewer than 50 questions. We also provide a preliminary investigation of an infinite-horizon setting with an additional discount factor for dynamic risk aversion, establishing qualitative identifiability in this case.","authors":["Ziteng Cheng","Anthony Coache","Sebastian Jaimungal"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20983v1","updated":"2025-12-24T06:17:21Z","published":"2025-12-24T06:17:21Z","title":"Automatic Replication of LLM Mistakes in Medical Conversations","summary":"Large language models (LLMs) are increasingly evaluated in clinical settings using multi-dimensional rubrics which quantify reasoning quality, safety, and patient-centeredness. Yet, replicating specific mistakes in other LLM models is not straightforward and often requires manual effort. We introduce MedMistake, an automatic pipeline that extracts mistakes LLMs make in patient-doctor conversations and converts them into a benchmark of single-shot QA pairs. Our pipeline (1) creates complex, conversational data between an LLM patient and LLM doctor, (2) runs an evaluation with a committee of 2 LLM judges across a variety of dimensions and (3) creates simplified single-shot QA scenarios from those mistakes. We release MedMistake-All, a dataset of 3,390 single-shot QA pairs where GPT-5 and Gemini 2.5 Pro are currently failing to answer correctly, as judged by two LLM judges. We used medical experts to validate a subset of 211/3390 questions (MedMistake-Bench), which we used to run a final evaluation of 12 frontier LLMs: Claude Opus 4.5, Claude Sonnet 4.5, DeepSeek-Chat, Gemini 2.5 Pro, Gemini 3 Pro, GPT-4o, GPT-5, GPT-5.1, GPT-5.2, Grok 4, Grok 4.1, Mistral Large. We found that GPT models, Claude and Grok obtained the best performance on MedMistake-Bench. We release both the doctor-validated benchmark (MedMistake-Bench), as well as the full dataset (MedMistake-All) at https://huggingface.co/datasets/TheLumos/MedicalMistakeBenchmark.","authors":["Oleksii Proniakin","Diego Fajardo","Ruslan Nazarenko","Razvan Marinescu"],"pdf_url":"","comment":"48 pages, 3 figures, 4 tables"},{"id":"http://arxiv.org/abs/2512.20978v1","updated":"2025-12-24T06:13:02Z","published":"2025-12-24T06:13:02Z","title":"GenTSE: Enhancing Target Speaker Extraction via a Coarse-to-Fine Generative Language Model","summary":"Language Model (LM)-based generative modeling has emerged as a promising direction for TSE, offering potential for improved generalization and high-fidelity speech. We present GenTSE, a two-stage decoder-only generative LM approach for TSE: Stage-1 predicts coarse semantic tokens, and Stage-2 generates fine acoustic tokens. Separating semantics and acoustics stabilizes decoding and yields more faithful, content-aligned target speech. Both stages use continuous SSL or codec embeddings, offering richer context than discretized-prompt methods. To reduce exposure bias, we employ a Frozen-LM Conditioning training strategy that conditions the LMs on predicted tokens from earlier checkpoints to reduce the gap between teacher-forcing training and autoregressive inference. We further employ DPO to better align outputs with human perceptual preferences. Experiments on Libri2Mix show that GenTSE surpasses previous LM-based systems in speech quality, intelligibility, and speaker consistency.","authors":["Haoyang Li","Xuyi Zhuang","Azmat Adnan","Ye Ni","Wei Rao","Shreyas Gopal","Eng Siong Chng"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2408.06672v3","updated":"2025-12-24T06:12:56Z","published":"2024-08-13T06:47:59Z","title":"TimeBridge: Better Diffusion Prior Design with Bridge Models for Time Series Generation","summary":"Time series generation is widely used in real-world applications such as simulation, data augmentation, and hypothesis testing. Recently, diffusion models have emerged as the de facto approach to time series generation, enabling diverse synthesis scenarios. However, the fixed standard-Gaussian diffusion prior may be ill-suited for time series data, which exhibit properties such as temporal order and fixed time points. In this paper, we propose TimeBridge, a framework that flexibly synthesizes time series data by using diffusion bridges to learn paths between a chosen prior and the data distribution. We then explore several prior designs tailored to time series synthesis. Our framework covers (i) data- and time-dependent priors for unconditional generation and (ii) scale-preserving priors for conditional generation. Experiments show that our framework with data-driven priors outperforms standard diffusion models on time series generation.","authors":["Jinseong Park","Seungyun Lee","Woojin Jeong","Yujin Choi","Jaewook Lee"],"pdf_url":"","comment":"KDD 2026"},{"id":"http://arxiv.org/abs/2512.20974v1","updated":"2025-12-24T06:00:51Z","published":"2025-12-24T06:00:51Z","title":"Generalised Linear Models in Deep Bayesian RL with Learnable Basis Functions","summary":"Bayesian Reinforcement Learning (BRL) provides a framework for generalisation of Reinforcement Learning (RL) problems from its use of Bayesian task parameters in the transition and reward models. However, classical BRL methods assume known forms of transition and reward models, reducing their applicability in real-world problems. As a result, recent deep BRL methods have started to incorporate model learning, though the use of neural networks directly on the joint data and task parameters requires optimising the Evidence Lower Bound (ELBO). ELBOs are difficult to optimise and may result in indistinctive task parameters, hence compromised BRL policies. To this end, we introduce a novel deep BRL method, Generalised Linear Models in Deep Bayesian RL with Learnable Basis Functions (GLiBRL), that enables efficient and accurate learning of transition and reward models, with fully tractable marginal likelihood and Bayesian inference on task parameters and model noises. On challenging MetaWorld ML10/45 benchmarks, GLiBRL improves the success rate of one of the state-of-the-art deep BRL methods, VariBAD, by up to 2.7x. Comparing against representative or recent deep BRL / Meta-RL methods, such as MAML, RL2, SDVT, TrMRL and ECET, GLiBRL also demonstrates its low-variance and decent performance consistently.","authors":["Jingyang You","Hanna Kurniawati"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20967v1","updated":"2025-12-24T05:47:27Z","published":"2025-12-24T05:47:27Z","title":"Deadline-Aware Online Scheduling for LLM Fine-Tuning with Spot Market Predictions","summary":"As foundation models grow in size, fine-tuning them becomes increasingly expensive. While GPU spot instances offer a low-cost alternative to on-demand resources, their volatile prices and availability make deadline-aware scheduling particularly challenging. We tackle this difficulty by using a mix of spot and on-demand instances. Distinctively, we show the predictability of prices and availability in a spot instance market, the power of prediction in enabling cost-efficient scheduling and its sensitivity to estimation errors. An integer programming problem is formulated to capture the use of mixed instances under both the price and availability dynamics. We propose an online allocation algorithm with prediction based on the committed horizon control approach that leverages a \\emph{commitment level} to enforce the partial sequence of decisions. When this prediction becomes inaccurate, we further present a complementary online algorithm without predictions. An online policy selection algorithm is developed that learns the best policy from a pool constructed by varying the parameters of both algorithms. We prove that the prediction-based algorithm achieves tighter performance bounds as prediction error decreases, while the policy selection algorithm possesses a regret bound of $\\mathcal{O}(\\sqrt{T})$. Experimental results demonstrate that our online framework can adaptively select the best policy under varying spot market dynamics and prediction quality, consistently outperforming baselines and improving utility by up to 54.8\\%.","authors":["Linggao Kong","Yuedong Xu","Lei Jiao","Chuan Xu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20963v1","updated":"2025-12-24T05:40:40Z","published":"2025-12-24T05:40:40Z","title":"Generalization of Diffusion Models Arises with a Balanced Representation Space","summary":"Diffusion models excel at generating high-quality, diverse samples, yet they risk memorizing training data when overfit to the training objective. We analyze the distinctions between memorization and generalization in diffusion models through the lens of representation learning. By investigating a two-layer ReLU denoising autoencoder (DAE), we prove that (i) memorization corresponds to the model storing raw training samples in the learned weights for encoding and decoding, yielding localized \"spiky\" representations, whereas (ii) generalization arises when the model captures local data statistics, producing \"balanced\" representations. Furthermore, we validate these theoretical findings on real-world unconditional and text-to-image diffusion models, demonstrating that the same representation structures emerge in deep generative models with significant practical implications. Building on these insights, we propose a representation-based method for detecting memorization and a training-free editing technique that allows precise control via representation steering. Together, our results highlight that learning good representations is central to novel and meaningful generative modeling.","authors":["Zekai Zhang","Xiao Li","Xiang Li","Lianghe Shi","Meng Wu","Molei Tao","Qing Qu"],"pdf_url":"","comment":"40 pages, 19 figures. The first two authors contributed equally"},{"id":"http://arxiv.org/abs/2512.20959v1","updated":"2025-12-24T05:31:42Z","published":"2025-12-24T05:31:42Z","title":"Can Agentic AI Match the Performance of Human Data Scientists?","summary":"Data science plays a critical role in transforming complex data into actionable insights across numerous domains. Recent developments in large language models (LLMs) have significantly automated data science workflows, but a fundamental question persists: Can these agentic AI systems truly match the performance of human data scientists who routinely leverage domain-specific knowledge? We explore this question by designing a prediction task where a crucial latent variable is hidden in relevant image data instead of tabular features. As a result, agentic AI that generates generic codes for modeling tabular data cannot perform well, while human experts could identify the important hidden variable using domain knowledge. We demonstrate this idea with a synthetic dataset for property insurance. Our experiments show that agentic AI that relies on generic analytics workflow falls short of methods that use domain-specific insights. This highlights a key limitation of the current agentic AI for data science and underscores the need for future research to develop agentic AI systems that can better recognize and incorporate domain knowledge.","authors":["An Luo","Jin Du","Fangqiao Tian","Xun Xian","Robert Specht","Ganghua Wang","Xuan Bi","Charles Fleming","Jayanth Srinivasa","Ashish Kundu","Mingyi Hong","Jie Ding"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20958v1","updated":"2025-12-24T05:29:35Z","published":"2025-12-24T05:29:35Z","title":"ReACT-Drug: Reaction-Template Guided Reinforcement Learning for de novo Drug Design","summary":"De novo drug design is a crucial component of modern drug development, yet navigating the vast chemical space to find synthetically accessible, high-affinity candidates remains a significant challenge. Reinforcement Learning (RL) enhances this process by enabling multi-objective optimization and exploration of novel chemical space - capabilities that traditional supervised learning methods lack. In this work, we introduce \\textbf{ReACT-Drug}, a fully integrated, target-agnostic molecular design framework based on Reinforcement Learning. Unlike models requiring target-specific fine-tuning, ReACT-Drug utilizes a generalist approach by leveraging ESM-2 protein embeddings to identify similar proteins for a given target from a knowledge base such as Protein Data Base (PDB). Thereafter, the known drug ligands corresponding to such proteins are decomposed to initialize a fragment-based search space, biasing the agent towards biologically relevant subspaces. For each such fragment, the pipeline employs a Proximal Policy Optimization (PPO) agent guiding a ChemBERTa-encoded molecule through a dynamic action space of chemically valid, reaction-template-based transformations. This results in the generation of \\textit{de novo} drug candidates with competitive binding affinities and high synthetic accessibility, while ensuring 100\\% chemical validity and novelty as per MOSES benchmarking. This architecture highlights the potential of integrating structural biology, deep representation learning, and chemical synthesis rules to automate and accelerate rational drug design. The dataset and code are available at https://github.com/YadunandanRaman/ReACT-Drug/.","authors":["R Yadunandan","Nimisha Ghosh"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20956v1","updated":"2025-12-24T05:27:20Z","published":"2025-12-24T05:27:20Z","title":"Solving Functional PDEs with Gaussian Processes and Applications to Functional Renormalization Group Equations","summary":"We present an operator learning framework for solving non-perturbative functional renormalization group equations, which are integro-differential equations defined on functionals. Our proposed approach uses Gaussian process operator learning to construct a flexible functional representation formulated directly on function space, making it independent of a particular equation or discretization. Our method is flexible, and can apply to a broad range of functional differential equations while still allowing for the incorporation of physical priors in either the prior mean or the kernel design. We demonstrate the performance of our method on several relevant equations, such as the Wetterich and Wilson--Polchinski equations, showing that it achieves equal or better performance than existing approximations such as the local-potential approximation, while being significantly more flexible. In particular, our method can handle non-constant fields, making it promising for the study of more complex field configurations, such as instantons.","authors":["Xianjin Yang","Matthieu Darcy","Matthew Hudes","Francis J. Alexander","Gregory Eyink","Houman Owhadi"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20950v1","updated":"2025-12-24T05:14:40Z","published":"2025-12-24T05:14:40Z","title":"MultiMind at SemEval-2025 Task 7: Crosslingual Fact-Checked Claim Retrieval via Multi-Source Alignment","summary":"This paper presents our system for SemEval-2025 Task 7: Multilingual and Crosslingual Fact-Checked Claim Retrieval. In an era where misinformation spreads rapidly, effective fact-checking is increasingly critical. We introduce TriAligner, a novel approach that leverages a dual-encoder architecture with contrastive learning and incorporates both native and English translations across different modalities. Our method effectively retrieves claims across multiple languages by learning the relative importance of different sources in alignment. To enhance robustness, we employ efficient data preprocessing and augmentation using large language models while incorporating hard negative sampling to improve representation learning. We evaluate our approach on monolingual and crosslingual benchmarks, demonstrating significant improvements in retrieval accuracy and fact-checking performance over baselines.","authors":["Mohammad Mahdi Abootorabi","Alireza Ghahramani Kure","Mohammadali Mohammadkhani","Sina Elahimanesh","Mohammad Ali Ali Panah"],"pdf_url":"","comment":"11 pages Published at the SemEval-2025 workshop"},{"id":"http://arxiv.org/abs/2512.20943v1","updated":"2025-12-24T04:57:30Z","published":"2025-12-24T04:57:30Z","title":"AirGS: Real-Time 4D Gaussian Streaming for Free-Viewpoint Video Experiences","summary":"Free-viewpoint video (FVV) enables immersive viewing experiences by allowing users to view scenes from arbitrary perspectives. As a prominent reconstruction technique for FVV generation, 4D Gaussian Splatting (4DGS) models dynamic scenes with time-varying 3D Gaussian ellipsoids and achieves high-quality rendering via fast rasterization. However, existing 4DGS approaches suffer from quality degradation over long sequences and impose substantial bandwidth and storage overhead, limiting their applicability in real-time and wide-scale deployments. Therefore, we present AirGS, a streaming-optimized 4DGS framework that rearchitects the training and delivery pipeline to enable high-quality, low-latency FVV experiences. AirGS converts Gaussian video streams into multi-channel 2D formats and intelligently identifies keyframes to enhance frame reconstruction quality. It further combines temporal coherence with inflation loss to reduce training time and representation size. To support communication-efficient transmission, AirGS models 4DGS delivery as an integer linear programming problem and design a lightweight pruning level selection algorithm to adaptively prune the Gaussian updates to be transmitted, balancing reconstruction quality and bandwidth consumption. Extensive experiments demonstrate that AirGS reduces quality deviation in PSNR by more than 20% when scene changes, maintains frame-level PSNR consistently above 30, accelerates training by 6 times, reduces per-frame transmission size by nearly 50% compared to the SOTA 4DGS approaches.","authors":["Zhe Wang","Jinghang Li","Yifei Zhu"],"pdf_url":"","comment":"This paper is accepted by IEEE International Conference on Computer Communications (INFOCOM), 2026"},{"id":"http://arxiv.org/abs/2406.16087v8","updated":"2025-12-24T04:53:14Z","published":"2024-06-23T12:02:17Z","title":"Imperative Learning: A Self-supervised Neuro-Symbolic Learning Framework for Robot Autonomy","summary":"Data-driven methods such as reinforcement and imitation learning have achieved remarkable success in robot autonomy. However, their data-centric nature still hinders them from generalizing well to ever-changing environments. Moreover, labeling data for robotic tasks is often impractical and expensive. To overcome these challenges, we introduce a new self-supervised neuro-symbolic (NeSy) computational framework, imperative learning (IL), for robot autonomy, leveraging the generalization abilities of symbolic reasoning. The framework of IL consists of three primary components: a neural module, a reasoning engine, and a memory system. We formulate IL as a special bilevel optimization (BLO), which enables reciprocal learning over the three modules. This overcomes the label-intensive obstacles associated with data-driven approaches and takes advantage of symbolic reasoning concerning logical reasoning, physical principles, geometric analysis, etc. We discuss several optimization techniques for IL and verify their effectiveness in five distinct robot autonomy tasks including path planning, rule induction, optimal control, visual odometry, and multi-robot routing. Through various experiments, we show that IL can significantly enhance robot autonomy capabilities and we anticipate that it will catalyze further research across diverse domains.","authors":["Chen Wang","Kaiyi Ji","Junyi Geng","Zhongqiang Ren","Taimeng Fu","Fan Yang","Yifan Guo","Haonan He","Xiangyu Chen","Zitong Zhan","Qiwei Du","Shaoshu Su","Bowen Li","Yuheng Qiu","Yi Du","Qihang Li","Yifan Yang","Xiao Lin","Zhipeng Zhao"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20941v1","updated":"2025-12-24T04:53:11Z","published":"2025-12-24T04:53:11Z","title":"A Multi-fidelity Double-Delta Wing Dataset and Empirical Scaling Laws for GNN-based Aerodynamic Field Surrogate","summary":"Data-driven surrogate models are increasingly adopted to accelerate vehicle design. However, open-source multi-fidelity datasets and empirical guidelines linking dataset size to model performance remain limited. This study investigates the relationship between training data size and prediction accuracy for a graph neural network (GNN) based surrogate model for aerodynamic field prediction. We release an open-source, multi-fidelity aerodynamic dataset for double-delta wings, comprising 2448 flow snapshots across 272 geometries evaluated at angles of attack from 11 (degree) to 19 (degree) at Ma=0.3 using both Vortex Lattice Method (VLM) and Reynolds-Averaged Navier-Stokes (RANS) solvers. The geometries are generated using a nested Saltelli sampling scheme to support future dataset expansion and variance-based sensitivity analysis. Using this dataset, we conduct a preliminary empirical scaling study of the MF-VortexNet surrogate by constructing six training datasets with sizes ranging from 40 to 1280 snapshots and training models with 0.1 to 2.4 million parameters under a fixed training budget. We find that the test error decreases with data size with a power-law exponent of -0.6122, indicating efficient data utilization. Based on this scaling law, we estimate that the optimal sampling density is approximately eight samples per dimension in a d-dimensional design space. The results also suggest improved data utilization efficiency for larger surrogate models, implying a potential trade-off between dataset generation cost and model training budget.","authors":["Yiren Shen","Juan J. Alonso"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2412.08893v2","updated":"2025-12-24T04:42:33Z","published":"2024-12-12T03:14:47Z","title":"Optimal Control with Natural Images: Efficient Reinforcement Learning using Overcomplete Sparse Codes","summary":"Optimal control and sequential decision making are widely used in many complex tasks. Optimal control over a sequence of natural images is a first step towards understanding the role of vision in control. Here, we formalize this problem as a reinforcement learning task, and derive general conditions under which an image includes enough information to implement an optimal policy. Reinforcement learning is shown to provide a computationally efficient method for finding optimal policies when natural images are encoded into \"efficient\" image representations. This is demonstrated by introducing a new reinforcement learning benchmark that easily scales to large numbers of states and long horizons. In particular, by representing each image as an overcomplete sparse code, we are able to efficiently solve an optimal control task that is orders of magnitude larger than those tasks solvable using complete codes. Theoretical justification for this behaviour is provided. This work also demonstrates that deep learning is not necessary for efficient optimal control with natural images.","authors":["Peter N. Loxley"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.18725v2","updated":"2025-12-24T04:31:46Z","published":"2025-12-21T12:59:45Z","title":"ML Inference Scheduling with Predictable Latency","summary":"Machine learning (ML) inference serving systems can schedule requests to improve GPU utilization and to meet service level objectives (SLOs) or deadlines. However, improving GPU utilization may compromise latency-sensitive scheduling, as concurrent tasks contend for GPU resources and thereby introduce interference. Given that interference effects introduce unpredictability in scheduling, neglecting them may compromise SLO or deadline satisfaction. Nevertheless, existing interference prediction approaches remain limited in several respects, which may restrict their usefulness for scheduling. First, they are often coarse-grained, which ignores runtime co-location dynamics and thus restricts their accuracy in interference prediction. Second, they tend to use a static prediction model, which may not effectively cope with different workload characteristics. In this paper, we evaluate the potential limitations of existing interference prediction approaches, finding that coarse-grained methods can lead to noticeable deviations in prediction accuracy and that static models degrade considerably under changing workloads.","authors":["Haidong Zhao","Nikolaos Georgantas"],"pdf_url":"","comment":"Accepted at MAIoT@Middleware 2025"},{"id":"http://arxiv.org/abs/2512.20932v1","updated":"2025-12-24T04:25:31Z","published":"2025-12-24T04:25:31Z","title":"Guardrailed Elasticity Pricing: A Churn-Aware Forecasting Playbook for Subscription Strategy","summary":"This paper presents a marketing analytics framework that operationalizes subscription pricing as a dynamic, guardrailed decision system, uniting multivariate demand forecasting, segment-level price elasticity, and churn propensity to optimize revenue, margin, and retention. The approach blends seasonal time-series models with tree-based learners, runs Monte Carlo scenario tests to map risk envelopes, and solves a constrained optimization that enforces business guardrails on customer experience, margin floors, and allowable churn. Validated across heterogeneous SaaS portfolios, the method consistently outperforms static tiers and uniform uplifts by reallocating price moves toward segments with higher willingness-to-pay while protecting price-sensitive cohorts. The system is designed for real-time recalibration via modular APIs and includes model explainability for governance and compliance. Managerially, the framework functions as a strategy playbook that clarifies when to shift from flat to dynamic pricing, how to align pricing with CLV and MRR targets, and how to embed ethical guardrails, enabling durable growth without eroding customer trust.","authors":["Deepit Sapru"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20924v1","updated":"2025-12-24T04:04:20Z","published":"2025-12-24T04:04:20Z","title":"Clever Hans in Chemistry: Chemist Style Signals Confound Activity Prediction on Public Benchmarks","summary":"Can machine learning models identify which chemist made a molecule from structure alone? If so, models trained on literature data may exploit chemist intent rather than learning causal structure-activity relationships. We test this by linking CHEMBL assays to publication authors and training a 1,815-class classifier to predict authors from molecular fingerprints, achieving 60% top-5 accuracy under scaffold-based splitting. We then train an activity model that receives only a protein identifier and an author-probability vector derived from structure, with no direct access to molecular descriptors. This author-only model achieves predictive power comparable to a simple baseline that has access to structure. This reveals a \"Clever Hans\" failure mode: models can predict bioactivity largely by inferring chemist goals and favorite targets without requiring a lab-independent understanding of chemistry. We analyze the sources of this leakage, propose author-disjoint splits, and recommend dataset practices to decouple chemist intent from biological outcomes.","authors":["Andrew D. Blevins","Ian K. Quigley"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2507.11005v3","updated":"2025-12-24T04:04:10Z","published":"2025-07-15T05:49:37Z","title":"AdaMuon: Adaptive Muon Optimizer","summary":"We propose AdaMuon, a novel optimizer that combines element-wise adaptivity with orthogonal updates for large-scale neural network training. AdaMuon incorporates two tightly coupled mechanisms: (1) an element-wise second momentum estimator applied to orthogonalized update directions, and (2) a sign-stabilized orthogonal update, where the momentum is first sign-transformed before orthogonalization. These two components jointly enable variance-adaptive scaling while maintaining stable update geometry. In addition, AdaMuon employs an RMS-aligned rescaling strategy to match the root-mean-square update magnitude to Adam, allowing direct reuse of existing learning rate schedules without extra tuning. Experiments demonstrate that AdaMuon not only maintains stability but can surpass Adam by more than 40\\% training efficiency in large-scale scenarios.","authors":["Chongjie Si","Debing Zhang","Wei Shen"],"pdf_url":"","comment":"Codes are available at https://github.com/Chongjie-Si/AdaMuon"},{"id":"http://arxiv.org/abs/2512.20920v1","updated":"2025-12-24T03:56:58Z","published":"2025-12-24T03:56:58Z","title":"RevFFN: Memory-Efficient Full-Parameter Fine-Tuning of Mixture-of-Experts LLMs with Reversible Blocks","summary":"Full parameter fine tuning is a key technique for adapting large language models (LLMs) to downstream tasks, but it incurs substantial memory overhead due to the need to cache extensive intermediate activations for backpropagation. This bottleneck makes full fine tuning of contemporary large scale LLMs challenging in practice. Existing distributed training frameworks such as DeepSpeed alleviate this issue using techniques like ZeRO and FSDP, which rely on multi GPU memory or CPU offloading, but often require additional hardware resources and reduce training speed. We introduce RevFFN, a memory efficient fine tuning paradigm for mixture of experts (MoE) LLMs. RevFFN employs carefully designed reversible Transformer blocks that allow reconstruction of layer input activations from outputs during backpropagation, eliminating the need to store most intermediate activations in memory. While preserving the expressive capacity of MoE architectures, this approach significantly reduces peak memory consumption for full parameter fine tuning. As a result, RevFFN enables efficient full fine tuning on a single consumer grade or server grade GPU.","authors":["Ningyuan Liu","Jing Yang","Kaitong Cai","Keze Wang"],"pdf_url":"","comment":"Under submission"},{"id":"http://arxiv.org/abs/2512.15745v2","updated":"2025-12-24T03:46:46Z","published":"2025-12-10T09:26:18Z","title":"LLaDA2.0: Scaling Up Diffusion Language Models to 100B","summary":"This paper presents LLaDA2.0 -- a tuple of discrete diffusion large language models (dLLM) scaling up to 100B total parameters through systematic conversion from auto-regressive (AR) models -- establishing a new paradigm for frontier-scale deployment. Instead of costly training from scratch, LLaDA2.0 upholds knowledge inheritance, progressive adaption and efficiency-aware design principle, and seamless converts a pre-trained AR model into dLLM with a novel 3-phase block-level WSD based training scheme: progressive increasing block-size in block diffusion (warm-up), large-scale full-sequence diffusion (stable) and reverting back to compact-size block diffusion (decay). Along with post-training alignment with SFT and DPO, we obtain LLaDA2.0-mini (16B) and LLaDA2.0-flash (100B), two instruction-tuned Mixture-of-Experts (MoE) variants optimized for practical deployment. By preserving the advantages of parallel decoding, these models deliver superior performance and efficiency at the frontier scale. Both models were open-sourced.","authors":["Tiwei Bie","Maosong Cao","Kun Chen","Lun Du","Mingliang Gong","Zhuochen Gong","Yanmei Gu","Jiaqi Hu","Zenan Huang","Zhenzhong Lan","Chengxi Li","Chongxuan Li","Jianguo Li","Zehuan Li","Huabin Liu","Lin Liu","Guoshan Lu","Xiaocheng Lu","Yuxin Ma","Jianfeng Tan","Lanning Wei","Ji-Rong Wen","Yipeng Xing","Xiaolu Zhang","Junbo Zhao","Da Zheng","Jun Zhou","Junlin Zhou","Zhanchao Zhou","Liwang Zhu","Yihong Zhuang"],"pdf_url":"","comment":"19 pages"},{"id":"http://arxiv.org/abs/2512.20915v1","updated":"2025-12-24T03:43:54Z","published":"2025-12-24T03:43:54Z","title":"Towards a General Framework for Predicting and Explaining the Hardness of Graph-based Combinatorial Optimization Problems using Machine Learning and Association Rule Mining","summary":"This study introduces GCO-HPIF, a general machine-learning-based framework to predict and explain the computational hardness of combinatorial optimization problems that can be represented on graphs. The framework consists of two stages. In the first stage, a dataset is created comprising problem-agnostic graph features and hardness classifications of problem instances. Machine-learning-based classification algorithms are trained to map graph features to hardness categories. In the second stage, the framework explains the predictions using an association rule mining algorithm. Additionally, machine-learning-based regression models are trained to predict algorithmic computation times. The GCO-HPIF framework was applied to a dataset of 3287 maximum clique problem instances compiled from the COLLAB, IMDB, and TWITTER graph datasets using five state-of-the-art algorithms, namely three exact branch-and-bound-based algorithms (Gurobi, CliSAT, and MOMC) and two graph-neural-network-based algorithms (EGN and HGS). The framework demonstrated excellent performance in predicting instance hardness, achieving a weighted F1 score of 0.9921, a minority-class F1 score of 0.878, and an ROC-AUC score of 0.9083 using only three graph features. The best association rule found by the FP-Growth algorithm for explaining the hardness predictions had a support of 0.8829 for hard instances and an overall accuracy of 87.64 percent, underscoring the framework's usefulness for both prediction and explanation. Furthermore, the best-performing regression model for predicting computation times achieved a percentage RMSE of 5.12 and an R2 value of 0.991.","authors":["Bharat Sharman","Elkafi Hassini"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.10952v2","updated":"2025-12-24T03:33:33Z","published":"2025-12-11T18:59:55Z","title":"Hierarchical Dataset Selection for High-Quality Data Sharing","summary":"The success of modern machine learning hinges on access to high-quality training data. In many real-world scenarios, such as acquiring data from public repositories or sharing across institutions, data is naturally organized into discrete datasets that vary in relevance, quality, and utility. Selecting which repositories or institutions to search for useful datasets, and which datasets to incorporate into model training are therefore critical decisions, yet most existing methods select individual samples and treat all data as equally relevant, ignoring differences between datasets and their sources. In this work, we formalize the task of dataset selection: selecting entire datasets from a large, heterogeneous pool to improve downstream performance under resource constraints. We propose Dataset Selection via Hierarchies (DaSH), a dataset selection method that models utility at both dataset and group (e.g., collections, institutions) levels, enabling efficient generalization from limited observations. Across two public benchmarks (Digit-Five and DomainNet), DaSH outperforms state-of-the-art data selection baselines by up to 26.2% in accuracy, while requiring significantly fewer exploration steps. Ablations show DaSH is robust to low-resource settings and lack of relevant datasets, making it suitable for scalable and adaptive dataset selection in practical multi-source learning workflows.","authors":["Xiaona Zhou","Yingyan Zeng","Ran Jin","Ismini Lourentzou"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20905v1","updated":"2025-12-24T03:10:00Z","published":"2025-12-24T03:10:00Z","title":"DiEC: Diffusion Embedded Clustering","summary":"Deep clustering hinges on learning representations that are inherently clusterable. However, using a single encoder to produce a fixed embedding ignores the representation trajectory formed by a pretrained diffusion model across network hierarchies and noise timesteps, where clusterability varies substantially. We propose DiEC (Diffusion Embedded Clustering), which performs unsupervised clustering by directly reading internal activations from a pretrained diffusion U-Net.\n  DiEC formulates representation selection as a two-dimensional search over layer x timestep, and exploits a weak-coupling property to decompose it into two stages. Specifically, we first fix the U-Net bottleneck layer as the Clustering-friendly Middle Layer (CML), and then use Optimal Timestep Search (OTS) to identify the clustering-optimal timestep (t*). During training, we extract bottleneck features at the fixed t* and obtain clustering representations via a lightweight residual mapping. We optimize a DEC-style KL self-training objective, augmented with adaptive graph regularization and entropy regularization to strengthen cluster structures. In parallel, we introduce a denoising-consistency branch at random timesteps to stabilize the representations and preserve generative consistency. Experiments show that DiEC achieves competitive clustering performance on multiple standard benchmarks.","authors":["Haidong Hu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20610v2","updated":"2025-12-24T02:38:59Z","published":"2025-12-23T18:57:53Z","title":"FedPOD: the deployable units of training for federated learning","summary":"This paper proposes FedPOD, which ranked first in the 2024 Federated Tumor Segmentation (FeTS) Challenge, for optimizing learning efficiency and communication cost in federated learning among multiple clients. Inspired by FedPIDAvg, we define a round-wise task for FedPOD to enhance training efficiency. FedPIDAvg achieved performance improvement by incorporating the training loss reduction for prediction entropy as weights using differential terms. Furthermore, by modeling data distribution with a Poisson distribution and using a PID controller, it reduced communication costs even in skewed data distribution. However, excluding participants classified as outliers based on the Poisson distribution can limit data utilization. Additionally, PID controller requires the same participants to be maintained throughout the federated learning process as it uses previous rounds' learning information in the current round. In our approach, FedPOD addresses these issues by including participants excluded as outliers, eliminating dependency on previous rounds' learning information, and applying a method for calculating validation loss at each round. In this challenge, FedPOD presents comparable performance to FedPIDAvg in metrics of Dice score, 0.78, 0.71 and 0.72 for WT, ET and TC in average, and projected convergence score, 0.74 in average. Furthermore, the concept of FedPOD draws inspiration from Kubernetes' smallest computing unit, POD, designed to be compatible with Kubernetes auto-scaling. Extending round-wise tasks of FedPOD to POD units allows flexible design by applying scale-out similar to Kubernetes' auto-scaling. This work demonstrated the potentials of FedPOD to enhance federated learning by improving efficiency, flexibility, and performance in metrics.","authors":["Daewoon Kim","Si Young Yie","Jae Sung Lee"],"pdf_url":"","comment":"12 pages, 12 figures, MICCAI"},{"id":"http://arxiv.org/abs/2512.20893v1","updated":"2025-12-24T02:33:08Z","published":"2025-12-24T02:33:08Z","title":"Time-Efficient Evaluation and Enhancement of Adversarial Robustness in Deep Neural Networks","summary":"With deep neural networks (DNNs) increasingly embedded in modern society, ensuring their safety has become a critical and urgent issue. In response, substantial efforts have been dedicated to the red-blue adversarial framework, where the red team focuses on identifying vulnerabilities in DNNs and the blue team on mitigating them. However, existing approaches from both teams remain computationally intensive, constraining their applicability to large-scale models. To overcome this limitation, this thesis endeavours to provide time-efficient methods for the evaluation and enhancement of adversarial robustness in DNNs.","authors":["Runqi Lin"],"pdf_url":"","comment":"Ph.D. Thesis, The University of Sydney"},{"id":"http://arxiv.org/abs/2512.20885v1","updated":"2025-12-24T02:05:46Z","published":"2025-12-24T02:05:46Z","title":"From GNNs to Symbolic Surrogates via Kolmogorov-Arnold Networks for Delay Prediction","summary":"Accurate prediction of flow delay is essential for optimizing and managing modern communication networks. We investigate three levels of modeling for this task. First, we implement a heterogeneous GNN with attention-based message passing, establishing a strong neural baseline. Second, we propose FlowKANet in which Kolmogorov-Arnold Networks replace standard MLP layers, reducing trainable parameters while maintaining competitive predictive performance. FlowKANet integrates KAMP-Attn (Kolmogorov-Arnold Message Passing with Attention), embedding KAN operators directly into message-passing and attention computation. Finally, we distill the model into symbolic surrogate models using block-wise regression, producing closed-form equations that eliminate trainable weights while preserving graph-structured dependencies. The results show that KAN layers provide a favorable trade-off between efficiency and accuracy and that symbolic surrogates emphasize the potential for lightweight deployment and enhanced transparency.","authors":["Sami Marouani","Kamal Singh","Baptiste Jeudy","Amaury Habrard"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2511.21701v2","updated":"2025-12-24T01:52:42Z","published":"2025-11-16T06:08:41Z","title":"47B Mixture-of-Experts Beats 671B Dense Models on Chinese Medical Examinations","summary":"The rapid advancement of large language models(LLMs) has prompted significant interest in their potential applications in medical domains. This paper presents a comprehensive benchmark evaluation of 27 state-of-the-art LLMs on Chinese medical examination questions, encompassing seven medical specialties across two professional levels. We introduce a robust evaluation framework that assesses model performance on 2,800 carefully curated questions from cardiovascular, gastroenterology, hematology, infectious diseases, nephrology, neurology, and respiratory medicine domains. Our dataset distinguishes between attending physician and senior physician difficulty levels, providing nuanced insights into model capabilities across varying complexity. Our empirical analysis reveals substantial performance variations among models, with Mixtral-8x7B achieving the highest overall accuracy of 74.25%, followed by DeepSeek-R1-671B at 64.07%. Notably, we observe no consistent correlation between model size and performance, as evidenced by the strong performance of smaller mixture-of-experts architectures. The evaluation demonstrates significant performance gaps between medical specialties, with models generally performing better on cardiovascular and neurology questions compared to gastroenterology and nephrology domains. Furthermore, our analysis indicates minimal performance degradation between attending and senior physician levels for top-performing models, suggesting robust generalization capabilities. This benchmark provides critical insights for the deployment of LLMs in medical education and clinical decision support systems, highlighting both the promise and current limitations of these technologies in specialized medical contexts.","authors":["Chiung-Yi Tseng","Danyang Zhang","Tianyang Wang","Hongying Luo","Lu Chen","Junming Huang","Jibin Guan","Junfeng Hao","Junhao Song","Xinyuan Song","Ziqian Bi"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2408.08056v2","updated":"2025-12-24T01:49:13Z","published":"2024-08-15T09:50:11Z","title":"DATTA: Domain Diversity Aware Test-Time Adaptation for Dynamic Domain Shift Data Streams","summary":"Test-Time Adaptation (TTA) addresses domain shifts between training and testing. However, existing methods assume a homogeneous target domain (e.g., single domain) at any given time. They fail to handle the dynamic nature of real-world data, where single-domain and multiple-domain distributions change over time. We identify that performance drops in multiple-domain scenarios are caused by batch normalization errors and gradient conflicts, which hinder adaptation. To solve these challenges, we propose Domain Diversity Adaptive Test-Time Adaptation (DATTA), the first approach to handle TTA under dynamic domain shift data streams. It is guided by a novel domain-diversity score. DATTA has three key components: a domain-diversity discriminator to recognize single- and multiple-domain patterns, domain-diversity adaptive batch normalization to combine source and test-time statistics, and domain-diversity adaptive fine-tuning to resolve gradient conflicts. Extensive experiments show that DATTA significantly outperforms state-of-the-art methods by up to 13%. Code is available at https://github.com/DYW77/DATTA.","authors":["Chuyang Ye","Dongyan Wei","Zhendong Liu","Yuanyi Pang","Yixi Lin","Qinting Jiang","Jingyan Jiang","Dongbiao He"],"pdf_url":"","comment":"Accepted to 2025 IEEE International Conference on Multimedia and Expo (ICME), Oral Presentation"},{"id":"http://arxiv.org/abs/2507.01041v3","updated":"2025-12-24T01:45:18Z","published":"2025-06-23T07:14:04Z","title":"Fast AI Model Splitting over Edge Networks","summary":"Split learning (SL) has emerged as a computationally efficient approach for artificial intelligence (AI) model training, which can alleviate device-side computational workloads. However, complex AI model architectures pose high computational complexity to obtain the optimal model splitting. In this paper, we represent an arbitrary AI model as a directed acyclic graph (DAG), and then reformulate the optimal model splitting problem as a minimum s-t cut search problem. To solve the problem, we propose a fast DAG-based model splitting algorithm, which restructures the DAG to enable the optimal model splitting identification via a maximum flow method. Theoretical analysis indicates that the proposed algorithm is optimal. Furthermore, considering AI models with block structures, we propose a block-wise model splitting algorithm to reduce computational complexity. The algorithm abstracts each block, i.e., a component consisting of multiple layers, into a single vertex, thereby obtaining the optimal model splitting via a simplified DAG. Extensive experimental results demonstrate that the proposed algorithms can determine the optimal model splitting within milliseconds, as well as reduce training delay by 24.62%-38.95% in dynamic edge networks as compared to the state-of-the-art benchmarks.","authors":["Zuguang Li","Wen Wu","Shaohua Wu","Songge Zhang","Ye Wang"," Xuemin"," Shen"],"pdf_url":"","comment":"This version lacks sufficient detail in key technical parts, including the equivalence proof for the s-t cut transformation and the computational complexity analysis (Sections VI-D). We are withdrawing it to prepare a revised, more complete manuscript"},{"id":"http://arxiv.org/abs/2512.20877v1","updated":"2025-12-24T01:36:50Z","published":"2025-12-24T01:36:50Z","title":"Architectural Trade-offs in Small Language Models Under Compute Constraints","summary":"We present a systematic empirical study of small language models under strict compute constraints, analyzing how architectural choices and training budget interact to determine performance. Starting from a linear next-token predictor, we progressively introduce nonlinearities, self-attention, and multi-layer transformer architectures, evaluating each on character-level modeling of Tiny Shakespeare and word-level modeling of Penn Treebank (PTB) and WikiText-2. We compare models using test negative log-likelihood (NLL), parameter count, and approximate training FLOPs to characterize accuracy-efficiency trade-offs. Our results show that attention-based models dominate MLPs in per-FLOP efficiency even at small scale, while increasing depth or context without sufficient optimization can degrade performance. We further examine rotary positional embeddings (RoPE), finding that architectural techniques successful in large language models do not necessarily transfer to small-model regimes.","authors":["Shivraj Singh Bhatti"],"pdf_url":"","comment":"15 pages, 11 images"},{"id":"http://arxiv.org/abs/2511.11767v4","updated":"2025-12-24T01:31:19Z","published":"2025-11-14T07:51:56Z","title":"Learning Fair Representations with Kolmogorov-Arnold Networks","summary":"Despite recent advances in fairness-aware machine learning, predictive models often exhibit discriminatory behavior towards marginalized groups. Such unfairness might arise from biased training data, model design, or representational disparities across groups, posing significant challenges in high-stakes decision-making domains such as college admissions. While existing fair learning models aim to mitigate bias, achieving an optimal trade-off between fairness and accuracy remains a challenge. Moreover, the reliance on black-box models hinders interpretability, limiting their applicability in socially sensitive domains. To circumvent these issues, we propose integrating Kolmogorov-Arnold Networks (KANs) within a fair adversarial learning framework. Leveraging the adversarial robustness and interpretability of KANs, our approach facilitates stable adversarial learning. We derive theoretical insights into the spline-based KAN architecture that ensure stability during adversarial optimization. Additionally, an adaptive fairness penalty update mechanism is proposed to strike a balance between fairness and accuracy. We back these findings with empirical evidence on two real-world admissions datasets, demonstrating the proposed framework's efficiency in achieving fairness across sensitive attributes while preserving predictive performance.","authors":["Amisha Priyadarshini","Sergio Gago-Masague"],"pdf_url":"","comment":"This article has been accepted at AAAI-26 (The 40th Annual AAAI Conference on Artificial Intelligence)"},{"id":"http://arxiv.org/abs/2512.20872v1","updated":"2025-12-24T01:21:38Z","published":"2025-12-24T01:21:38Z","title":"Better Call Graphs: A New Dataset of Function Call Graphs for Malware Classification","summary":"Function call graphs (FCGs) have emerged as a powerful abstraction for malware detection, capturing the behavioral structure of applications beyond surface-level signatures. Their utility in traditional program analysis has been well established, enabling effective classification and analysis of malicious software. In the mobile domain, especially in the Android ecosystem, FCG-based malware classification is particularly critical due to the platform's widespread adoption and the complex, component-based structure of Android apps. However, progress in this direction is hindered by the lack of large-scale, high-quality Android-specific FCG datasets. Existing datasets are often outdated, dominated by small or redundant graphs resulting from app repackaging, and fail to reflect the diversity of real-world malware. These limitations lead to overfitting and unreliable evaluation of graph-based classification methods. To address this gap, we introduce Better Call Graphs (BCG), a comprehensive dataset of large and unique FCGs extracted from recent Android application packages (APKs). BCG includes both benign and malicious samples spanning various families and types, along with graph-level features for each APK. Through extensive experiments using baseline classifiers, we demonstrate the necessity and value of BCG compared to existing datasets. BCG is publicly available at https://erdemub.github.io/BCG-dataset.","authors":["Jakir Hossain","Gurvinder Singh","Lukasz Ziarek","Ahmet Erdem Sarıyüce"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20865v1","updated":"2025-12-24T00:49:47Z","published":"2025-12-24T00:49:47Z","title":"Robustness Certificates for Neural Networks against Adversarial Attacks","summary":"The increasing use of machine learning in safety-critical domains amplifies the risk of adversarial threats, especially data poisoning attacks that corrupt training data to degrade performance or induce unsafe behavior. Most existing defenses lack formal guarantees or rely on restrictive assumptions about the model class, attack type, extent of poisoning, or point-wise certification, limiting their practical reliability. This paper introduces a principled formal robustness certification framework that models gradient-based training as a discrete-time dynamical system (dt-DS) and formulates poisoning robustness as a formal safety verification problem. By adapting the concept of barrier certificates (BCs) from control theory, we introduce sufficient conditions to certify a robust radius ensuring that the terminal model remains safe under worst-case ${\\ell}_p$-norm based poisoning. To make this practical, we parameterize BCs as neural networks trained on finite sets of poisoned trajectories. We further derive probably approximately correct (PAC) bounds by solving a scenario convex program (SCP), which yields a confidence lower bound on the certified robustness radius generalizing beyond the training set. Importantly, our framework also extends to certification against test-time attacks, making it the first unified framework to provide formal guarantees in both training and test-time attack settings. Experiments on MNIST, SVHN, and CIFAR-10 show that our approach certifies non-trivial perturbation budgets while being model-agnostic and requiring no prior knowledge of the attack or contamination level.","authors":["Sara Taheri","Mahalakshmi Sabanayagam","Debarghya Ghoshdastidar","Majid Zamani"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20861v1","updated":"2025-12-24T00:41:13Z","published":"2025-12-24T00:41:13Z","title":"Memory-Efficient Acceleration of Block Low-Rank Foundation Models on Resource Constrained GPUs","summary":"Recent advances in transformer-based foundation models have made them the default choice for many tasks, but their rapidly growing size makes fitting a full model on a single GPU increasingly difficult and their computational cost prohibitive. Block low-rank (BLR) compression techniques address this challenge by learning compact representations of weight matrices. While traditional low-rank (LR) methods often incur sharp accuracy drops, BLR approaches such as Monarch and BLAST can better capture the underlying structure, thus preserving accuracy while reducing computations and memory footprints. In this work, we use roofline analysis to show that, although BLR methods achieve theoretical savings and practical speedups for single-token inference, multi-token inference often becomes memory-bound in practice, increasing latency despite compiler-level optimizations in PyTorch. To address this, we introduce custom Triton kernels with partial fusion and memory layout optimizations for both Monarch and BLAST. On memory-constrained NVIDIA GPUs such as Jetson Orin Nano and A40, our kernels deliver up to $3.76\\times$ speedups and $3\\times$ model size compression over PyTorch dense baselines using CUDA backend and compiler-level optimizations, while supporting various models including Llama-7/1B, GPT2-S, DiT-XL/2, and ViT-B. Our code is available at https://github.com/pabillam/mem-efficient-blr .","authors":["Pierre Abillama","Changwoo Lee","Juechu Dong","David Blaauw","Dennis Sylvester","Hun-Seok Kim"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2507.10884v2","updated":"2025-12-24T00:39:09Z","published":"2025-07-15T00:56:21Z","title":"Learning from Imperfect Data: Robust Inference of Dynamic Systems using Simulation-based Generative Model","summary":"System inference for nonlinear dynamic models, represented by ordinary differential equations (ODEs), remains a significant challenge in many fields, particularly when the data are noisy, sparse, or partially observable. In this paper, we propose a Simulation-based Generative Model for Imperfect Data (SiGMoID) that enables precise and robust inference for dynamic systems. The proposed approach integrates two key methods: (1) physics-informed neural networks with hyper-networks that constructs an ODE solver, and (2) Wasserstein generative adversarial networks that estimates ODE parameters by effectively capturing noisy data distributions. We demonstrate that SiGMoID quantifies data noise, estimates system parameters, and infers unobserved system components. Its effectiveness is validated validated through realistic experimental examples, showcasing its broad applicability in various domains, from scientific research to engineered systems, and enabling the discovery of full system dynamics.","authors":["Hyunwoo Cho","Hyeontae Jo","Hyung Ju Hwang"],"pdf_url":"","comment":"20 pages, 9 figures, AAAI2026 (paper id: 20546)"},{"id":"http://arxiv.org/abs/2512.20856v1","updated":"2025-12-24T00:24:05Z","published":"2025-12-24T00:24:05Z","title":"NVIDIA Nemotron 3: Efficient and Open Intelligence","summary":"We introduce the Nemotron 3 family of models - Nano, Super, and Ultra. These models deliver strong agentic, reasoning, and conversational capabilities. The Nemotron 3 family uses a Mixture-of-Experts hybrid Mamba-Transformer architecture to provide best-in-class throughput and context lengths of up to 1M tokens. Super and Ultra models are trained with NVFP4 and incorporate LatentMoE, a novel approach that improves model quality. The two larger models also include MTP layers for faster text generation. All Nemotron 3 models are post-trained using multi-environment reinforcement learning enabling reasoning, multi-step tool use, and support granular reasoning budget control. Nano, the smallest model, outperforms comparable models in accuracy while remaining extremely cost-efficient for inference. Super is optimized for collaborative agents and high-volume workloads such as IT ticket automation. Ultra, the largest model, provides state-of-the-art accuracy and reasoning performance. Nano is released together with its technical report and this white paper, while Super and Ultra will follow in the coming months. We will openly release the model weights, pre- and post-training software, recipes, and all data for which we hold redistribution rights.","authors":[" NVIDIA"," :","Aaron Blakeman","Aaron Grattafiori","Aarti Basant","Abhibha Gupta","Abhinav Khattar","Adi Renduchintala","Aditya Vavre","Akanksha Shukla","Akhiad Bercovich","Aleksander Ficek","Aleksandr Shaposhnikov","Alex Kondratenko","Alexander Bukharin","Alexandre Milesi","Ali Taghibakhshi","Alisa Liu","Amelia Barton","Ameya Sunil Mahabaleshwarkar","Amir Klein","Amit Zuker","Amnon Geifman","Amy Shen","Anahita Bhiwandiwalla","Andrew Tao","Anjulie Agrusa","Ankur Verma","Ann Guan","Anubhav Mandarwal","Arham Mehta","Ashwath Aithal","Ashwin Poojary","Asif Ahamed","Asit Mishra","Asma Kuriparambil Thekkumpate","Ayush Dattagupta","Banghua Zhu","Bardiya Sadeghi","Barnaby Simkin","Ben Lanir","Benedikt Schifferer","Besmira Nushi","Bilal Kartal","Bita Darvish Rouhani","Boris Ginsburg","Brandon Norick","Brandon Soubasis","Branislav Kisacanin","Brian Yu","Bryan Catanzaro","Carlo del Mundo","Chantal Hwang","Charles Wang","Cheng-Ping Hsieh","Chenghao Zhang","Chenhan Yu","Chetan Mungekar","Chintan Patel","Chris Alexiuk","Christopher Parisien","Collin Neale","Cyril Meurillon","Damon Mosk-Aoyama","Dan Su","Dane Corneil","Daniel Afrimi","Daniel Lo","Daniel Rohrer","Daniel Serebrenik","Daria Gitman","Daria Levy","Darko Stosic","David Mosallanezhad","Deepak Narayanan","Dhruv Nathawani","Dima Rekesh","Dina Yared","Divyanshu Kakwani","Dong Ahn","Duncan Riach","Dusan Stosic","Edgar Minasyan","Edward Lin","Eileen Long","Eileen Peters Long","Elad Segal","Elena Lantz","Ellie Evans","Elliott Ning","Eric Chung","Eric Harper","Eric Tramel","Erick Galinkin","Erik Pounds","Evan Briones","Evelina Bakhturina","Evgeny Tsykunov","Faisal Ladhak","Fay Wang","Fei Jia","Felipe Soares","Feng Chen","Ferenc Galko","Frank Sun","Frankie Siino","Gal Hubara Agam","Ganesh Ajjanagadde","Gantavya Bhatt","Gargi Prasad","George Armstrong","Gerald Shen","Gorkem Batmaz","Grigor Nalbandyan","Haifeng Qian","Harsh Sharma","Hayley Ross","Helen Ngo","Herbert Hum","Herman Sahota","Hexin Wang","Himanshu Soni","Hiren Upadhyay","Huizi Mao","Huy C Nguyen","Huy Q Nguyen","Iain Cunningham","Ido Galil","Ido Shahaf","Igor Gitman","Ilya Loshchilov","Itamar Schen","Itay Levy","Ivan Moshkov","Izik Golan","Izzy Putterman","Jan Kautz","Jane Polak Scowcroft","Jared Casper","Jatin Mitra","Jeffrey Glick","Jenny Chen","Jesse Oliver","Jian Zhang","Jiaqi Zeng","Jie Lou","Jimmy Zhang","Jinhang Choi","Jining Huang","Joey Conway","Joey Guman","John Kamalu","Johnny Greco","Jonathan Cohen","Joseph Jennings","Joyjit Daw","Julien Veron Vialard","Junkeun Yi","Jupinder Parmar","Kai Xu","Kan Zhu","Kari Briski","Katherine Cheung","Katherine Luna","Keith Wyss","Keshav Santhanam","Kevin Shih","Kezhi Kong","Khushi Bhardwaj","Kirthi Shankar","Krishna C. Puvvada","Krzysztof Pawelec","Kumar Anik","Lawrence McAfee","Laya Sleiman","Leon Derczynski","Li Ding","Lizzie Wei","Lucas Liebenwein","Luis Vega","Maanu Grover","Maarten Van Segbroeck","Maer Rodrigues de Melo","Mahdi Nazemi","Makesh Narsimhan Sreedhar","Manoj Kilaru","Maor Ashkenazi","Marc Romeijn","Marcin Chochowski","Mark Cai","Markus Kliegl","Maryam Moosaei","Matt Kulka","Matvei Novikov","Mehrzad Samadi","Melissa Corpuz","Mengru Wang","Meredith Price","Michael Andersch","Michael Boone","Michael Evans","Miguel Martinez","Mikail Khona","Mike Chrzanowski","Minseok Lee","Mohammad Dabbah","Mohammad Shoeybi","Mostofa Patwary","Nabin Mulepati","Najeeb Nabwani","Natalie Hereth","Nave Assaf","Negar Habibi","Neta Zmora","Netanel Haber","Nicola Sessions","Nidhi Bhatia","Nikhil Jukar","Nikki Pope","Nikolai Ludwig","Nima Tajbakhsh","Nir Ailon","Nirmal Juluru","Nishant Sharma","Oleksii Hrinchuk","Oleksii Kuchaiev","Olivier Delalleau","Oluwatobi Olabiyi","Omer Ullman Argov","Omri Puny","Oren Tropp","Ouye Xie","Parth Chadha","Pasha Shamis","Paul Gibbons","Pavlo Molchanov","Pawel Morkisz","Peter Dykas","Peter Jin","Pinky Xu","Piotr Januszewski","Pranav Prashant Thombre","Prasoon Varshney","Pritam Gundecha","Przemek Tredak","Qing Miao","Qiyu Wan","Rabeeh Karimi Mahabadi","Rachit Garg","Ran El-Yaniv","Ran Zilberstein","Rasoul Shafipour","Rich Harang","Rick Izzo","Rima Shahbazyan","Rishabh Garg","Ritika Borkar","Ritu Gala","Riyad Islam","Robert Hesse","Roger Waleffe","Rohit Watve","Roi Koren","Ruoxi Zhang","Russell Hewett","Russell J. Hewett","Ryan Prenger","Ryan Timbrook","Sadegh Mahdavi","Sahil Modi","Samuel Kriman","Sangkug Lim","Sanjay Kariyappa","Sanjeev Satheesh","Saori Kaji","Satish Pasumarthi","Saurav Muralidharan","Sean Narentharen","Sean Narenthiran","Seonmyeong Bak","Sergey Kashirsky","Seth Poulos","Shahar Mor","Shanmugam Ramasamy","Shantanu Acharya","Shaona Ghosh","Sharath Turuvekere Sreenivas","Shelby Thomas","Shiqing Fan","Shreya Gopal","Shrimai Prabhumoye","Shubham Pachori","Shubham Toshniwal","Shuoyang Ding","Siddharth Singh","Simeng Sun","Smita Ithape","Somshubra Majumdar","Soumye Singhal","Stas Sergienko","Stefania Alborghetti","Stephen Ge","Sugam Dipak Devare","Sumeet Kumar Barua","Suseella Panguluri","Suyog Gupta","Sweta Priyadarshi","Syeda Nahida Akter","Tan Bui","Teodor-Dumitru Ene","Terry Kong","Thanh Do","Tijmen Blankevoort","Tim Moon","Tom Balough","Tomer Asida","Tomer Bar Natan","Tomer Ronen","Tugrul Konuk","Twinkle Vashishth","Udi Karpas","Ushnish De","Vahid Noorozi","Vahid Noroozi","Venkat Srinivasan","Venmugil Elango","Victor Cui","Vijay Korthikanti","Vinay Rao","Vitaly Kurin","Vitaly Lavrukhin","Vladimir Anisimov","Wanli Jiang","Wasi Uddin Ahmad","Wei Du","Wei Ping","Wenfei Zhou","Will Jennings","William Zhang","Wojciech Prazuch","Xiaowei Ren","Yashaswi Karnati","Yejin Choi","Yev Meyer","Yi-Fu Wu","Yian Zhang","Yigong Qin","Ying Lin","Yonatan Geifman","Yonggan Fu","Yoshi Subara","Yoshi Suhara","Yubo Gao","Zach Moshe","Zhen Dong","Zhongbo Zhu","Zihan Liu","Zijia Chen","Zijie Yan"],"pdf_url":"","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2512.21076v1","updated":"2025-12-24T09:49:56Z","published":"2025-12-24T09:49:56Z","title":"Blurb-Refined Inference from Crowdsourced Book Reviews using Hierarchical Genre Mining with Dual-Path Graph Convolutions","summary":"Accurate book genre classification is fundamental to digital library organization, content discovery, and personalized recommendation. Existing approaches typically model genre prediction as a flat, single-label task, ignoring hierarchical genre structure and relying heavily on noisy, subjective user reviews, which often degrade classification reliability. We propose HiGeMine, a two-phase hierarchical genre mining framework that robustly integrates user reviews with authoritative book blurbs. In the first phase, HiGeMine employs a zero-shot semantic alignment strategy to filter reviews, retaining only those semantically consistent with the corresponding blurb, thereby mitigating noise, bias, and irrelevance. In the second phase, we introduce a dual-path, two-level graph-based classification architecture: a coarse-grained Level-1 binary classifier distinguishes fiction from non-fiction, followed by Level-2 multi-label classifiers for fine-grained genre prediction. Inter-genre dependencies are explicitly modeled using a label co-occurrence graph, while contextual representations are derived from pretrained language models applied to the filtered textual content. To facilitate systematic evaluation, we curate a new hierarchical book genre dataset. Extensive experiments demonstrate that HiGeMine consistently outperformed strong baselines across hierarchical genre classification tasks. The proposed framework offers a principled and effective solution for leveraging both structured and unstructured textual data in hierarchical book genre analysis.","authors":["Suraj Kumar","Utsav Kumar Nareti","Soumi Chattopadhyay","Chandranath Adak","Prolay Mallick"],"pdf_url":"","comment":"10 pages, 4 figures, 3 tables"},{"id":"http://arxiv.org/abs/2512.12284v3","updated":"2025-12-24T07:46:59Z","published":"2025-12-13T11:02:04Z","title":"V-Rex: Real-Time Streaming Video LLM Acceleration via Dynamic KV Cache Retrieval","summary":"Streaming video large language models (LLMs) are increasingly used for real-time multimodal tasks such as video captioning, question answering, conversational agents, and augmented reality. However, these models face fundamental memory and computational challenges because their key-value (KV) caches grow substantially with continuous streaming video input. This process requires an iterative prefill stage, which is a unique feature of streaming video LLMs. Due to its iterative prefill stage, it suffers from significant limitations, including extensive computation, substantial data transfer, and degradation in accuracy. Crucially, this issue is exacerbated for edge deployment, which is the primary target for these models.\n  In this work, we propose V-Rex, the first software-hardware co-designed accelerator that comprehensively addresses both algorithmic and hardware bottlenecks in streaming video LLM inference. At its core, V-Rex introduces ReSV, a training-free dynamic KV cache retrieval algorithm. ReSV exploits temporal and spatial similarity-based token clustering to reduce excessive KV cache memory across video frames. To fully realize these algorithmic benefits, V-Rex offers a compact, low-latency hardware accelerator with a dynamic KV cache retrieval engine (DRE), featuring bit-level and early-exit based computing units. V-Rex achieves unprecedented real-time of 3.9-8.3 FPS and energy-efficient streaming video LLM inference on edge deployment with negligible accuracy loss. While DRE only accounts for 2.2% power and 2.0% area, the system delivers 1.9-19.7x speedup and 3.1-18.5x energy efficiency improvements over AGX Orin GPU. This work is the first to comprehensively tackle KV cache retrieval across algorithms and hardware, enabling real-time streaming video LLM inference on resource-constrained edge devices.","authors":["Donghyuk Kim","Sejeong Yang","Wonjin Shin","Joo-Young Kim"],"pdf_url":"","comment":"14 pages, 20 figures, conference, accepted by HPCA 2026"},{"id":"http://arxiv.org/abs/2512.20943v1","updated":"2025-12-24T04:57:30Z","published":"2025-12-24T04:57:30Z","title":"AirGS: Real-Time 4D Gaussian Streaming for Free-Viewpoint Video Experiences","summary":"Free-viewpoint video (FVV) enables immersive viewing experiences by allowing users to view scenes from arbitrary perspectives. As a prominent reconstruction technique for FVV generation, 4D Gaussian Splatting (4DGS) models dynamic scenes with time-varying 3D Gaussian ellipsoids and achieves high-quality rendering via fast rasterization. However, existing 4DGS approaches suffer from quality degradation over long sequences and impose substantial bandwidth and storage overhead, limiting their applicability in real-time and wide-scale deployments. Therefore, we present AirGS, a streaming-optimized 4DGS framework that rearchitects the training and delivery pipeline to enable high-quality, low-latency FVV experiences. AirGS converts Gaussian video streams into multi-channel 2D formats and intelligently identifies keyframes to enhance frame reconstruction quality. It further combines temporal coherence with inflation loss to reduce training time and representation size. To support communication-efficient transmission, AirGS models 4DGS delivery as an integer linear programming problem and design a lightweight pruning level selection algorithm to adaptively prune the Gaussian updates to be transmitted, balancing reconstruction quality and bandwidth consumption. Extensive experiments demonstrate that AirGS reduces quality deviation in PSNR by more than 20% when scene changes, maintains frame-level PSNR consistently above 30, accelerates training by 6 times, reduces per-frame transmission size by nearly 50% compared to the SOTA 4DGS approaches.","authors":["Zhe Wang","Jinghang Li","Yifei Zhu"],"pdf_url":"","comment":"This paper is accepted by IEEE International Conference on Computer Communications (INFOCOM), 2026"},{"id":"http://arxiv.org/abs/2512.20916v1","updated":"2025-12-24T03:44:25Z","published":"2025-12-24T03:44:25Z","title":"MMSRARec: Summarization and Retrieval Augumented Sequential Recommendation Based on Multimodal Large Language Model","summary":"Recent advancements in Multimodal Large Language Models (MLLMs) have demonstrated significant potential in recommendation systems. However, the effective application of MLLMs to multimodal sequential recommendation remains unexplored: A) Existing methods primarily leverage the multimodal semantic understanding capabilities of pre-trained MLLMs to generate item embeddings or semantic IDs, thereby enhancing traditional recommendation models. These approaches generate item representations that exhibit limited interpretability, and pose challenges when transferring to language model-based recommendation systems. B) Other approaches convert user behavior sequence into image-text pairs and perform recommendation through multiple MLLM inference, incurring prohibitive computational and time costs. C) Current MLLM-based recommendation systems generally neglect the integration of collaborative signals. To address these limitations while balancing recommendation performance, interpretability, and computational cost, this paper proposes MultiModal Summarization-and-Retrieval-Augmented Sequential Recommendation. Specifically, we first employ MLLM to summarize items into concise keywords and fine-tune the model using rewards that incorporate summary length, information loss, and reconstruction difficulty, thereby enabling adaptive adjustment of the summarization policy. Inspired by retrieval-augmented generation, we then transform collaborative signals into corresponding keywords and integrate them as supplementary context. Finally, we apply supervised fine-tuning with multi-task learning to align the MLLM with the multimodal sequential recommendation. Extensive evaluations on common recommendation datasets demonstrate the effectiveness of MMSRARec, showcasing its capability to efficiently and interpretably understand user behavior histories and item information for accurate recommendations.","authors":["Haoyu Wang","Yitong Wang","Jining Wang"],"pdf_url":"","comment":"Under Review"},{"id":"http://arxiv.org/abs/2512.20871v1","updated":"2025-12-24T01:21:25Z","published":"2025-12-24T01:21:25Z","title":"NeRV360: Neural Representation for 360-Degree Videos with a Viewport Decoder","summary":"Implicit neural representations for videos (NeRV) have shown strong potential for video compression. However, applying NeRV to high-resolution 360-degree videos causes high memory usage and slow decoding, making real-time applications impractical. We propose NeRV360, an end-to-end framework that decodes only the user-selected viewport instead of reconstructing the entire panoramic frame. Unlike conventional pipelines, NeRV360 integrates viewport extraction into decoding and introduces a spatial-temporal affine transform module for conditional decoding based on viewpoint and time. Experiments on 6K-resolution videos show that NeRV360 achieves a 7-fold reduction in memory consumption and a 2.5-fold increase in decoding speed compared to HNeRV, a representative prior work, while delivering better image quality in terms of objective metrics.","authors":["Daichi Arai","Kyohei Unno","Yasuko Sugito","Yuichi Kusakabe"],"pdf_url":"","comment":"2026 IIEEJ International Conference on Image Electronics and Visual Computing (IEVC)"}]}}